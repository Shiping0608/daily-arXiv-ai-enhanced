<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 24]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [nlin.PS](#nlin.PS) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [math.DG](#math.DG) [Total: 4]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A note on the space-time variational formulation for the wave equation with source term in $L^2(Q)$](https://arxiv.org/abs/2512.23807)
*Marco Zank*

Main category: math.NA

TL;DR: A variational formulation for the scalar wave equation with homogeneous initial conditions on bounded Lipschitz domains, using a new solution space and L² test space, with proven inf-sup stability and uniqueness results.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous variational framework for the scalar wave equation that supports space-time discretization methods, particularly for space-time boundary element methods and least-squares approaches, while addressing regularity and uniqueness issues.

Method: Derived a variational formulation in a bounded space-time cylinder Q with homogeneous initial conditions, using a new solution space and test space L²(Q) for L² source terms. Applied inf-sup theory to prove well-posedness and established an isomorphism as solution operator.

Result: Proved the variational setting fits inf-sup theory with existence and uniqueness in H¹(Q), established an isomorphism as solution operator, and showed the new solution space is not a subspace of H²(Q).

Conclusion: The new variational framework provides crucial uniqueness and solvability results essential for space-time discretization methods, regularity analysis, and space-time boundary integral equations, forming the foundation for space-time boundary element methods.

Abstract: We derive a variational formulation for the scalar wave equation in the second-order formulation on bounded Lipschitz domains and homogeneous initial conditions. We investigate a variational framework in a bounded space-time cylinder $Q$ with a new solution space and the test space $L^2(Q)$ for source terms in $L^2(Q)$. Using existence and uniqueness results in $H^1(Q)$, we prove that this variational setting fits the inf-sup theory, including an isomorphism as solution operator. Moreover, we show that the new solution space is not a subspace of $H^2(Q)$. This new uniqueness and solvability result is not only crucial for discretizations using space-time methods, including least-squares approaches, but also important for regularity results and the analysis of related space-time boundary integral equations, which form the basis for space-time boundary element methods.

</details>


### [2] [Greedy Rational Approximation for Frequency-Domain Model Reduction of Parametric LTI Systems](https://arxiv.org/abs/2512.23814)
*Filip Bělík,Yanlai Chen,Akil Narayan*

Main category: math.NA

TL;DR: The paper proposes using reduced basis method (RBM) for model reduction of parametric LTI systems by constructing low-order rational approximations of high-order rational functions through an iterative greedy approach.


<details>
  <summary>Details</summary>
Motivation: Model reduction of parametric linear time-invariant (LTI) dynamical systems is computationally challenging. When formulated in the frequency domain, this becomes a problem of approximating high-order rational functions with low-order ones. There's a need for principled approaches to rational compression of these complex systems.

Method: The authors propose using a standard reduced basis method (RBM) to construct low-order rational function approximations. This is implemented as an iterative greedy algorithm where the greedy objective is evaluated using an error estimator that exploits the linearity of the frequency domain representation. The method is theoretically motivated by rational approximability results.

Result: The paper presents a computational framework for rational compression of high-order rational functions. The greedy approach provides a systematic way to construct reduced-order models for parametric LTI systems, leveraging theoretical foundations of rational approximation.

Conclusion: The proposed reduced basis method offers a principled approach to model reduction of parametric LTI systems through rational function approximation. The greedy framework provides an effective computational pathway for constructing reduced-order models while maintaining theoretical guarantees about approximation quality.

Abstract: We investigate model reduction of parametric linear time-invariant (LTI) dynamical systems. When posed in the frequency domain, this problem can be formulated as seeking a low-order rational function approximation of a high-order rational function. We propose to use a standard reduced basis method (RBM) to construct this low-order rational function. Algorithmically, this procedure is an iterative greedy approach, where the greedy objective is evaluated through an error estimator that exploits the linearity of the frequency domain representation. The greedy framework is motivated through theoretical results of rational approximability of functions. This framework provides a principled approach to rational compression of high-order rational functions, and provides a computational pathway for model reduction of parametric LTI systems.

</details>


### [3] [Deep learning methods for inverse problems using connections between proximal operators and Hamilton-Jacobi equations](https://arxiv.org/abs/2512.23829)
*Oluwatosin Akande,Gabriel P. Langlois,Akwum Onwunta*

Main category: math.NA

TL;DR: Proposes using Hamilton-Jacobi PDE connections with proximal operators to create deep learning architectures for learning priors in inverse problems, enabling direct prior learning without inversion after training.


<details>
  <summary>Details</summary>
Motivation: Inverse problems require regularization/priors but existing methods often learn priors indirectly or require inversion after training. There's an opportunity to leverage mathematical connections between proximal operators and Hamilton-Jacobi PDEs for more direct and efficient prior learning.

Method: Develops novel deep learning architectures that exploit connections between proximal operators and Hamilton-Jacobi PDEs to learn priors directly, avoiding the need to invert priors after training.

Result: Demonstrates efficiency of the proposed method in high-dimensional settings through several numerical experiments.

Conclusion: The Hamilton-Jacobi PDE framework provides an effective approach for direct prior learning in inverse problems, offering advantages over existing methods that require post-training inversion.

Abstract: Inverse problems are important mathematical problems that seek to recover model parameters from noisy data. Since inverse problems are often ill-posed, they require regularization or incorporation of prior information about the underlying model or unknown variables. Proximal operators, ubiquitous in nonsmooth optimization, are central to this because they provide a flexible and convenient way to encode priors and build efficient iterative algorithms. They have also recently become key to modern machine learning methods, e.g., for plug-and-play methods for learned denoisers and deep neural architectures for learning priors of proximal operators. The latter was developed partly due to recent work characterizing proximal operators of nonconvex priors as subdifferential of convex potentials. In this work, we propose to leverage connections between proximal operators and Hamilton-Jacobi partial differential equations (HJ PDEs) to develop novel deep learning architectures for learning the prior. In contrast to other existing methods, we learn the prior directly without recourse to inverting the prior after training. We present several numerical results that demonstrate the efficiency of the proposed method in high dimensions.

</details>


### [4] [Multimodal sampling via Schrödinger-Föllmer samplers with temperatures](https://arxiv.org/abs/2512.23965)
*Xiaojie Wang,Xiaoyan Zhang*

Main category: math.NA

TL;DR: The paper introduces temperature-parameterized Schrödinger-Föllmer samplers (SFS) with improved O(h) convergence rate in L²-Wasserstein distance, outperforming Langevin samplers for multimodal distributions.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing Schrödinger-Föllmer samplers which had only O(√h) convergence rate, and to develop more effective samplers for complex multimodal distributions that outperform traditional Langevin samplers.

Method: Incorporates temperature parameter into Schrödinger-Föllmer process, uses Euler discretization with novel error analysis approach, operates in unit interval [0,1] without requiring gradient computations or ergodicity assumptions.

Result: Achieves enhanced O(h) convergence rate in L²-Wasserstein distance (improving from O(√h)), demonstrates superior performance over Langevin samplers especially for multimodal distributions, with numerical experiments confirming the theoretical results.

Conclusion: Temperature-parameterized SFS provides gradient-free, efficient sampling with improved convergence rates, particularly effective for challenging multimodal distributions where traditional methods struggle.

Abstract: Generating samples from complex and high-dimensional distributions is ubiquitous in various scientific fields of statistical physics, Bayesian inference, scientific computing and machine learning. Very recently, Huang et al. (IEEE Trans. Inform. Theory, 2025) proposed new Schrödinger-Föllmer samplers (SFS), based on the Euler discretization of the Schrödinger-Föllmer diffusion evolving on the unit interval $[0, 1]$. There, a convergence rate of order $\mathcal{O}(\sqrt{h})$ in the $L^2$-Wasserstein distance was obtained for the Euler discretization with a uniform time step-size $h>0$.
  By incorporating a temperature parameter, different samplers are introduced in this paper, based on the Euler discretization of the Schrödinger-Föllmer process with temperatures. As revealed by numerical experiments, high temperatures are vital, particularly in sampling from multimodal distributions. Further, a novel approach of error analysis is developed for the time discretization and an enhanced convergence rate of order ${ \mathcal{O}(h)}$ is obtained in the $L^2$-Wasserstein distance, under certain smoothness conditions on the drift. This significantly improves the existing order-half convergence in the aforementioned paper. Unlike Langevin samplers, SFS is of gradient-free, works in a unit interval $[0, 1]$ and does not require any ergodicity. Numerical experiments confirm the convergence rate and show that, the SFS substantially outperforms vanilla Langevin samplers, particularly in sampling from multimodal distributions.

</details>


### [5] [High order numerical discretizations of the Einstein-Euler equations in the Generalized Harmonic formulation](https://arxiv.org/abs/2512.24121)
*Stefano Muzzolon,Michael Dumbser,Olindo Zanotti,Elena Gaburro*

Main category: math.NA

TL;DR: Two new numerical schemes for solving Einstein-Euler equations: FD-CWENO on Cartesian meshes and ADER-DG on unstructured polygonal meshes, both with well-balancing for exact preservation of stationary solutions.


<details>
  <summary>Details</summary>
Motivation: To develop robust numerical methods for solving coupled Einstein-Euler equations in numerical relativity, particularly advancing toward 3D simulations on moving unstructured meshes for complex astrophysical sources.

Method: 1) Finite Difference Central Weighted Essentially Non-Oscillatory (FD-CWENO) scheme on Cartesian meshes. 2) ADER discontinuous Galerkin (DG) scheme on 2D unstructured polygonal meshes. Both incorporate well-balancing to preserve equilibrium of known stationary solutions at discrete level.

Result: Successful validation through standard vacuum tests (robust stability, linearized wave, gauge wave), long-term stable evolutions of stationary black holes (including extreme spin Kerr black holes), and matter coupling tests (spherical accretion onto Schwarzschild black hole, perturbed non-rotating neutron star evolution).

Conclusion: The schemes provide solid foundation for addressing more complex astrophysical simulations through DG schemes on unstructured 3D meshes, representing preliminary step toward full 3D numerical relativity calculations on moving meshes.

Abstract: We propose two new alternative numerical schemes to solve the coupled Einstein-Euler equations in the Generalized Harmonic formulation. The first one is a finite difference (FD) Central Weighted Essentially Non-Oscillatory (CWENO) scheme on a traditional Cartesian mesh, while the second one is an ADER (Arbitrary high order Derivatives) discontinuous Galerkin (DG) scheme on 2D unstructured polygonal meshes. The latter, in particular, represents a preliminary step in view of a full 3D numerical relativity calculation on moving meshes. Both schemes are equipped with a well-balancing (WB) property, which allows to preserve the equilibrium of a priori known stationary solutions exactly at the discrete level. We validate our numerical approaches by successfully reproducing standard vacuum test cases, such as the robust stability, the linearized wave, and the gauge wave tests, as well as achieving long-term stable evolutions of stationary black holes, including Kerr black holes with extreme spin. Concerning the coupling with matter, modeled by the relativistic Euler equations, we perform a classical test of spherical accretion onto a Schwarzschild black hole, as well as an evolution of a perturbed non-rotating neutron star, demonstrating the capability of our schemes to operate also on the full Einstein-Euler system. Altogether, these results provide a solid foundation for addressing more complex and challenging simulations of astrophysical sources through DG schemes on unstructured 3D meshes.

</details>


### [6] [Structure-preserving schemes for nonlinear symmetric hyperbolic and thermodynamically compatible systems of partial differential equations](https://arxiv.org/abs/2512.24127)
*Alessia Lucca,Michael Dumbser*

Main category: math.NA

TL;DR: This paper develops exactly energy-conservative and structure-preserving finite volume schemes for symmetric-hyperbolic thermodynamically compatible (SHTC) systems, with applications to nonlinear acoustics and Maxwell equations.


<details>
  <summary>Details</summary>
Motivation: SHTC systems in continuum physics have thermodynamic compatibility that ensures total energy conservation and often satisfy stationary differential constraints (involutions). Existing numerical methods may not preserve these important physical properties at the discrete level.

Method: Two approaches: 1) A simple semi-discrete cell-centered HTC finite volume scheme on collocated grids that conserves total energy but not involutions. 2) A fully discrete semi-implicit vertex-based staggered scheme that exactly conserves total energy and satisfies involution constraints by preserving vector calculus identities ∇·∇×A=0 and ∇×∇φ=0 at discrete level, using symmetric-hyperbolic Godunov-form discretization.

Result: The schemes are applied to three SHTC systems: nonlinear acoustics, nonlinear Maxwell equations without charges, and nonlinear Maxwell-GLM system. Numerical results demonstrate the stated properties of exact energy conservation and involution preservation.

Conclusion: The paper successfully develops structure-preserving finite volume schemes that maintain key physical properties (energy conservation and involution constraints) at the discrete level for SHTC systems, with the staggered semi-implicit scheme being particularly effective for preserving both properties simultaneously.

Abstract: This paper aims at developing exactly energy-conservative and structure-preserving finite volume schemes for the discretisation of first-order symmetric-hyperbolic and thermodynamically compatible (SHTC) systems of partial differential equations in continuum physics. Due to their thermodynamic compatibility the class of SHTC systems satisfies an additional conservation law for the total energy and many PDE in this class of equations also satisfy stationary differential constraints (involutions). First, we propose a simple semi-discrete cell-centered HTC finite volume scheme that employs collocated grids and that is compatible with the total energy conservation law, but which does not satisfy the involutions. Second, we develop a fully discrete semi-implicit finite volume scheme that conserves total energy and which can be proven to satisfy also the involution constraints exactly at the discrete level. This method is a vertex-based staggered semi-implicit scheme that preserves the basic vector calculus identities $\nabla \cdot \nabla \times A = 0$ and $\nabla \times \nabla φ= 0$ for any vector and scalar field, respectively, exactly at the discrete level and which is also exactly totally energy conservative. The main key ingredient of the proposed implicit scheme is the fact that it uses a discrete version of the symmetric-hyperbolic Godunov-form of the governing PDE system. This leads naturally to sequences of symmetric and positive definite linear algebraic systems to be solved inside an iterative fixed-point method used in each time step. We apply our new schemes to three different SHTC systems. In particular, we consider the equations of nonlinear acoustics, the nonlinear Maxwell equations in the absence of charges and a nonlinear version of the Maxwell-GLM system. We also show some numerical results to provide evidence of the stated properties of the proposed schemes.

</details>


### [7] [Sufficient and Necessary Conditions for Eckart-Young-like Result for Tubal Tensors](https://arxiv.org/abs/2512.24405)
*Uria Mor*

Main category: math.NA

TL;DR: Characterizes which tubal tensor products yield Eckart-Young type theorems for low-rank approximation.


<details>
  <summary>Details</summary>
Motivation: The tubal tensor framework extends matrix algebra concepts to tensors, but it's unclear which specific tubal products preserve the Eckart-Young property (best low-rank approximation via truncated SVD).

Method: Provides theoretical characterization of tubal product families that guarantee Eckart-Young type results, then validates with experiments on video data and data-driven dynamical systems.

Result: Complete characterization of tubal products yielding Eckart-Young theorems, with practical validation through experimental demonstrations.

Conclusion: Identifies specific conditions under which tubal tensor products maintain the desirable Eckart-Young property for low-rank approximation, bridging theory and practical applications.

Abstract: A valuable feature of the tubal tensor framework is that many familiar constructions from matrix algebra carry over to tensors, including SVD and notions of rank. Most importantly, it has been shown that for a specific family of tubal products, an Eckart-Young type theorem holds, i.e., the best low-rank approximation of a tensor under the Frobenius norm is obtained by truncating its tubal SVD. In this paper, we provide a complete characterization of the family of tubal products that yield an Eckart-Young type result. We demonstrate the practical implications of our theoretical findings by conducting experiments with video data and data-driven dynamical systems.

</details>


### [8] [Fast high-order spectral solvers for PDEs on triangulated surfaces with applications to deforming surfaces](https://arxiv.org/abs/2512.24456)
*Gentian Zavalani*

Main category: math.NA

TL;DR: Extends quadrilateral-based hierarchical Poincaré-Steklov (HPS) framework to triangles using two high-order strategies: reduced quadrilateralization and triangle-based spectral elements with Dubiner polynomials, preserving spectral accuracy and fast direct-solver structure.


<details>
  <summary>Details</summary>
Motivation: The classical HPS method is limited to quadrilateral meshes with tensor-product spectral discretizations, restricting its application to triangulated geometries common in computational science and engineering.

Method: Two complementary high-order strategies: 1) Reduced quadrilateralization approach (straightforward implementation), and 2) Triangle-based spectral element method using Dubiner polynomials. Both preserve HPS framework's spectral accuracy and fast direct-solver structure.

Result: Numerical demonstrations show the extensions preserve spectral accuracy, efficiency, and fast direct-solver structure. The method is successfully applied to time-dependent problems, evolving surfaces, reaction-diffusion systems, and geometry-driven surface evolution.

Conclusion: Successfully extends HPS framework to triangular elements while maintaining its key advantages, enabling broader application to complex geometries and time-dependent surface evolution problems.

Abstract: In this paper, we extend the classical quadrilateral based hierarchical Poincaré-Steklov (HPS) framework to triangulated geometries. Traditionally, the HPS method takes as input an unstructured, high-order quadrilateral mesh and relies on tensor-product spectral discretizations on each element. To overcome this restriction, we introduce two complementary high-order strategies for triangular elements: a reduced quadrilateralization approach which is straightforward to implement, and triangle based spectral element method based on Dubiner polynomials. We show numerically that these extensions preserve the spectral accuracy, efficiency, and fast direct-solver structure of the HPS framework. The method is further extended to time dependent and evolving surfaces, and its performance is demonstrated through numerical experiments on reaction-diffusion systems, and geometry driven surface evolution.

</details>


### [9] [Exponential Convergence of Deep Composite Polynomial Approximation for Cusp-Type Functions](https://arxiv.org/abs/2512.24523)
*Kingsley Yeon,Steven B. Damelin,Michael Werman*

Main category: math.NA

TL;DR: Deep composite polynomial approximations achieve exponential convergence for functions with algebraic cusp singularities, outperforming classical polynomial methods with only algebraic rates.


<details>
  <summary>Details</summary>
Motivation: Classical polynomial approximations for functions with cusp singularities (like |x-a|^α with α∈(0,1)) only achieve algebraic convergence rates. The paper aims to develop more efficient approximations that can handle these challenging non-differentiable functions with multiple cusp terms.

Method: Proposes a constructive deep composite polynomial scheme with two layers: an inner layer using division-free polynomial iteration for fractional powers, and an outer layer for analytic polynomial fitting. The method handles functions consisting of finitely many cusp terms |x-a_j|^α_j with rational exponents on a real-analytic background.

Result: The composite structure achieves exponential convergence in the number of scalar coefficients (parameter budget) for L^p([-1,1]) approximation error. Numerical experiments confirm theoretical rates and demonstrate parameter efficiency for both single and multiple cusp configurations.

Conclusion: Deep composite polynomial constructions provide an efficient approximation framework for functions with algebraic cusp singularities, achieving exponential convergence that significantly outperforms classical single-layer polynomial methods.

Abstract: We investigate deep composite polynomial approximations of continuous but non-differentiable functions with algebraic cusp singularities. The functions in focus consist of finitely many cusp terms of the form $|x-a_j|^{α_j}$ with rational exponents $α_j\in(0,1)$ on a real-analytic background. We propose a constructive approximation scheme that combines a division-free polynomial iteration for fractional powers with an outer layer for the analytic polynomial fitting. Our main result shows that this composite structure achieves exponential convergence in the the number of scalar coefficients in the inner and outer polynomial layers. Specifically, the $L^p([-1,1])$ approximation error, decays exponentially with respect to the parameter budget, in contrast to the algebraic rates obtained by classical single-layer polynomial approximation for cusp-type functions. Numerical experiments for both single and multiple cusp configurations confirm the theoretical rates and demonstrate the parameter efficiency of deep composite polynomial constructions.

</details>


### [10] [Newton-Krylov Methods for Computing Steady States of Particle Timesteppers via Optimal Transport](https://arxiv.org/abs/2512.24567)
*Hannes Vandecasteele,Nicholas Karris,Alexander Cloninger,Ioannis G. Kevrekidis*

Main category: math.NA

TL;DR: A matrix-free framework for computing steady-state distributions of stochastic particle systems using timesteppers reinterpreted as operators on probability measures, enabling efficient computation even under high noise.


<details>
  <summary>Details</summary>
Motivation: Stochastic particle simulations have intrinsic randomness that prevents direct steady state extraction, unlike deterministic systems where timesteppers can implicitly encode steady states and stability information.

Method: Reinterpret stochastic timesteppers in optimal transport language as operators acting on probability measures rather than individual trajectories. Construct smooth (I)CDF timesteppers that evolve distributions instead of particles, combined with matrix-free Newton-Krylov solvers.

Result: Error analysis shows convergence can be obtained even in high noise regimes. Higher-dimensional generalizations based on smooth CDF-related representations work effectively on non-trivial 2D distributions.

Conclusion: Establishes a unified variational framework for computing meaningful steady states of both deterministic and stochastic timesteppers, enabling efficient computation of steady-state distributions in stochastic particle simulations.

Abstract: Timesteppers constitute a powerful tool in modern computational science and engineering. Although they are typically used to advance the system forward in time, they can also be viewed as nonlinear mappings that implicitly encode steady states and stability information. In this work, we present an extension of the matrix-free framework for calculating, via timesteppers, steady states of deterministic systems to stochastic particle simulations, where intrinsic randomness prevents direct steady state extraction. By formulating stochastic timesteppers in the language of optimal transport, we reinterpret them as operators acting on probability measures rather than on individual particle trajectories. This perspective enables the construction of smooth cumulative- and inverse-cumulative-distribution-function ((I)CDF) timesteppers that evolve distributions rather than particles. Combined with matrix-free Newton-Krylov solvers, these smooth timesteppers allow efficient computation of steady-state distributions even under high stochastic noise. We perform an error analysis quantifying how noise affects finite-difference Jacobian action approximations, and demonstrate that convergence can be obtained even in high noise regimes. Finally, we introduce higher-dimensional generalizations based on smooth CDF-related representations of particles and validate their performance on a non-trivial two-dimensional distribution. Together, these developments establish a unified variational framework for computing meaningful steady states of both deterministic and stochastic timesteppers.

</details>


### [11] [Solving the inverse Source Problems for wave equation with final time measurements by a data driven approach](https://arxiv.org/abs/2512.24647)
*Qiling Gu,Wenlong Zhang,Zhidong Zhang*

Main category: math.NA

TL;DR: A discrete data-driven approach for solving wave equation inverse source problems using L²-Tikhonov regularization with final time measurements, establishing convergence rates without classical source conditions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inverse source problem for wave equations with final time measurements, which is ill-posed and requires regularization. The motivation is to develop a practical, data-driven approach that doesn't rely on a priori information or classical source conditions, making it more applicable to real-world scenarios with discrete noisy measurements.

Method: Uses L²-Tikhonov regularization with spectral decomposition analysis and noise separation technique. The method handles two different noise models for discrete spatial observations. Extends to fully discrete case with finite element discretization, analyzing error dependencies on noise level, regularization parameter, time step size, and spatial mesh size.

Result: Establishes error bounds for reconstructed solution u and source term f without requiring classical source conditions. Derives expected convergence rate for source error in weaker topology. Shows overall error depends only on noise level, regularization parameter, time step size, and spatial mesh size. Provides basis for optimal regularization parameter selection in data-driven manner.

Conclusion: The proposed discrete data-driven approach effectively solves wave equation inverse source problems with final time measurements. Theoretical convergence results are validated by numerical experiments, demonstrating the algorithm's efficiency and practical applicability without needing a priori information.

Abstract: This paper develops a discrete data-driven approach for solving the inverse source problem of the wave equation with final time measurements. Focusing on the $L^2$-Tikhonov regularization method, we analyze its convergence under two different noise models, using noisy discrete spatial observations. By exploiting the spectral decomposition of the forward operator and introducing a noise separation technique into the variational framework, we establish error bounds for the reconstructed solution $u$ and the source term $f$ without requiring classical source conditions. Moreover, an expected convergence rate for the source error is derived in a weaker topology. We also extend the analysis to the fully discrete case with finite element discretization, showing that the overall error depends only on the noise level, regularization parameter, time step size, and spatial mesh size. These estimates provide a basis for selecting the optimal regularization parameter in a data-driven manner, without a priori information. Numerical experiments validate the theoretical results and demonstrate the efficiency of the proposed algorithm.

</details>


### [12] [Boundary error control for numerical solution of BSDEs by the convolution-FFT method](https://arxiv.org/abs/2512.24714)
*Xiang Gao,Cody Hyndman*

Main category: math.NA

TL;DR: The paper improves the CFFT method for solving BSDEs in option pricing by modifying damping and shifting schemes to reduce boundary errors, with time-dependent shifting showing significant improvement.


<details>
  <summary>Details</summary>
Motivation: The original CFFT approach for solving BSDEs in option valuation suffers from boundary errors when transforming functions for Fourier analysis. These boundary errors reduce accuracy in option pricing applications.

Method: The authors modify the damping and shifting schemes used in the original CFFT formulation. They introduce time-dependent shifting to transform target functions into bounded periodic functions suitable for Fourier transforms, specifically addressing boundary error issues.

Result: Time-dependent shifting significantly reduces boundary errors. Numerical results and detailed error analysis demonstrate improved accuracy and convergence of the modified convolution method compared to the original approach.

Conclusion: The proposed modifications to the CFFT method for BSDEs effectively address boundary error problems in option valuation, leading to enhanced numerical accuracy and convergence properties for practical applications.

Abstract: We first review the convolution fast-Fourier-transform (CFFT) approach for the numerical solution of backward stochastic differential equations (BSDEs) introduced in (Hyndman and Oyono Ngou, 2017). We then propose a method for improving the boundary errors obtained when valuing options using this approach. We modify the damping and shifting schemes used in the original formulation, which transforms the target function into a bounded periodic function so that Fourier transforms can be applied successfully. Time-dependent shifting reduces boundary error significantly. We present numerical results for our implementation and provide a detailed error analysis showing the improved accuracy and convergence of the modified convolution method.

</details>


### [13] [A structure-preserving parametric approximation for anisotropic geometric flows via an $α$-surface energy matrix](https://arxiv.org/abs/2512.24875)
*Weizhu Bao,Yifei Li,Wenjun Ying,Yulin Zhang*

Main category: math.NA

TL;DR: Proposes structure-preserving parametric approximation for anisotropic geometric flows with optimal energy stability at α=-1.


<details>
  <summary>Details</summary>
Motivation: Existing formulations for anisotropic geometric flows lack a unified framework with optimal energy stability guarantees. The paper aims to develop a structure-preserving parametric approximation that can handle general anisotropic effects while ensuring energy stability.

Method: Introduces hyperparameter α to construct unified surface energy matrix Ĝₖᵅ(θ) encompassing all existing formulations. Applies this to anisotropic curvature flow and proves α=-1 achieves optimal energy stability under minimal conditions. Extends framework to general anisotropic geometric flows through unified velocity discretization.

Result: Proves α=-1 is unique choice achieving optimal energy stability under necessary and sufficient condition 3γ̂(θ)≥γ̂(θ-π), while other α≠-1 require strictly stronger conditions. Numerical experiments validate theoretical optimality of α=-1 and demonstrate effectiveness and robustness.

Conclusion: The proposed framework provides a unified structure-preserving parametric approximation for anisotropic geometric flows with guaranteed energy stability, where α=-1 represents the optimal choice with minimal stability requirements.

Abstract: We propose a structure-preserving parametric approximation for geometric flows with general anisotropic effects. By introducing a hyperparameter $α$, we construct a unified surface energy matrix $\hat{\boldsymbol{G}}_k^α(θ)$ that encompasses all existing formulations of surface energy matrices, and apply it to anisotropic curvature flow. We prove that $α=-1$ is the unique choice achieving optimal energy stability under the necessary and sufficient condition $3\hatγ(θ)\geq\hatγ(θ-π)$, while all other $α\neq-1$ require strictly stronger conditions. The framework extends naturally to general anisotropic geometric flows through a unified velocity discretization that ensures energy stability. Numerical experiments validate the theoretical optimality of $α=-1$ and demonstrate the effectiveness and robustness.

</details>


### [14] [Random compressible Euler flows](https://arxiv.org/abs/2512.24879)
*Maria Lukacova-Medvidova,Simon Schneider*

Main category: math.NA

TL;DR: Finite volume stochastic collocation method for random Euler system with convergence proof under bounded discrete differential quotients assumption.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods for solving random Euler systems (compressible fluid flow with uncertainty) and establish rigorous convergence guarantees for stochastic finite volume approximations.

Method: Finite volume stochastic collocation method combining deterministic finite volume discretization with stochastic collocation for uncertainty quantification. Convergence analysis uses deterministic FV convergence results plus stochastic compactness arguments (Skorokhod and Gyöngy-Krylov theorems).

Result: Rigorous proof of convergence for random finite volume solutions under the assumption that discrete differential quotients remain bounded in probability.

Conclusion: The proposed method provides a theoretically sound framework for solving random Euler systems with guaranteed convergence, combining deterministic finite volume techniques with stochastic analysis tools.

Abstract: We propose a finite volume stochastic collocation method for the random Euler system. We rigorously prove the convergence of random finite volume solutions under the assumption that the discrete differential quotients remain bounded in probability. Convergence analysis combines results on the convergence of a deterministic FV method with stochastic compactness arguments due to Skorokhod and Gyöngy-Krylov.

</details>


### [15] [A finite element approach for minimizing line and surface energies arising in the study of singularities in liquid crystals](https://arxiv.org/abs/2512.24928)
*Dominik Stantejsky*

Main category: math.NA

TL;DR: Numerical algorithm for Plateau-like problem with area, boundary length, and obstacle constraints, applied to defect structures in nematic liquid crystals.


<details>
  <summary>Details</summary>
Motivation: Study of defect structures in nematic liquid crystals, specifically colloidal particles, requires solving a Plateau-like problem with complex constraints.

Method: ADMM-based algorithm using finite elements to minimize discretized energy containing surface area, boundary length, obstacle constraints, and surface energy on obstacle.

Result: Algorithm successfully handles different inclusion shapes, revealing rich minimizing configurations with physical interpretations for colloidal particles in nematic liquid crystals.

Conclusion: The developed ADMM-based finite element method effectively solves complex Plateau-like problems relevant to nematic liquid crystal defect structures, generalizing TV-minimization approaches.

Abstract: Motivated by a problem originating in the study of defect structures in nematic liquid crystals, we describe and study a numerical algorithm for the resolution of a Plateau-like problem. The energy contains the area of a two-dimensional surface $T$ and the length of its boundary $\partial T$ reduced by a prescribed curve to make our problem non-trivial. We additionally include an obstacle $E$ for $T$ and pose a surface energy on $E$. We present an algorithm based on the Alternating Direction Method of Multipliers that minimizes a discretized version of the energy using finite elements, generalizing existing TV-minimization methods. We study different inclusion shapes demonstrating the rich structure of minimizing configurations and provide physical interpretation of our findings for colloidal particles in nematic liquid crystal.

</details>


### [16] [Approximating evolution operators of linear delay equations: a general framework for the convergence analysis](https://arxiv.org/abs/2512.24964)
*Alessia andò,Giusy Bosco,Dimitri Breda,Davide Liessi*

Main category: math.NA

TL;DR: A framework for discretizing evolution operators of linear delay equations to approximate spectra for stability analysis, unifying existing methods and providing formal convergence analysis for previously unproven methods.


<details>
  <summary>Details</summary>
Motivation: To investigate stability properties of nonlinear delay equations via linearized stability principle, requiring approximation of spectra of linear delay evolution operators.

Method: Develops general convergence analysis framework using fixed-point equation reformulation, with hypotheses on regularization properties and approximation convergence on suitable subspaces. Unifies pseudospectral discretization methods and applies to weighted residual methods.

Result: Provides unified convergence proofs for pseudospectral discretization methods and formal convergence analysis for previously unproven weighted residual methods.

Conclusion: The framework successfully unifies discretization methods for linear delay equations, provides rigorous convergence analysis, and demonstrates generality by covering both pseudospectral and weighted residual approaches.

Abstract: We consider the problem of discretizing evolution operators of linear delay equations with the aim of approximating their spectra, which is useful in investigating the stability properties of (nonlinear) equations via the principle of linearized stability. We develop a general convergence analysis based on a reformulation of the operators by means of a fixed-point equation, providing a list of hypotheses related to the regularization properties of the equation and the convergence of the chosen approximation techniques on suitable subspaces. This framework unifies the proofs for some methods based on pseudospectral discretization, which we present here in this new form. To exemplify the generality of the framework, we also apply it to a method of weighted residuals found in the literature, which was previously lacking a formal convergence analysis.

</details>


### [17] [At the intersection of Numerical Analysis and Spectral Geometry](https://arxiv.org/abs/2512.25012)
*Nilima Nigam*

Main category: math.NA

TL;DR: Survey paper exploring the intersection of spectral geometry and numerical analysis, focusing on how computational methods can be used to study operator spectra and geometric properties, with emphasis on choosing appropriate discretization strategies based on research goals.


<details>
  <summary>Details</summary>
Motivation: To bridge spectral geometry (studying how domain geometry affects operator spectra) and numerical analysis (computing accurate approximations), exploring how computational tools can both guide conjectures and serve as proof strategies in spectral geometry.

Method: Expository survey reviewing various discretization methods and approximation strategies, analyzing trade-offs between efficiency/accuracy for conjecture formulation versus rigorous error bounds for proof strategies.

Result: Identifies that computational spectral geometry requires different approaches depending on goals: rapid specialized methods for conjecture formulation versus schemes with guaranteed error bounds for proof strategies, with mutual benefits between the fields.

Conclusion: Computational spectral geometry represents a fruitful intersection where spectral geometry's demands motivate new numerical analysis developments, while computational tools enable both conjecture guidance and rigorous proof strategies in spectral geometry.

Abstract: How do the geometric properties of a domain impact the spectrum of an operator defined on it? How do we compute accurate and reliable approximations of these spectra? The former question is studied in spectral geometry, and the latter is a central concern in numerical analysis. In this short expository survey we revisit the process of eigenvalue approximation, from the perspective of computational spectral geometry. Over the years a multitude of methods -- for discretizing the operator and for the resultant discrete system -- have been developed and analyzed in the field of numerical analysis. High-accuracy and provably convergent discretization approaches can be used to examine the interplay between the spectrum of an operator and the geometric properties of the spatial domain or manifold it is defined on. While computations have been used to guide conjectures in spectral geometry, in recent years approximation-theoretic tools and validated computations are also being used as part of proof strategies in spectral geometry.
  Given a particular spectral feature of interest, should we discretize the original problem, or seek a reformulation? Of the many possible approximation strategies, which should we choose? These choices are inextricably linked to the objective: on the one hand, rapid, specialized methods are often ideal for conjecture formulation (prioritizing efficiency and accuracy), whereas schemes with guaranteed, computable error bounds are needed when computation is incorporated into a proof strategy. We also review instances where the demanding requirements of spectral geometry -- the need for rigorous error control or the robust calculation of higher eigenvalues -- motivate new developments in numerical analysis.

</details>


### [18] [Convergence of the generalization error for deep gradient flow methods for PDEs](https://arxiv.org/abs/2512.25017)
*Chenguang Liu,Antonis Papapantoleon,Jasper Rou*

Main category: math.NA

TL;DR: DGFMs provide mathematical foundation for solving high-dimensional PDEs using neural networks, with generalization error converging to zero as neurons and training time increase.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for deep gradient flow methods (DGFMs) in solving high-dimensional PDEs, addressing the need for theoretical guarantees in neural network-based PDE solvers.

Method: Decompose generalization error into approximation and training errors. First prove PDE solutions can be approximated by neural networks (approximation error → 0 as neurons → ∞). Then derive gradient flow in "wide network limit" and analyze its limit as training time → ∞.

Result: Theoretical proof that generalization error of DGFMs tends to zero as both number of neurons and training time tend to infinity, providing convergence guarantees for neural network PDE solvers.

Conclusion: DGFMs offer mathematically sound approach for solving high-dimensional PDEs with provable convergence properties, bridging theoretical foundations with practical deep learning methods.

Abstract: The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit'' and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [Fractal Mehler kernels and nonlinear geometric flows](https://arxiv.org/abs/2512.23830)
*Nicola Garofalo*

Main category: math.AP

TL;DR: The paper introduces a two-parameter family of Mehler kernels and connects them to Baouendi-Grushin flows in fractal dimension, while also linking to geometric fully nonlinear equations and posing two open questions.


<details>
  <summary>Details</summary>
Motivation: To establish connections between Mehler kernels, Baouendi-Grushin flows in fractal settings, and geometric fully nonlinear equations, exploring relationships between these mathematical structures.

Method: Introduces a two-parameter family of Mehler kernels and establishes connections with Baouendi-Grushin flows in fractal dimension. Also highlights links with geometric fully nonlinear equations.

Result: Establishes connections between the introduced Mehler kernels, Baouendi-Grushin flows in fractal dimension, and geometric fully nonlinear equations.

Conclusion: The paper successfully connects these mathematical concepts and formulates two open questions for further research in this area.

Abstract: In this paper we introduce a two-parameter family of Mehler kernels and connect them to a class of Baouendi-Grushin flows in fractal dimension. We also highlight a link with a geometric fully nonlinear equation and formulate two questions.

</details>


### [20] [A nonlinear instability result to the Navier-Stokes equations with Navier slip boundary conditions](https://arxiv.org/abs/2512.23946)
*Tien-Tai Nguyen*

Main category: math.AP

TL;DR: The paper proves linear and nonlinear instability of trivial steady states for incompressible viscous fluids with Navier-slip boundary conditions using operator methods and boundary layer analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate instability of trivial steady states in incompressible viscous fluids with Navier-slip boundary conditions, providing a different approach from previous work by Ding, Li and Xin (2018).

Method: Uses operator method of Lafitte and Nguyen (2022) to show existence of infinitely many normal mode solutions for linear instability, then adapts framework of Desjardins and Grenier (2003) for boundary layer analysis to prove nonlinear instability by obtaining two separated solutions at escaping time.

Result: Proves both linear and nonlinear instability of trivial steady states for incompressible viscous fluids with Navier-slip boundary conditions.

Conclusion: The paper successfully demonstrates instability using a novel approach that differs from previous work, establishing theoretical foundations for understanding fluid instability with Navier-slip boundary conditions.

Abstract: In this paper, we investigate the instability of the trivial steady states to the incompressible viscous fluid with Navier-slip boundary conditions. For the linear instability, the existence of infinitely many normal mode solutions to the linearized equations is shown via the operator method of Lafitte and Nguyen (2022). Hence, we prove the nonlinear instability by adapting the framework of Desjardins and Grenier (2003) studying some classes of viscous boundary layers to obtain two separated solutions at escaping time. Our work performs a different approach from that of Ding, Li and Xin (2018).

</details>


### [21] [A regularity theory for second-order parabolic partial differential equations in weighted mixed norm Sobolev-Zygmund spaces](https://arxiv.org/abs/2512.24020)
*Jae-Hwan Choi,Junhee Ryu*

Main category: math.AP

TL;DR: Optimal regularity theory for parabolic PDEs in weighted mixed norm Sobolev-Zygmund spaces, extending Schauder estimates to time-measurable coefficients and integer-order regularity, with sharp trace theorem for initial data.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive regularity theory for parabolic PDEs that goes beyond classical Schauder estimates, handling more general coefficient regularity (merely measurable in time) and addressing the critical case of integer-order regularity, while properly treating nonzero initial data.

Method: Develops optimal regularity theory using weighted mixed norm Sobolev-Zygmund spaces, extends classical Schauder estimates to accommodate coefficients that are only measurable in time, addresses integer-order regularity cases, and incorporates a sharp trace theorem for initial data treatment.

Result: Establishes optimal regularity results for parabolic PDEs in the specified function spaces, successfully extends Schauder estimates to time-measurable coefficients and integer-order regularity cases, and provides proper treatment of nonzero initial data through a sharp trace theorem.

Conclusion: The paper develops a comprehensive optimal regularity theory for parabolic PDEs that significantly extends classical results, handling more general coefficient regularity and providing optimal treatment of initial data, representing an important advancement in parabolic regularity theory.

Abstract: We develop an optimal regularity theory for parabolic partial differential equations in weighted mixed norm Sobolev-Zygmund spaces. The results extend the classical Schauder estimates to coefficients that are merely measurable in time and to the critical case of integer-order regularity. In addition, nonzero initial data are treated in the optimal trace space via a sharp trace theorem.

</details>


### [22] [$L^p$ Estimates for Numerical Approximation of Hamilton-Jacobi Equations](https://arxiv.org/abs/2512.24051)
*Alessio Basti,Fabio Camilli*

Main category: math.AP

TL;DR: L^p error estimates for monotone schemes approximating Hamilton-Jacobi equations on torus, with order one L^1 bound via adjoint method and interpolation to L^p estimates.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous error estimates for numerical schemes approximating Hamilton-Jacobi equations, which are important in optimal control, differential games, and geometric optics, but challenging due to their nonlinear nature.

Method: Uses adjoint method to prove L^1 error bound of order one for finite-difference and semi-Lagrangian schemes under convex Hamiltonian assumptions, then extends to L^p estimates via interpolation techniques.

Result: Proves L^1 error bound of order one for monotone schemes, obtains L^p estimates for all finite p>1 through interpolation, covering broad class of schemes and improving existing results.

Conclusion: Provides unified framework for discrete error estimates of monotone schemes for Hamilton-Jacobi equations, establishing rigorous L^p error bounds that generalize and improve previous results.

Abstract: We establish $L^p$ error estimates for monotone numerical schemes approximating Hamilton-Jacobi equations on the $d$-dimensional torus. Using the adjoint method, we first prove a $L^1$ error bound of order one for finite-difference and semi-Lagrangian schemes under standard convexity assumptions on the Hamiltonian. By interpolation, we also obtain $L^p$ estimates for every finite $p>1$. Our analysis covers a broad class of schemes, improves several existing results, and provides a unified framework for discrete error estimates.

</details>


### [23] [Propagation of chaos for the homogeneous Boltzmann equation with moderately soft potentials](https://arxiv.org/abs/2512.24065)
*Nicolas Fournier,Stéphane Mischler*

Main category: math.AP

TL;DR: Kac particle system converges to Boltzmann equation for moderately soft potentials, proving propagation of chaos via Fisher information control.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous connection between microscopic particle systems (Kac model) and macroscopic kinetic theory (Boltzmann equation) for moderately soft potentials, proving propagation of chaos.

Method: Adapt recent work by Imbert, Silvestre and Villani to show Fisher information is nonincreasing along solutions to Kac master equation, using this to control interaction singularity.

Result: Proves convergence of Kac particle system to homogeneous Boltzmann equation as particle number → ∞ for moderately soft potentials (γ ∈ (-2,0)), establishing propagation of chaos.

Conclusion: Fisher information monotonicity provides key analytical tool to handle singular interactions, enabling rigorous derivation of Boltzmann equation from particle dynamics for moderately soft potentials.

Abstract: We show that the Kac particle system converges, as the number of particles tends to infinity, to the solution of the homogeneous Boltzmann equation, in the regime of moderately soft potentials, $γ\in (-2,0)$ with the common notation. This proves the propagation of chaos. We adapt the recent work of Imbert, Silvestre and Villani, to show that the Fisher information is nonincreasing in time along solutions to the Kac master equation. This estimate allows us to control the singularity of the interaction.

</details>


### [24] [Dirac solitons in one-dimensional nonlinear Schrödinger equations](https://arxiv.org/abs/2512.24089)
*William Borrelli,Elena Danesi,Simone Dovetta,Lorenzo Tentarelli*

Main category: math.AP

TL;DR: The paper studies stationary cubic NLS equations with periodic potentials having Dirac points, constructs "Dirac solitons" via perturbation theory, and rigorously justifies the NLD equation as an effective model for NLS.


<details>
  <summary>Details</summary>
Motivation: To understand how nonlinear standing waves (solitons) can emerge in periodic potentials with Dirac points, and to provide rigorous mathematical justification for using the nonlinear Dirac equation as an effective model for the more complex nonlinear Schrödinger equation in such systems.

Method: Study stationary cubic NLS equations with periodic potentials featuring Dirac points in the dispersion relation. Introduce periodic perturbations to open spectral gaps around Dirac-point energy. Construct standing waves (Dirac solitons) by modulating Bloch waves using spinor components solving an appropriate cubic nonlinear Dirac equation.

Result: Successfully constructs Dirac solitons - standing waves of the NLS equation whose leading-order profile is a modulation of Bloch waves governed by a cubic nonlinear Dirac equation. Provides rigorous mathematical justification for the NLD equation as an effective model for the original NLS equation.

Conclusion: The nonlinear Dirac equation serves as a valid effective model for the nonlinear Schrödinger equation in periodic potentials with Dirac points, enabling the construction and understanding of Dirac solitons through perturbation theory and rigorous mathematical analysis.

Abstract: In this paper we study a family of one-dimensional stationary cubic nonlinear Schrödinger (NLS) equations with periodic potentials and linear part displaying Dirac points in the dispersion relation. By introducing a suitable periodic perturbation, one can open a spectral gap around the Dirac-point energy. This allows to construct standing waves of the NLS equation whose leading-order profile is a modulation of Bloch waves by means of the components of a spinor solving an appropriate cubic nonlinear Dirac (NLD) equation. We refer to these solutions as Dirac solitons. Our analysis thus provides a rigorous justification for the use of the NLD equation as an effective model for the original NLS equation.

</details>


### [25] [An Equivalence Result on the Order of Differentiability in Frobenius' Theorem](https://arxiv.org/abs/2512.24218)
*Yuhki Hosoya*

Main category: math.AP

TL;DR: This paper analyzes total differential equations in foliation theory without smoothness assumptions, revealing asymmetry in solution differentiability. It shows integral manifolds have higher regularity than solutions, provides counterexamples, and characterizes quasi-convex solutions to optimization problems.


<details>
  <summary>Details</summary>
Motivation: To study total differential equations in foliation theory without imposing smoothness assumptions, revealing inherent asymmetries in solution differentiability that require resolution through analysis of integral manifold regularity.

Method: Examines total differential equations in simplest cases, analyzes differentiability of integral manifolds, provides counterexamples, and characterizes quasi-convex solutions through necessary and sufficient conditions for optimization problems.

Result: For locally Lipschitz systems, solutions are only locally Lipschitz but integral manifolds are C¹. For C^k systems, solutions are C^k but integral manifolds are C^{k+1}. Counterexample shows C¹ system without C² solution. Characterizes minimizers for optimization with quasi-convex solutions.

Conclusion: Integral manifolds exhibit higher regularity than solutions to total differential equations, revealing fundamental asymmetry in differentiability. The paper provides complete characterization of this phenomenon and connects it to optimization with quasi-convex solutions.

Abstract: This paper examines the simplest case of total differential equations that appears in the theory of foliation structures, without imposing the smoothness assumptions. This leads to a peculiar asymmetry in the differentiability of solutions. To resolve this asymmetry, this paper focuses on the differentiability of the integral manifold. When the system is locally Lipschitz, a solution is ensured to be only locally Lipschitz, but the integral manifolds must be $C^1$. When the system is $C^k$, we can only ensure the existence of a $C^k$ solution, but the integral manifolds must be $C^{k+1}$. In addition, we see a counterexample in which the system is $C^1$, but there is no $C^2$ solution. Moreover, we characterize a minimizer of an optimization problem whose objective function is a quasi-convex solution to a total differential equation. In this connection, we examine two necessary and sufficient conditions for the system in which any solution is quasi-convex.

</details>


### [26] [Multi-bump solutions for sublinear elliptic equations with nonsymmetric coefficients](https://arxiv.org/abs/2512.24234)
*Chengxiang Zhang,Xu Zhang*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many nonnegative bump solutions with arbitrarily many bumps for a sublinear elliptic equation with nonsymmetric potential K, using sharp support estimates and truncated functional spaces.


<details>
  <summary>Details</summary>
Motivation: To study existence of multiple bump solutions for sublinear elliptic equations without symmetry assumptions on the potential, addressing challenges from sensitive bump interactions due to compact support of limiting profiles.

Method: Uses truncated functional space approach to control bump interactions, derives qualitative local stability estimates in region-wise maximum norms to precisely control support sets and minimize overlap between bumps.

Result: Constructs infinitely many nonnegative bump solutions with arbitrarily many bumps when ‖K-1‖_{L^p_loc} is sufficiently small, with uniform estimates independent of number of bumps.

Conclusion: The truncated functional space method with sharp support estimates successfully overcomes sensitive bump interaction challenges, enabling construction of infinitely many bump solutions for sublinear elliptic equations with nonsymmetric potentials.

Abstract: We investigate the existence of nonnegative bump solutions to the sublinear elliptic equation \[ \begin{cases} -Δv - K(x)v + |v|^{q-2}v = 0 & \text{in } \mathbb{R}^N, \\ v(x) \to 0 & \text{as } |x| \to \infty, \end{cases} \] where $q \in (1,2)$, $ N \geq 2$, and the potential $K \in L^p_{\mathrm{loc}}(\mathbb{R}^N)$ with $p > N/2$ is a function without any symmetry assumptions. Under the condition that $\|K - 1\|_{L^p_{\mathrm{loc}}}$ is sufficiently small, we construct infinitely many solutions with arbitrarily many bumps. The construction is challenged by the sensitive interaction between bumps, whose limiting profiles have compact support. The key to ensuring their effective separation lies in obtaining sharp estimates of the support sets. Our method, based on a truncated functional space, provides precisely such control. We derive qualitative local stability estimates in region-wise maximum norms that govern the size of each bump's essential support, confining its core to a designated region and minimizing overlap. Crucially, these estimates are uniform in the number of bumps, which is the pivotal step in establishing the existence of solutions with infinitely many bumps.

</details>


### [27] [Mean-Field Limits of Deterministic and Stochastic Flocking Models with Nonlinear Velocity Alignment](https://arxiv.org/abs/2512.24383)
*Vinh Nguyen,Roman Shvydkoy,Changhui Tan*

Main category: math.AP

TL;DR: Mean-field limit analysis for agent-based flocking models with nonlinear velocity alignment and power-law coupling, extended to both deterministic and stochastic settings with improved convergence rates.


<details>
  <summary>Details</summary>
Motivation: Extend classical Cucker-Smale flocking theory to nonlinear framework with power-law velocity coupling, which has received recent attention in literature. Address both deterministic and stochastic versions of agent-based models.

Method: Study agent-based models with communication protocol φ and nonlinear velocity coupling A(v) = |v|^{p-2}v (p>2). Prove mean-field limit in deterministic and stochastic settings. Provide quantitative propagation of chaos estimates for deterministic case with fat-tailed kernels. Address stochastic version with multiplicative noise depending on local interaction intensity.

Result: Proved mean-field limit for both deterministic and stochastic settings. Showed improved convergence rate of k-particle marginals to Vlasov equation solution for deterministic case with fat-tailed kernels. Derived Fokker-Planck-Alignment equation for stochastic version with multiplicative noise.

Conclusion: Successfully extended Cucker-Smale theory to nonlinear framework with power-law velocity alignment. Established rigorous mean-field limits and propagation of chaos results for both deterministic and stochastic agent-based flocking models.

Abstract: We study the mean-field limit for a class of agent-based models describing flocking with nonlinear velocity alignment. Each agent interacts through a communication protocol $φ$ and a non-linear coupling of velocities given by the power law $A(\bv) = |\bv|^{p-2}\bv$, $p > 2$. The mean-field limit is proved in two settings -- deterministic and stochastic. We then provide quantitative estimates on propagation of chaos for deterministic case in the case of the classical fat-tailed kernels, showing an improved convergence rate of the $k$-particle marginals to a solution of the corresponding Vlasov equation. The stochastic version is addressed with multiplicative noise depending on the local interaction intensity, which leads to the associated Fokker-Planck-Alignment equation.
  Our results extend the classical Cucker-Smale theory to the nonlinear framework which has received considerable attention in the literature recently.

</details>


### [28] [Stability of the reconstruction of the heat reflection coefficient in the phonon transport equation](https://arxiv.org/abs/2512.24394)
*Peiyi Chen,Irene M. Gamba,Qin Li,Anjali Nair*

Main category: math.AP

TL;DR: The paper analyzes stability of inverse problem for determining phonon reflection coefficient from temperature measurements, showing ill-posedness in ballistic-to-diffusive transition.


<details>
  <summary>Details</summary>
Motivation: The reflection coefficient is crucial for nanoscale thermal properties, but determining it requires solving inverse problems from macroscopic measurements. Previous studies show discrepancies about well-posedness of this inverse problem.

Method: The authors investigate stability of the inverse problem for inferring reflection coefficient in phonon transport equation, analyzing how stability changes with Knudsen number (ballistic-to-diffusive transition).

Result: The problem becomes ill-posed as system transitions from ballistic to diffusive regime (Knudsen number → 0). The study quantifies deterioration rate of stability with respect to Knudsen number and provides numerical confirmation.

Conclusion: The stability analysis clarifies previous discrepancies about well-posedness and provides quantitative understanding of how stability degrades during ballistic-diffusive transition, important for nanoscale thermal property determination.

Abstract: The reflection coefficient is an important thermal property of materials, especially at the nanoscale, and determining this property requires solving an inverse problem based on macroscopic temperature measurements. In this manuscript, we investigate the stability of this inverse problem to infer the reflection coefficient in the phonon transport equation. We show that the problem becomes ill-posed as the system transitions from the ballistic to the diffusive regime, characterized by the Knudsen number converging to zero. Such a stability estimate clarifies the discrepancy observed in previous studies on the well-posedness of this inverse problem. Furthermore, we quantify the rate at which the stability deteriorates with respect to the Knudsen number and confirm the theoretical result with numerical evidence.

</details>


### [29] [Solvability conditions for some non-Fredholm operators with shifted arguments](https://arxiv.org/abs/2512.24476)
*Vitali Vougalter,Vitaly Volpert*

Main category: math.AP

TL;DR: Existence and convergence results for solutions to linear differential and integro-differential equations with shifted arguments in Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To establish existence and convergence properties for solutions to equations involving argument shifts, which arise in various applications where delayed or advanced terms appear in differential equations.

Method: Two-part approach: 1) For nonhomogeneous linear differential equations with shifted arguments, analyze convergence in L² of source terms to prove existence and convergence in H² of solutions. 2) For integro-differential equations with shifted arguments, study convergence in L¹ of integral kernels to establish existence and convergence in H² of solutions.

Result: Under reasonable technical conditions: 1) L² convergence of source terms implies existence and H² convergence of solutions for differential equations. 2) L¹ convergence of integral kernels yields existence and H² convergence of solutions for integro-differential equations. The Fredholm property of the second-order operator depends on the shift constant.

Conclusion: The paper establishes rigorous existence and convergence results for solutions to differential and integro-differential equations with shifted arguments in Sobolev spaces, providing conditions under which convergence of source terms/kernels implies convergence of solutions in stronger norms.

Abstract: In the first part of the article we establish the existence in the sense of sequences of solutions in $H^{2}(R)$ for some nonhomogeneous linear differential equation in which one of the terms has the argument translated by a constant. It is shown that under the reasonable technical conditions the convergence in $L^{2}(R)$ of the source terms implies the existence and the convergence in $H^{2}(R)$ of the solutions. The second part of the work deals with the solvability in the sense of sequences in $H^{2}(R)$ of the integro-differential equation in which one of the terms has the argument shifted by a constant. It is demonstrated that under the appropriate auxiliary assumptions the convergence in $L^{1}(R)$ of the integral kernels yields the existence and the convergence in $H^{2}(R)$ of the solutions. Both equations considered involve the second order differential operator with or without the Fredholm property depending on the value of the constant by which the argument gets translated.

</details>


### [30] [Steady Self-Propelled Motion of a Rigid Body in a Viscous Fluid with Navier-Slip Boundary Conditions](https://arxiv.org/abs/2512.24510)
*Sarka Necasova,Arnab Roy,Ana Leonor Silvestre*

Main category: math.AP

TL;DR: Existence of steady self-propelled motion for a rigid body in viscous fluid with Navier-slip boundary conditions, establishing weak solutions and propulsion conditions.


<details>
  <summary>Details</summary>
Motivation: To analyze propulsion in microfluidic and rough-surface regimes where partial slip effects are significant, extending classical Dirichlet-based theory to more realistic Navier-slip boundary conditions.

Method: Analysis in body-fixed reference frame with fluid in exterior domain; use of nonhomogeneous Navier-slip boundary conditions; derivation of Korn-type inequality for exterior domains with rigid-body motion; introduction of finite-dimensional thrust space via auxiliary Stokes problems.

Result: Established existence of weak steady solutions under smallness assumptions; derived necessary and sufficient condition for slip velocity to induce nontrivial translational/rotational motion; clarified how boundary effects generate propulsion.

Conclusion: Successfully extended fluid-structure interaction theory from Dirichlet to Navier-slip boundaries, providing mathematical framework for understanding propulsion in slip-dominated regimes with practical applications in microfluidics.

Abstract: We investigate the steady self-propelled motion of a rigid body immersed in a three-dimensional incompressible viscous fluid governed by the Navier-Stokes equations. The analysis is performed in a body-fixed reference frame, so that the fluid occupies an exterior domain and the propulsion mechanism is modeled through nonhomogeneous Navier-slip boundary conditions at the fluid-body interface. Such conditions provide a realistic description of propulsion in microfluidic and rough-surface regimes, where partial slip effects are significant. Under suitable smallness assumptions on the boundary flux and on the normal component of the prescribed surface velocity, we establish the existence of weak steady solutions to the coupled fluid-structure system. A key analytical ingredient is the derivation of a Korn-type inequality adapted to exterior domains with rigid-body motion and Navier-slip interfaces, which yields uniform control of both the fluid velocity and the translational and rotational velocities of the body. Beyond existence, we provide a necessary and sufficient condition under which a prescribed slip velocity on the body surface induces nontrivial translational or rotational motion of the rigid body. This is achieved through the introduction of a finite-dimensional thrust space, defined via auxiliary exterior Stokes problems with Navier boundary conditions, which captures the effective contribution of boundary-driven flows to the rigid-body motion. Our results clarify how boundary effects generate propulsion and extend the classical Dirichlet-based theory to the Navier-slip setting.

</details>


### [31] [Anomalous Dissipation at Onsager-Critical Regularity](https://arxiv.org/abs/2512.24568)
*Alexey Cheskidov,Qirui Peng*

Main category: math.AP

TL;DR: The paper constructs 3D Euler solutions with anomalous dissipation via vanishing viscosity, extending 2.5D constructions and establishing sharp Onsager-critical energy criteria.


<details>
  <summary>Details</summary>
Motivation: To understand anomalous dissipation in 3D Euler equations through vanishing viscosity limits, extending previous 2.5D constructions and establishing sharp energy criteria in Onsager's sense.

Method: Extends 2.5D constructions from previous works, uses vanishing viscosity limit approach, establishes Onsager-critical energy criterion adapted to such flows, and provides fully 3D dissipative Euler example with slightly rough external force.

Result: Constructs solutions exhibiting anomalous dissipation in finite time, shows sharpness of Onsager-critical energy criterion, and provides fully 3D dissipative Euler example that is sharp in Onsager's sense.

Conclusion: The work successfully extends anomalous dissipation constructions to 3D Euler equations, establishes sharp energy criteria, and demonstrates the phenomenon can occur with slightly rough external forces, contributing to understanding of turbulence and dissipation mechanisms.

Abstract: We construct solutions to the three-dimensional Euler equations exhibiting anomalous dissipation in finite time through a vanishing viscosity limit. Inspired by \cite{BDL23} and \cite{cheskidov2023dissipation}, we extend the \(2\frac{1}{2}\)-dimensional constructions and establish an Onsager-critical energy criterion adapted to such flows, showing its sharpness. Moreover, we provide a fully three-dimensional dissipative Euler example, sharp in Onsager's sense, driven by a slightly rough external force, following the framework of \cite{CL21}.

</details>


### [32] [Propagation of space-time singularities for perturbed harmonic oscillators](https://arxiv.org/abs/2512.24582)
*Kenichi Ito,Tomoya Tagawa*

Main category: math.AP

TL;DR: Analysis of space-time singularity propagation for quantum harmonic oscillator with time-dependent perturbations using semiclassical wave front set methods.


<details>
  <summary>Details</summary>
Motivation: To understand how singularities propagate in quantum systems with time-dependent perturbations, particularly for the harmonic oscillator where time becomes a base variable rather than just a parameter.

Method: Reformulate Lascar's quasi-homogeneous wave front set in semiclassical manner, adapt Nakamura's argument for spatial singularities to handle time-dependent case where time is part of base variables.

Result: Obtain characterization of singularity appearance in perturbed system compared to unperturbed one, despite non-trivial application due to time's role as base variable.

Conclusion: Successfully extended singularity analysis methods to time-dependent quantum systems, providing framework for understanding singularity propagation in perturbed harmonic oscillators.

Abstract: We discuss propagation of space-time singularities for the quantum harmonic oscillator with time-dependent metric and potential perturbations. Reformulating the quasi-homogeneous wave front set according to Lascar (1977) in a semiclassical manner, we obtain a characterization of its appearance in comparison with the unperturbed system. The idea of our proof is based on the argument of Nakamura (2009), which was originally devised for the analysis of spatial singularities of the Schrödinger equation, however, the application is non-trivial since the time is no more a parameter, but takes a part in the base variables.

</details>


### [33] [Phase transition thresholds and chiral magnetic fields of general degree](https://arxiv.org/abs/2512.24598)
*Slim Ibrahim,Tatsuya Miura,Carlos Román,Ikkei Shimizu*

Main category: math.AP

TL;DR: The paper analyzes variational problems for Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in 2D micromagnetics, focusing on the Bogomol'nyi regime. It determines minimal energy for arbitrary topological degree, reveals phase transitions, proves uniqueness/nonexistence of minimizers, and studies stability transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the variational properties of Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in 2D micromagnetics, particularly in the Bogomol'nyi regime. The research aims to characterize energy minimization, phase transitions, and stability properties relevant to skyrmion physics.

Method: The authors use variational analysis and mathematical techniques to study the Landau-Lifshitz energy functional with Dzyaloshinskii-Moriya interactions. They focus on the Bogomol'nyi regime and employ topological degree theory to analyze energy minimization problems.

Result: 1) Determined minimal energy for arbitrary topological degree, revealing two types of phase transitions consistent with physical observations. 2) Proved uniqueness of energy minimizers for degrees 0 and -1, and nonexistence of minimizers for all other degrees. 3) Showed homogeneous state remains stable beyond threshold where skyrmion loses stability. 4) Uncovered new stability transition driven by Zeeman energy.

Conclusion: The study provides comprehensive understanding of energy minimization and stability in 2D micromagnetics with Dzyaloshinskii-Moriya interactions. The results reveal important phase transitions, characterize existence/nonexistence of minimizers, and identify stability properties that enhance understanding of skyrmion physics in the Bogomol'nyi regime.

Abstract: We study a variational problem for the Landau--Lifshitz energy with Dzyaloshinskii--Moriya interactions arising in 2D micromagnetics, focusing on the Bogomol'nyi regime. We first determine the minimal energy for arbitrary topological degree, thereby revealing two types of phase transitions consistent with physical observations. In addition, we prove the uniqueness of the energy minimizer in degrees $0$ and $-1$, and nonexistence of minimizers for all other degrees. Finally, we show that the homogeneous state remains stable even beyond the threshold at which the skyrmion loses stability, and we uncover a new stability transition driven by the Zeeman energy.

</details>


### [34] [Half-space minimizing solutions of a two dimensional Allen-Cahn system](https://arxiv.org/abs/2512.24610)
*Zhiyuan Geng*

Main category: math.AP

TL;DR: Complete classification of half-space minimizing solutions to 2D Allen-Cahn system with multi-well potential, based on blow-down limits at infinity, plus characterization of asymptotic behavior near sharp interfaces.


<details>
  <summary>Details</summary>
Motivation: Study minimizing solutions to Allen-Cahn systems on half-planes with Dirichlet boundary conditions to understand pattern formation and interface behavior in phase transition models with multiple stable phases.

Method: Analyze solutions through blow-down limits at infinity, classify minimizing solutions based on these asymptotic limits, and characterize asymptotic behavior near sharp interfaces using analytical techniques.

Result: Complete classification of half-space minimizing solutions in terms of their blow-down limits, providing characterization of asymptotic behavior near sharp interfaces in the 2D Allen-Cahn system.

Conclusion: The classification framework based on blow-down limits provides comprehensive understanding of minimizing solutions and their interface behavior in half-space Allen-Cahn systems with multi-well potentials.

Abstract: This paper studies minimizing solutions to a two dimensional Allen-Cahn system on the upper half plane, subject to Dirichlet boundary conditions, \begin{equation*}
  Δu-\nabla_u W(u)=0, \quad u: \mathbb{R}_+^2\to \mathbb{R}^2,\ u=u_0 \text{ on } \partial \mathbb{R}_+^2, \end{equation*} where $W: \mathbb{R}^2\to [0,\infty)$ is a multi-well potential. We give a complete classification of such half-space minimizing solutions in terms of their blow-down limits at infinity. In addition, we characterize the asymptotic behavior of solutions near the associated sharp interfaces.

</details>


### [35] [A unified spatiotemporal formulation with physics-preserving structure for time-dependent convection-diffusion problems](https://arxiv.org/abs/2512.24650)
*James H. Adler,Xiaozhe Hu,Seulip Lee*

Main category: math.AP

TL;DR: A 4D spatiotemporal framework for time-dependent convection-diffusion problems using exterior calculus, treating time as space-like coordinate to create stationary 4D equations with physical structure preservation.


<details>
  <summary>Details</summary>
Motivation: To develop a unified formulation for time-dependent convection-diffusion problems that preserves underlying physical structures and handles the full family of problems on H(grad), H(curl), and H(div) spaces.

Method: Treat time as additional space-like coordinate to reformulate evolution problems as stationary convection-diffusion equations in 4D space-time domain using exterior calculus. Introduce 4D Hodge-Laplacian operator with spatiotemporal diffusion tensor and convection field, plus temporal perturbation for nondegeneracy. Develop exponentially-fitted 4D spatiotemporal flux operator for symmetrization.

Result: The formulation naturally incorporates fundamental physical constraints (divergence-free and curl-free conditions) and enables well-posed variational formulation. The temporally-perturbed formulation converges to original time-dependent model as perturbation parameter tends to zero.

Conclusion: A unified 4D spatiotemporal framework successfully preserves physical structures for convection-diffusion problems across different function spaces, with mathematical guarantees of convergence to original time-dependent models.

Abstract: We propose a unified four-dimensional (4D) spatiotemporal formulation for time-dependent convection-diffusion problems that preserves underlying physical structures. By treating time as an additional space-like coordinate, the evolution problem is reformulated as a stationary convection-diffusion equation on a 4D space-time domain. Using exterior calculus, we extend this framework to the full family of convection-diffusion problems posed on $H(\textbf{grad})$, $H(\textbf{curl})$, and $H(\text{div})$. The resulting formulation is based on a 4D Hodge-Laplacian operator with a spatiotemporal diffusion tensor and convection field, augmented by a small temporal perturbation to ensure nondegeneracy. This formulation naturally incorporates fundamental physical constraints, including divergence-free and curl-free conditions. We further introduce an exponentially-fitted 4D spatiotemporal flux operator that symmetrizes the convection-diffusion operator and enables a well-posed variational formulation. Finally, we prove that the temporally-perturbed formulation converges to the original time-dependent convection-diffusion model as the perturbation parameter tends to zero.

</details>


### [36] [$L_p$-estimates for nonlocal equations with general Lévy measures](https://arxiv.org/abs/2512.24704)
*Hongjie Dong,Junhee Ryu*

Main category: math.AP

TL;DR: The paper establishes continuity and unique strong solvability of nonlocal parabolic equations with singular Lévy measures in Lp spaces, and investigates their treatment in weighted mixed-norm spaces.


<details>
  <summary>Details</summary>
Motivation: To study nonlocal operators with general Lévy measures that can be very singular and have no time regularity, extending the theory of nonlocal parabolic equations beyond classical settings.

Method: Analysis of nonlocal operators defined by Lévy measures of order σ∈(0,2), allowing singular measures without time regularity assumptions, using Lp space techniques and investigating weighted mixed-norm spaces.

Result: Continuity of operators and unique strong solvability of corresponding nonlocal parabolic equations in Lp spaces are established. The treatment in weighted mixed-norm spaces depends on ranges of σ and d.

Conclusion: The theory extends to singular Lévy measures without time regularity, with applicability in weighted mixed-norm spaces depending on parameter ranges, providing a comprehensive framework for nonlocal parabolic equations.

Abstract: We consider nonlocal operators of the form \begin{equation*}
  L_t u(x) = \int_{\mathbb{R}^d} \left( u(x+y)-u(x)-\nabla u(x)\cdot y^{(σ)} \right) ν_t(dy), \end{equation*} where $ν_t$ is a general Lévy measure of order $σ\in(0,2)$. We allow this class of Lévy measures to be very singular and impose no regularity assumptions in the time variable. Continuity of the operators and the unique strong solvability of the corresponding nonlocal parabolic equations in $L_p$ spaces are established. We also demonstrate that, depending on the ranges of $σ$ and $d$, the operator can or cannot be treated in weighted mixed-norm spaces.

</details>


### [37] [Global spherically symmetric classical solutions for arbitrary large initial data of the multi-dimensional non-isentropic compressible Navier-Stokes equations](https://arxiv.org/abs/2512.24799)
*Yongteng Gu,Xiangdi Huang*

Main category: math.AP

TL;DR: The paper proves global classical solutions for arbitrary large initial data to the viscous shallow water system with transported entropy in spherically symmetric 2D and 3D cases, extending previous results and relaxing dimension/adiabatic index restrictions.


<details>
  <summary>Details</summary>
Motivation: The shallow water equations have been an open problem since 1871 regarding global classical solutions for arbitrary large initial data. Recent breakthroughs proved existence for radial symmetric cases, but with restrictions on dimensions and adiabatic index. This paper aims to generalize these results to non-isentropic compressible fluids with transported entropy.

Method: Develops a new BD entropy inequality for non-isentropic compressible fluids, uses different density lower bound estimates than previous work, and applies these to the viscous shallow water system with transport entropy in spherically symmetric initial-boundary value problems.

Result: Proves existence of global classical solutions for arbitrary large initial data in both 2D and 3D spherically symmetric cases, extending the admissible parameter range from N=2, γ≥3/2 to N=2, γ>1 and N=3, 1<γ<3.

Conclusion: The paper successfully generalizes previous shallow water results to non-isentropic fluids with transported entropy, establishing global classical solutions for large data in broader dimensional and parameter ranges through new BD entropy inequality and density estimates.

Abstract: In 1871, Saint-Venant introduced the shallow water equations. Since then, the global classical solutions for arbitrary large initial data of the multi-dimensional viscous Saint-Venant system have remained a well-known open problem. It was only recently that [Huang-Meng-Zhang, http:arXiv:2512.15029, 2025], under the assumption of radial symmetry, first proved the existence of global classical solutions for arbitrary large initial data to the initial-boundary value problem of the two-dimensional viscous shallow water equations. At the same time, [Chen-Zhang-Zhu, http:arXiv:2512.18545, 2025] also independently proved the existence of global large solutions to the Cauchy problem of this system. Notably, in the work of Huang-Meng-Zhang, they also established the existence of global classical solutions for arbitrary large initial data to the isentropic compressible Navier-Stokes equations satisfying the BD entropy equality in both two and three dimensions, and the viscous shallow water equations are precisely a specific class of isentropic compressible fluids subject to the BD entropy equality. In this paper, we prove a new BD entropy inequality for a class of non-isentropic compressible fluids, which can be regarded as a generalization of the shallow water equations with transported entropy. Employing new estimates on the lower bound of density different from that of Huang-Meng-Zhang's work, we show the "viscous shallow water system with transport entropy" will admit global classical solutions for arbitrary large initial data to the spherically symmetric initial-boundary value problem in both two and three dimensions. Our results also relax the restrictions on the dimension and adiabatic index imposed in Huang-Meng-Zhang's work on the shallow water equations, extending the range from $N=2,\ γ\ge \frac{3}{2}$ to $N=2,\ γ> 1$ and $N=3,\ 1<γ<3$.

</details>


### [38] [Hölder continuity of weak solutions to the thin-film equation in $d=2$](https://arxiv.org/abs/2512.24809)
*Federico Cornalba,Julian Fischer,Erika Maringová Kokavcová*

Main category: math.AP

TL;DR: Proves Hölder continuity of energy-dissipating weak solutions to the 2D thin-film equation, solving a major open problem about boundedness of solutions.


<details>
  <summary>Details</summary>
Motivation: The thin-film equation models viscous liquid film spreading on surfaces. While existence theory for weak solutions was established decades ago, boundedness of solutions in 2D remained a major unsolved problem due to the equation's fourth-order degenerate parabolic structure.

Method: Uses the hole-filling technique to overcome challenges posed by the degenerate parabolicity of the fourth-order PDE. The approach works around the inapplicability of De Giorgi-Nash-Moser theory for fourth-order equations.

Result: Successfully proves Hölder continuity of energy-dissipating weak solutions to the thin-film equation in two spatial dimensions (d=2), establishing boundedness of solutions.

Conclusion: Solves a longstanding open problem in thin-film equation theory by proving regularity of weak solutions in the physically most relevant 2D case, using innovative techniques to handle fourth-order degenerate parabolic structure.

Abstract: The thin-film equation $\partial_t u = -\nabla \cdot (u^n \nabla Δu)$ describes the evolution of the height $u=u(x,t)\geq 0$ of a viscous thin liquid film spreading on a flat solid surface. We prove Hölder continuity of energy-dissipating weak solutions to the thin-film equation in the physically most relevant case of two spatial dimensions $d=2$. While an extensive existence theory of weak solutions to the thin-film equation was established more than two decades ago, even boundedness of weak solutions in $d=2$ has remained a major unsolved problem in the theory of the thin-film equation. Due the fourth-order structure of the thin-film equation, De Giorgi-Nash-Moser theory is not applicable. Our proof is based on the hole-filling technique, the challenge being posed by the degenerate parabolicity of the fourth-order PDE.

</details>


### [39] [Bol's type inequality for singular metrics and its application to prescribing $Q$-curvature problems](https://arxiv.org/abs/2512.24828)
*Mrityunjoy Ghosh,Ali Hyder*

Main category: math.AP

TL;DR: Higher-order Bol's inequality applied to radial normal solutions of singular Liouville equations yields existence criteria for singular Q-curvature problems and uniform bounds on total Q-curvature.


<details>
  <summary>Details</summary>
Motivation: To establish existence conditions for radial normal solutions to singular Q-curvature problems using higher-order Bol's inequality and compactness arguments.

Method: Apply higher-order Bol's inequality to radial normal solutions of singular Liouville equations, use compactness arguments, and analyze under suitable Q-curvature assumptions.

Result: Derived necessary and sufficient conditions for existence of radial normal solutions to singular Q-curvature problems, and obtained uniform bounds on total Q-curvature.

Conclusion: Higher-order Bol's inequality combined with compactness provides powerful tools for analyzing existence and bounds in singular Q-curvature problems with radial normal solutions.

Abstract: In this article, we study higher-order Bol's inequality for radial normal solutions to a singular Liouville equation. By applying these inequalities along with compactness arguments, we derive necessary and sufficient conditions for the existence of radial normal solutions to a singular $Q$-curvature problem. Moreover, under suitable assumptions on the $Q$-curvature, we obtain uniform bounds on the total $Q$-curvature.

</details>


### [40] [Boundedness of Fourier Integral Operators with complex phases on Fourier Lebesgue spaces](https://arxiv.org/abs/2512.24854)
*Duván Cardona,William Obeng-Denteh,Frederick Opoku*

Main category: math.AP

TL;DR: Complex phase Fourier integral operators are bounded on Fourier Lebesgue spaces under spatial factorization condition with sharp order bound.


<details>
  <summary>Details</summary>
Motivation: Extend boundedness results for Fourier integral operators from real canonical relations to complex canonical relations, establishing complex analogues of known real-phase results.

Method: Develop boundedness estimates for Fourier integral operators with complex phase functions on Fourier Lebesgue spaces, using spatial factorization condition of rank ϰ.

Result: Fourier integral operator is bounded on ℱL^p when order m ≤ -ϰ|1/p - 1/2|, 1 ≤ p ≤ ∞, and this condition on m is sharp.

Conclusion: Successfully established complex analogue of real-phase boundedness results with optimal order conditions for Fourier integral operators on Fourier Lebesgue spaces.

Abstract: In this paper, we develop boundedness estimates for Fourier integral operators on Fourier Lebesgue spaces when the associated canonical relation is parametrised by a complex phase function. Our result constitutes the complex analogue of those obtained for real canonical relations by Rodino, Nicola, and Cordero. We prove that, under the spatial factorization condition of rank $\varkappa$, the corresponding Fourier integral operator is bounded on the Fourier Lebesgue space $\mathcal{F}L^p,$ provided that the order $m$ of the operator satisfies that $ m \leq -\varkappa\left|\frac{1}{p}-\frac{1}{2}\right|, 1 \leq p \leq \infty. $ This condition on the order $m$ is sharp.

</details>


### [41] [Global boundedness and absorbing sets in two-dimensional chemotaxis-Navier-Stokes systems with weakly singular sensitivity and a sub-logistic source](https://arxiv.org/abs/2512.24892)
*Minh Le,Alexey Cheskidov*

Main category: math.AP

TL;DR: The paper proves global existence and boundedness of classical solutions for a chemotaxis-fluid system with logistic growth and modified chemotactic sensitivity in 2D bounded domains.


<details>
  <summary>Details</summary>
Motivation: To establish global well-posedness for a chemotaxis-fluid model with modified chemotactic sensitivity (n∇c/c^k) and logistic growth with logarithmic damping, which extends previous results on chemotaxis-fluid systems.

Method: Uses energy estimates and functional analysis techniques to establish a priori bounds, proving global existence of classical solutions under no-flux/no-flux/Dirichlet boundary conditions with suitable initial data.

Result: The system admits globally bounded classical solutions and possesses an absorbing set in the topology of C⁰(Ω̄) × W¹,∞(Ω) × C⁰(Ω̄; ℝ²).

Conclusion: The modified chemotaxis-fluid system with logistic growth and logarithmic damping is well-posed in 2D, with solutions remaining globally bounded and exhibiting absorbing dynamics.

Abstract: This paper studies the following chemotaxis-fluid system in a two-dimensional bounded domain $Ω$: \begin{equation*}
  \begin{cases}
  n_t + u \cdot \nabla n &= Δn - χ\nabla \cdot \left (n \frac{\nabla c}{c^k} \right ) + r n - \frac{μn^2}{\log^η(n+e)},
  c_t + u \cdot \nabla c &= Δc - αc + βn,
  u_t + u \cdot \nabla u &= Δu - \nabla P + n \nabla φ+ f,
  \nabla \cdot u &= 0,
  \end{cases} \end{equation*} where $r, μ, α, β, χ$ are positive parameters, $k, η\in (0,1)$, $φ\in W^{2,\infty}(Ω)$, and $f \in C^1\left(\barΩ\times [0, \infty)\right) \cap L^\infty\left(Ω\times (0, \infty)\right)$. We show that, under suitable conditions on the initial data and with no-flux/no-flux/Dirichlet boundary conditions, this system admits a globally bounded classical solution. Furthermore, the system possesses an absorbing set in the topology of $C^0(\barΩ) \times W^{1, \infty}(Ω) \times C^0(\barΩ; \mathbb{R}^2)$.

</details>


### [42] [On exact Observability for Compactly perturbed infinite dimension system](https://arxiv.org/abs/2512.25041)
*Nisrine Charaf,Faouzi Triki*

Main category: math.AP

TL;DR: Study of observability preservation for compactly perturbed infinite-dimensional systems with self-adjoint generators.


<details>
  <summary>Details</summary>
Motivation: To understand when exact observability is preserved under compact perturbations in infinite-dimensional systems, which is important for control theory applications where systems may be subject to perturbations.

Method: Assuming an infinite-dimensional system with self-adjoint generator is exactly observable, derive sufficient conditions on compact self-adjoint perturbations to maintain observability. Analysis based on asymptotic estimation of spectral elements of perturbed unbounded operators in terms of the perturbation.

Result: Developed sufficient conditions for compact self-adjoint perturbations that guarantee preservation of exact observability. Derived intermediate results on asymptotic estimation of spectral elements that are important in their own right.

Conclusion: The paper provides theoretical conditions under which compact perturbations preserve observability in infinite-dimensional systems, with spectral analysis techniques that yield valuable intermediate mathematical results.

Abstract: In this paper, we study the observability of compactly perturbed infinite dimensional systems. Assuming that a given infinite-dimensional system with self-adjoint generator is exactly observable we derive sufficient conditions on a compact self adjoint perturbation to guarantee that the perturbed system stays exactly observable. The analysis is based on a careful asymptotic estimation of the spectral elements of the perturbed unbounded operator in terms of the compact perturbation. These intermediate results are of importance themselves.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [43] [Learning Density Functionals to Bridge Particle and Continuum Scales](https://arxiv.org/abs/2512.23840)
*Edoardo Monti,Peter Yatsyshin,Konstantinos Gkagkas,Andrew B. Duncan*

Main category: physics.comp-ph

TL;DR: A physics-informed learning framework that augments classical density functional theory with neural corrections trained against molecular dynamics data, preserving thermodynamic consistency while improving predictive accuracy for interfacial thermodynamics.


<details>
  <summary>Details</summary>
Motivation: Predicting interfacial thermodynamics across molecular and continuum scales is challenging. Classical density functional theory (cDFT) connects microscopic interactions with macroscopic observables but depends on approximate free-energy functionals that are difficult to generalize and lack accuracy.

Method: Introduces a physics-informed learning framework that augments cDFT with neural corrections. Rather than replacing theory with black-box surrogates, it embeds compact neural networks within the Helmholtz free-energy functional. The neural corrections are trained directly against molecular-dynamics data through adjoint optimization, learning both local and nonlocal corrections that preserve thermodynamic consistency while capturing missing correlations.

Result: Applied to Lennard-Jones fluids, the augmented excess free-energy functional quantitatively reproduces equilibrium density profiles, coexistence curves, and surface tensions across a broad temperature range. It also accurately predicts contact angles and droplet shapes far beyond the training regime.

Conclusion: This approach combines the interpretability of statistical mechanics with the adaptability of modern machine learning, establishing a general route to learned thermodynamic functionals that bridge molecular simulations and continuum-scale models.

Abstract: Predicting interfacial thermodynamics across molecular and continuum scales remains a central challenge in computational science. Classical density functional theory (cDFT) provides a first-principles route to connect microscopic interactions with macroscopic observables, but its predictive accuracy depends on approximate free-energy functionals that are difficult to generalize. Here we introduce a physics-informed learning framework that augments cDFT with neural corrections trained directly against molecular-dynamics data through adjoint optimization. Rather than replacing the theory with a black-box surrogate, we embed compact neural networks within the Helmholtz free-energy functional, learning local and nonlocal corrections that preserve thermodynamic consistency while capturing missing correlations. Applied to Lennard-Jones fluids, the resulting augmented excess free-energy functional quantitatively reproduces equilibrium density profiles, coexistence curves, and surface tensions across a broad temperature range, and accurately predicts contact angles and droplet shapes far beyond the training regime. This approach combines the interpretability of statistical mechanics with the adaptability of modern machine learning, establishing a general route to learned thermodynamic functionals that bridge molecular simulations and continuum-scale models.

</details>


### [44] [BF-APNN: A Low-Memory Method for Accelerating the Solution of Radiative Transfer Equations](https://arxiv.org/abs/2512.24534)
*Xizhe Xie,Wengu Chen,Weiming Li,Peng Song,Han Wang*

Main category: physics.comp-ph

TL;DR: BF-APNN accelerates radiative transfer equation solutions using basis function expansion to reduce high-dimensional integral computations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Radiative Transfer Equations are high-dimensional and multiscale, making conventional methods computationally intensive. Existing deep learning methods struggle with high-dimensional or nonlinear RTEs.

Method: BF-APNN extends RT-APNN by applying basis function expansion on the microscopic component from micro-macro decomposition, reducing computational burden of high-dimensional integrals during training.

Result: BF-APNN substantially reduces training time compared to RT-APNN while preserving high accuracy, and performs well on challenging RTE scenarios with nonlinearity, discontinuities, and multiscale behavior.

Conclusion: BF-APNN shows superior performance for complex, high-dimensional RTE problems and has potential as a robust tool for radiative transfer computations.

Abstract: The Radiative Transfer Equations (RTEs) exhibit high dimensionality and multiscale characteristics, rendering conventional numerical methods computationally intensive. Existing deep learning methods perform well in low-dimensional or linear RTEs, but still face many challenges with high-dimensional or nonlinear RTEs. To overcome these challenges, we propose the Basis Function Asymptotically Preserving Neural Network (BF-APNN), a framework that inherits the advantages of Radiative Transfer Asymptotically Preserving Neural Network (RT-APNN) and accelerates the solution process. By employing basis function expansion on the microscopic component, derived from micro-macro decomposition, BF-APNN effectively mitigates the computational burden associated with evaluating high-dimensional integrals during training. Numerical experiments, which involve challenging RTE scenarios featuring, nonlinearity, discontinuities, and multiscale behavior, demonstrate that BF-APNN substantially reduces training time compared to RT-APNN while preserving high solution accuracy. Moreover, BF-APNN exhibits superior performance in addressing complex, high-dimensional RTE problems, underscoring its potential as a robust tool for radiative transfer computations.

</details>


### [45] [Random Batch Sum-of-Gaussians Method for Molecular Dynamics of Born-Mayer-Huggins Systems](https://arxiv.org/abs/2512.24970)
*Chen Chen,Jiuyang Liang,Zhenli Xu,Qianru Zhang*

Main category: physics.comp-ph

TL;DR: Extends RBSOG method to BMH systems with random batch list scheme for efficient large-scale MD simulations of ionic materials.


<details>
  <summary>Details</summary>
Motivation: Large-scale MD simulations of BMH systems are computationally expensive due to Coulomb, dispersion, and short-range interactions. Existing methods have limitations in computation, communication, and memory costs.

Method: Extends RBSOG method to BMH systems and incorporates random batch list (RBL) scheme. Uses SOG decomposition to split potential into short- and long-range parts, with importance sampling in Fourier space for long-range part. RBL accelerates short-range interactions via random batch neighbor list.

Result: Achieves 4-10× speedup vs Ewald-based P3M and 2× speedup vs RBSOG-only method using 1000 cores for systems up to 5×10⁶ atoms. Maintains structural/thermodynamic accuracy with reduced memory usage.

Conclusion: The unified framework provides efficient and scalable treatment of both long- and short-range interactions in BMH systems, demonstrating attractive performance for large-scale MD simulations with long-range interactions.

Abstract: The Born-Mayer-Huggins (BMH) potential, which combines Coulomb interactions with dispersion and short-range exponential repulsion, is widely used for ionic materials such as molten salts. However, large-scale molecular dynamics simulations of BMH systems are often limited by computation, communication, and memory costs. We recently proposed the random batch sum-of-Gaussians (RBSOG) method, which accelerates Coulomb calculations by using a sum-of-Gaussians (SOG) decomposition to split the potential into short- and long-range parts and by applying importance sampling in Fourier space for the long-range part. In this work, we extend the RBSOG to BMH systems and incorporate a random batch list (RBL) scheme to further accelerate the short-range part, yielding a unified framework for efficient simulations with the BMH potential. The combination of the SOG decomposition and the RBL enables an efficient and scalable treatment of both long- and short-range interactions in BMH system, particularly the RBL well handles the medium-range exponential repulsion and dispersion by the random batch neighbor list. Error estimate is provided to show the theoretical convergence of the RBL force. We evaluate the framework on molten NaCl and mixed alkali halide with up to $5\times10^6$ atoms on $2048$ CPU cores. Compared to the Ewald-based particle-particle particle-mesh method and the RBSOG-only method, our method achieves approximately $4\sim10\times$ and $2\times$ speedups while using $1000$ cores, respectively, under the same level of structural and thermodynamic accuracy and with a reduced memory usage. These results demonstrate the attractive performance of our method in accuracy and scalability for MD simulations with long-range interactions.

</details>


### [46] [Fast Poisson brackets and constraint algebras in canonical gravity](https://arxiv.org/abs/2512.25007)
*Will Barker*

Main category: physics.comp-ph

TL;DR: A computer algebra package for efficiently computing Poisson brackets and constraint algebras in gravity theories, tested on GR and modified gravity.


<details>
  <summary>Details</summary>
Motivation: Dirac's Hamiltonian constraint algorithm is essential for analyzing propagating modes and gauge symmetries in gravity theories, but its implementation is notoriously arduous and computationally intensive.

Method: Developed a simple computer algebra package specifically designed to efficiently compute Poisson brackets and reconstruct constraint algebras in canonical approaches to gravity.

Result: The package was successfully stress-tested against pure general relativity and various modified gravity theories, including order reduction of general relativity at two loops.

Conclusion: The presented computer algebra tool provides an efficient solution to the computational challenges of implementing Dirac's Hamiltonian constraint algorithm in gravity theories, facilitating analysis of pathologies, gauge symmetries, and quantization.

Abstract: In the study of alternative or extended theories of gravity, Dirac's Hamiltonian constraint algorithm is invaluable for enumerating the propagating modes and gauge symmetries. For gravity, this canonical approach is frequently applied as a means for finding pathologies such as strongly coupled modes; more generally it facilitates the reconstruction of gauge symmetries and the quantization of gauge theories. For gravity, however, the algorithm can become notoriously arduous to implement. We present a simple computer algebra package for efficiently computing Poisson brackets and reconstructing constraint algebras. The tools are stress-tested against pure general relativity and modified gravity, including the order reduction of general relativity at two loops.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [47] [Autoregressive long-horizon prediction of plasma edge dynamics](https://arxiv.org/abs/2512.23884)
*Hunor Csala,Sebastian De Pascuale,Paul Laiu,Jeremy Lore,Jae-Sun Park,Pei Zhang*

Main category: physics.plasm-ph

TL;DR: Transformer-based autoregressive surrogate models for fast prediction of 2D time-dependent plasma edge states, trained on SOLPS-ITER data, enabling orders-of-magnitude faster simulations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: High-fidelity edge fluid/neutral codes like SOLPS-ITER are computationally expensive, limiting parameter scans and long transient studies needed for fusion device design. There's a need for faster surrogates to enable rapid scenario exploration and control-oriented studies.

Method: Transformer-based autoregressive surrogate models trained on SOLPS-ITER spatiotemporal data to forecast electron temperature, electron density, and radiated power. Models were evaluated with increasing autoregressive horizons (1-100 steps) on short- and long-horizon prediction tasks.

Result: Longer-horizon training improves rollout stability and reduces error accumulation, enabling stable predictions over hundreds to thousands of steps while reproducing key dynamical features. The surrogate is orders of magnitude faster than SOLPS-ITER in wall-clock time. Accuracy degrades when predicting physical regimes not in training data.

Conclusion: Transformer-based surrogates provide fast, accurate alternatives to computationally intensive plasma edge simulations, supporting rapid parameter exploration, control studies, and progress toward real-time applications in fusion devices, though data enrichment and physics-informed constraints are needed for robustness.

Abstract: Accurate modeling of scrape-off layer (SOL) and divertor-edge dynamics is vital for designing plasma-facing components in fusion devices. High-fidelity edge fluid/neutral codes such as SOLPS-ITER capture SOL physics with high accuracy, but their computational cost limits broad parameter scans and long transient studies. We present transformer-based, autoregressive surrogates for efficient prediction of 2D, time-dependent plasma edge state fields. Trained on SOLPS-ITER spatiotemporal data, the surrogates forecast electron temperature, electron density, and radiated power over extended horizons. We evaluate model variants trained with increasing autoregressive horizons (1-100 steps) on short- and long-horizon prediction tasks. Longer-horizon training systematically improves rollout stability and mitigates error accumulation, enabling stable predictions over hundreds to thousands of steps and reproducing key dynamical features such as the motion of high-radiation regions. Measured end-to-end wall-clock times show the surrogate is orders of magnitude faster than SOLPS-ITER, enabling rapid parameter exploration. Prediction accuracy degrades when the surrogate enters physical regimes not represented in the training dataset, motivating future work on data enrichment and physics-informed constraints. Overall, this approach provides a fast, accurate surrogate for computationally intensive plasma edge simulations, supporting rapid scenario exploration, control-oriented studies, and progress toward real-time applications in fusion devices.

</details>


### [48] [The role of particle feedback on particle acceleration in magnetic reconnection](https://arxiv.org/abs/2512.24054)
*Shimin Liang,Nianyu Yi*

Main category: physics.plasm-ph

TL;DR: Particle feedback in magnetic reconnection amplifies shear flows in magnetic islands, strengthening convective electric fields and boosting particle acceleration, leading to higher maximum energies and harder spectra, while guide fields suppress these effects.


<details>
  <summary>Details</summary>
Motivation: To understand how particle feedback affects magnetic reconnection and particle acceleration processes in astrophysical plasmas, particularly the interplay between feedback mechanisms, guide fields, and reconnection dynamics.

Method: 2.5D magnetohydrodynamic (MHD) simulations using a co-evolving fluid-particle framework to investigate particle feedback effects on reconnection and acceleration.

Result: Particle feedback amplifies shear flows within magnetic islands, strengthening convective electric fields and boosting particle acceleration, resulting in higher maximum particle energies and harder non-thermal energy spectra. Guide fields suppress both gas internal energy increase and particle acceleration.

Conclusion: The study reveals complex feedback mechanisms in magnetic reconnection where particle acceleration is enhanced through shear flow amplification, while guide fields play a suppressing role, highlighting important interactions in astrophysical plasma processes.

Abstract: Magnetic reconnection is a ubiquitous process in astrophysical plasmas and an efficient mechanism for particle acceleration. Using 2.5D magnetohydrodynamic (MHD) simulations with a co-evolving fluid-particle framework, we investigate how particle feedback affects reconnection and acceleration. Our simulations demonstrate that particle feedback to the fluid amplifies shear flows within magnetic islands, which strengthens the convective electric field and thereby boosts particle acceleration. This mechanism results in a higher maximum particle energy and a harder non-thermal energy spectrum. The guide field suppresses both the increase in gas internal energy and particle acceleration. These findings highlight the complex interplay between feedback, guide fields, and reconnection dynamics.

</details>


### [49] [Coordinates based on a magnetic mirror field](https://arxiv.org/abs/2512.24305)
*R. D. Hazeltine*

Main category: physics.plasm-ph

TL;DR: Construction of a coordinate system tailored to the geometry of cylindrically symmetric magnetic fields


<details>
  <summary>Details</summary>
Motivation: Standard coordinate systems may not optimally represent the geometry of magnetic fields, especially cylindrical symmetric ones. A specialized coordinate system could better capture field structure and simplify analysis.

Method: Develop a mathematical framework to construct coordinate systems that align with the geometry of given cylindrically symmetric magnetic fields, likely using field lines as coordinate curves.

Result: A coordinate system specifically designed to fit the geometry of cylindrically symmetric magnetic fields, potentially enabling more efficient field analysis and calculations.

Conclusion: Specialized coordinate systems tailored to magnetic field geometry can provide better representation and analytical advantages over standard coordinate systems for studying cylindrically symmetric magnetic fields.

Abstract: We construct a coordinate system fitting the geometry of a given, cylindrically symmetric, magnetic field.

</details>


### [50] [Computing Flux-Surface Shapes in Tokamaks and Stellarators](https://arxiv.org/abs/2512.24544)
*M. J. Gerard,M. J. Pueschel,S. Stewart,H. O. M. Hillebrecht,B. Geiger*

Main category: physics.plasm-ph

TL;DR: A new method for characterizing stellarator magnetic field geometry using Fourier analysis of flux-surface shaping modes, revealing that quasi-symmetry emerges from spatial resonance between shape complexity and rotation about the magnetic axis.


<details>
  <summary>Details</summary>
Motivation: There is currently no agreed-upon methodology for characterizing stellarator magnetic field geometry, despite modern stellarator designs achieving high levels of magnetic-field quasi-symmetry through careful flux-surface shaping.

Method: Introduces a general method using Fourier mode analysis to define shaping modes (elongation, triangularity, squareness, etc.) of cross-sections that can be non-planar. The framework works for both axisymmetric and non-axisymmetric configurations, with the additional degree of freedom in non-axisymmetric equilibria manifesting as rotation of each shaping mode about the magnetic axis.

Result: Analysis of non-axisymmetric configurations with precise quasi-symmetry and cases from the QUASR database reveals that quasi-symmetry results from a spatial resonance between shape complexity and shape rotation about the magnetic axis. The quantitative features of this resonance correlate closely with a configuration's rotational transform and number of field periods.

Conclusion: The shaping paradigm can facilitate systematic investigations into the relationship between general flux-surface geometries and other figures of merit, providing a new framework for understanding and designing stellarator magnetic configurations.

Abstract: There is currently no agreed-upon methodology for characterizing a stellarator magnetic field geometry, and yet modern stellarator designs routinely attain high levels of magnetic-field quasi-symmetry through careful flux-surface shaping. Here, we introduce a general method for computing the shape of an ideal-MHD equilibrium that can be used in both axisymmetric and non-axisymmetric configurations. This framework uses a Fourier mode analysis to define the shaping modes (e.g. elongation, triangularity, squareness, etc.) of cross-sections that can be non-planar. Relative to an axisymmetric equilibrium, the additional degree of freedom in a non-axisymmetric equilibrium manifests as a rotation of each shaping mode about the magnetic axis. Using this method, a shaping analysis is performed on non-axisymmetric configurations with precise quasi-symmetry and select cases from the QUASR database spanning a range of quasi-symmetry quality. Empirically, we find that quasi-symmetry results from a spatial resonance between shape complexity and shape rotation about the magnetic axis. The quantitative features of this resonance correlate closely with a configuration's rotational transform and number of field periods. Based on these observations, it is conjectured that this shaping paradigm can facilitate systematic investigations into the relationship between general flux-surface geometries and other figures of merit.

</details>


### [51] [Cataloging the nonlinear waves excited by moving a charged body in the dusty plasma medium](https://arxiv.org/abs/2512.24723)
*Swathi S Krishna,S. K. Mishra,S. Jaiswal*

Main category: physics.plasm-ph

TL;DR: Study examines nonlinear waves generated by charged bodies moving through dusty plasma, described by forced KdV equation, showing source parameters (amplitude, width, speed) crucially shape wave evolution beyond just Mach number.


<details>
  <summary>Details</summary>
Motivation: To understand how charged bodies moving through dusty plasma generate diverse nonlinear waves (precursors, pinned solitons) and investigate how source characteristics influence wave formation and evolution.

Method: Theoretical analysis using forced Korteweg-de Vries (fKdV) equation under weakly nonlinear and dispersive limits, examining effects of three source parameters: amplitude, width, and flow speed.

Result: Found that nonlinear structure excitation depends not just on Mach number but also source features (amplitude, width). Discovered novel lagging structures that maintain shape and speed while propagating behind the source.

Conclusion: Source characteristics play crucial role in shaping nonlinear waves in dusty plasma, with theoretical discovery of lagging structures representing first depiction of such phenomena.

Abstract: The nonlinear waves excited by the movement of a charged body in the dusty plasma medium are studied. A charged body moving through a dusty plasma medium can generate diverse nonlinear waves, such as precursors and pinned solitons. These wave excitations under weakly nonlinear and dispersive limits are described theoretically by the forced Korteweg-de Vries (fKdV) type equation. We have examined the role of the driver in shaping and evolving these wave excitations. In particular we studied the effect of primarily three source parameters, namely, amplitude, width, and flow speed, on the evolution of nonlinear structures. The driver generates a perturbation in the stable system configuration, which couples with medium characteristics and eventually evolves into propagating excitations. Our finding shows that the excitation of nonlinear structure by a moving body in a plasma medium is not just dictated by the mach number but also the features of the source such as amplitude and width. As a novel finding apart from pinned and precursor solitons, we observe another nonlinear structure that lags behind the source term, maintaining its shape and speed as it propagates. These features are the first ever theoretical depiction of such lagging structures.

</details>


### [52] [Runaway electron avalanche and macroscopic beam formation: simulations of the DTT full power scenario](https://arxiv.org/abs/2512.24760)
*E. Emanuelli,F. Vannini,M. Hoelzl,E. Nardon,V. Bandaru,N. Schwarz,D. Bonfiglio,G. Ramogida,F. Subba,JOREK Team*

Main category: physics.plasm-ph

TL;DR: The DTT facility's transition from low-current (2 MA) to full-power (5.5 MA) operation dramatically increases runaway electron avalanche risk during disruptions, requiring careful impurity injection balancing between thermal load mitigation and RE avoidance.


<details>
  <summary>Details</summary>
Motivation: Previous studies showed safe margins against runaway electron formation in DTT's low-current commissioning phase, but the transition to full-power operation introduces critical changes in RE dynamics that need investigation for safe operation.

Method: Used JOREK non-linear MHD code for comprehensive 2D simulations of current quench phase during disruptions, systematically scanning initial RE seed currents and injected impurity levels across different disruption scenarios.

Result: In full-power scenario (5.5 MA), avalanche multiplication factor reaches ~1.3×10⁵, converting tiny 5.5 A seed currents into 0.7 MA RE beams with impurities. Higher seeds can produce 3.2 MA RE currents (80% of total plasma current).

Conclusion: Full-power DTT operation requires disruption mitigation strategy that carefully balances thermal load mitigation with RE avoidance through precisely controlled impurity injection, unlike the safer low-current commissioning phase.

Abstract: The transition of the Divertor Tokamak Test (DTT) facility from its initial commissioning phase (Day-0, plasma current $I_{p}=2$ MA) to the full power scenario ($I_{p}=5.5$ MA) introduces a critical shift in the dynamics of runaway electrons (REs) generation. While previous predictive studies of the low-current scenario indicated a robust safety margin against RE beam formation, this work reveals that the exponential scaling of the RE avalanche gain with plasma current severely narrows the safe operational window in the full power scenario. Using the non-linear magnetohydrodynamic code JOREK, we perform comprehensive 2D simulations of the current quench (CQ) phase of several disruption scenarios, systematically scanning initial RE seed currents and injected impurity levels. The results demonstrate that in the full power scenario, the avalanche multiplication factor is sufficiently high ($G_\text{av} \approx 1.3 \cdot 10^5$) to convert a mere 5.5 A seed current into macroscopic RE beams of $\approx 0.7$ MA when large amounts of impurities are present. For even higher RE seeds, the RE current can peak at $ \approx 3.2$ MA, constituting up to $\approx$ 80% of the total plasma current during the CQ. These findings suggest that, unlike the Day-0 phase, the disruption mitigation strategy for the full power scenario involves a careful balance between thermal load mitigation and RE avoidance, necessitating a well-chosen quantity of injected impurities. This work provides the baseline needed for future estimations of RE loads on the plasma-facing components of DTT, which will be essential for designing and positioning mitigation components like sacrificial limiters.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [53] [Assessing generative modeling approaches for free energy estimates in condensed matter](https://arxiv.org/abs/2512.23930)
*Maximilian Schebek,Jiajun He,Emil Hoffmann,Yuanqi Du,Frank Noé,Jutta Rogal*

Main category: cond-mat.stat-mech

TL;DR: Systematic review and benchmarking of generative-model-based methods for free energy estimation in condensed-matter systems, comparing normalizing flows and adaptive transport approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional free energy estimation methods require computationally expensive sampling of multiple intermediate states. Recent generative-model-based approaches bypass this need but lack systematic comparison of their trade-offs between efficiency, accuracy, and scalability.

Method: Systematically review generative-model-based methods and benchmark selected approaches including discrete/continuous normalizing flows in targeted free energy perturbation, FEAT with escorted Jarzynski equality, using coarse-grained monatomic ice and Lennard-Jones solids as benchmark systems.

Result: Evaluation of accuracy, data efficiency, computational cost, and scalability with system size provides quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.

Conclusion: The study provides systematic guidance for choosing appropriate free energy estimation methods based on specific requirements of condensed-phase molecular simulations, addressing the trade-offs between computational efficiency and accuracy.

Abstract: The accurate estimation of free energy differences between two states is a long-standing challenge in molecular simulations. Traditional approaches generally rely on sampling multiple intermediate states to ensure sufficient overlap in phase space and are, consequently, computationally expensive. Several generative-model-based methods have recently addressed this challenge by learning a direct bridge between distributions, bypassing the need for intermediate states. However, it remains unclear which approaches provide the best trade-off between efficiency, accuracy, and scalability. In this work, we systematically review these methods and benchmark selected approaches with a focus on condensed-matter systems. In particular, we investigate the performance of discrete and continuous normalizing flows in the context of targeted free energy perturbation as well as FEAT (Free energy Estimators with Adaptive Transport) together with the escorted Jarzynski equality, using coarse-grained monatomic ice and Lennard-Jones solids as benchmark systems. We evaluate accuracy, data efficiency, computational cost, and scalability with system size. Our results provide a quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [54] [Bridging Visual Intuition and Chemical Expertise: An Autonomous Analysis Framework for Nonadiabatic Dynamics Simulations via Mentor-Engineer-Student Collaboration](https://arxiv.org/abs/2512.24133)
*Yifei Zhu,Jiahui Zhang,Binni Huang,Zhenggang Lan*

Main category: physics.chem-ph

TL;DR: VisU is a vision-driven AI framework that uses large language models in a "Mentor-Engineer-Student" paradigm to autonomously analyze nonadiabatic molecular dynamics trajectories, reducing reliance on expert intuition and manual interpretation.


<details>
  <summary>Details</summary>
Motivation: Traditional analysis of nonadiabatic molecular dynamics trajectories heavily relies on expert intuition and visual pattern recognition, which is difficult to formalize and scale. There's a need for more systematic, automated approaches to analyze excited-state dynamics simulation results.

Method: VisU uses two state-of-the-art large language models in a "virtual research collective" with a "Mentor-Engineer-Student" paradigm. The Mentor provides physical intuition through visual reasoning, the Engineer adaptively constructs analysis scripts, and the Student executes pipelines and manages data. The framework autonomously orchestrates a four-stage workflow: Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary.

Result: VisU identifies reaction channels and key nuclear motions while generating professional academic reports. It bridges visual insight with chemical expertise, enabling more intuitive and scalable mechanistic discovery in excited-state dynamics analysis.

Conclusion: VisU establishes a new paradigm for human-AI collaboration in analyzing excited-state dynamics simulation results, significantly reducing dependence on manual interpretation and enabling systematic, scalable analysis of complex molecular dynamics data.

Abstract: Analyzing nonadiabatic molecular dynamics trajectories traditionally heavily relies on expert intuition and visual pattern recognition, a process that is difficult to formalize. We present VisU, a vision-driven framework that leverages the complementary strengths of two state-of-the-art large language models to establish a "virtual research collective." This collective operates through a "Mentor-Engineer-Student" paradigm that mimics the collaborative intelligence of a professional chemistry laboratory. Within this ecosystem, the Mentor provides physical intuition through visual reasoning, while the Engineer adaptively constructs analysis scripts, and the Student executes the pipeline and manages the data and results. VisU autonomously orchestrates a four-stage workflow comprising Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary. This systematic approach identifies reaction channels and key nuclear motions while generating professional academic reports. By bridging visual insight with chemical expertise, VisU establishes a new paradigm for human-AI collaboration in the analysis of excited-state dynamics simulation results, significantly reducing dependence on manual interpretation and enabling more intuitive, scalable mechanistic discovery.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [55] [Upscaling from ab initio atomistic simulations to electrode scale: The case of manganese hexacyanoferrate, a cathode material for Na-ion batteries](https://arxiv.org/abs/2512.24816)
*Yuan-Chi Yang,Eric Woillez,Quentin Jacquet,Ambroise van Roekeghem*

Main category: cond-mat.mtrl-sci

TL;DR: A multiscale computational framework bridges atomistic to device scales for predictive modeling of insertion-type electrode materials, demonstrated on sodium manganese hexacyanoferrate cathode for sodium-ion batteries.


<details>
  <summary>Details</summary>
Motivation: To enable rational computational design of next-generation insertion-type materials (like battery electrodes) by systematically translating atomistic insights into continuum-scale predictions, overcoming scale limitations in materials modeling.

Method: Active-learning strategy trains Moment Tensor Potential via iterative hybrid grand-canonical Monte Carlo-molecular dynamics sampling. The ML interatomic potential captures configuration spaces at all sodiation levels, then feeds computed parameters (diffusivities, interfacial/strain energies, free-energy landscapes) into pseudo-2D phase-field simulations for electrode-scale predictions.

Result: The framework accurately reproduces experimental properties (volume expansion, operating voltage, structural transformations) and reveals 4-order-of-magnitude difference in sodium diffusivity between rhombohedral (sodium-rich) and tetragonal (sodium-poor) phases at 300K. Successfully predicts phase-boundary propagation and rate-dependent performance across electrode scales.

Conclusion: The multiscale workflow establishes a blueprint for rational computational design of insertion-type materials, demonstrating systematic translation of atomistic insights into continuum-scale predictions for battery electrode materials and beyond.

Abstract: We present a generalizable scale-bridging computational framework that enables predictive modeling of insertion-type electrode materials from atomistic to device scales. Applied to sodium manganese hexacyanoferrate, a promising cathode material for grid-scale sodium-ion batteries, our methodology employs an active-learning strategy to train a Moment Tensor Potential through iterative hybrid grand-canonical Monte Carlo--molecular dynamics sampling, robustly capturing configuration spaces at all sodiation levels. The resulting machine learning interatomic potential accurately reproduces experimental properties including volume expansion, operating voltage, and sodium concentration-dependent structural transformations, while revealing a four-order-of-magnitude difference in sodium diffusivity between the rhombohedral (sodium-rich) and tetragonal (sodium-poor) phases at 300 K. We directly compute all critical parameters -- temperature- and concentration-dependent diffusivities, interfacial and strain energies, and complete free-energy landscapes -- to feed them into pseudo-2D phase-field simulations that predict phase-boundary propagation and rate-dependent performances across electrode length scales. This multiscale workflow establishes a blueprint for rational computational design of next-generation insertion-type materials, such as battery electrode materials, demonstrating how atomistic insights can be systematically translated into continuum-scale predictions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [56] [Stochastic Galerkin Method and Hierarchical Preconditioning for PDE-constrained Optimization](https://arxiv.org/abs/2512.23804)
*Zhendong Li,Akwum Onwunta,Bedřich Sousedík*

Main category: math.OC

TL;DR: Efficient hierarchical preconditioners for PDE-constrained optimal control with uncertain coefficients, using stochastic Galerkin and polynomial chaos expansions to accelerate iterative solvers.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of solving large-scale, ill-conditioned linear systems arising in uncertainty quantification for PDE-constrained optimal control problems with uncertain coefficients.

Method: Discretize-then-optimize framework combining finite elements, stochastic Galerkin approximation, and advanced time-discretization. Derives hierarchical preconditioners exploiting sparsity in generalized polynomial chaos expansions, using truncated stochastic expansions.

Result: Proposed preconditioners significantly accelerate convergence of iterative solvers compared to existing methods, providing robust and efficient solvers for both steady-state and time-dependent optimal control under uncertainty.

Conclusion: Hierarchical preconditioners based on truncated stochastic expansions effectively balance computational cost and preconditioning quality, enabling efficient solution of large-scale optimal control problems with uncertain parameters.

Abstract: We develop efficient hierarchical preconditioners for optimal control problems governed by partial differential equations with uncertain coefficients. Adopting a discretize-then-optimize framework that integrates finite element discretization, stochastic Galerkin approximation, and advanced time-discretization schemes, the approach addresses the challenge of large-scale, ill-conditioned linear systems arising in uncertainty quantification. By exploiting the sparsity inherent in generalized polynomial chaos expansions, we derive hierarchical preconditioners based on truncated stochastic expansion that strike an effective balance between computational cost and preconditioning quality. Numerical experiments demonstrate that the proposed preconditioners significantly accelerate the convergence of iterative solvers compared to existing methods, providing robust and efficient solvers for both steady-state and time-dependent optimal control applications under uncertainty.

</details>


### [57] [The Flow-Limit of Reflect-Reflect-Relax: Existence, Stability, and Discrete-Time Behavior](https://arxiv.org/abs/2512.23843)
*Manish Krishan Lal*

Main category: math.OC

TL;DR: The paper analyzes the Reflect-Reflect-Relax (RRR) algorithm's small-step regime, showing it forms a hyperbolic sink with exponential convergence, excludes chaotic behavior, and explains optimal relaxation parameters through flow discretization.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior and dynamics of the Reflect-Reflect-Relax (RRR) algorithm in its small-step regime, particularly how it relates to continuous flows and why certain relaxation parameters emerge as optimal.

Method: Analyzes RRR as a forward-Euler discretization of a continuous flow, uses Lyapunov function analysis with squared gap measure, studies Filippov sliding dynamics, and applies percolation/renormalization group heuristics for performance analysis near limits.

Result: Proves RRR forms a hyperbolic sink with exponential convergence, excludes recurrent/chaotic behavior in a basin, shows finite-time capture into solution domains, and explains emergence of iteration-optimal relaxation parameters through flow discretization.

Conclusion: Small-step RRR behaves like a discretized continuous flow with predictable convergence properties, optimal relaxation parameters emerge naturally from the flow structure, and performance near Douglas-Rachford limits can be understood through mesoscopic percolation frameworks.

Abstract: We study the Reflect-Reflect-Relax (RRR) algorithm in its small-step (flow-limit) regime. In the smooth transversal setting, we show that the transverse dynamics form a hyperbolic sink, yielding exponential decay of a natural gap measure. Under uniform geometric assumptions, we construct a tubular neighborhood of the feasible manifold on which the squared gap defines a strict Lyapunov function, excluding recurrent dynamics and chaotic behavior within this basin.
  In the discrete setting, the induced flow is piecewise constant on W-domains and supports Filippov sliding along convergent boundaries, leading to finite-time capture into a solution domain. We prove that small-step RRR is a forward-Euler discretization of this flow, so that solution times measured in rescaled units converge to a finite limit while iteration counts diverge, explaining the emergence of iteration-optimal relaxation parameters. Finally, we introduce a heuristic mesoscopic framework based on percolation and renormalization group to organize performance deterioration near the Douglas-Rachford limit.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [58] [Soliton profiles: Classical Numerical Schemes vs. Neural Network - Based Solvers](https://arxiv.org/abs/2512.24634)
*Chandler Haight,Svetlana Roudenko,Zhongming Wang*

Main category: nlin.PS

TL;DR: Comparative study shows classical numerical solvers outperform neural network methods for single-instance 1D solitary-wave computations, while operator-learning methods offer advantages for repeated simulations.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of classical numerical methods versus modern neural network approaches for computing ground states/profiles of solitary-wave solutions in 1D dispersive PDEs, assessing their relative strengths and limitations.

Method: Comparative analysis of three approaches: 1) Classical numerical solvers (Petviashvili's method, finite difference with Newton iterations), 2) Physics-informed neural networks (PINNs), and 3) Operator-learning methods, applied to nonlinear Schrödinger, nonlinear Klein-Gordon, and generalized KdV equations.

Result: Classical methods maintain high-order accuracy and computational efficiency for single-instance problems; PINNs produce qualitative solutions but are less accurate and efficient due to expensive training; operator-learning methods offer rapid inference after pretraining for repeated simulations but have lower accuracy for single instances.

Conclusion: Classical numerical solvers remain superior for single-instance 1D problems, while neural network methods (particularly operator-learning) show promise for applications requiring repeated simulations or real-time predictions despite current accuracy limitations.

Abstract: We present a comparative study of classical numerical solvers, such as Petviashvili's method or finite difference with Newton iterations, and neural network-based methods for computing ground states or profiles of solitary-wave solutions to the one-dimensional dispersive PDEs that include the nonlinear Schrödinger, the nonlinear Klein-Gordon and the generalized KdV equations. We confirm that classical approaches retain high-order accuracy and strong computational efficiency for single-instance problems in the one-dimensional setting. Physics-informed neural networks (PINNs) are also able to reproduce qualitative solutions but are generally less accurate and less efficient in low dimensions than classical solvers due to expensive training and slow convergence. We also investigate the operator-learning methods, which, although computationally intensive during training, can be reused across many parameter instances, providing rapid inference after pretraining, making them attractive for applications involving repeated simulations or real-time predictions. For single-instance computations, however, the accuracy of operator-learning methods remains lower than that of classical methods or PINNs, in general.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [59] [Numerical study of solitary waves in Dirac--Klein--Gordon system](https://arxiv.org/abs/2512.24954)
*Andrew Comech,Julien Ricaud,Marco Roque*

Main category: math-ph

TL;DR: Numerical construction of solitary waves in Dirac-Klein-Gordon systems, studying energy/charge dependence on frequency ω, using iterative methods and comparing with shooting for massless case.


<details>
  <summary>Details</summary>
Motivation: To understand the properties of solitary waves in Dirac-Klein-Gordon systems across different spatial dimensions (1D and 3D), particularly how energy and charge depend on frequency ω, and to develop reliable numerical methods for their construction.

Method: Uses iterative numerical procedure starting from nonlinear Dirac equation solitary waves, computes corresponding scalar field, adjusts coupling constant. Compares with shooting method for massless scalar field case. Employs virial identities to control simulation errors.

Result: Successfully constructs solitary waves in both 1D and 3D Dirac-Klein-Gordon systems, obtains dependence of energy and charge on frequency ω, validates numerical methods through virial identity error control, and establishes comparison between iterative and shooting methods for massless case.

Conclusion: The developed numerical methods provide reliable construction of solitary waves in Dirac-Klein-Gordon systems, with implications for understanding spectral stability of these waves based on the obtained energy/charge dependencies.

Abstract: We use numerics to construct solitary waves in Dirac--Klein--Gordon (in one and three spatial dimensions) and study the dependence of energy and charge on $ω$. For the construction, we use the iterative procedure, starting from solitary waves of nonlinear Dirac equation, computing the corresponding scalar field, and adjusting the coupling constant. We also consider the case of massless scalar field, when the iteration procedure could be compared with the shooting method. We use the virial identities to control the error of simulations. We also discuss possible implications from the obtained results for the spectral stability of solitary waves.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [60] [Polynomial mixing for the stochastic Schrödinger equation with large damping in the whole space](https://arxiv.org/abs/2512.24599)
*Hung D. Nguyen,Kihoon Seong*

Main category: math.PR

TL;DR: The paper establishes polynomial mixing rates for the stochastic nonlinear Schrödinger equation with large damping in dimensions d ≤ 3.


<details>
  <summary>Details</summary>
Motivation: While unique ergodicity is known for the stochastic nonlinear Schrödinger equation under strong damping, the rate of convergence to equilibrium has remained unknown. The paper aims to quantify this mixing rate.

Method: The approach uses a coupling strategy combined with pathwise Strichartz estimates to analyze the convergence toward the invariant measure.

Result: In the large damping regime, solutions are attracted to the unique invariant probability measure at polynomial rates of arbitrary order.

Conclusion: The work successfully quantifies the mixing behavior of the stochastic nonlinear Schrödinger equation, establishing polynomial convergence rates for the first time in this context.

Abstract: We study the long-time mixing behavior of the stochastic nonlinear Schrödinger equation in $\mathbb{R}^d$, $d\le 3$. It is well known that, under a sufficiently strong damping force, the system admits unique ergodicity, although the rate of convergence toward equilibrium has remained unknown. In this work, we address the mixing property in the regime of large damping and establish that solutions are attracted toward the unique invariant probability measure at polynomial rates of arbitrary order. Our approach is based on a coupling strategy with pathwise Strichartz estimates.

</details>


### [61] [Heat kernel estimates for Markov processes with jump kernels blowing-up at the boundary](https://arxiv.org/abs/2512.24807)
*Soobin Cho,Panki Kim,Renming Song,Zoran Vondraček*

Main category: math.PR

TL;DR: The paper establishes sharp two-sided heat kernel estimates for purely discontinuous symmetric Markov processes with jump kernels that blow up at the boundary, overcoming challenges from unbounded jump measure tails.


<details>
  <summary>Details</summary>
Motivation: To extend the framework for conservative self-similar Markov processes to a broader geometric setting where jump kernels can blow up at the boundary, covering important examples like traces of isotropic α-stable processes and processes related to nonlocal Neumann problems.

Method: Employ recently developed weighted functional inequalities specifically designed for jump kernels blowing up at the boundary, as standard techniques for jump processes with uniformly bounded jump measure tails are not applicable.

Result: Establish sharp two-sided heat kernel estimates for purely discontinuous symmetric Markov processes on closed subsets of ℝ^d with jump kernels of the form J(x,y)=|x-y|^{-d-α}ℬ(x,y), where ℬ(x,y) may blow up at the boundary.

Conclusion: The paper successfully extends heat kernel analysis to Markov processes with boundary-blowing jump kernels using weighted functional inequalities, covering important geometric settings and applications including traces of stable processes and nonlocal Neumann problems.

Abstract: In this paper, we study purely discontinuous symmetric Markov processes on closed subsets of ${\mathbb R}^d$, $d\ge 1$, with jump kernels of the form $J(x,y)=|x-y|^{-d-α}{\mathcal B}(x,y)$, $α\in (0,2)$, where the function ${\mathcal B}(x,y)$ may blow up at the boundary of the state space. This extends the framework developed recently for conservative self-similar Markov processes on the upper half-space to a broader geometric setting. Examples of Markov processes that fall into our general framework include traces of isotropic $α$-stable processes in $C^{1,\rm Dini}$ sets, processes in Lipschitz sets arising in connection with the nonlocal Neumann problem, and a large class of resurrected self-similar processes in the closed upper half-space.
  We establish sharp two-sided heat kernel estimates for these Markov processes. A fundamental difficulty in accomplishing this task is that, in contrast to the existing literature on heat kernels for jump processes, the tails of the associated jump measures in our setting are not uniformly bounded. Thus, standard techniques in the existing literature used to study heat kernels are not applicable. To overcome this obstacle, we employ recently developed weighted functional inequalities specifically designed for jump kernels blowing up at the boundary.

</details>


### [62] [Uniqueness for stochastic differential equations in Hilbert spaces with irregular drift](https://arxiv.org/abs/2512.25003)
*Lukas Anzeletti,Oleg Butkovsky,Máté Gerencsér,Alexander Shaposhnikov*

Main category: math.PR

TL;DR: The paper presents a framework for proving strong existence and uniqueness of SDEs in Hilbert spaces with irregular drift, extending previous work by removing structural assumptions on the drift.


<details>
  <summary>Details</summary>
Motivation: To establish strong existence and uniqueness results for stochastic differential equations in Hilbert spaces with irregular drift coefficients, going beyond the structural assumptions required in previous seminal work by Da Prato and Flandoli (2010).

Method: Develops a new technique combining Lê's theory of stochastic sewing in Hilbert spaces, Gaussian analysis, and a method of Lasry and Lions for approximation in Hilbert spaces, avoiding the use of infinite-dimensional Kolmogorov equations.

Result: Proves that the SDE has a unique strong solution provided that α > 2γ/(1+γ), where α is the Hölder exponent of the drift and γ is a parameter related to the noise covariance.

Conclusion: The paper substantially extends previous results by removing structural assumptions on the drift function, providing a more general framework for studying SDEs with irregular coefficients in Hilbert spaces.

Abstract: We present a versatile framework to study strong existence and uniqueness for stochastic differential equations (SDEs) in Hilbert spaces with irregular drift. We consider an SDE in a separable Hilbert space $H$ \begin{equation*} dX_t= (A X_t + b(X_t))dt +(-A)^{-γ/2}dW_t,\quad X_0=x_0 \in H, \end{equation*} where $A$ is a self-adjoint negative definite operator with purely atomic spectrum, $W$ is a cylindrical Wiener process, $b$ is $α$-Hölder continuous function $H\to H$, and a nonnegative parameter $γ$ such that the stochastic convolution takes values in $H$. We show that this equation has a unique strong solution provided that $α> 2γ/(1+γ)$. This substantially extends the seminal work of Da Prato and Flandoli (2010) as no structural assumption on $b$ is imposed. To obtain this result, we do not use infinite-dimensional Kolmogorov equations but instead develop a new technique combining Lê's theory of stochastic sewing in Hilbert spaces, Gaussian analysis, and a method of Lasry and Lions for approximation in Hilbert spaces.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [63] [Mathematical Theory for Photonic Hall Effect in Honeycomb Photonic Crystals](https://arxiv.org/abs/2512.24477)
*Wei Li,Junshan Lin,Jiayu Qiu,Hai Zhang*

Main category: physics.optics

TL;DR: The paper develops a mathematical theory for the photonic Hall effect, proving existence of guided electromagnetic waves at interfaces between two honeycomb photonic crystals, analogous to electronic edge states.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for the photonic Hall effect and understand how topological properties in photonic crystals can create guided interface waves, similar to edge states in electronic topological insulators.

Method: Start with symmetric honeycomb photonic crystals with Dirac points at K and K' points. Introduce two classes of perturbations that lift Dirac degeneracy, creating spectral band valleys with well-defined topological phases. Use layer potential techniques and spectral analysis to investigate guided waves at interfaces between two perturbed crystals.

Result: Proves existence of guided electromagnetic waves propagating along interfaces between two honeycomb photonic crystals. Shows these interface modes are induced by topological Hall effect and elucidate relationship between interface mode existence and nature of perturbations on the two periodic media.

Conclusion: Establishes mathematical theory for photonic Hall effect, demonstrating topological interface states in photonic systems analogous to electronic edge states, with guided waves dependent on perturbation-induced topological phases.

Abstract: In this work, we develop a mathematical theory for the photonic Hall effect and prove the existence of guided electromagnetic waves at the interface of two honeycomb photonic crystals. The guided wave resembles the edge states in electronic systems: it is induced by the topological Hall effect, and the wave propagates along the interface but not in the bulk media. Starting from a symmetric honeycomb photonic crystal that attains Dirac points at the high-symmetry points of the Brillouin zone, $K$ and $K'$, we introduce two classes of perturbations for the periodic medium. The perturbations lift the Dirac degeneracy, forming a spectral band valley at the points $K$ and $K'$ with well-defined topological phase that depends on the sign of the perturbation parameters. By employing the layer potential techniques and spectral analysis, we investigate the existence of guided wave along an interface when two honeycomb photonic crystals are glued together. In particular, we elucidate the relationship between the existence of the interface mode and the nature of perturbations imposed on the two periodic media separated by the interface.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [64] [Classification of ancient cylindrical mean curvature flows and the Mean Convex Neighborhood Conjecture](https://arxiv.org/abs/2512.24524)
*Richard H. Bamler,Yi Lai*

Main category: math.DG

TL;DR: The paper resolves the Mean Convex Neighborhood Conjecture for mean curvature flows, showing that near cylindrical singularities the flow is mean-convex with level set structure, and establishes a canonical neighborhood theorem.


<details>
  <summary>Details</summary>
Motivation: To prove the Mean Convex Neighborhood Conjecture for mean curvature flows in all dimensions, which concerns the local structure near cylindrical singularities and has been a long-standing open problem in geometric analysis.

Method: Complete classification of ancient, asymptotically cylindrical flows; refined asymptotic analysis with novel "leading mode condition"; "induction over thresholds" argument; independent approach from prior work.

Result: Resolved the conjecture: near multiplicity-one cylindrical singularities, flow is mean-convex with level set structure, all nearby tangent flows are cylindrical, established canonical neighborhood theorem with quantitative structural description.

Conclusion: The work provides a complete solution to the Mean Convex Neighborhood Conjecture, characterizes asymptotically cylindrical flows into three canonical families, and offers new proofs and parameterizations in mean curvature flow theory.

Abstract: We resolve the Mean Convex Neighborhood Conjecture for mean curvature flows in all dimensions and for all types of cylindrical singularities. Specifically, we show that if the tangent flow at a singular point is a multiplicity-one cylinder, then in a neighborhood of that point the flow is mean-convex, its time-slices arise as level sets of a continuous function, and all nearby tangent flows are cylindrical. Moreover, we establish a canonical neighborhood theorem near such points, which characterizes the flow via local models. We also obtain a more uniform version of the Mean Convex Neighborhood Conjecture, which only requires closeness to a cylinder at some initial time and yields a quantitative version of this structural description.
  Our proof relies on a complete classification of ancient, asymptotically cylindrical flows. We prove that any such flow is non-collapsed, convex, rotationally symmetric, and belongs to one of three canonical families: ancient ovals, the bowl soliton, or the flying wing translating solitons. Central to our method is a refined asymptotic analysis and a novel \emph{leading mode condition,} together with a new ``induction over thresholds'' argument. In addition, our approach provides a full parameterization of the space of asymptotically cylindrical flows and gives a new proof of the existence of flying wing solitons.
  Our method is independent of prior work and, together with our prequel paper, this work is largely self-contained.

</details>


### [65] [Isocapacitary constants for the $p$-Laplacian on compact manifolds](https://arxiv.org/abs/2512.24725)
*Lili Wang,Tao Wang*

Main category: math.DG

TL;DR: Introduces Steklov and Neumann isocapacitary constants for p-Laplacian on compact manifolds, providing two-sided bounds for (p,α)-Sobolev constants and bounds for first nontrivial eigenvalues when α=1.


<details>
  <summary>Details</summary>
Motivation: To develop isocapacitary constants that can provide quantitative bounds for Sobolev constants and eigenvalues of the p-Laplacian on compact manifolds, extending classical results to nonlinear operators.

Method: Introduces Steklov and Neumann isocapacitary constants specifically for the p-Laplacian operator on compact manifolds, relating them to (p,α)-Sobolev constants through two-sided bounds.

Result: The new isocapacitary constants yield two-sided bounds for (p,α)-Sobolev constants, and when α=1, these bounds degenerate to upper and lower bounds for the first nontrivial Steklov and Neumann eigenvalues of the p-Laplacian.

Conclusion: Steklov and Neumann isocapacitary constants provide effective tools for bounding Sobolev constants and eigenvalues of the p-Laplacian on compact manifolds, with applications to spectral geometry and nonlinear analysis.

Abstract: In this paper, we introduce Steklov and Neumann isocapacitary constants for the $p$-Laplacian on compact manifolds. These constants yield two-sided bounds for the $(p,α)$-Sobolev constants, which degenerate to upper and lower bounds for the first nontrivial Steklov and Neumann eigenvalues of the $p$-Laplacian when $α= 1$.

</details>


### [66] [A Liouville-Weierstrass correspondence for Spacelike and Timelike Minimal Surfaces in $\mathbb{L}^3$](https://arxiv.org/abs/2512.24908)
*Adriana A. Cintra,Iury Domingos,Irene I. Onnis*

Main category: math.DG

TL;DR: The paper establishes a correspondence between solutions of the Liouville equation and minimal surfaces (both spacelike and timelike) in Lorentz-Minkowski space, using complex/paracomplex analysis to unify both causal types.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between solutions of the Liouville equation and minimal surfaces in Lorentz-Minkowski space, providing a unified treatment for both spacelike and timelike surfaces using complex and paracomplex analysis.

Method: Using complex analysis for spacelike surfaces and paracomplex analysis for timelike surfaces, studying the action of pseudo-isometries via Möbius-type transformations, establishing correspondences with Lorentz group rotations, and deriving Gauss maps and Weierstrass data from Liouville equation solutions.

Result: Established a unified correspondence between Liouville equation solutions and minimal surfaces in L³, showed how pseudo-isometries act via Möbius transformations corresponding to Lorentz rotations, derived Gauss maps and Weierstrass data from local solutions, and provided explicit examples of both spacelike and timelike minimal surfaces.

Conclusion: The Liouville equation provides a fundamental link to minimal surfaces in Lorentz-Minkowski space, with complex/paracomplex analysis offering a unified framework for both causal types, enabling explicit construction of surfaces and revealing symmetries through Möbius transformations.

Abstract: We investigate a correspondence between solutions $λ(x,y)$ of the Liouville equation \[ Δλ= -\varepsilon e^{-4λ}, \] and the Weierstrass representations of spacelike ($\varepsilon = 1$) and timelike ($\varepsilon = -1$) minimal surfaces with diagonalizable Weingarten map in the three-dimensional Lorentz--Minkowski space $\mathbb{L}^3$. Using complex and paracomplex analysis, we provide a unified treatment of both causal types. We study the action of pseudo-isometries of $\mathbb{L}^3$ on minimal surfaces via Möbius-type transformations, establishing a correspondence between these transformations and rotations in the special orthochronous Lorentz group. Furthermore, we show how local solutions of the Liouville equation determine the Gauss map and the associated Weierstrass data. Finally, we present explicit examples of spacelike and timelike minimal surfaces in $\mathbb{L}^3$ arising from solutions of the Liouville equation.

</details>


### [67] [The PDE-ODI principle and cylindrical mean curvature flows](https://arxiv.org/abs/2512.25050)
*Richard H. Bamler,Yi Lai*

Main category: math.DG

TL;DR: A new PDE-ODI principle converts parabolic PDEs into ODE inequalities, enabling high-order asymptotic expansions for ancient MCF solutions and singularities modeled on cylinders, with applications to uniqueness results.


<details>
  <summary>Details</summary>
Motivation: To develop a more streamlined approach for analyzing ancient solutions and singularities of mean curvature flow that are locally modeled on cylinders, bypassing delicate analytic estimates used in previous work.

Method: Introduces the PDE-ODI principle that converts parabolic differential equations into systems of ordinary differential inequalities, enabling stronger asymptotic control and high-order expansions without using Łojasiewicz-Simon inequality.

Result: Establishes uniqueness of bowl soliton times Euclidean factor among ancient cylindrical flows with dominant linear mode; obtains complete asymptotic expansions for quadratic mode case; recovers classical results (uniqueness of tangent flows, rigidity of cylinders) with new proofs.

Conclusion: The PDE-ODI principle provides a powerful, self-contained framework for analyzing cylindrical mean curvature flow solutions, yielding stronger asymptotic control and unifying classical results while enabling future work on quadratic mode cases.

Abstract: We introduce a new approach for analyzing ancient solutions and singularities of mean curvature flow that are locally modeled on a cylinder. Its key ingredient is a general mechanism, called the \emph{PDE--ODI principle}, which converts a broad class of parabolic differential equations into systems of ordinary differential inequalities. This principle bypasses many delicate analytic estimates used in previous work, and yields asymptotic expansions to arbitrarily high order.
  As an application, we establish the uniqueness of the bowl soliton times a Euclidean factor among ancient, cylindrical flows with dominant linear mode. This extends previous results on this problem to the most general setting and is made possible by the stronger asymptotic control provided by our analysis. In the other case, when the quadratic mode dominates, we obtain a complete asymptotic expansion to arbitrary polynomial order, which will form the basis for a subsequent paper. Our framework also recovers and unifies several classical results. In particular, we give new proofs of the uniqueness of tangent flows (due to Colding-Minicozzi) and the rigidity of cylinders among shrinkers (due to Colding-Ilmanen-Minicozzi) by reducing both problems to a single ordinary differential inequality, without using the Łojasiewicz-Simon inequality.
  Our approach is independent of prior work and the paper is largely self-contained.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [NeuralCrop: Combining physics and machine learning for improved crop yield predictions](https://arxiv.org/abs/2512.20177)
*Yunan Lin,Sebastian Bathiany,Maha Badri,Maximilian Gelbrecht,Philipp Hess,Brian Groenke,Jens Heinke,Christoph Müller,Niklas Boers*

Main category: cs.LG

TL;DR: NeuralCrop is a hybrid crop model combining process-based GGCMs with machine learning, outperforming state-of-the-art models in yield prediction and robustness under climate change.


<details>
  <summary>Details</summary>
Motivation: Traditional GGCMs have substantial uncertainties due to limited process understanding, while pure machine learning models fail to generalize to changing climate conditions outside their training distributions. There's a need for models that combine process understanding with data-driven approaches for reliable climate impact assessments.

Method: NeuralCrop is a hybrid GGCM that integrates an advanced process-based GGCM with data-driven machine learning components. The model is first trained to emulate a competitive GGCM, then fine-tuned on observational data, combining explicit process representation with data-driven optimization.

Result: NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions (2000-2019). It accurately reproduces interannual yield anomalies in European wheat regions and US Corn Belt, with particularly strong improvements under drought extremes. It maintains robust projections under unseen conditions while pure ML models show substantial performance degradation.

Conclusion: The hybrid crop modeling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions, demonstrating the value of combining process-based and data-driven methods.

Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

</details>


### [69] [Generative forecasting with joint probability models](https://arxiv.org/abs/2512.24446)
*Patrick Wyrod,Ashesh Chattopadhyay,Daniele Venturi*

Main category: cs.LG

TL;DR: Joint generative forecasting learns probability distributions over lagged system states for chaotic dynamics, enabling improved predictions and uncertainty quantification without ground truth.


<details>
  <summary>Details</summary>
Motivation: Chaotic systems have fundamental forecasting limits due to sensitivity to initial conditions and multiscale processes. Existing generative models focus on next-step prediction rather than underlying dynamics structure.

Method: Reframe forecasting as generative problem by learning joint probability distribution of lagged system states over short windows, with forecasts via marginalization. Introduce model-agnostic training/inference framework with three uncertainty metrics: ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift.

Result: Joint generative models show improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behavior than conventional conditional next-step models on Lorenz-63 and Kuramoto-Sivashinsky systems.

Conclusion: Joint generative forecasting captures nonlinear temporal dependencies and enables robust uncertainty quantification, offering superior approach for chaotic dynamical systems compared to traditional next-step prediction methods.

Abstract: Chaotic dynamical systems exhibit strong sensitivity to initial conditions and often contain unresolved multiscale processes, making deterministic forecasting fundamentally limited. Generative models offer an appealing alternative by learning distributions over plausible system evolutions; yet, most existing approaches focus on next-step conditional prediction rather than the structure of the underlying dynamics. In this work, we reframe forecasting as a fully generative problem by learning the joint probability distribution of lagged system states over short temporal windows and obtaining forecasts through marginalization. This new perspective allows the model to capture nonlinear temporal dependencies, represent multistep trajectory segments, and produce next-step predictions consistent with the learned joint distribution. We also introduce a general, model-agnostic training and inference framework for joint generative forecasting and show how it enables assessment of forecast robustness and reliability using three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift), without access to ground truth. We evaluate the performance of the proposed method on two canonical chaotic dynamical systems, the Lorenz-63 system and the Kuramoto-Sivashinsky equation, and show that joint generative models yield improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behaviour than conventional conditional next-step models.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [70] [Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features](https://arxiv.org/abs/2512.24440)
*Theodore MacMillan,Nicholas T. Ouellette*

Main category: physics.ao-ph

TL;DR: Researchers adapt LLM interpretability tools to analyze GraphCast's internal representations, discovering interpretable weather features like tropical cyclones and atmospheric rivers, and demonstrate feature manipulation for physically consistent hurricane modifications.


<details>
  <summary>Details</summary>
Motivation: While data-driven physics models like GraphCast achieve high accuracy, their internal computations are largely unknown and it's unclear whether their representations are interpretable or physically consistent, creating a "black box" problem for scientific trustworthiness.

Method: Adapt interpretability tools from Large Language Models to GraphCast, using sparse autoencoders to discover interpretable features in the model's neuron space, and perform interventions on prediction steps to probe feature abstractions.

Result: Discovered distinct interpretable features corresponding to various weather phenomena (tropical cyclones, atmospheric rivers, diurnal/seasonal behavior, precipitation patterns, geographical coding, sea-ice extent) and demonstrated that sparse modifications to tropical cyclone features produce interpretable, physically consistent hurricane modifications.

Conclusion: These interpretability methods provide insight into data-driven physics models' black-box behavior and represent progress toward making them trustworthy predictors and scientifically valuable discovery tools.

Abstract: Large data-driven physics models like DeepMind's weather model GraphCast have empirically succeeded in parameterizing time operators for complex dynamical systems with an accuracy reaching or in some cases exceeding that of traditional physics-based solvers. Unfortunately, how these data-driven models perform computations is largely unknown and whether their internal representations are interpretable or physically consistent is an open question. Here, we adapt tools from interpretability research in Large Language Models to analyze intermediate computational layers in GraphCast, leveraging sparse autoencoders to discover interpretable features in the neuron space of the model. We uncover distinct features on a wide range of length and time scales that correspond to tropical cyclones, atmospheric rivers, diurnal and seasonal behavior, large-scale precipitation patterns, specific geographical coding, and sea-ice extent, among others. We further demonstrate how the precise abstraction of these features can be probed via interventions on the prediction steps of the model. As a case study, we sparsely modify a feature corresponding to tropical cyclones in GraphCast and observe interpretable and physically consistent modifications to evolving hurricanes. Such methods offer a window into the black-box behavior of data-driven physics models and are a step towards realizing their potential as trustworthy predictors and scientifically valuable tools for discovery.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [71] [A positive eigenvalue result for semilinear differential equations in Banach spaces with functional initial conditions](https://arxiv.org/abs/2512.23876)
*Gennaro Infante,Paola Rubbioni*

Main category: math.CA

TL;DR: Existence of positive eigenvalues with nonnegative eigenfunctions for abstract initial value problems with functional/nonlocal initial conditions in Banach spaces.


<details>
  <summary>Details</summary>
Motivation: To establish existence results for positive eigenvalues with associated nonnegative eigenfunctions in abstract initial value problems, particularly those with functional or nonlocal initial conditions (periodic, multipoint, integral average conditions) that arise in various applied models.

Method: Uses nonlinear analysis, topological methods, and theory of strongly continuous semigroups to develop abstract framework applicable to wide range of models.

Result: Develops abstract theory for existence of positive eigenvalues with nonnegative mild eigenfunctions, with application to reaction-diffusion equation with nonlocal initial condition from heat flow problem.

Conclusion: Provides general framework for studying eigenvalue problems with nonlocal initial conditions, demonstrating applicability through concrete reaction-diffusion example.

Abstract: We study the existence of positive eigenvalues with associated nonnegative mild eigenfunctions for a class of abstract initial value problems in Banach spaces with functional, possibly nonlocal, initial conditions. The framework includes periodic, multipoint, and integral average conditions. Our approach relies on nonlinear analysis, topological methods, and the theory of strongly continuous semigroups, yielding results applicable to a wide range of models. As an illustration, we apply the abstract theory to a reaction-diffusion equation with a nonlocal initial condition arising from a heat flow problem.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [72] [Achieving High Efficiency And Enhanced Beam Quality In Laser Wakefield Acceleration](https://arxiv.org/abs/2512.24719)
*Jia Wang,Ming Zeng,Dazhang Li,Wentao Wang,Song Li,Ke Feng,Jie Gao*

Main category: physics.acc-ph

TL;DR: Shorter laser pulses enable two-step dechirping for high-charge electron beams with 1% energy spread and 10-30% energy transfer efficiency in laser wakefield acceleration.


<details>
  <summary>Details</summary>
Motivation: Laser wakefield acceleration offers compact, cost-effective particle acceleration with extremely high gradients (>100 GV/m), but faces challenges in improving energy transfer efficiency while maintaining beam quality suitable for practical applications like particle colliders and light sources.

Method: Using shorter laser pulse duration enables a two-step dechirping process for accelerated electron beams with nanocoulomb-level charge. The approach works across a large parameter space.

Result: Achieved electron beams with 1% energy spread and 10-30% energy transfer efficiency. Demonstrated example: 420 MeV electron beam with 5.5 nC charge and 2% RMS energy spread using 8.3 J laser pulse with 7.2 fs duration.

Conclusion: Shorter laser pulses facilitate effective dechirping and enable high-quality electron beams with improved energy transfer efficiency, advancing laser wakefield acceleration toward practical applications.

Abstract: Laser wakefield acceleration, characterized by the extremely high electric field gradient exceeding 100GV/m, is regarded as a compact and cost affordable technology for the next generation of particle colliders and light sources. However, it has always been a major challenge to effectively increase the energy transfer efficiency from the laser to the accelerated beam, while ensuring the beam quality remains suitable for practical applications. This study demonstrates that the laser with shorter pulse duration allows for a two-step dechirping process of the accelerated electron beam with charge of nanocoulomb level. The electron beams with an energy spread of 1% can be generated with the energy transfer efficiency of 10% to 30% in a large parameter space. For example, one electron beam with the energy of 420MeV, the charge of 5.5nC and the RMS energy spread of 2% can be produced using an 8.3J laser pulse with 7.2fs duration.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [73] [Green's function on the Tate curve](https://arxiv.org/abs/2512.24935)
*An Huang,Rebecca Rohrlich,Yaojia Sun,Eric Whyman*

Main category: math.NT

TL;DR: Defined a Laplacian operator and Green's function on the Tate curve for p-adic string worldsheet action in genus one.


<details>
  <summary>Details</summary>
Motivation: To define a p-adic string worldsheet action in genus one (elliptic curves/torus), which requires understanding differential operators and Green's functions in the p-adic setting.

Method: Defined a Laplacian operator on the Tate curve (p-adic elliptic curve) and studied its Green's function properties.

Result: Proved existence of the Green's function and provided an explicit formula, showing it's a non-Archimedean counterpart to the Archimedean Green's function on a flat torus.

Conclusion: Successfully constructed the mathematical framework needed for p-adic string theory on genus one surfaces by establishing the Laplacian and Green's function on Tate curves.

Abstract: Motivated by the question of defining a $p$-adic string worldsheet action in genus one, we define a Laplacian operator on the Tate curve, and study its Green's function. We show that the Green's function exists. We provide an explicit formula for the Green's function, which turns out to be a non-Archimedean counterpart of the Archimedean Green's function on a flat torus.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [74] [Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis](https://arxiv.org/abs/2512.24999)
*Seunghoon Paik,Kangjie Zhou,Matus Telgarsky,Ryan J. Tibshirani*

Main category: math.ST

TL;DR: The paper introduces "basic inequalities" as a unified framework connecting implicit and explicit regularization in first-order optimization algorithms, enabling statistical analysis of training dynamics and prediction risk.


<details>
  <summary>Details</summary>
Motivation: To develop a simple, versatile framework that connects implicit and explicit regularization in optimization algorithms, providing a unified tool for statistical analysis that can handle various algorithms beyond just gradient descent.

Method: Introduces "basic inequalities" that bound the objective function difference f(θ_T)-f(z) in terms of accumulated step sizes and distances between initial point, current iterate, and reference point. This translates iteration count into effective regularization coefficients.

Result: Provides new theoretical results for mirror descent with Bregman divergence projection, generalized linear models trained by gradient descent and exponentiated gradient descent, and randomized predictors. Also refines known results on gradient descent.

Conclusion: The basic inequalities framework successfully connects implicit and explicit regularization, offering a versatile tool for analyzing optimization algorithms' statistical properties, with experimental validation on generalized linear models.

Abstract: We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Given a first-order iterative algorithm initialized at $θ_0$ with current iterate $θ_T$, the basic inequality upper bounds $f(θ_T)-f(z)$ for any reference point $z$ in terms of the accumulated step sizes and the distances between $θ_0$, $θ_T$, and $z$. The bound translates the number of iterations into an effective regularization coefficient in the loss function. We demonstrate this framework through analyses of training dynamics and prediction risk bounds. In addition to revisiting and refining known results on gradient descent, we provide new results for mirror descent with Bregman divergence projection, for generalized linear models trained by gradient descent and exponentiated gradient descent, and for randomized predictors. We illustrate and supplement these theoretical findings with experiments on generalized linear models.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [75] [Computational Analysis of Disease Progression in Pediatric Pulmonary Arterial Hypertension](https://arxiv.org/abs/2512.25027)
*Omar Said,Christopher Tossas-Betancourt,Mary K. Olive,Jimmy C. Lu,Adam Dorfman,C. Alberto Figueroa*

Main category: physics.med-ph

TL;DR: Researchers developed patient-specific computational models for pediatric pulmonary arterial hypertension using longitudinal MRI/catheterization data, creating a framework that couples 3D pulmonary artery models with heart/circulation models to simulate hemodynamics and track disease progression.


<details>
  <summary>Details</summary>
Motivation: Pediatric PAH is understudied due to limited data and lack of targeted diagnostic/therapeutic strategies. There's a need for better tools to understand disease progression and inform treatment decisions for children with this progressive cardiopulmonary disease.

Method: Developed multi-scale patient-specific cardiovascular models for 4 pediatric PAH patients using longitudinal MRI and catheterization data collected ~2 years apart. Used CRIMSON framework to couple 3D fluid-structure interaction models of pulmonary arteries with 0D lumped-parameter heart and Windkessel models. Created automated Python-based optimizer to calibrate boundary conditions by minimizing discrepancies between simulated and clinical metrics.

Result: Successfully reduced calibration time from weeks to days. Model-derived metrics (arterial stiffness, pulse wave velocity, resistance, compliance) aligned with clinical indicators of disease severity and progression. Demonstrated computational modeling can non-invasively capture patient-specific hemodynamic adaptation over time.

Conclusion: Computational modeling offers a promising non-invasive tool for monitoring pediatric PAH progression and informing future treatment strategies, potentially addressing current limitations in pediatric PAH research and clinical management.

Abstract: Pulmonary arterial hypertension (PAH) is a progressive cardiopulmonary disease that leads to increased pulmonary pressures, vascular remodeling, and eventual right ventricular (RV) failure. Pediatric PAH remains understudied due to limited data and the lack of targeted diagnostic and therapeutic strategies. In this study, we developed and calibrated multi-scale, patient-specific cardiovascular models for four pediatric PAH patients using longitudinal MRI and catheterization data collected approximately two years apart. Using the CRIMSON simulation framework, we coupled three-dimensional fluid-structure interaction (FSI) models of the pulmonary arteries with zero-dimensional (0D) lumped-parameter heart and Windkessel models to simulate patient hemodynamics. An automated Python-based optimizer was developed to calibrate boundary conditions by minimizing discrepancies between simulated and clinical metrics, reducing calibration time from weeks to days. Model-derived metrics such as arterial stiffness, pulse wave velocity, resistance, and compliance were found to align with clinical indicators of disease severity and progression. Our findings demonstrate that computational modeling can non-invasively capture patient-specific hemodynamic adaptation over time, offering a promising tool for monitoring pediatric PAH and informing future treatment strategies.

</details>


### [76] [Finite element analysis of very large bone models based on micro-CT scans](https://arxiv.org/abs/2512.24401)
*Shani Martinez-Weissberg,Will Pazner,Zohar Yosibash*

Main category: physics.med-ph

TL;DR: Open-source μFE framework enables large-scale biomechanical analysis of rabbit femur with experimental validation, showing 40μm resolution balances accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: High-resolution μFE models from μCT imaging are computationally challenging at anatomically relevant scales, requiring scalable solutions for detailed bone mechanics analysis.

Method: Integrated open-source framework using MIA clustering for segmentation, MFEM library for solving voxel-based μFE meshes, with experimental validation via Digital Image Correlation on rabbit femur under compression.

Result: Successfully solved models with over 800M DOFs using moderate HPC; 40μm resolution preserves accuracy while reducing cost; segmentation parameters affect mechanical response; calibrated effective bone material properties.

Conclusion: Large-scale experimentally informed μFE modeling is feasible with open-source tools, providing robust foundation for preclinical bone mechanics assessment and treatment risk evaluation.

Abstract: High-resolution voxel-based micro-finite element ($μ$FE) models derived from $μ$CT imaging enable detailed investigation of bone mechanics but remain computationally challenging at anatomically relevant scales. This study presents a comprehensive $μ$FE framework for large-scale biomechanical analysis of an intact New Zealand White (NZW) rabbit femur, integrating advanced segmentation, scalable finite element solvers, and experimental validation using predominantly open-source libraries. Bone geometries were segmented from $μ$CT data using the MIA clustering algorithm and converted into voxel-based $μ$FE meshes, which were solved using the open-source MFEM library with algorithms designed for large-scale linear elasticity systems.
  The numerical solutions were verified by comparing with a commercial finite element solver, and by evaluating the performance of full assembly and element-by-element formulations within MFEM. Models containing over $8\times10^{8}$ DOFs were solved using moderate HPC resources, demonstrating the feasibility of anatomically realistic $μ$FE simulations at this scale. Resolution effects were investigated by comparing models with voxel sizes of 20, 40, and 80 $μ$m, revealing that 40 $μ$m preserves boundary displacement and principal strain distributions with minimal bias while significantly reducing computational cost. Sensitivity analyses further showed that segmentation parameters influence the global mechanical response.
  Finally, $μ$FE predictions were coupled with Digital Image Correlation measurements on an NZW rabbit femur under compression to calibrate effective bone material properties at the micron scale. The results demonstrate that large-scale, experimentally informed $μ$FE modeling can be achieved using open-source tools, providing a robust foundation for preclinical assessment of bone mechanics and treatment-related risks.

</details>
