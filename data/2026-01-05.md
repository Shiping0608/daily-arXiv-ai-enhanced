<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 9]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [nucl-th](#nucl-th) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Finite element exterior calculus for time-dependent Hamiltonian partial differential equations](https://arxiv.org/abs/2601.00103)
*Ari Stern,Enrico Zampa*

Main category: math.NA

TL;DR: Combines finite element exterior calculus (FEEC) for spatial discretization with symplectic integrators for time discretization to create structure-preserving methods for Hamiltonian PDEs that satisfy local multisymplectic conservation laws.


<details>
  <summary>Details</summary>
Motivation: Extend the success of symplectic integrators for Hamiltonian ODEs to Hamiltonian PDEs by developing structure-preserving numerical methods that capture finer Hamiltonian structure information than global function space approaches.

Method: Combine finite element exterior calculus (FEEC) for spatial semidiscretization with symplectic integrators for time discretization, with particular attention to conforming FEEC methods and hybridizable discontinuous Galerkin (HDG) methods.

Result: Constructed a large class of methods that satisfy a local multisymplectic conservation law in space and time, which generalizes the symplectic conservation law of Hamiltonian ODEs.

Conclusion: The proposed approach provides structure-preserving numerical methods for Hamiltonian PDEs that capture local Hamiltonian structure information, demonstrated through application to the semilinear Hodge wave equation.

Abstract: The success of symplectic integrators for Hamiltonian ODEs has led to a decades-long program of research seeking analogously structure-preserving numerical methods for Hamiltonian PDEs. In this paper, we construct a large class of such methods by combining finite element exterior calculus (FEEC) for spatial semidiscretization with symplectic integrators for time discretization. The resulting methods satisfy a local multisymplectic conservation law in space and time, which generalizes the symplectic conservation law of Hamiltonian ODEs, and which carries finer information about Hamiltonian structure than other approaches based on global function spaces. We give particular attention to conforming FEEC methods and hybridizable discontinuous Galerkin (HDG) methods. The theory and methods are illustrated by application to the semilinear Hodge wave equation.

</details>


### [2] [Affine Invariant Langevin Dynamics for rare-event sampling](https://arxiv.org/abs/2601.00107)
*Deepyaman Chakraborty,Ruben Harris,Rupert Klein,Guillermo Olicón-Méndez,Sebastian Reich,Claudia Schillings*

Main category: math.NA

TL;DR: ALDI framework for rare event estimation in nonlinear dynamical systems using smooth approximation of nonsmooth limit-state functions and affine invariant Langevin dynamics.


<details>
  <summary>Details</summary>
Motivation: Need efficient estimation of rare events in nonlinear dynamical systems, which are challenging due to nonsmooth limit-state functions and local anisotropy in posterior distributions.

Method: Formulate rare events as Bayesian inverse problems, use smooth approximation of nonsmooth limit-state functions, sample posterior with affine invariant Langevin dynamics (ALDI) - a derivative-free interacting particle system that adapts to local anisotropy.

Result: ALDI successfully concentrates near relevant near-critical sets, provides accurate proposal distributions for importance sampling across low-dimensional algebraic/dynamical problems and point-vortex atmospheric blocking model.

Conclusion: ALDI is a computationally robust, potentially gradient-free tool well-suited for rare-event estimation in unstable regimes of dynamical systems with strong geometric anisotropy.

Abstract: We introduce an affine invariant Langevin dynamics (ALDI) framework for the efficient estimation of rare events in nonlinear dynamical systems. Rare events are formulated as Bayesian inverse problems through a nonsmooth limit-state function whose zero level set characterises the event of interest. To overcome the nondifferentiability of this function, we propose a smooth approximation that preserves the failure set and yields a posterior distribution satisfying the small-noise limit. The resulting potential is sampled by ALDI, a (derivative-free) interacting particle system whose affine invariance allows it to adapt to the local anisotropy of the posterior.
  We demonstrate the performance of the method across a hierarchy of benchmarks, namely two low-dimensional examples (an algebraic problem with convex geometry and a dynamical problem of saddle-type instability) and a point-vortex model for atmospheric blockings. In all cases, ALDI concentrates near the relevant near-critical sets and provides accurate proposal distributions for self-normalised importance sampling. The framework is computationally robust, potentially gradient-free, and well-suited for complex forward models with strong geometric anisotropy. These results highlight ALDI as a promising tool for rare-event estimation in unstable regimes of dynamical systems.

</details>


### [3] [Fast Ewald Summation with Prolates for Charged Systems in the NPT Ensemble](https://arxiv.org/abs/2601.00161)
*Jiuyang Liang,Libin Lu,Shidong Jiang*

Main category: math.NA

TL;DR: NPT extension of ESP method for accurate pressure/stress evaluation in molecular dynamics simulations with improved Fourier grid efficiency and production implementations in LAMMPS/GROMACS.


<details>
  <summary>Details</summary>
Motivation: To develop a thermodynamically consistent pressure/stress evaluation method for periodic charged systems in the isothermal-isobaric ensemble that improves upon current mesh-Ewald methods in terms of accuracy and computational efficiency.

Method: Extends ESP framework using prolate spheroidal wave functions as splitting/spreading kernels, derives unified pressure-tensor formulation for various cell types, and implements long-range pressure evaluation with single forward FFT plus diagonal scaling.

Result: Demonstrates reduced Fourier grid size requirements for prescribed pressure accuracy compared to Gaussian/B-spline methods, validates accuracy on bulk water, ionic liquids, and transmembrane systems, and shows strong scaling on up to 3000 CPU cores with reduced communication costs.

Conclusion: ESP-NPT provides a spectrally accurate, scalable particle-mesh method for pressure evaluation in molecular dynamics with production-ready implementations in major MD packages, offering improved efficiency for NPT simulations.

Abstract: We present an NPT extension of Ewald summation with prolates (ESP), a spectrally accurate and scalable particle-mesh method for molecular dynamics simulations of periodic, charged systems. Building on the recently introduced ESP framework, this work focuses on rigorous and thermodynamically consistent pressure/stress evaluation in the isothermal--isobaric ensemble. ESP employs prolate spheroidal wave functions as both splitting and spreading kernels, reducing the Fourier grid size needed to reach a prescribed pressure accuracy compared with current widely used mesh-Ewald methods based on Gaussian splitting and B-spline spreading. We derive a unified pressure-tensor formulation applicable to isotropic, semi-isotropic, anisotropic, and fully flexible cells, and show that the long-range pressure can be evaluated with a single forward FFT followed by diagonal scaling, whereas force evaluation requires both forward and inverse transforms. We provide production implementations in LAMMPS and GROMACS and validate pressure and force accuracy on bulk water, LiTFSI ionic liquids, and a transmembrane system. Benchmarks on up to $3\times 10^3$ CPU cores demonstrate strong scaling and reduced communication cost at matched accuracy, particularly for NPT pressure evaluation.

</details>


### [4] [Temporal Two-Grid Compact Difference Scheme for Benjamin-Bona-Mahony-Burgers Equation](https://arxiv.org/abs/2601.00193)
*Lisen Ding,Xiangyi Peng,Dongling Wang*

Main category: math.NA

TL;DR: A temporal two-grid compact difference scheme for solving BBMB equations that reduces computational cost while maintaining high-order accuracy through coarse-fine time grid decomposition.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving the Benjamin-Bona-Mahony-Burgers (BBMB) equation that reduces computational cost while preserving high accuracy, addressing the need for practical numerical solutions to this nonlinear partial differential equation.

Method: Three-step temporal two-grid compact difference scheme: 1) Solve nonlinear system on coarse time grid (τ_c), 2) Use linear Lagrange interpolation to get coarse approximation on fine time grid (τ_f), 3) Solve linearized scheme on fine grid for corrected solution.

Result: The method achieves convergence of order O(τ_c² + τ_f² + h⁴) in maximum norm, reduces computational cost without sacrificing accuracy, and demonstrates conservation property, unique solvability, convergence, and stability through rigorous proof using energy method.

Conclusion: The TTCD scheme provides an effective and feasible strategy for solving BBMB equations with high accuracy and reduced computational cost, validated by both theoretical analysis and numerical experiments.

Abstract: This paper proposes a temporal two-grid compact difference (TTCD) scheme for solving the Benjamin-Bona-Mahony-Burgers (BBMB) equation with initial and periodic boundary conditions. The method consists of three main steps: first, solving a nonlinear system on a coarse time grid of size $τ_c$; then obtaining a coarse approximation on the fine time grid of size $τ_f$ via linear Lagrange interpolation; and finally solving a linearized scheme on the fine grid to obtain the corrected solution. The TTCD scheme reduces computational cost without sacrificing accuracy. Moreover, using the energy method, we rigorously prove the conservation property, unique solvability, convergence, and stability of the proposed scheme. It is shown that the method achieves convergence of order $\mathcal{O}(τ_c^2 + τ_f^2 + h^4)$ in the maximum norm, where $h$ is space step size. Finally, some numerical experiments are provided to demonstrate the effectiveness and feasibility of the proposed strategy.

</details>


### [5] [Spectral Schur analysis of structured moment matrices for quadratic histopolation](https://arxiv.org/abs/2601.00301)
*Allal Guessab,Federico Nudo*

Main category: math.NA

TL;DR: Study of parameter-dependent moment matrices from quadratic histopolation on simplicial meshes, focusing on stability analysis and optimization of reconstruction systems.


<details>
  <summary>Details</summary>
Motivation: To improve stability and conditioning of quadratic reconstruction systems on simplicial meshes by analyzing the spectral properties of structured moment matrices arising from weighted quadratic histopolation.

Method: Construct compatible face densities and orthogonal decomposition of quadratic polynomial space into face/interior components. Identify reduced Schur complement that characterizes stability. Formulate eigenvalue optimization problem using degrees of freedom, density, and scaling parameters as design variables.

Result: Developed spectral criteria for invertibility of local moment systems, identified spectrally preferable basis choices, and demonstrated improved stability, conditioning, and convergence through 3D experiments on uniform/quasi-uniform meshes.

Conclusion: The matrix-based spectral analysis provides effective tools for optimizing quadratic reconstruction systems, leading to improved numerical stability and conditioning while maintaining convergence properties.

Abstract: In this paper we study parameter-dependent structured moment matrices with a canonical block form arising from weighted quadratic histopolation on simplicial meshes. For a strictly positive density on a simplex, we construct compatible face densities and an orthogonal decomposition of the quadratic polynomial space into face and interior components, which induces a natural face-interior block structure. A reduced Schur complement is identified that fully characterizes enrichment and well-posedness and provides a sharp spectral stability result. We show that this quantity coincides with the square root of the smallest eigenvalue of a low-dimensional symmetric positive definite operator. This matrix-based viewpoint yields simple spectral criteria for the invertibility of local moment systems and motivates spectrally preferable choices of face and interior bases with improved conditioning. Using the resulting degrees of freedom together with density and scaling parameters as design variables, we formulate a small eigenvalue optimization problem aimed at improving stability and reducing the condition number of the global reconstruction system. Three-dimensional experiments on uniform and quasi-uniform simplicial meshes illustrate the predicted stability, conditioning, and convergence behaviour of the enriched quadratic reconstruction.

</details>


### [6] [A weak Galerkin least squares finite element method for linear convection equations in non-divergence form](https://arxiv.org/abs/2601.00399)
*Chunmei Wang,Shangyou Zhang*

Main category: math.NA

TL;DR: A weak Galerkin least-squares finite element method for first-order linear convection equations in non-divergence form, using discontinuous functions without coercivity requirements, yielding symmetric positive definite systems on general meshes with optimal error estimates.


<details>
  <summary>Details</summary>
Motivation: To develop a robust finite element method for first-order linear convection equations that doesn't require coercivity assumptions on convection vector or reaction coefficient, works on general polygonal/polyhedral meshes, and produces symmetric positive definite systems.

Method: Weak Galerkin least-squares (WG-LS) finite element method using discontinuous finite element functions for first-order linear convection equations in non-divergence form. The method formulates the problem without coercivity requirements and produces symmetric positive definite linear systems.

Result: Optimal-order error estimates established for WG-LS approximation in a suitable energy norm under minimal regularity assumptions. Numerical experiments confirm theoretical convergence results and demonstrate accuracy and efficiency of the method.

Conclusion: The WG-LS method provides an effective approach for solving first-order linear convection equations with advantages including no coercivity requirements, symmetric positive definite systems, applicability to general meshes, and proven optimal convergence rates.

Abstract: This article develops a weak Galerkin least-squares (WG--LS) finite element method for first-order linear convection equations in non-divergence form. The method is formulated using discontinuous finite element functions and does not require any coercivity assumption on the convection vector or reaction coefficient. The resulting discrete problem leads to a symmetric and positive definite linear system and is applicable to general polygonal and polyhedral meshes. Under minimal regularity assumptions on the coefficients, optimal-order error estimates are established for the WG--LS approximation in a suitable energy norm. Numerical experiments are presented to confirm the theoretical convergence results and to demonstrate the accuracy and efficiency of the proposed method.

</details>


### [7] [Guaranteed stability bounds for second-order PDE problems satisfying a Garding inequality](https://arxiv.org/abs/2601.00404)
*T. Chaumont-Frelet*

Main category: math.NA

TL;DR: Algorithm to numerically determine well-posedness of second-order linear PDEs with Gårding inequality and provide lower bound for inf-sup constant for a posteriori error estimation.


<details>
  <summary>Details</summary>
Motivation: Need computational method to verify well-posedness of PDE problems and obtain quantitative stability constants for reliable error estimation in numerical solutions.

Method: Algorithm based on two discrete singular value problems using Lagrange finite element discretization coupled with flux reconstruction techniques for a posteriori error estimation.

Result: The algorithm provides numerical lower bound for inf-sup constant; with sufficiently rich discretization, it underestimates optimal constant by factor roughly equal to two.

Conclusion: Proposed method enables practical verification of PDE well-posedness and provides reliable stability constants for error estimation in computational PDE analysis.

Abstract: We propose an algorithm to numerically determined whether a second-order linear PDE problem satisfying a Garding inequality is well-posed. This algorithm further provides a lower bound to the inf-sup constant of the weak formulation, which may in turn be used for a posteriori error estimation purposes. Our numerical lower bound is based on two discrete singular value problems involving a Lagrange finite element discretization coupled with an a posteriori error estimator based on flux reconstruction techniques. We show that if the finite element discretization is sufficiently rich, our lower bound underestimates the optimal constant only by a factor roughly equal to two.

</details>


### [8] [Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs](https://arxiv.org/abs/2601.00672)
*Seungchan Ko,Jiyeon Kim,Dongwook Shin*

Main category: math.NA

TL;DR: Proposes a sparse network architecture for FEONet to improve computational efficiency for large-scale parametric PDE problems while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: FEONet is effective for parametric PDEs but suffers from increasing computational cost and potential accuracy deterioration as the number of finite elements grows, making it challenging for large-scale problems.

Method: Introduces a new sparse network architecture motivated by the structure of finite elements to reduce computational complexity while maintaining approximation capabilities.

Result: Extensive numerical experiments show the sparse network achieves substantial improvements in computational cost and efficiency while maintaining comparable accuracy to the original FEONet.

Conclusion: The proposed sparse FEONet architecture successfully addresses scalability issues while preserving accuracy, with theoretical guarantees for approximation capability and training stability.

Abstract: In this paper, we study the finite element operator network (FEONet), an operator-learning method for parametric problems, originally introduced in J. Y. Lee, S. Ko, and Y. Hong, Finite Element Operator Network for Solving Elliptic-Type Parametric PDEs, SIAM J. Sci. Comput., 47(2), C501-C528, 2025. FEONet realizes the parameter-to-solution map on a finite element space and admits a training procedure that does not require training data, while exhibiting high accuracy and robustness across a broad class of problems. However, its computational cost increases and accuracy may deteriorate as the number of elements grows, posing notable challenges for large-scale problems. In this paper, we propose a new sparse network architecture motivated by the structure of the finite elements to address this issue. Throughout extensive numerical experiments, we show that the proposed sparse network achieves substantial improvements in computational cost and efficiency while maintaining comparable accuracy. We also establish theoretical results demonstrating that the sparse architecture can approximate the target operator effectively and provide a stability analysis ensuring reliable training and prediction.

</details>


### [9] [A Unified Trace-Optimization Framework for Multidimensionality Reduction](https://arxiv.org/abs/2601.00729)
*Mohamed El Guide,Alaa El Ichi,Khalide Jbilou,Lothar Reichel,Hessah Alqahtani*

Main category: math.NA

TL;DR: This paper provides a unified framework for multidimensional dimensionality reduction methods including MPCA, MONPP, MLLE, and MLE, with both linear and kernel-based extensions for handling nonlinear relationships.


<details>
  <summary>Details</summary>
Motivation: To establish a comprehensive understanding and unified framework for various multidimensional dimensionality reduction techniques, enabling better comparison and selection of appropriate methods for different applications.

Method: Formulates dimensionality reduction as trace optimization problems (maximization/minimization), presents linear methods (MPCA, MONPP) and their kernel-based extensions, and provides comparative analysis of theoretical foundations, assumptions, and computational efficiency.

Result: A unified framework for multidimensional reduction methods with both linear and nonlinear (kernel-based) approaches, along with comparative insights into their theoretical properties and practical applicability.

Conclusion: The study provides guidelines for selecting appropriate dimensionality reduction techniques based on theoretical foundations, computational efficiency, and practical application requirements.

Abstract: This paper presents a comprehensive overview of several multidimensional reduction methods focusing on Multidimensional Principal Component Analysis (MPCA), Multilinear Orthogonal Neighborhood Preserving Projection (MONPP), Multidimensional Locally Linear Embedding (MLLE), and Multidimensional Laplacian Eigenmaps (MLE). These techniques are formulated within a unified framework based on trace optimization, where the dimensionality reduction problem is expressed as maximization or minimization problems. In addition to the linear MPCA and MONPP approaches, kernel-based extensions of these methods also are presented. The latter methods make it possible to capture nonlinear relations between high-dimensional data. A comparative analysis highlights the theoretical foundations, assumptions, and computational efficiency of each method, as well as their practical applicability. The study provides insights and guidelines for selecting an appropriate dimensionality reduction technique suited to the application at hand.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [10] [Stability of time-periodic solutions to the Navier-Stokes-Fourier system](https://arxiv.org/abs/2601.00034)
*Naoto Deguchi*

Main category: math.AP

TL;DR: Existence and stability of time-periodic solutions to Navier-Stokes-Fourier system in 3D whole space with small periodic forcing.


<details>
  <summary>Details</summary>
Motivation: To establish the existence and stability of time-periodic solutions to the Navier-Stokes-Fourier system when subjected to small time-periodic external forces, which is important for understanding forced fluid dynamics with heat transfer.

Method: Mathematical analysis of the Navier-Stokes-Fourier system in three-dimensional whole space, proving existence of time-periodic solutions when external forcing is sufficiently small, and deriving time decay estimates for perturbations around these solutions.

Result: Proved existence and stability of time-periodic solutions, and derived time decay estimates for perturbations when initial perturbations are small and belong to L^p spaces for 1 ≤ p ≤ 2.

Conclusion: The Navier-Stokes-Fourier system admits stable time-periodic solutions under small periodic forcing, with perturbations decaying over time under appropriate initial conditions.

Abstract: We prove the existence and stability of a time-periodic solution to the Navier-Stokes-Fourier system in the three-dimensional whole space when a time-periodic external force is sufficiently small. The time decay estimate of the perturbation around the time-periodic solution is derived under the assumption that an initial perturbation is small and belongs to $L^p$ for some $1\leq p\leq 2$.

</details>


### [11] [Existence and (in)stability of standing waves for the nonlinear Schrödinger Equations on looping-edge graphs with $δ'$-type interactions](https://arxiv.org/abs/2601.00158)
*Jaime Angulo Pava,Alexander Muñoz*

Main category: math.AP

TL;DR: Existence and orbital stability analysis of standing-wave solutions for cubic NLS on looping-edge graphs with δ'-type vertex conditions.


<details>
  <summary>Details</summary>
Motivation: Study nonlinear wave propagation on quantum graphs, specifically looping-edge graphs with nontrivial vertex conditions, to understand existence and stability of standing-wave solutions.

Method: Use Implicit Function Theorem to prove existence of standing-wave profiles; apply perturbation theory and Kreĭn-von Neumann extension theory for stability analysis.

Result: Established existence of families of standing-wave solutions with dnoidal profiles on circular component and soliton tails on half-lines; analyzed orbital (in)stability.

Conclusion: Developed analytical framework for studying NLS on looping graphs with δ'-type conditions; approach generalizable to other bound states on non-compact metric graphs.

Abstract: In this work, we investigate the existence and orbital (in)stability of several branches of standing--wave solutions for the cubic nonlinear Schrödinger equation (NLS) posed on a looping--edge graph $\mathcal{G}$, consisting of a circle and a finite number $N$ of infinite half--lines attached to a common vertex. The model is endowed with $δ'$--type interaction boundary conditions at the vertex, which enforce continuity of the derivatives of the wave functions, while continuity of the wave function itself is not required. By means of the Implicit Function Theorem, we establish the existence of families of standing--wave profiles that converge, on the circular component of the graph, to Jacobi elliptic solutions of dnoidal type, coupled with soliton--type tail profiles on the half--lines. Tools from perturbation theory and Kreĭn--von Neumann extension theory for symmetric operators play a central role in the (in)stability analysis of such standing wave solutions. Our approach may be extended to other bound states for the NLS on looping graphs or more general non--compact metric graphs.

</details>


### [12] [Harnack Inequality for Nonlinear Equations Driven by the Normalized Infinity-Laplacian](https://arxiv.org/abs/2601.00177)
*Ahmed Mohammed,Carson Pocock*

Main category: math.AP

TL;DR: Harnack inequality for non-negative viscosity solutions of normalized infinity Laplacian PDE with nonlinear absorption and gradient terms


<details>
  <summary>Details</summary>
Motivation: To establish Harnack inequality for solutions of normalized infinity Laplacian equations with additional nonlinear terms (absorption and gradient-dependent terms), which extends classical results to more general PDEs

Method: Analyze the PDE Δ_∞^N u = f(u) + g(u)|Du|^q where 0 ≤ q ≤ 1, with f and g being non-decreasing continuous functions satisfying growth conditions at infinity; study non-negative viscosity solutions

Result: Established Harnack inequality for non-negative viscosity solutions of the given PDE under specified conditions on f, g, and q

Conclusion: Successfully proved Harnack inequality for normalized infinity Laplacian with nonlinear absorption and gradient terms, extending classical theory to more general equations with additional nonlinear structure

Abstract: This paper aims to investigate a Harnack inequality for non-negative solutions of the normalized infinity Laplacian with nonlinear absorption and gradient terms. More specifically, we establish a Harnack inequality for non-negative viscosity solutions of the PDE $Δ_\infty^Nu=f(u)+g(u)|Du|^q$, where $0\le q\le 1$, and for a large class of non-decreasing continuous functions $f$ and $g$ that meet suitable growth conditions at infinity.

</details>


### [13] [Uniqueness of the maximal solution of the supercooled Stefan problem in 1D](https://arxiv.org/abs/2601.00234)
*Kai Hong Chau,Young-Heon Kim,Mathav Murugan*

Main category: math.AP

TL;DR: The paper proves uniqueness of maximal weak solutions to the 1D supercooled Stefan problem by showing optimal transport solutions are cost-independent, and establishes stability properties despite lacking monotonicity and Lipschitz continuity.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness results for the supercooled Stefan problem in one dimension, which is important for understanding phase transition phenomena in materials science and mathematical physics, particularly for problems involving melting/solidification with supercooling effects.

Method: The authors use optimal transport theory, specifically analyzing the corresponding free target optimal transport problem. They prove that in 1D, the optimal solution is independent of the choice of cost function, which leads to uniqueness of maximal weak solutions.

Result: 1) Uniqueness of maximal weak solutions to the supercooled Stefan problem in 1D. 2) The optimal transport solution is cost-independent in 1D. 3) The problem lacks monotonicity and L¹-Lipschitz stability (unlike similar problems). 4) In 1D, it has stability in the weak convergence of measures.

Conclusion: The supercooled Stefan problem in one dimension has unique maximal weak solutions due to cost-independence of optimal transport solutions, and while it lacks certain stability properties available in similar problems, it maintains stability under weak convergence of measures in 1D.

Abstract: We prove uniqueness of the maximal weak solutions to the supercooled Stefan problem in 1 dimension. This follows by showing that in 1 dimension, the optimal solution of the corresponding free target optimal transport problem given in \cite{GeneralDimensions}, is independent of the choice of the cost function. Moreover, we show that the supercooled Stefan problem lacks monotonicity and $L^1$-Lipschitz stability, which are available in a similar problem considered in a previous paper \cite{freetarget}. However, in $1$ dimension, it has stability in the weak convergence of measures.

</details>


### [14] [Sharp nonuniqueness for the forced 2D Navier-Stokes and dissipative SQG equations](https://arxiv.org/abs/2601.00331)
*Francisco Mengual,Marcos Solera*

Main category: math.AP

TL;DR: The paper proves sharp nonuniqueness results for forced generalized SQG equations, showing nonunique solutions below established regularity classes.


<details>
  <summary>Details</summary>
Motivation: To establish sharp nonuniqueness results for forced generalized SQG equations, particularly addressing whether solutions in certain regularity classes are unique or not.

Method: The authors prove a sharp nonuniqueness result through mathematical analysis, likely using constructive counterexamples or analytical techniques to demonstrate nonunique solutions below established regularity thresholds.

Result: Two main results: 1) Nonunique $\dot{H}^s$-energy solutions below the Miura-Ju class, showing Resnick and Marchand's dissipative SQG solutions are not necessarily unique; 2) Nonuniqueness below Ladyzhenskaya-Prodi-Serrin class for 2D Navier-Stokes, and below Constantin-Wu and Dong-Chen-Zhao-Liu classes for dissipative SQG.

Conclusion: The paper establishes sharp nonuniqueness thresholds for various fluid equations, demonstrating that previously constructed solutions may not be unique and identifying precise regularity classes where uniqueness fails.

Abstract: We prove a sharp nonuniqueness result for the forced generalized SQG equation. First, this yields nonunique $\dot{H}^s$- energy solutions below the Miura-Ju class. In particular, this shows that the solutions constructed by Resnick and Marchand for the dissipative SQG equation are not necessarily unique. Second, this establishes nonuniqueness below the Ladyzhenskaya-Prodi-Serrin class for the 2D Navier-Stokes equation, as well as below the Constantin-Wu and Dong-Chen-Zhao-Liu classes for the dissipative SQG equation.

</details>


### [15] [A Deep Learning-Enhanced Fourier Method for the Multi-Frequency Inverse Source Problem with Sparse Far-Field Data](https://arxiv.org/abs/2601.00427)
*Hao Chen,Yan Chang,Yukun Guo,Yuliang Wang*

Main category: math.AP

TL;DR: Hybrid Fourier-CNN framework for multi-frequency inverse source problems using physics-informed Fourier approximation as U-Net input, with noise transfer learning for robustness.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multi-frequency inverse source problems with sparse and noisy far-field data, where traditional methods struggle with artifacts and limited resolution.

Method: Integrates classical Fourier method (physics-informed low-frequency approximation) with U-Net CNN. Uses high-to-low noise transfer learning: pre-train on high-noise data for global features, then fine-tune on lower-noise data.

Result: Achieves accurate reconstructions with noise levels up to 100%, significantly outperforms traditional spectral methods under sparse measurements, and generalizes well to unseen source geometries.

Conclusion: The hybrid framework effectively combines physics-based methods with deep learning to overcome limitations of sparse noisy data, providing robust and accurate source reconstruction with good generalization.

Abstract: This paper introduces a hybrid computational framework for the multi-frequency inverse source problem governed by the Helmholtz equation. By integrating a classical Fourier method with a deep convolutional neural network, we address the challenges inherent in sparse and noisy far-field data. The Fourier method provides a physics-informed, low-frequency approximation of the source, which serves as the input to a U-Net. The network is trained to map this coarse approximation to a high-fidelity source reconstruction, effectively suppressing truncation artifacts and recovering fine-scale geometric details. To enhance computational efficiency and robustness, we propose a high-to-low noise transfer learning strategy: a model pre-trained on high-noise regimes captures global topological features, offering a robust initialization for fine-tuning on lower-noise data. Numerical experiments demonstrate that the framework achieves accurate reconstructions with noise levels up to 100%, significantly outperforms traditional spectral methods under sparse measurement constraints, and generalizes well to unseen source geometries.

</details>


### [16] [Global compactness results for fractional $p$-Laplace Hardy Sobolev operator on a bounded domain](https://arxiv.org/abs/2601.00589)
*Nirjan Biswas*

Main category: math.AP

TL;DR: Global compactness result for fractional p-Laplace Hardy-Sobolev problems with critical exponent


<details>
  <summary>Details</summary>
Motivation: To establish Struwe-type global compactness for nonlinear critical Hardy-Sobolev problems with fractional p-Laplace operators, addressing compactness issues in critical exponent problems

Method: Develop Struwe-type global compactness framework for fractional p-Laplace Hardy-Sobolev operators with critical exponent, likely using concentration-compactness principles and variational methods

Result: Established global compactness result analogous to Struwe's classical result, applicable to fractional p-Laplace Hardy-Sobolev critical problems

Conclusion: Provides compactness framework for studying existence and multiplicity of solutions to critical Hardy-Sobolev problems with fractional p-Laplace operators

Abstract: In this paper, we establish a Struwe type global compactness result for a class of nonlinear critical Hardy-Sobolev exponent problems driven by the fractional $p$-Laplace Hardy-Sobolev operator.

</details>


### [17] [Limiting Behavior of Non-Autonomous Stochastic Reversible Selkov Lattice Systems Driven by Locally Lipschitz Lévy Noises](https://arxiv.org/abs/2601.00600)
*Guofu Li,Jianxin Wu,Yunshun Wu*

Main category: math.AP

TL;DR: The paper studies long-term behavior of reversible Selkov lattice systems on ℤ driven by Lévy noises, establishing existence of unique pullback measure attractors and their periodicity under periodic forcing.


<details>
  <summary>Details</summary>
Motivation: To understand the distributional dynamics of infinite-dimensional stochastic lattice systems with polynomial nonlinear couplings and Lévy noise, overcoming challenges from lack of compactness and bidirectional energy dissipation.

Method: Define continuous non-autonomous dynamical system on probability measure space using dual-Lipschitz distance, prove global well-posedness, establish pullback measure attractors via measure-valued solutions, use uniform tail-end estimates to handle compactness issues.

Result: Existence of unique pullback measure attractor characterized by measure-valued complete solutions; periodic attractors under periodic forcing; upper semicontinuity results for parameter limits; overcome infinite-dimensional compactness challenges.

Conclusion: The reversible Selkov lattice system with Lévy noise admits well-defined long-term distributional behavior through pullback measure attractors, with structural properties preserved under periodic forcing and parameter limits.

Abstract: This work investigates the long-term distributional behavior of the reversible Selkov lattice systems defined on the set $\mathbb{Z}$ and driven by locally Lipschitz \emph{Lévy noises}, which possess two pairs of oppositely signed nonlinear terms and whose nonlinear couplings can grow polynomially with any order $p \geq 1$. Firstly, based on the global-in-time well-posedness in $L^{2}(Ω, \ell^2 \times \ell^2)$, we define a \emph{continuous} non-autonomous dynamical system (NDS) on the metric space $(\mathcal{P}_{2}(\ell^2 \times \ell^2), d_{\mathcal{P}(\ell^2 \times \ell^2)})$, where $d_{\mathcal{P}(\ell^2 \times \ell^2)}$ is the dual-Lipschitz distance on $\mathcal{P}(\ell^2 \times \ell^2)$, the space of probability measures on $\ell^2 \times \ell^2$. Specifically, we establish that this non-autonomous dynamical system admits a unique pullback measure attractor, characterized via measure-valued complete solutions and orbits in the sense of Wang (DOI.org/10.1016/j.jde.2012.05.015). Moreover, when the deterministic external forcing terms are periodic in time, we demonstrate that the pullback measure attractors are also periodic. We also study the upper semicontinuity of pullback measure attractors as $(ε_1, ε_2, γ_1, γ_2) \rightarrow (0, 0, 0, 0)$. The main difficulty in proving the pullback asymptotic compactness of the NDS in $(\mathcal{P}_{2}(\ell^2 \times \ell^2), d_{\mathcal{P}(\ell^2 \times \ell^2)})$ is caused by the lack of compactness in infinite-dimensional lattice systems, which is overcome by using uniform tail-ends estimates. And the inherent structure of the Selkov system precludes the possibility of any unidirectional dissipative influence arising from the interaction between the two coupled equations, thereby obstructing the emergence of a dominant energy-dissipation mechanism along a single directional pathway.

</details>


### [18] [Global Dynamics and Stabilization of Zero-Mode Singularities in Multi-Scale Reaction-Diffusion Systems via Negative Coupling](https://arxiv.org/abs/2601.00638)
*Pengyue Hou*

Main category: math.AP

TL;DR: The paper establishes a rigorous mathematical framework for Multi-Scale Negative Coupled Systems (MNCS), proving global well-posedness, existence of compact global attractors, and deriving explicit fractal dimension estimates that scale with negative coupling strength.


<details>
  <summary>Details</summary>
Motivation: To address the stabilization of reaction-diffusion systems with hierarchical state spaces and directed, sign-structured interactions, particularly overcoming the "zero-mode singularity" problem where the Laplacian operator has a trivial zero eigenvalue that provides no linear dissipation for spatial means.

Method: Develops mathematical framework for MNCS on bounded domains with Neumann boundary conditions; uses Moser-Alikakos iteration technique to establish uniform L^∞ bounds; applies Kaplan-Yorke trace formula for fractal dimension estimates; validates with stiff-stable Second-Order Exponential Time Differencing (ETD2) scheme with Discrete Cosine Transform (DCT).

Result: Proves global well-posedness and existence of compact global attractor in phase space; establishes uniform L^∞ bounds overcoming Sobolev embedding limitations; derives explicit fractal dimension scaling as d_F(A) ∼ max{0, K_A - γ}^{d/2}, showing negative coupling strength γ acts as global regularizer compressing phase space.

Conclusion: The negative coupling strength in MNCS systems serves as an effective global regularizer that compresses the phase space and stabilizes the dynamics, with rigorous mathematical proofs and numerical validation confirming the theoretical framework.

Abstract: This paper establishes a rigorous mathematical framework for the Multi-Scale Negative Coupled System (MNCS), a dynamical model describing hierarchical state spaces with directed, sign-structured interactions. We address the stabilization of reaction-diffusion systems on bounded domains $Ω\subset \mathbb{R}^d$ ($d \le 3$) subject to homogeneous Neumann boundary conditions. A critical feature of this setting is the "zero-mode singularity," where the Laplacian operator possesses a trivial zero eigenvalue ($λ_0=0$), providing no linear dissipation for the spatial mean. We rigorously prove the global well-posedness of the system and the existence of a compact global attractor $\mathcal{A}$ in the phase space $\mathbb{H}=(L^2(Ω))^N$. Utilizing the Moser-Alikakos iteration technique, we establish uniform $L^\infty(Ω)$ bounds, overcoming the lack of Sobolev embedding from $H^1$ into $L^\infty$ in three dimensions. These bounds enable the derivation of explicit upper estimates for the fractal dimension of the attractor via the Kaplan-Yorke trace formula. We show that the dimension scales as $d_F(\mathcal{A}) \sim \max\{0, \mathcal{K}_{\mathcal{A}}-γ\}^{d/2}$, confirming that the negative coupling strength $γ$ acts as a global regularizer that compresses the phase space. The theoretical results are validated using a stiff-stable Second-Order Exponential Time Differencing (ETD2) scheme with Discrete Cosine Transform (DCT) to strictly enforce no-flux boundary conditions.

</details>


### [19] [Lipschitz Stability for an Inverse Problem of Biharmonic Wave Equations with Damping](https://arxiv.org/abs/2601.00648)
*Minghui Bi,Yixian Gao*

Main category: math.AP

TL;DR: Lipschitz stability for recovering density coefficient and initial displacement in damped biharmonic wave equation using boundary Cauchy data of Laplacian of solution.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide rigorous theoretical basis for applications in non-destructive testing and dynamic inversion by establishing stability for parameter identification in damped biharmonic wave equations.

Method: First proves the system operator generates a contraction semigroup for well-posedness. Then derives key observability inequality via multiplier techniques. Finally obtains explicit stability estimates for the inverse problem.

Result: Establishes Lipschitz stability for simultaneous recovery of variable density coefficient and initial displacement. Shows biharmonic structure enhances stability with explicit dependence on damping coefficient via factor (1 + γ)^{1/2}.

Conclusion: The work provides rigorous theoretical foundation showing biharmonic structure improves parameter identification stability, with practical applications in non-destructive testing and dynamic inversion.

Abstract: This paper establishes Lipschitz stability for the simultaneous recovery of a variable density coefficient and the initial displacement in a damped biharmonic wave equation. The data consist of the boundary Cauchy data for the Laplacian of the solution, \(Δu |_{\partial Ω}\) and \( \partial_{n}(Δu)|_{\partial Ω}.\) We first prove that the associated system operator generates a contraction semigroup, which ensures the well-posedness of the forward problem. A key observability inequality is then derived via multiplier techniques. Building on this foundation, explicit stability estimates for the inverse problem are obtained. These estimates demonstrate that the biharmonic structure inherently enhances the stability of parameter identification, with the stability constants exhibiting an explicit dependence on the damping coefficient via the factor \( (1 + γ)^{1/2} \). This work provides a rigorous theoretical basis for applications in non-destructive testing and dynamic inversion.

</details>


### [20] [Global regularity estimates for $p(x)$-Laplacian variational inequalities with singular or degenerate matrix-valued weights](https://arxiv.org/abs/2601.00652)
*Minh-Phuong Tran,Duc-Quang Bui,Thanh-Nhan Nguyen*

Main category: math.AP

TL;DR: Global gradient bounds for weak solutions to elliptic variational inequalities with two-sided obstructions and p(x)-Laplacian operators with degenerate/singular matrix weights.


<details>
  <summary>Details</summary>
Motivation: To investigate how the structure of elliptic variational inequalities with two-sided obstructions affects the integrability properties of solutions, particularly when dealing with p(x)-Laplacian operators involving degenerate or singular matrix weights.

Method: Develop regularity theory using weighted Calderón-Zygmund-type and general weighted Orlicz-type estimates. Employ a constructive level-set approach that minimizes dependence of scaling parameters on structural constants.

Result: Establish global gradient bounds under optimal regularity assumptions on matrix weights, geometric domain flatness, and prescribed data. The estimates are sharp, enabling construction of level-set estimates with nearly optimal scaling parameters.

Conclusion: The paper provides sharp regularity results for elliptic variational inequalities with two-sided obstructions, demonstrating that through careful level-set analysis, optimal gradient bounds can be achieved with minimal parameter dependence on structural constants.

Abstract: We establish the global gradient bounds for weak solutions to the elliptic variational inequality with two-sided obstructions, associated with a $p(x)$-Laplacian type operator involving degenerate or singular matrix weights. Under the optimal regularity assumptions on the matrix-valued weight, suitable geometric flatness of the domain, and the prescribed data, we aim to investigate the effects of the problem structure on the level of integrability properties of solutions. To this end, we develop regularity in two regards: weighted Calderón-Zygmund-type and general weighted Orlicz-type estimates. A notable feature of our results is that, through a constructive level-set approach, the estimates can be derived with minimal dependence of the scaling parameter on the structural constants. The regularity results are then sharp in the sense that they enable the construction of a level-set estimate with nearly optimal scaling parameters, within admissible parameter sets.

</details>


### [21] [Error bounds for Physics Informed Neural Networks in Generalized KdV Equations placed on unbounded domains](https://arxiv.org/abs/2601.00779)
*Ricardo Freire,Claudio Muñoz,Nicolás Valenzuela*

Main category: math.AP

TL;DR: Deep neural network approximation of generalized Korteweg-de Vries (gKdV) equations on unbounded real line using Physics Informed Neural Networks (PINNs) with rigorous error bounds for subcritical and critical nonlinearities.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of numerically approximating gKdV models on the unbounded real line using deep learning techniques. The difficulty arises from the intricate oscillatory estimates required for gKdV analysis (from Kato, Bourgain, Kenig, Ponce, Vega) and the need to adapt these theoretical frameworks to the deep learning setting.

Method: Uses Physics Informed Neural Networks (PINNs) combined with Kenig-Ponce-Vega suitable norms to create an approximation scheme. The approach adapts classical oscillatory estimates from gKdV theory to the deep learning context, providing a rigorous framework for neural network approximation.

Result: Proves rigorous bounds on the approximation error for both critical and subcritical gKdV models. Demonstrates clear approximation results for various gKdV nonlinear patterns including solitons, multi-solitons, breathers, and other solutions.

Conclusion: The paper establishes a rigorous framework for deep neural network approximation of gKdV equations on unbounded domains, successfully bridging classical PDE analysis with modern machine learning techniques through PINNs and adapted functional norms.

Abstract: In this paper we study a rigorous setting for the numerical approximation via deep neural networks of the generalized Korteweg-de Vries (gKdV) model in one dimension, for subcritical and critical nonlinearities, and assuming that the domain is the unbounded real line. The fact that the model is posed on the real line makes the problem difficult from the point of view of learning techniques, since the setting required to model gKdV is structured on intricate oscillatory estimates dating from Kato, Bourgain and Kenig, Ponce and Vega, among others. Therefore, a first task is to adapt the setting of these techniques to the deep learning setting. We shall use a battery of Kenig-Ponce-Vega suitable norms and Physics Informed Neural Networks (PINNs) to describe this approximative scheme, proving rigorous bounds on the approximation for each critical and subcritical gKdV model. We shall use this results to provide clear approximation results in the case of several gKdV nonlinear patterns such as solitons, multi-solitons, breathers, among other solutions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [22] [New RVE concept in thermoelasticity of periodic composites subjected to compact support loading](https://arxiv.org/abs/2601.00018)
*V. A. Buryachenko*

Main category: physics.comp-ph

TL;DR: A new computational micromechanics framework using additive general integral equations for thermoelastic composites with periodic microstructures, featuring data-driven RVEs based on loading scale and ML integration.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional RVE approaches in micromechanics, including boundary effects, finite-size dependencies, and inability to handle localized loading conditions like laser heating.

Method: Develops an exact Additive General Integral Equation (AGIE) for compact loading, establishes general integral equations for arbitrary mechanical/thermal loading, creates unified iterative solver, and introduces generalized RVE concept based on loading scale rather than fixed geometry.

Result: A framework that reduces infinite periodic medium analysis to finite data-driven domains, automatically filters nonrepresentative parameters, eliminates boundary effects, and integrates with ML/NN architectures for physics-informed surrogate operators.

Conclusion: The AGIE-based CAM framework provides a novel approach to micromechanics that overcomes traditional RVE limitations, enables accurate analysis of localized loading, and facilitates machine learning integration for efficient computational modeling.

Abstract: This paper introduces an advanced Computational Analytical Micromechanics (CAM) framework for linear thermoelastic composites (CMs) with periodic microstructures. The approach is based on an exact new Additive General Integral Equation (AGIE), formulated for compactly supported loading conditions, such as body forces and localized thermal effects (for example laser heating). In addition, new general integral equations (GIEs) are established for arbitrary mechanical and thermal loading. A unified iterative scheme is developed for solving the static AGIEs, where the compact support of loading serves as a new fundamental training parameter. At the core of the methodology lies a generalized Representative Volume Element (RVE) concept that extends Hill classical definition of the RVE. Unlike conventional RVEs, this generalized RVE is not fixed geometrically but emerges naturally from the characteristic scale of localized loading, thereby reducing the analysis of an infinite periodic medium to a finite, data-driven domain. This formulation automatically filters out nonrepresentative subsets of effective parameters while eliminating boundary effects, edge artifacts, and finite-size sample dependencies. Furthermore, the AGIE-based CAM framework integrates seamlessly with machine learning (ML) and neural network (NN) architectures, supporting the development of accurate, physics-informed surrogate nonlocal operators.

</details>


### [23] [Additive general integral equations in thermoelastic micromechanics of composites](https://arxiv.org/abs/2601.00019)
*Valeriy A. Buryachenko*

Main category: physics.comp-ph

TL;DR: Enhanced computational micromechanics framework for thermoelastic composites with random microstructure using additive integral equations and generalized RVE concept that emerges from loading scale, enabling machine learning integration.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective framework for analyzing linear thermoelastic composite materials with random microstructure, addressing limitations of traditional approaches like boundary effects, finite size limitations, and difficulties with arbitrary loading conditions including thermal changes.

Method: Proposes an enhanced Computational Analytical Micromechanics (CAM) framework based on exact Additive General Integral Equation (AGIE) for compactly supported loading. Develops new general integral equations for arbitrary mechanical and thermal loading, with unified iterative solution strategy for both perfectly and imperfectly bonded interfaces. Introduces generalized Representative Volume Element (RVE) concept that emerges from loading scale rather than being predefined geometrically.

Result: Framework reduces analysis of infinite randomly heterogeneous medium to finite data-driven domain, automatically excludes unrepresentative subsets of effective parameters, eliminates boundary effects and finite size limitations. The AGIE-based approach is naturally compatible with machine learning and neural network architectures for constructing accurate surrogate nonlocal operators.

Conclusion: The enhanced CAM framework provides a robust, data-driven approach for thermoelastic composite analysis that overcomes traditional limitations through generalized RVE concept and AGIE formulation, while enabling seamless integration with modern machine learning techniques for efficient surrogate modeling.

Abstract: This work presents an enhanced Computational Analytical Micromechanics (CAM) framework for the analysis of linear thermoelastic composite materials (CMs) with random microstructure. The proposed approach is grounded in an exact Additive General Integral Equation (AGIE), specifically formulated for compactly supported loading, including both body forces and localized thermal changes (such as those from laser heating). New general integral equations (GIEs) for arbitrary mechanical and thermal loading are proposed. A unified iterative solution strategy is developed for the static AGIE, applicable to CMs with both perfectly and imperfectly bonded interfaces, where the compact support of loading is introduced as a new fundamental training parameter. Central to this methodology is a generalized Representative Volume Element (RVE) concept, which extends Hill classical definition. The resulting RVE is not predefined geometrically, but rather emerges from the characteristic scale of the localized loading, effectively reducing the analysis of an infinite, randomly heterogeneous medium to a finite, data-driven domain. This generalized RVE approach enables automatic exclusion of unrepresentative subsets of effective parameters, while inherently eliminating boundary effects, edge artifacts, and finite size limitations. Moreover, the AGIE-based CAM framework is naturally compatible with machine learning (ML) and neural network (NN) architectures, facilitating the construction of accurate and physically informed surrogate nonlocal operators.

</details>


### [24] [Materials Informatics: Emergence To Autonomous Discovery In The Age Of AI](https://arxiv.org/abs/2601.00742)
*Turab Lookman,YuJie Liu,Zhibin Gao*

Main category: physics.comp-ph

TL;DR: Materials informatics has evolved from physics/information theory foundations to AI-driven ecosystem, now transitioning toward autonomous "human-out-of-the-loop" discovery with LLMs and self-driving labs.


<details>
  <summary>Details</summary>
Motivation: To trace the evolution of materials informatics from its foundational roots to its current AI-driven state, examining how the field has matured and is transitioning toward autonomous discovery processes.

Method: Perspective review analyzing key methodologies including Bayesian Optimization, Reinforcement Learning, Transformers, and LLM integration strategies (specialist vs generalist models), with focus on uncertainty quantification and retrieval-augmented generation (RAG).

Result: The field has evolved from early physics-based approaches through Materials Genome Initiative to current AI/LLM integration, creating an ecosystem that enables inverse design and autonomous laboratories, though practical LLM integration challenges remain.

Conclusion: Materials informatics is transitioning from AI as predictive tool to collaborative research partner, moving toward "human-out-of-the-loop" autonomous discovery through active learning and RAG, representing a new era of autonomous materials science.

Abstract: This perspective explores the evolution of materials informatics, from its foundational roots in physics and information theory to its maturation through artificial intelligence (AI). We trace the field's trajectory from early milestones to the transformative impact of the Materials Genome Initiative and the recent advent of large language models (LLMs). Rather than a mere toolkit, we present materials informatics as an evolving ecosystem, reviewing key methodologies such as Bayesian Optimization, Reinforcement Learning, and Transformers that drive inverse design and autonomous self-driving laboratories. We specifically address the practical challenges of LLM integration, comparing specialist versus generalist models and discussing solutions for uncertainty quantification. Looking forward, we assess the transition of AI from a predictive tool to a collaborative research partner. By leveraging active learning and retrieval-augmented generation (RAG), the field is moving toward a new era of autonomous materials science, increasingly characterized by "human-out-of-the-loop" discovery processes.

</details>


### [25] [Kinetic Turing Instability and Emergent Spectral Scaling in Chiral Active Turbulence](https://arxiv.org/abs/2508.21012)
*Magnus F Ivarsen*

Main category: physics.comp-ph

TL;DR: Active chiral agents with Kuramoto coupling transition from chaos to turbulence via kinetic Turing instability, exhibiting quantized currents, coherent clustering, and universal power-law scaling.


<details>
  <summary>Details</summary>
Motivation: To understand how coherent structures spontaneously emerge from chaotic backgrounds in biological swarms, bridging discrete chimera states with continuous fluid turbulence.

Method: Simulate ensemble of polar chiral active agents with local Kuramoto coupling, derive continuum kinetic theory, and analyze transition via kinetic Turing instability.

Result: System shows quantized loop phase currents and coherent clustering during chaos-to-turbulence transition, with competition between phase-locking and motility selecting critical wavenumber, leading to active turbulence with stable power-law spectral density.

Conclusion: Statistical scaling laws of active turbulence arise from fundamental kinetic instability criteria, bridging chimera states and fluid turbulence, suggesting universality across turbulent phenomena.

Abstract: The spontaneous emergence of coherent structures from chaotic backgrounds is a hallmark of active biological swarms. We investigate this self-organization by simulating an ensemble of polar chiral active agents that couple locally via a Kuramoto interaction. We demonstrate that the system's transition from chaos to active turbulence is characterized by quantized loop phase currents and coherent clustering, and that this transition is strictly governed by a kinetic Turing instability. By deriving the continuum kinetic theory for the model, we identify that the competition between local phase-locking and active agent motility selects a critical structural wavenumber. The instability then drives the system into a state of developed, active turbulence that exhibits stable, robust power-laws in spectral density, suggestive of universality and consistent with observations from a broad range of turbulent phenomena. Our results bridge the gap between discrete chimera states and continuous fluid turbulence, suggesting that the statistical scaling laws of active turbulence can arise from fundamental kinetic instability criteria.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [26] [Generation and characterization of coherent terahertz radiation from 100-TW laser-wakefield acceleration](https://arxiv.org/abs/2601.00134)
*Taegyu Pak,Dae Hee Wi,Sang Beom Kim,Jaewon Lim,Jae Hee Sung,Seong Ku Lee,Ki-Yong Kim*

Main category: physics.plasm-ph

TL;DR: THz radiation from 100-TW laser-wakefield acceleration shows quadratic scaling with charge/laser energy, coherent broadband emission, and supports coherent acceleration radiation as the dominant mechanism.


<details>
  <summary>Details</summary>
Motivation: To characterize THz radiation from high-power laser-wakefield acceleration and understand its generation mechanisms and scaling properties for potential applications.

Method: Experimental characterization using simultaneous measurements of laser energy, electron-bunch charge, and THz energy; microbolometer-based beam profiling; and single-shot THz interferometry.

Result: THz energy shows quadratic dependence on both charge and laser energy; THz beam has ~0.2 rad divergence; THz pulse is sub-picosecond and broadband (1-20 THz).

Conclusion: Results support coherent acceleration radiation as the dominant mechanism for THz generation in 100-TW laser-wakefield acceleration.

Abstract: We experimentally characterized terahertz (THz) radiation emitted from laser-wakefield acceleration (LWFA) driven at 100-TW laser power. Simultaneous measurements of the laser energy, electron-bunch charge, and THz energy reveal a quadratic dependence of the THz energy on both charge and laser energy. This behavior indicates coherent collective emission in the generation process and provides a useful scaling law for THz output. Microbolometer-based beam profiling shows a relatively large THz beam divergence (~0.2 rad). Single-shot THz interferometry further shows that the emitted THz pulse is sub-picosecond in duration and broadband. Combining the beam-profile and interferometric measurements, the THz spectrum is expected to span approximately 1-20 THz. Together, these results support coherent acceleration radiation as the dominant mechanism for THz generation in 100-TW LWFA.

</details>


### [27] [Transient spark dielectric barrier post-discharge plasma reactor with a liquid electrode for dye degradation: A primary study](https://arxiv.org/abs/2601.00236)
*Mangilal Choudhary,Vanshika,Surya*

Main category: physics.plasm-ph

TL;DR: Researchers developed a dielectric barrier post-discharge plasma reactor with liquid electrode for synthetic dye degradation in textile wastewater treatment, achieving promising results for industrial application.


<details>
  <summary>Details</summary>
Motivation: The potential application of non-thermal plasma in treating textile industrial wastewater motivates development of innovative techniques for wastewater mineralization at laboratory scale.

Method: Built a dielectric barrier post-discharge plasma reactor with liquid electrode, optimized operating conditions, studied reaction kinetics of crystal violet degradation, and tested on other synthetic dyes (wastewater model samples).

Result: Achieved higher degradation efficiency at given discharge conditions through optimization, with promising results for treating dye effluents from textile industry.

Conclusion: The proposed dielectric barrier post-discharge plasma reactor offers a promising solution for treating textile dye effluents, demonstrating potential for industrial wastewater treatment applications.

Abstract: The potential application of non-thermal plasma in treating textile industrial wastewater motivates researchers to develop innovative techniques at a laboratory scale to achieve the goal of wastewater mineralization. In line with this objective, a dielectric barrier post-discharge plasma reactor with a liquid electrode has been built for the study of synthetic dye degradation. The plasma reactor was optimized by altering various operating conditions to achieve a higher degradation efficiency at given discharge conditions. The reaction kinetics of crystal violet degradation were studied, and the same plasma reactor was tested for other synthetic dyes (wastewater model samples). The results suggest that the proposed dielectric barrier post-discharge plasma reactor may offer a promising solution for treating dye effluents from the textile industry.

</details>


### [28] [Enhanced wakefield generation in homogeneous plasma via two co-propagating laser pulses](https://arxiv.org/abs/2601.00298)
*Abhishek Kumar Maurya,Dinkar Mishra,Bhupesh Kumar,Ramesh C Sharma,Lal C Mangal,Binoy K Das,Vijay K Saraswat,Brijesh Kumar*

Main category: physics.plasm-ph

TL;DR: Two co-propagating laser pulses (seed + trailing) enhance plasma wakefield amplitude when separated by plasma wavelength, with optimization for pulse widths and intensities.


<details>
  <summary>Details</summary>
Motivation: To achieve stronger plasma wakefield excitation for various applications by using two co-propagating laser pulses instead of single pulse.

Method: Used two co-propagating linearly polarized laser pulses with identical parameters (seed followed by trailing pulse). Conducted analytical modeling and particle-in-cell simulations to optimize wakefield enhancement for various pulse widths and intensities at fixed spatial separation.

Result: Maximum wakefield amplification occurs when spatial separation equals plasma wavelength (λ_p). Spatial intervals between pulses critically influence amplification. Two-pulse scheme provides stronger wakefield excitation than single pulse.

Conclusion: Two co-propagating laser pulses offer promising route for enhanced plasma wakefield excitation, potentially important for various applications requiring stronger wakefields.

Abstract: This investigation deals with enhanced plasma wakefield amplitude generated using two co-propagating laser pulses in homogeneous plasma. The configuration consists of a seed pulse followed by a trailing pulse, both linearly polarized and sharing identical laser parameters. The enhancement in wakefield amplitude corresponding to fixed spatial separation is optimized for various pulse widths and intensities of the seed and trailing lasers. Analytical modelling and particle-in-cell simulations reveal that the maximum amplification in wakefield amplitude is obtained when spatial separation equals the plasma wavelength (λ_p). The spatial intervals between laser pulses critically influence the wakefield amplification. These findings confirm that the two co-propagating lasers scheme provides a promising route toward stronger plasma wakefield excitation, potentially important for various applications.

</details>


### [29] [Influence of Cathode Boundary and Initial Electron Swarm Width on Electron Swarm Parameter Determination with the Pulsed Townsend Experiment](https://arxiv.org/abs/2601.00365)
*Mücahid Akbas*

Main category: physics.plasm-ph

TL;DR: Improved analysis method for Pulsed Townsend experiments that accounts for initial and boundary conditions to extract more accurate electron transport parameters, with publicly available code.


<details>
  <summary>Details</summary>
Motivation: Existing analysis techniques for Pulsed Townsend experiments lack accurate representation of experimental initial and boundary conditions, leading to less accurate extraction of electron transport properties.

Method: Developed an improved evaluation approach that appropriately considers both initial and boundary conditions in the analysis of Pulsed Townsend measurement data.

Result: Verification through simulative and experimental measurements shows increased evaluation accuracy, and the longitudinal diffusion coefficient can now be accurately extracted from Pulsed Townsend measurements.

Conclusion: The improved method provides more accurate swarm parameters from Pulsed Townsend experiments, with the developed curve fitting code made publicly available for broader use.

Abstract: The Pulsed Townsend experiment enables the extraction of relevant electron transport properties in different gases such as the electron drift velocity $W$ (or equivalently the mobility $μ$), the longitudinal diffusion coefficient $D_{\mathrm{L}}$, and the effective ionization rate $R_{\mathrm{net}}$ (or equivalently the effective ionization coefficient $α$). Existing analysis techniques lack an accurate representation of the experimental initial and boundary conditions. This work aims to provide an improved evaluation approach by appropriately considering both initial and boundary conditions in order to extract more accurate swarm parameters from measurement data. Simulative and experimental measurement results verify an increased evaluation accuracy. Furthermore, the longitudinal diffusion coefficient $D_{\mathrm{L}}$ can now be accurately extracted from Pulsed Townsend measurements. The developed curve fitting code is made publicly available.

</details>


### [30] [Update on the design of the Columbia Stellarator eXperiment](https://arxiv.org/abs/2601.00673)
*Antoine Baillod,Avigdor Veksler,Rohan Lopez,Dylan Schmeling,Michael Campagna,Elizabeth Paul,Alexey Knyazev*

Main category: physics.plasm-ph

TL;DR: Final configuration selected for Columbia Stellarator eXperiment (CSX) that satisfies all constraints, with analysis of coil effects, sensitivity, and plasma physics confirming experimental goals.


<details>
  <summary>Details</summary>
Motivation: To present the final build configuration for the CSX stellarator experiment that meets all design constraints and confirms experimental feasibility.

Method: Building upon previous optimization work, the paper describes the final configuration selection process, including analysis of coil finite build effects, sensitivity analyses, and plasma neoclassical physics properties using the SFINCS code.

Result: A configuration that satisfies all constraints is identified, with post-processing calculations confirming that CSX's experimental goals can be achieved with this design.

Conclusion: The presented final configuration for CSX is validated through comprehensive analysis and meets all requirements for the stellarator experiment.

Abstract: We present the final configuration chosen to be build for the Columbia Stellarator eXperiment (CSX), a new stellartor experiment at Columbia University. In a recent publication, Baillod et al. (NF, 2025) discussed in detail the different objectives, constraints, and optimization algorithms used to find an optimal configuration for CSX. In this paper, we build upon this first publication and find a configuration that satisfies all the constraints. We describe this final configuration including discussion of the coil finite build effects, sensitivity analyses, and the plasma neoclassical physics properties using the SFINCS code. These post-processing calculations provide a confirmation that the experimental goals of CSX can be achieved with the presented configuration.

</details>


### [31] [Stimulation of surface ionization waves by pulsed laser irradiation](https://arxiv.org/abs/2601.00686)
*Thomas Orrière,David Z. Pai*

Main category: physics.plasm-ph

TL;DR: Semiconductor composite barriers enable uniform surface ionization waves in air regardless of electric field polarity, and laser stimulation enhances propagation and energy when timed within 3 μs of plasma generation.


<details>
  <summary>Details</summary>
Motivation: To achieve uniform propagation of surface ionization waves in atmospheric air regardless of electric field polarity, overcoming limitations of purely dielectric barriers which produce non-uniform discharges.

Method: Using semiconductor (silicon) composite barriers and stimulating with 2-ns pulsed laser at 532 nm (1.3 mJ/cm² fluence) at specific timing relative to plasma generation.

Result: Laser stimulation within 3 μs before plasma generation enhances SIW propagation distance and optical emission intensity, increasing discharge energy by up to 7%. No effect observed beyond 3 μs delay.

Conclusion: Semiconductor composite barriers enable polarity-independent uniform SIW propagation, and laser stimulation timing sensitivity (3 μs window) indicates enhancement is due to photoexcited carrier dynamics at Si-SiO₂ interface rather than surface charge desorption.

Abstract: The inclusion of semiconducting material within a composite barrier enables the perfectly uniform propagation of surface ionization waves (SIW) in air at atmospheric pressure regardless of the polarity of the applied electric field, unlike surface discharges generated using purely dielectric barriers. We exploit the photonic properties of silicon to stimulate the SIW using external irradiation by a 2-ns pulsed laser at 532 nm, with a fluence of 1.3 mJ/cm$^2$ per pulse at the surface. No effect is observed when irradiation occurs more than 3 $μ$s before plasma generation. This timescale is attributed to the ambipolar diffusion of photoexcited carriers away from the Si-SiO$_2$ interface. When this delay shortens to less than 3 $μ$s, the SIW propagates farther and with more intense optical emission. Furthermore, the energy of the discharge increases by up to 7%. The sensitivity to the laser-plasma delay demonstrates that the observed stimulation of the SIW cannot be due to the desorption of surface charge by irradiation.

</details>


### [32] [Gravitational instability in partially ionized plasmas: A two-fluid approach](https://arxiv.org/abs/2601.00719)
*A. P. Misra,V. Krishan*

Main category: physics.plasm-ph

TL;DR: Two-fluid model for partially ionized plasma shows ion-neutral collisions and wave geometry modify gravitational instability growth rates in ionosphere and solar atmosphere.


<details>
  <summary>Details</summary>
Motivation: To understand how gravitational instability (Rayleigh-Taylor) in partially ionized magnetoplasmas is affected by ion-neutral collisions and transverse wave geometry, with applications to ionospheric E-region and solar atmospheric phenomena.

Method: Developed a new two-fluid model where electrons+neutrals form one fluid and positive ions form another fluid. Analyzed instability growth rates as functions of collision frequency ratio (f = ν_in/Ω_ci) and wave number ratio (κ = k_x/k_y).

Result: Growth rates can be enhanced or decreased depending on κ and f values. Maximum growth occurs for κ,f≪1 or κ>1 with f∼0.64; minimum growth for f≫1 regardless of κ. Timescales: 1-2 minutes in solar atmosphere, 1-80 minutes in E-region.

Conclusion: Ion-neutral collisions significantly modify gravitational instability in partially ionized plasmas, with growth rates controllable by collision frequency and wave geometry. The 1-80 minute timescales in E-region align with solar prominence thread lifetimes.

Abstract: We propose a new two-fluid model for a partially ionized magnetoplasma under gravity, where electrons and neutrals are treated as a single fluid, and singly charged positive ions are a separate fluid. We observe that the classical result of gravitational instability (also known as Rayleigh-Taylor instability) in fully ionized plasmas is significantly modified by the influence of ion-neutral collisions (with frequency $ν_{\rm{in}}$) and transverse wave numbers ($k_x$ and $k_y$). The instability growth rate can be enhanced or decreased depending on the values of the ratios $κ\equiv k_x/k_y$ and $f\equivν_{\rm{in}}/Ω_{\rm{ci}}$, where $Ω_{\rm{ci}}$ is the ion-cyclotron frequency. We also estimate the growth rates relevant to the ionospheric E-region and solar atmosphere, noting that such growth rates can be maximized for $κ,~f\ll1$, or for $κ>1$ and $f\sim0.64$, and minimized for $f\gg1$ irrespective of the value of $κ$. Furthermore, the timescale of instability ranges from $1$ minute to $2$ minutes in the solar atmosphere, while in the E region, it ranges from $1$ minute to $80$ minutes. The latter can be a satisfactory result for the reported lifetime of solar prominence threads.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [33] [Combining multiple interface set path ensembles with MBAR reweighting](https://arxiv.org/abs/2601.00458)
*Rik S. Breebaart,Peter G. Bolhuis*

Main category: cond-mat.stat-mech

TL;DR: A method combining transition interface sampling simulations with different collective variables using MBAR to compute reweighted path ensembles, improving statistical accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve statistical accuracy in computing reweighted path ensembles by effectively combining transition interface sampling simulations that use different collective variables.

Method: Uses Multistate Bennett Acceptance Ratio (MBAR) methodology applied to entire trajectories to combine transition interface sampling simulations conditioned on different collective variables.

Result: Demonstrated on simple 2D potential models and a complex host-guest system, showing significantly improved statistics compared to straightforward combination approaches.

Conclusion: The MBAR-based approach provides a robust method for computing reweighted path ensembles with enhanced statistical accuracy by optimally combining simulations with different collective variables.

Abstract: We introduce a method to compute the reweighted path ensemble by combining transition interface sampling simulations conditioned on different collective variables. The approach is based on the Multistate Bennett Acceptance Ratio (MBAR) methodology applied to entire trajectories. Illustrating the technique with simple 2D potential models and a more complex host-guest system, we show that the statistics can significantly improve compared to a straightforward combination.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [34] [Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference](https://arxiv.org/abs/2601.00038)
*Shane A. McQuarrie,Mengwu Guo,Anirban Chaudhuri*

Main category: stat.ML

TL;DR: Active learning framework for data-driven reduced-order models using Bayesian operator inference with adaptive sampling to improve stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Data-driven ROMs are sensitive to training data quality, and we need to identify optimal training parameters to create the best possible parametric ROMs for digital twin applications.

Method: Probabilistic parametric operator inference (Bayesian linear regression) with prediction uncertainties used to design sequential adaptive sampling scheme for selecting new training parameters.

Result: Adaptive sampling consistently yields more stable and accurate ROMs than random sampling under the same computational budget across multiple nonlinear parametric PDE systems.

Conclusion: The proposed active learning framework with Bayesian operator inference and adaptive sampling effectively improves ROM quality by intelligently selecting training parameters.

Abstract: This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [35] [Coupled Modal-Nonmodal Interactions Due to Periodic, Infinite Train of Convecting Vortices (TCV)](https://arxiv.org/abs/2601.00295)
*Jyothi Kumar Puttam,Prasannabalaji Sundaram,Vajjala K. Suman,Ankan Sarkar,Tapan K. Sengupta,Tirupathur N. Venkatesh,Rakesh K. Mathpal*

Main category: physics.flu-dyn

TL;DR: Computational study shows strong interaction between modal and non-modal routes to turbulence when freestream is excited by convecting vortices, causing spectacular disturbance growth.


<details>
  <summary>Details</summary>
Motivation: To understand severe turbulence encounters caused by convective vortical disturbances on shear layers, particularly how freestream excitation from vortex trains interacts with different transition routes.

Method: Computational investigation of strong freestream excitation caused by a train of convecting vortices (TCV excitation).

Result: Strong interaction between modal and non-modal components causes spectacular growth of disturbances, proposed as mechanism for severe encounters due to convective vortical disturbances.

Conclusion: TCV excitation creates a powerful interaction between modal and non-modal transition routes, explaining severe turbulence encounters from convective vortical disturbances on shear layers.

Abstract: Events during transition to turbulence either follow modal or non-modal routes, or combinations of the two. Here, we report a computational investigation of strong freestream excitation caused by a train of convecting vortices. For this TCV excitation, we show a strong interaction of modal and non-modal components causing a spectacular growth of disturbances. We propose this as the mechanism for the severe encounters due to convective vortical disturbances on the underlying shear layer.

</details>


### [36] [Solving nonlinear subsonic compressible flow in infinite domain via multi-stage neural networks](https://arxiv.org/abs/2601.00342)
*Xuehui Qian,Hongkai Tao,Yongji Wang*

Main category: physics.flu-dyn

TL;DR: PINNs solve nonlinear compressible potential flow in infinite domains using coordinate transforms and multi-stage optimization, achieving near-machine precision accuracy and quantifying errors from traditional domain truncation/linearization.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for subsonic compressible flow over airfoils use linearized equations or finite domains, introducing significant errors that limit real-world applicability, especially at higher Mach numbers.

Method: Physics-Informed Neural Networks (PINNs) with coordinate transformation for unbounded domains, embedding physical asymptotic constraints, and Multi-Stage PINN (MS-PINN) approach for iterative residual minimization.

Result: Validated on circular and elliptical geometries, achieving solution accuracy approaching machine precision, quantifying noticeable discrepancies from domain truncation and linearization, especially at higher Mach numbers.

Conclusion: The framework provides a robust, high-fidelity tool for computational fluid dynamics that overcomes limitations of traditional approaches for unbounded-domain compressible flow problems.

Abstract: In aerodynamics, accurately modeling subsonic compressible flow over airfoils is critical for aircraft design. However, solving the governing nonlinear perturbation velocity potential equation presents computational challenges. Traditional approaches often rely on linearized equations or finite, truncated domains, which introduce non-negligible errors and limit applicability in real-world scenarios. In this study, we propose a novel framework utilizing Physics-Informed Neural Networks (PINNs) to solve the full nonlinear compressible potential equation in an unbounded (infinite) domain. We address the unbounded-domain and convergence challenges inherent in standard PINNs by incorporating a coordinate transformation and embedding physical asymptotic constraints directly into the network architecture. Furthermore, we employ a Multi-Stage PINN (MS-PINN) approach to iteratively minimize residuals, achieving solution accuracy approaching machine precision. We validate this framework by simulating flow over circular and elliptical geometries, comparing our results against traditional finite-domain and linearized solutions. Our findings quantify the noticeable discrepancies introduced by domain truncation and linearization, particularly at higher Mach numbers, and demonstrate that this new framework is a robust, high-fidelity tool for computational fluid dynamics.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [37] [Full grid solution for multi-asset options pricing with tensor networks](https://arxiv.org/abs/2601.00009)
*Lucas Arenstein,Michael Kastoryano*

Main category: q-fin.CP

TL;DR: QTT methods enable solving high-dimensional Black-Scholes PDEs for multi-asset options on personal computers, overcoming the curse of dimensionality that limits classical full-grid solvers to 3 assets.


<details>
  <summary>Details</summary>
Motivation: Pricing multi-asset options via Black-Scholes PDE suffers from curse of dimensionality - classical solvers scale exponentially with number of underlyings and are limited to 3 assets. Practitioners rely on Monte Carlo methods for complex multi-asset instruments, but PDE methods would be preferable for accuracy and Greeks computation.

Method: Use quantized tensor trains (QTT) to transform d-asset Black-Scholes PDE into tractable high-dimensional problem. Construct QTT representations of operator, payoffs, and boundary conditions with polynomial scaling in d and polylogarithmic scaling in grid size. Build two solvers: time-stepping algorithm for European/American options, and space-time algorithm for European options.

Result: Successfully compute full-grid prices and Greeks for correlated basket and max-min options in 3-5 dimensions with high accuracy. Methods can be extended to 10-15 underlyings with further optimization and compute power, demonstrating practical feasibility on personal computers.

Conclusion: QTT methods make high-dimensional Black-Scholes PDE solving practical, enabling accurate pricing of multi-asset options with full-grid precision where classical methods fail, potentially replacing Monte Carlo for many applications requiring Greeks computation.

Abstract: Pricing multi-asset options via the Black-Scholes PDE is limited by the curse of dimensionality: classical full-grid solvers scale exponentially in the number of underlyings and are effectively restricted to three assets. Practitioners typically rely on Monte Carlo methods for computing complex instrument involving multiple correlated underlyings. We show that quantized tensor trains (QTT) turn the d-asset Black-Scholes PDE into a tractable high-dimensional problem on a personal computer. We construct QTT representations of the operator, payoffs, and boundary conditions with ranks that scale polynomially in d and polylogarithmically in the grid size, and build two solvers: a time-stepping algorithm for European and American options and a space-time algorithm for European options. We compute full-grid prices and Greeks for correlated basket and max-min options in three to five dimensions with high accuracy. The methods introduced can comfortably be pushed to full-grid solutions on 10-15 underlyings, with further algorithmic optimization and more compute power.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [38] [High-Temperature Deformation Behavior of Co-Free Non-Equiatomic CrMnFeNi Alloy](https://arxiv.org/abs/2601.00619)
*F. J. Dominguez-Gutierrez,M. Frelek-Kozak,G. Markovic,M. A. Strozyk,A. Daramola,M. Traversier,A. Fraczkiewicz,A. Zaborowska,T. Khvan,I. Jozwik,L. Kurpaska*

Main category: cond-mat.mtrl-sci

TL;DR: Cobalt-free CrMnFeNi high-entropy alloy shows temperature-dependent mechanical strength reduction, with dislocation activity, stacking fault formation, and twinning observed via experiments and simulations.


<details>
  <summary>Details</summary>
Motivation: Develop cobalt-free high-entropy alloys for nuclear structural applications that avoid long-lived Co radioisotopes while maintaining good mechanical performance, thermal stability, and radiation resistance.

Method: Experimental tensile tests, molecular dynamics simulations of single- and polycrystals, electron backscatter diffraction (EBSD) analysis, and Schmid factor mapping to study deformation mechanisms.

Result: Temperature-dependent reduction in mechanical strength observed, with dislocation activity, stacking fault formation, and twin nucleation captured in simulations. Enhanced high-temperature strength compared to Cantor alloy due to Co absence.

Conclusion: Cobalt-free CrMnFeNi HEA demonstrates promising deformation behavior with balanced stacking fault energies for enhanced strain hardening and ductility, making it suitable for nuclear applications.

Abstract: Cobalt-free high-entropy alloys (HEAs) have garnered interest for nuclear structural applications due to their good mechanical performance, thermal stability, and resistance to radiation-induced degradation, while avoiding long-lived Co radioisotopes. This study presents an experimental and computational investigation of the plastic deformation behavior of a non-equatomic CrMnFeNi alloy, designed to maintain a stability of fcc phase in a large domain of temperatures and to balance stacking fault (SF) energies for enhanced strain hardening and ductility. Tensile tests reveal a temperature-dependent reduction in mechanical strength, attributed to thermally activated deformation mechanisms and microstructural evolution. Molecular dynamics simulations of single- and polycrystals capture dislocation activity, SF formation, and twin nucleation as a function of strain and temperature. Electron backscatter diffraction (EBSD) confirms twin formation and grain boundary activity. The Schmid factor mapping is drawn to interpret local slip activity and anisotropic deformation behavior. The absence of Co leads to enhanced high-temperature strength compared to the Cantor alloy.

</details>


### [39] [Electronic-Entropy-Driven Solid-Solid Phase Transitions in Elemental Metals](https://arxiv.org/abs/2601.00740)
*S. Azadi,S. M. Vinko,A. Principi,T. D. Kuehne,M. S. Bahramy*

Main category: cond-mat.mtrl-sci

TL;DR: Researchers used finite-temperature DFT to compute phase diagrams of 17 elemental metals, finding electronic entropy drives solid-solid phase transitions at high electronic temperatures.


<details>
  <summary>Details</summary>
Motivation: To understand how electronic entropy affects structural stability in metals under strong electronic excitation, particularly identifying solid-solid phase transitions driven purely by electronic effects rather than lattice vibrations.

Method: Used finite-temperature density functional theory to compute Helmholtz free-energy differences between hcp, fcc, and bcc phases as functions of electronic temperature up to 7 eV for 17 elemental metals in their ground-state structures.

Result: Found that all studied systems except Mg and Pb undergo one or two solid-solid phase transitions caused purely by electronic entropy, with transition temperatures extracted from free-energy crossings.

Conclusion: Electronic entropy is a key factor governing structural stability in metals under strong electronic excitation, establishing its importance in phase transitions at high electronic temperatures.

Abstract: We compute the thermodynamic phase diagram of seventeen elemental metals with hexagonal close-packed (hcp), face-centered cubic (fcc), and body-centered cubic (bcc) crystal structures using finite-temperature density functional theory. Helmholtz free-energy differences between competing hcp, fcc, and bcc phases are evaluated as functions of electronic temperature up to 7 eV, allowing us to identify solid-solid phase transitions driven by electronic entropy. The systems studied include Zr, Ti, Cd, Zn, Co, and Mg (hcp), Ni, Cu, Ag, Al, Pt, and Pb (fcc), and Cr, W, V, Nb, and Mo (bcc) in their ground-state structures. From the free-energy crossings, we extract the transition electronic temperatures and analyze systematic trends across the metallic systems. We found that all the studied systems go through one or two solid-solid phase transition caused purely by electronic entropy except Mg and Pb. Our results establish electronic entropy as a key factor governing structural stability in metals under strong electronic excitation.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [40] [A POD-DeepONet Framework for Forward and Inverse Design of 2D Photonic Crystals](https://arxiv.org/abs/2601.00199)
*Yueqi Wang,Guanglian Li,Guang Lin*

Main category: physics.optics

TL;DR: A reduced-order operator-learning framework for forward/inverse band-structure design of 2D photonic crystals with binary, pixel-based p4m-symmetric unit cells using POD-DeepONet surrogate.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient framework for designing photonic crystals with specific band structures, addressing the computational cost of traditional methods and the non-uniqueness of inverse design problems.

Method: Construct POD-DeepONet surrogate combining Proper Orthogonal Decomposition (POD) trunk from high-fidelity finite-element band snapshots with neural branch network to predict reduced coefficients. Formulate two inverse design procedures: dispersion-to-structure and band-gap design with combined training objectives.

Result: The framework achieves accurate forward predictions and produces effective inverse designs on practical high-contrast, pixel-based photonic layouts, with proven continuity and approximation properties.

Conclusion: The proposed reduced-order operator-learning framework provides an efficient, differentiable approach for both forward modeling and inverse design of photonic crystals, addressing computational challenges and non-uniqueness issues in photonic band structure engineering.

Abstract: We develop a reduced-order operator-learning framework for forward and inverse band-structure design of two-dimensional photonic crystals with binary, pixel-based $p4m$-symmetric unit cells. We construct a POD--DeepONet surrogate for the discrete band map along the standard high-symmetry path by coupling a POD trunk extracted from high-fidelity finite-element band snapshots with a neural branch network that predicts reduced coefficients. This architecture yields a compact and differentiable forward model that is tailored to the underlying Bloch eigenvalue discretization. We establish continuity of the discrete band map on the relaxed design space and prove a uniform approximation property of the POD--DeepONet surrogate, leading to a natural decomposition of the total surrogate error into POD truncation and network approximation contributions. Building on this forward surrogate, we formulate two end-to-end neural inverse design procedures, namely dispersion-to-structure and band-gap inverse design, with training objectives that combine data misfit, binarity promotion, and supervised regularization to address the intrinsic non-uniqueness of the inverse mapping and to enable stable gradient-based optimization in the relaxed space. Our numerical results show that the proposed framework achieves accurate forward predictions and produces effective inverse designs on practical high-contrast, pixel-based photonic layouts.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [41] [On the Theory of Absorption of Sound Waves via the Bulk Viscosity in the Partially Ionized Solar Chromosphere](https://arxiv.org/abs/2601.00362)
*Albert M. Varonov,Todor M. Mishonov*

Main category: astro-ph.SR

TL;DR: Bulk viscosity in hydrogen-helium solar atmosphere provides sufficient acoustic wave damping for chromosphere heating without needing artificial viscosity or relying on shear viscosity.


<details>
  <summary>Details</summary>
Motivation: To determine if bulk viscosity alone can provide sufficient acoustic wave damping for solar chromosphere heating, addressing whether artificial viscosity or shear viscosity mechanisms are necessary.

Method: Used temperature and density height profiles of the solar atmosphere to calculate bulk viscosity, thermodynamic variables (internal energy, enthalpy, pressure, derivatives, heat capacities), and evaluate sound wave energy flux requirements.

Result: Found that bulk viscosity creates the dominating mechanism for acoustic wave damping, with necessary sound wave energy flux of 320 kW/m² for chromosphere heating.

Conclusion: Bulk viscosity alone is sufficient for chromosphere heating; no need for artificial viscosity or conclusions that shear viscosity is insufficient.

Abstract: Bulk viscosity and thermodynamic variables of a hydrogen-helium cocktail: internal energy, enthalpy, pressure, their derivatives, heat capacities per constant density and pressure are obtained using temperature and density height profiles of the solar atmosphere [Avrett & Loeser, ApJS Vol. 175, 229 (2008)]. The qualitative evaluation for the necessary sound wave energy flux to heat the solar chromosphere is determined to be 320 kW/m$^2$. It is concluded that the bulk viscosity creates the dominating mechanism of acoustic waves damping and it is not necessary to introduce artificial viscosity or to conclude that shear viscosity is not sufficient for chromosphere heating; the volume viscosity induced wave absorption is sufficient.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [42] [Bayesian optimization for re-analysis and calibration of extreme sea state events simulated with a spectral third-generation wave model](https://arxiv.org/abs/2601.00628)
*Cédric Goeury,Thierry Fouquet,Maria Teles,Michel Benoit*

Main category: physics.ao-ph

TL;DR: Bayesian Optimization with Tree Parzen Estimator improves wave model calibration for extreme sea states by optimizing physical parameters, reducing errors compared to observations.


<details>
  <summary>Details</summary>
Motivation: Accurate hindcasting of extreme sea state events is crucial for coastal engineering and risk assessment, but current numerical wave models have reliability limitations due to uncertainties in physical parameterizations and model inputs.

Method: A novel calibration framework using Bayesian Optimization (BO) with Tree structured Parzen Estimator (TPE) to efficiently estimate uncertain sink term parameters (bottom friction dissipation, depth induced breaking, wave dissipation from strong opposing currents) in the ANEMOC-3 hindcast wave model, enabling joint optimization of continuous parameters and discrete model structures.

Result: Applied to a one-month period with multiple intense storm events along the French Atlantic coast, the calibrated model showed improved agreement with buoy measurements, achieving lower bias, RMSE, and scatter index compared to the default configuration.

Conclusion: BO demonstrates potential to automate and enhance wave model calibration, offering a scalable and flexible approach applicable to various geophysical modeling problems, with future extensions including multi-objective optimization, uncertainty quantification, and integration of additional observational datasets.

Abstract: Accurate hindcasting of extreme sea state events is essential for coastal engineering, risk assessment, and climate studies. However, the reliability of numerical wave models remains limited by uncertainties in physical parameterizations and model inputs. This study presents a novel calibration framework based on Bayesian Optimization (BO), leveraging the Tree structured Parzen Estimator (TPE) to efficiently estimate uncertain sink term parameters, specifically bottom friction dissipation, depth induced breaking, and wave dissipation from strong opposing currents, in the ANEMOC-3 hindcast wave model. The proposed method enables joint optimization of continuous parameters and discrete model structures, significantly reducing discrepancies between model outputs and observations. Applied to a one month period encompassing multiple intense storm events along the French Atlantic coast, the calibrated model demonstrates improved agreement with buoy measurements, achieving lower bias, RMSE, and scatter index relative to the default sea$-$state solver configuration. The results highlight the potential of BO to automate and enhance wave model calibration, offering a scalable and flexible approach applicable to a wide range of geophysical modeling problems. Future extensions include multi-objective optimization, uncertainty quantification, and integration of additional observational datasets.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [43] [Revisiting p-$^{11}$B Fusion: Updated Cross-sections, Reactivity, and Energy Balance](https://arxiv.org/abs/2601.00241)
*Hong-Yi Wang,Yu-Qi Li,Qian Wu,Zhu-Fang Cui*

Main category: nucl-th

TL;DR: High-precision analytical parameterization of p-11B fusion cross-section using new experimental data, showing bremsstrahlung constraints don't preclude p-11B fusion viability.


<details>
  <summary>Details</summary>
Motivation: Recent experimental progress has improved cross-section data for p-11B fusion, particularly in previously unmeasured energy regions, enabling more accurate modeling and assessment of this fusion reaction's viability.

Method: Developed a high-precision analytical parameterization of p-11B reaction cross-section over 0-10 MeV range, incorporating new experimental data into continuous representation. Used this to evaluate thermonuclear reactivity, analyze effects of dominant resonance at 0.6 MeV and newly observed resonance at 4.7 MeV, and assess energy balance by comparing fusion power density with electron bremsstrahlung power density.

Result: The parameterization provides continuous and numerically efficient representation of cross-section data. Analysis shows p-11B fusion is not precluded by bremsstrahlung constraints when using contemporary cross-section data and self-consistent thermal modeling.

Conclusion: p-11B fusion remains a viable candidate for fusion energy when using updated experimental data and proper thermal modeling, contrary to previous concerns about bremsstrahlung losses.

Abstract: Recent experimental progress has substantially improved the available cross-section data for the p-$^{11}$B fusion reaction, particularly in energy regions that previously lacked direct measurements. In this study, we develop a high-precision analytical parameterization of the p-$^{11}$B reaction cross-section over the 0--10 MeV energy range, incorporating the new experimental data into a continuous and numerically efficient representation. Using this parameterization, we evaluate the thermonuclear reactivity of the p-$^{11}$B reaction and examine the effects of the dominant resonance at 0.6 MeV and a newly observed resonance around 4.7 MeV. Furthermore, we assess the energy balance by analyzing the fusion power density and the electron bremsstrahlung power density. Our results indicate that p-$^{11}$B fusion is not precluded by bremsstrahlung constraints when contemporary cross-section data and self-consistent thermal modeling are employed.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: RL framework for adaptive precision tuning of linear solvers using contextual bandit formulation with Q-learning to dynamically select optimal precision configurations, balancing accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: To advance mixed-precision numerical methods in scientific computing by automating precision selection, reducing computational costs while maintaining accuracy, and generalizing to diverse datasets.

Method: Formulated as contextual bandit problem using Q-learning with discretized state space. Features like condition number and matrix norm map to precision configurations via Q-table, optimized with epsilon-greedy strategy and multi-objective reward balancing accuracy and cost.

Result: Empirical results show effective precision selection that reduces computational cost while maintaining accuracy comparable to double-precision baselines. Framework generalizes to out-of-sample data and is the first RL-based precision autotuning work verified on unseen datasets.

Conclusion: The RL framework successfully enables adaptive precision tuning for linear solvers, demonstrating practical benefits for mixed-precision computing and potential extension to other numerical algorithms in scientific computing.

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [45] [Mass-loaded magnetic explosions in the context of Magnetar Giant Flares and Fast Radio Bursts](https://arxiv.org/abs/2601.00441)
*Konstantinos N. Gourgouliatos*

Main category: astro-ph.HE

TL;DR: The paper develops semi-analytical relativistic MHD solutions for spherical magnetic explosions, with two solution classes applicable to magnetar flares (overdense/high pressure) and fast radio bursts (underdense/magnetically dominated).


<details>
  <summary>Details</summary>
Motivation: To understand the connection between magnetar flares and fast radio bursts, both involving strong magnetic fields and relativistic plasma, by providing theoretical models for relativistic magnetic explosions that could explain these astrophysical phenomena.

Method: Semi-analytical solution of relativistic magnetohydrodynamics equations assuming self-similarity in time and radius, axial symmetry, and separation of variables for a spherical, mass-loaded magnetic explosion system.

Result: Two distinct solution classes emerge: overdense/high-pressure configurations (relevant to magnetar flares) and underdense/magnetically dominated configurations (relevant to fast radio bursts), with expansion velocity affected by field ratios and pressure/density parameters.

Conclusion: The derived relativistic MHD solutions provide a unified theoretical framework connecting magnetar flares and fast radio bursts, with different parameter regimes explaining each phenomenon based on density, pressure, and magnetic field configurations.

Abstract: Magnetar flares are highly energetic and rare events where intense high-energy emission is released from strongly magnetised neutron stars. Fast radio bursts are short and intense pulses of coherent radio emission. Their large dispersion measures support an extragalactic origin. While their exact origin still remains elusive, a substantial number of models associates them with strong magnetic field and high-energy relativistic plasma found in the vicinity of magnetars. There is growing evidence that some fast radio bursts are associated to flare-type events from magnetars. We aim to provide a set of configurations describing a relativistic, spherical, mass-loaded, magnetic explosion. We proceed by solving the equations of relativistic magnetohydrodynamics, for a system that expands while maintaining its internal equilibrium. We employ a semi-analytical approach for the solution of the equations of relativistic magnetohydrodynamics. We assume self-similarity in time and radius, axial symmetry, and separation of variables. There exists a dichotomy of solutions that correspond to higher and lower density and thermal pressure compared to the external one. The ratio of the poloidal to toroidal field and the inclusion of pressure and mass density affect the expansion velocity. The classes of these solutions can be applied to magnetar giant flares and fast radio bursts. The ones corresponding to overdensities and higher pressure can be associated to magnetar flares, whereas the ones corresponding to underdensities can be relevant to fast radio bursts corresponding to magnetically dominated events with low mass loading.

</details>


### [46] [High-energy Emission from Turbulent Electron-ion Coronae of Accreting Black Holes](https://arxiv.org/abs/2601.00518)
*Daniel Groselj,Alexander Philippov,Andrei M. Beloborodov,Richard Mushotzky*

Main category: astro-ph.HE

TL;DR: A turbulent black-hole corona model using 2D radiative particle-in-cell simulations shows self-regulated two-temperature plasma, nonthermal particle acceleration, and X-ray spectra matching observations with predicted MeV tails.


<details>
  <summary>Details</summary>
Motivation: To understand particle energization and emission mechanisms in strongly turbulent black-hole coronae, and to explain observed X-ray spectra from sources like NGC 4151.

Method: 2D radiative particle-in-cell simulations with electron-ion plasma composition, including injection/diffusive escape of photons/particles and self-consistent Compton scattering.

Result: The model produces extended nonthermal ion distributions, X-ray spectra consistent with observations (excellent match to NGC 4151), MeV tails from electron acceleration at current sheets, and self-regulated two-temperature corona with ions carrying 60-70% of dissipated power.

Conclusion: Turbulent black-hole coronae naturally develop two-temperature states with nonthermal particle acceleration, explaining observed X-ray spectra and predicting MeV emission detectable by future instruments.

Abstract: We develop a model of particle energization and emission from strongly turbulent black-hole coronae. Our local model is based on a set of 2D radiative particle-in-cell simulations with an electron-ion plasma composition, injection and diffusive escape of photons and charged particles, and self-consistent Compton scattering. We show that a radiatively compact turbulent corona generates extended nonthermal ion distributions, while producing X-ray spectra consistent with observations. As an example, we demonstrate excellent agreement with observed X-ray spectra of NGC 4151. The predicted emission spectra feature an MeV tail, which can be studied with future MeV-band instruments. The MeV tail is shaped by nonthermal electrons accelerated at turbulent current sheets. We also find that the corona regulates itself into a two-temperature state, with ions much hotter than electrons. The ions carry away roughly 60% to 70% of the dissipated power, and their energization is driven by a combination of shocks and reconnecting current sheets, embedded into the turbulent flow.

</details>
