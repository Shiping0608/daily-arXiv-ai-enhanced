<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 20]
- [math.AP](#math.AP) [Total: 24]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [astro-ph.HE](#astro-ph.HE) [Total: 3]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.CA](#math.CA) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 3]
- [math-ph](#math-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Simple Weak Galerkin Finite Element Method for Convection-Diffusion-Reaction Equations on Nonconvex Polytopal Meshes](https://arxiv.org/abs/2601.00986)
*Chunmei Wang,Shangyou Zhang*

Main category: math.NA

TL;DR: A weak Galerkin finite element method for convection-diffusion-reaction equations using discontinuous functions on polytopal meshes with proven error estimates and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible finite element method for convection-diffusion-reaction equations that can handle complex geometries and discontinuous solutions on general nonconvex polytopal meshes.

Method: A simple weak Galerkin finite element method using discontinuous approximating functions on general nonconvex polytopal meshes for solving convection-diffusion-reaction equations.

Result: Rigorous error estimates established within a suitable norm, with numerical experiments validating theoretical convergence rates and demonstrating computational efficiency.

Conclusion: The proposed weak Galerkin method provides an effective and flexible approach for solving convection-diffusion-reaction equations on complex meshes with proven accuracy and efficiency.

Abstract: This article introduces a simple weak Galerkin (WG) finite element method for solving convection-diffusion-reaction equation. The proposed method offers significant flexibility by supporting discontinuous approximating functions on general nonconvex polytopal meshes. We establish rigorous error estimates within a suitable norm. Finally, numerical experiments are presented to validate the theoretical convergence rates and demonstrate the computational efficiency of the approach.

</details>


### [2] [On solving nonlinear simultaneous equations arising from the double-exponential Sinc-collocation method for initial value problems](https://arxiv.org/abs/2601.01007)
*Yusaku Yamamoto*

Main category: math.NA

TL;DR: Analysis of convergence for Gauss-Seidel type fixed-point iteration in double-exponential Sinc-collocation method for solving ODEs.


<details>
  <summary>Details</summary>
Motivation: The double-exponential Sinc-collocation method requires solving nonlinear simultaneous equations in nN variables, which is computationally expensive. Recent empirical evidence shows Gauss-Seidel type fixed-point iteration works surprisingly well, but theoretical understanding of its convergence is needed.

Method: Theoretical analysis of Gauss-Seidel type fixed-point iteration convergence for the double-exponential Sinc-collocation method. Provides sufficient condition for global convergence and upper bound on convergence factor.

Result: Established theoretical foundation for the observed efficiency of Gauss-Seidel iteration, with numerical examples validating the analysis.

Conclusion: The paper provides rigorous convergence analysis explaining why Gauss-Seidel type fixed-point iteration works efficiently for solving nonlinear equations in the double-exponential Sinc-collocation method.

Abstract: The double-exponential Sinc-collocation method is known as a super-accurate method for solving initial value problems of ordinary differential equations, for which the error decreases almost exponentially as a function of the number of sample points in the temporal direction, $N$. However, this method requires solving nonlinear simultaneous equations in $nN$ variables when the problem dimension is $n$. Recently, Ogata pointed out that Gauss-Seidel type fixed-point iteration works surprisingly well for solving these equations, typically reducing the error by one or two orders of magnitude at each iteration. In this paper, we analyze the convergence of this iteration and give a sufficient condition for its global convergence. We also provide an upper bound on its convergence factor, which explains the efficiency of this iteration. Some numerical examples that illustrate the validity of our analysis are also provided.

</details>


### [3] [Flow Matching Transport for Quasi-Monte Carlo Integration](https://arxiv.org/abs/2601.01072)
*Zhijun Zeng,Jianlong Chen*

Main category: math.NA

TL;DR: FM-ISQMC combines Flow Matching with importance sampling and QMC to create unbiased high-order integration schemes for high-dimensional complex measures, achieving O(N^{-1+ε}) error rates.


<details>
  <summary>Details</summary>
Motivation: High-dimensional integration with complex target measures is challenging. Flow Matching has discretization bias and lacks rigorous convergence guarantees when combined with QMC methods, limiting its use for high-precision integration.

Method: Proposes FM-ISQMC framework: compose logistic base transformation with Euler-discretized neural ODE field, then use importance sampling to correct residual transport errors. General convergence analysis for QMC importance sampling with arbitrary transport maps.

Result: Establishes O(N^{-1+ε}) root-mean-square error rate for unbiased FM-ISQMC estimator. Numerical experiments show FM-ISQMC breaks through error floor of direct transport methods, delivering superior precision.

Conclusion: Bridges deep generative modeling with numerical integration by transforming biased generative flows into unbiased, high-order integration schemes with rigorous convergence guarantees.

Abstract: High-dimensional integration with respect to complex target measures remains a fundamental challenge in computational science. While Flow Matching (FM) offers a powerful paradigm for constructing continuous-time transport maps, its deployment in high-precision integration is severely limited by the discretization bias inherent to numerical ODE solvers and the lack of rigorous convergence guarantees when coupled with Quasi-Monte Carlo (QMC) methods. This paper addresses these critical gaps by proposing Flow Matching Importance Sampling Quasi-Monte Carlo (FM-ISQMC), a framework designed to transform biased generative flows into unbiased, high-order integration schemes. Methodologically, we construct a transport map by composing a logistic base transformation with an Euler-discretized neural ODE field and employ importance sampling to correct for residual transport errors. Our central contribution is twofold. First, we establish a general convergence analysis for QMC importance sampling with arbitrary transport maps, identifying sufficient growth conditions for the $\mathcal{O}(N^{-1+\varepsilon})$ root-mean-square error rate. Second, we rigorously prove that the specific transport architecture of Flow Matching satisfies these conditions. Consequently, we establish a $\mathcal{O}(N^{-1+\varepsilon})$ root-mean-square error for the unbiased FM-ISQMC estimator, extending classical QMC theory to the realm of generative models. Numerical experiments validate that FM-ISQMC consistently breaks through the error floor observed in direct transport methods, delivering superior precision. This work thus bridges the divide between deep generative modeling and numerical integration.

</details>


### [4] [SaddleScape V1.0: A Python Package for Constructing Solution Landscapes via High-index Saddle Dynamics](https://arxiv.org/abs/2601.01081)
*Yuyang Liu,Hua Su,Zixiang Xiao,Lei Zhang,Jin Zhao*

Main category: math.NA

TL;DR: SaddleScape V1.0 is a Python package for exploring solution landscapes in complex systems using High-index Saddle Dynamics (HiSD) to identify critical points like minima and saddle points.


<details>
  <summary>Details</summary>
Motivation: To provide researchers with an accessible tool for understanding the hierarchical structure of complex systems by systematically exploring solution landscapes, which reveal critical points and their relationships.

Method: Implements High-index Saddle Dynamics (HiSD) framework and variants including Generalized HiSD for non-gradient systems and Accelerated HiSD. Uses dynamic updating of state estimates and associated subspaces, supports automatic differentiation, numerical Hessian-vector products, eigenvalue solvers, and landscape construction algorithms.

Result: A comprehensive Python software package with user-friendly interface, flexible parameter configuration, visualization tools for trajectories and landscapes, and data export capabilities. The package efficiently implements advanced saddle dynamics for both gradient and non-gradient systems.

Conclusion: SaddleScape V1.0 facilitates solution landscape construction, empowering researchers across scientific disciplines to gain deeper insights into complex system hierarchies through accessible implementation of advanced saddle dynamics methods.

Abstract: We present SaddleScape V1.0, a Python software package designed for the exploration and construction of solution landscapes in complex systems. The package implements the High-index Saddle Dynamics (HiSD) framework and its variants, including the Generalized HiSD for non-gradient systems and the Accelerated HiSD. SaddleScape V1.0 enables the systematic identification of critical points, including both local minima and high-index saddle points, by dynamically updating both the state estimate and an associated subspace characterizing the saddle's local manifold. It supports both gradient systems, defined by energy functions/functionals, and general non-gradient autonomous dynamical systems. Key features include automatic differentiation for symbolic inputs, numerical approximation techniques for Hessian-vector products, diverse eigenvalue solvers, and algorithms for constructing solution landscapes. The software offers a user-friendly interface with flexible parameter configuration, tools for trajectory and landscape visualization, and data export capabilities. By providing an efficient and accessible implementation of advanced saddle dynamics, SaddleScape V1.0 facilitates the construction of solution landscapes, empowering researchers in various scientific disciplines to gain deeper insights into the hierarchical structure of complex systems. The source code is available at the repository https://github.com/HiSDpackage/saddlescape. The package's introductory website is available at https://hisdpackage.github.io/saddlescape.

</details>


### [5] [Adaptive finite difference methods for the Willmore flow: mesh redistribution algorithm and tangential velocity approach](https://arxiv.org/abs/2601.01433)
*Zhenghua Duan,Meng Li*

Main category: math.NA

TL;DR: Two adaptive finite difference methods for Willmore flow simulation using BDFk time discretization and monitor functions for dynamic mesh adaptation along evolving interfaces.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and accurate numerical methods for simulating Willmore flow that can handle complex interface geometries while maintaining computational efficiency and robustness.

Method: Two approaches: 1) Weighted arc-length equidistribution with adaptive monitor selection based on curvature and its variation; 2) Incorporation of tangential velocity into Willmore flow to eliminate explicit reparameterization, with mesh redistribution embedded in geometric evolution. Both use BDFk time discretization and monitor functions for dynamic mesh adaptation.

Result: The proposed BDFk-based adaptive schemes accurately capture geometric evolution of Willmore flow and exhibit excellent robustness and computational efficiency for problems involving complex interface geometries.

Conclusion: The developed adaptive finite difference methods with monitor functions provide effective numerical simulation of Willmore flow, with the second method enhanced by an energy-stable correction algorithm for theoretical stability guarantees.

Abstract: We develop two adaptive finite difference methods for the numerical simulation of the Willmore flow, employing the kth-order backward differentiation formula (BDFk) for time discretization, together with monitor functions for dynamic mesh adaptation along evolving interfaces. The first approach is based on a weighted arc-length equidistribution strategy driven by a monitor function to adaptively redistribute grid points. An adaptive monitor selection mechanism, constructed from the curvature and its variation, enhances spatial resolution in regions of strong geometric complexity while preserving mesh regularity. The second approach eliminates explicit reparameterization by incorporating a tangential velocity into the Willmore flow, with mesh redistribution inherently embedded in the geometric evolution. We further develop an energy-stable correction algorithm for the second method to guarantee discrete energy stability at the theoretical level. In both approaches, the monitor function serves as the core component of the adaptive framework, encoding essential geometric information -- such as curvature and curvature variation -- to guide mesh refinement and redistribution. Extensive numerical experiments demonstrate that the proposed BDFk-based adaptive schemes accurately capture the geometric evolution of the Willmore flow and exhibit excellent robustness and computational efficiency for problems involving complex interface geometries.

</details>


### [6] [Convergence Analysis of PINNs for Fractional Diffusion Equations in Bounded Domains](https://arxiv.org/abs/2601.01462)
*Elie Abdo,Lihui Chai,Ruimeng Hu,Xu Yang*

Main category: math.NA

TL;DR: PINNs converge for time-dependent fractional diffusion equations using spectrally-defined mollification to handle nonlocal operators and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: Fractional Laplacian operators in PDEs introduce nonlocal behavior and regularity constraints, and standard neural networks don't naturally enforce spectral boundary conditions, requiring new approaches for theoretical guarantees.

Method: Introduce spectrally-defined mollification strategy that preserves nonlocal operator structure while ensuring boundary compatibility, enabling rigorous energy estimates in Sobolev spaces.

Result: Prove convergence of PINN approximation in any space-time Sobolev norm H^k (k ∈ ℕ) for time-dependent fractional diffusion equations on bounded domains.

Conclusion: Mollified residuals enable theoretical guarantees for neural-network-based solvers of nonlocal PDEs, showing compatibility of PINN approximations with classical energy estimates.

Abstract: We establish the convergence of physics-informed neural networks (PINNs) for time-dependent fractional diffusion equations posed on bounded domains. The presence of fractional Laplacian operators introduces nonlocal behavior and regularity constraints, and standard neural network approximations do not naturally enforce the associated spectral boundary conditions. To address this challenge, we introduce a spectrally-defined mollification strategy that preserves the structure of the nonlocal operator while ensuring boundary compatibility. This enables the derivation of rigorous energy estimates in Sobolev spaces. Our results rely on analytical tools from PDE theory, highlighting the compatibility of PINN approximations with classical energy estimates for nonlocal equations. We prove convergence of the PINN approximation in any space-time Sobolev norm $H^k$ (with $k \in \N)$. The analysis highlights the role of mollified residuals in enabling theoretical guarantees for neural-network-based solvers of nonlocal PDEs.

</details>


### [7] [A parametric Keldysh decomposition](https://arxiv.org/abs/2601.01553)
*Linus Balicki,Mark Embree,Serkan Gugercin*

Main category: math.NA

TL;DR: Extension of Keldysh decomposition for parametric nonlinear eigenvalue problems, enabling contour integral algorithms for parameter-dependent matrix functions.


<details>
  <summary>Details</summary>
Motivation: Contour integral algorithms are effective for computing eigenvalues in complex plane regions, but existing methods don't handle parameter-dependent nonlinear eigenvalue problems where matrix functions depend analytically on additional parameters.

Method: Propose parametric Keldysh decomposition that extends the standard decomposition to matrix-valued functions with analytic parameter dependence. Develop algorithm for solving parametric nonlinear eigenvalue problems based on this decomposition.

Result: Established key properties of the parametric Keldysh decomposition and introduced a new algorithm for parametric nonlinear eigenvalue problems that leverages contour integration principles.

Conclusion: The parametric Keldysh decomposition enables extension of contour integral methods to parameter-dependent nonlinear eigenvalue problems, providing a foundation for efficient eigenvalue computation in parameter-varying systems.

Abstract: Contour integral algorithms seek to compute a small number of eigenvalues located within a bounded region of the complex plane. These methods can be applied to both linear and nonlinear matrix eigenvalue problems. In the latter case, the foundation of these methods comes from the Keldysh decomposition, which breaks the nonlinear matrix-valued function into two parts: a rational function whose poles match the desired eigenvalues, and a remainder term that is analytic within the target region. Under contour integration this analytic part vanishes (via Cauchy's theorem), leaving only the component containing the desired eigenvalues. We propose an extension of the Keldysh decomposition for matrix-valued functions that depend analytically on an additional parameter. We establish key properties of this parametric Keldysh decomposition, and introduce an algorithm for solving parametric nonlinear eigenvalue problems that is based upon it.

</details>


### [8] [A Unified Equilibrated Flux Recovery Framework with Robust A Posteriori Error Estimation](https://arxiv.org/abs/2601.01585)
*Cuiyu He*

Main category: math.NA

TL;DR: EARM is a unified flux-recovery framework for elliptic interface problems that works across various finite element discretizations (DG and conforming), dimensions (2D/3D), and polynomial orders, producing efficient recovered fluxes with proven conservation properties.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for a general framework for flux recovery in elliptic interface problems that can handle different finite element discretizations, dimensions, and polynomial orders while maintaining computational efficiency and robustness with respect to coefficient jumps.

Method: Develops Equilibrated Averaging Residual Method (EARM) as a unified flux-recovery framework. For DG methods, EARM yields explicit recovered fluxes. For conforming discretizations, proposes Orthogonal Null-space-Eliminated EARM (ON-EARM) to ensure uniqueness by restricting correction flux to orthogonal complement of divergence-free null space.

Result: Proves local conservation and establishes robust a posteriori error estimator for recovered flux in 2D with robustness measured with respect to jumps in diffusion coefficient. Numerical results in 2D and 3D confirm theoretical findings.

Conclusion: EARM provides a comprehensive framework for flux recovery in elliptic interface problems that is applicable to various discretizations, dimensions, and polynomial orders, with proven conservation properties and robust error estimation.

Abstract: We introduce the Equilibrated Averaging Residual Method (EARM), a unified equilibrated flux-recovery framework for elliptic interface problems that applies to a broad class of finite element discretizations. The method is applicable in both two and three dimensions and for arbitrary polynomial orders, and it enables the construction of computationally efficient recovered fluxes. We develop EARM for both discontinuous Galerkin (DG) and conforming finite element discretizations. For DG methods, EARM can be applied directly and yields an explicit recovered flux that coincides with state-of-the-art conservative flux reconstructions.
  For conforming discretizations, we further propose the Orthogonal Null-space--Eliminated EARM (ON-EARM), which ensures uniqueness by restricting the correction flux to the orthogonal complement of the divergence-free null space. We prove local conservation and establish a robust a~posteriori error estimator for the recovered flux in two dimensions, with robustness measured with respect to jumps in the diffusion coefficient. Numerical results in two and three dimensions confirm the theoretical findings.

</details>


### [9] [A generalized Scharfetter-Gummel scheme for nonlocal cross-diffusion systems](https://arxiv.org/abs/2601.01731)
*Ansgar Jüngel,Panchi Li,Zhiwei Sun*

Main category: math.NA

TL;DR: Implicit Euler finite-volume scheme for nonlocal cross-diffusion system preserves positivity, mass, and entropy structure; convergence proven despite degeneracy challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop and analyze a numerical scheme for nonlocal cross-diffusion systems that describe population dynamics with repulsive/attractive interactions. Such systems are challenging due to nonlocal flux terms and degeneracy issues in numerical approximations.

Method: Uses implicit Euler finite-volume scheme with generalized Scharfetter-Gummel discretization for nonlocal flux terms. The scheme preserves positivity, total mass, and entropy structure even for integrable kernel functions. Overcomes degeneracy by proving uniform estimates for discrete Fisher information using both Boltzmann and Rao entropy inequalities.

Result: Proves existence of discrete solutions and convergence to continuous solutions as mesh size tends to zero. Numerical simulations in 1D and 2D demonstrate scheme features. Key achievement is handling degeneracy in generalized Bernoulli function through Fisher information estimates.

Conclusion: The developed scheme successfully handles nonlocal cross-diffusion systems with rigorous mathematical analysis, overcoming degeneracy challenges while preserving essential physical properties like positivity and entropy structure.

Abstract: An implicit Euler finite-volume scheme for a nonlocal cross-diffusion system on the multidimensional torus is analyzed. The equations describe the dynamics of population species with repulsive or attractive interactions. The numerical scheme is based on a generalized Scharfetter-Gummel discretization of the nonlocal flux term. For merely integrable kernel functions, the scheme preserves the positivity, total mass, and entropy structure. The existence of a discrete solution and its convergence to a solution to the continuous problem, as the mesh size tends to zero, are shown. A key difficulty is the degeneracy of the generalized Bernoulli function in the Scharfetter-Gummel approximation. This issue is overcome by proving a uniform estimate for the discrete Fisher information, which requires both the Boltzmann and Rao entropy inequalities. Numerical simulations illustrate the features of the scheme in one and two space dimensions.

</details>


### [10] [Implicit and implicit--explicit high-order BDF methods for coupled elliptic--parabolic systems](https://arxiv.org/abs/2601.01742)
*Georgios Akrivis,Minghua Chen,Fan Yu*

Main category: math.NA

TL;DR: Higher-order (up to 6th) implicit and implicit-explicit BDF schemes for coupled elliptic-parabolic systems, with decoupled IMEX variants for efficiency and novel multipliers for error analysis.


<details>
  <summary>Details</summary>
Motivation: Existing first-order schemes exist, but extending to higher-order schemes is challenging due to difficulties in constructing G-stability matrices. There's a need for higher-order methods that maintain computational efficiency while improving accuracy.

Method: Develop fully implicit and implicit-explicit BDF schemes up to 6th order. IMEX variants are decoupled for efficiency. Use novel multipliers and energy technique for convergence analysis. Weak coupling condition required for IMEX schemes but not for fully implicit ones.

Result: Successfully developed higher-order schemes (3rd-6th order) that greatly improve accuracy with almost the same computational cost as first-order schemes. Established error estimates via energy technique with novel multipliers.

Conclusion: Higher-order BDF schemes provide significant accuracy improvements over first-order methods with comparable computational cost, offering efficient alternatives for solving coupled elliptic-parabolic systems.

Abstract: First-order fully implicit as well as implicit--explicit schemes for coupled elliptic-parabolic systems are discussed in [Ern and Meunier, ESAIM: M2AN, 2009] and [Altmann et al., Math.\ Comp., 2021], respectively. The extension of the analysis to higher-order (third-, fourth-, fifth-, and sixth-order) schemes is not straightforward since explicitly constructing $G$ matrices (G-stability) is often tricky. In this article, we develop fully implicit as well as implicit--explicit backward difference formula (BDF) schemes of order up to six. The implicit--explicit variants are decoupled, thereby enhancing computational efficiency; their convergence analysis requires a weak coupling condition on the poroelastic parameters. In contrast, no coupling conditions are needed for the fully implicit, coupled schemes. We determine novel and suitable multipliers for the two proposed classes and establish error estimates via the energy technique. A prominent advantage of these higher-order schemes is that, with almost the computational cost of first-order schemes, they greatly improve the accuracy.

</details>


### [11] [A Wachspress-based transfinite formulation for exactly enforcing Dirichlet boundary conditions on convex polygonal domains in physics-informed neural networks](https://arxiv.org/abs/2601.01756)
*N. Sukumar,Ritwick Roy*

Main category: math.NA

TL;DR: A Wachspress-based transfinite formulation for exact Dirichlet boundary enforcement in PINNs on convex polygons, using geometric blending functions to lift boundary conditions to interior.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in previous methods that used approximate distance functions for exact Dirichlet boundary enforcement in physics-informed neural networks, particularly addressing issues with bounded Laplacian and providing a framework for parametrized convex geometries.

Method: Uses Wachspress coordinates for n-gons in transfinite interpolation to lift boundary functions to interior. The trial function combines neural network output with boundary extension using transfinite interpolant g, ensuring kinematic admissibility in deep Ritz method.

Result: The method provides exact enforcement of Dirichlet boundary conditions with bounded Laplacian, generalizes bilinear Coons interpolation to convex polygons, and enables solving problems on parametrized convex geometries using neural networks.

Conclusion: The Wachspress-based transfinite formulation successfully addresses limitations of previous approaches, providing accurate boundary enforcement for physics-informed neural networks and deep Ritz method on convex polygonal domains with applications to forward, inverse, and parametrized geometric problems.

Abstract: In this paper, we present a Wachspress-based transfinite formulation on convex polygonal domains for exact enforcement of Dirichlet boundary conditions in physics-informed neural networks. This approach leverages prior advances in geometric design such as blending functions and transfinite interpolation over convex domains. For prescribed Dirichlet boundary function $\mathcal{B}$, the transfinite interpolant of $\mathcal{B}$, $g : \bar P \to C^0(\bar P)$, $\textit{lifts}$ functions from the boundary of a two-dimensional polygonal domain to its interior. The trial function is expressed as the difference between the neural network's output and the extension of its boundary restriction into the interior of the domain, with $g$ added to it. This ensures kinematic admissibility of the trial function in the deep Ritz method. Wachspress coordinates for an $n$-gon are used in the transfinite formula, which generalizes bilinear Coons transfinite interpolation on rectangles to convex polygons. The neural network trial function has a bounded Laplacian, thereby overcoming a limitation in a previous contribution where approximate distance functions were used to exactly enforce Dirichlet boundary conditions. For a point $\boldsymbol{x} \in \bar{P}$, Wachspress coordinates, $\boldsymbolλ : \bar P \to [0,1]^n$, serve as a geometric feature map for the neural network: $\boldsymbolλ$ encodes the boundary edges of the polygonal domain. This offers a framework for solving problems on parametrized convex geometries using neural networks. The accuracy of physics-informed neural networks and deep Ritz is assessed on forward, inverse, and parametrized geometric Poisson boundary-value problems.

</details>


### [12] [Sharp inverse statements for kernel approximation: Superconvergence and saturation](https://arxiv.org/abs/2601.01808)
*Tizian Wenzel*

Main category: math.NA

TL;DR: Sharp inverse and saturation theorems for kernel-based approximation with finitely smooth Sobolev kernels on bounded Lipschitz domains, establishing one-to-one correspondence between target function smoothness and achievable approximation rates.


<details>
  <summary>Details</summary>
Motivation: To extend existing kernel approximation theory beyond the escaping-the-native-space regime and provide a unified characterization covering the full scale of admissible smoothness spaces, particularly focusing on the superconvergence regime where direct statements have only recently been obtained.

Method: Analysis of kernel-based approximation using finitely smooth Sobolev kernels on bounded Lipschitz regions, establishing sharp inverse and saturation statements in the superconvergence regime, with smoothness quantified through power spaces.

Result: Establishes a one-to-one correspondence between the smoothness of target functions (quantified via power spaces) and achievable approximation rates by kernel-based approximation, extending results beyond escaping-the-native-space regime.

Conclusion: The theory provides a complete and unified characterization of kernel-based approximation covering the full range of admissible smoothness spaces, with sharp inverse and saturation statements for finitely smooth Sobolev kernels on bounded Lipschitz domains.

Abstract: This article establishes sharp inverse and saturation statements for kernel-based approximation using finitely smooth Sobolev kernels on bounded Lipschitz regions. The analysis focuses on the superconvergence regime, for which direct statements have only recently been obtained. The resulting theory yields a one-to-one correspondence between the smoothness of a target function - quantified in terms of power spaces - and the achievable approximation rates by kernel-based approximation. In this way, we extend existing results beyond the escaping-the-native-space regime and provide a unified characterization covering the full scale of admissible smoothness spaces.

</details>


### [13] [Approximation for stochastic time-space fractional cable equations driven by rough noise](https://arxiv.org/abs/2601.01889)
*Jiawei He,Jianhua Huang,Fang Su*

Main category: math.NA

TL;DR: Numerical analysis of stochastic nonlinear time-space fractional cable equation with rough noise using spectral Galerkin and backward Euler convolution quadrature methods.


<details>
  <summary>Details</summary>
Motivation: The time-space fractional cable equation models anomalous diffusion processes, extending generalized fractional Ohm's law. The paper addresses the need for numerical methods to solve stochastic nonlinear versions of this equation driven by rough noise.

Method: Operator theoretic approach for existence/uniqueness proofs; Wong-Zakai approximation to regularize rough noise; spatial discretization via spectral Galerkin method; temporal discretization via backward Euler convolution quadrature method.

Result: Established existence, uniqueness, and regularity of solutions; obtained convergence for regularized equation via Wong-Zakai approximation; developed numerical scheme with error estimates.

Conclusion: The paper provides a comprehensive numerical framework for solving stochastic nonlinear time-space fractional cable equations with rough noise, including theoretical foundations and practical implementation with error analysis.

Abstract: The time-space fractional cable equation arises from extending the generalized fractional Ohm's law to model anomalous diffusion processes. In this paper, we develop and analyze a numerical approximation for stochastic nonlinear time-space fractional cable equation driven by rough noise. The model involves both two nonlocal terms in time and one in space. By an operator theoretic approach, we establish the existence, uniqueness, and regularities of solutions. We also obtain a convergence result for the regularized equation via Wong-Zakai approximation to regularize the rough noise. The numerical scheme approximates the model in space by the standard spectral Galerkin method and in time by the backward Euler convolution quadrature method. After that, error estimates are established.

</details>


### [14] [An explicit scheme for stochastic Allen-Cahn equations with space-time white noise near the sharp interface limit](https://arxiv.org/abs/2601.01894)
*Yingsong Jiang,Chenxu Pang,Xiaojie Wang*

Main category: math.NA

TL;DR: The paper develops an explicit exponential integrator for Allen-Cahn SPDEs with space-time white noise near the sharp interface limit, achieving polynomial dependence on ε⁻¹ and T in error bounds with mild time-step restrictions.


<details>
  <summary>Details</summary>
Motivation: Standard numerical methods for Allen-Cahn type SPDEs near the sharp interface limit (ε→0) suffer from severe time-step restrictions (τ=O(ε^σ)) and exponential error dependence on ε⁻¹ and T, making them impractical for small ε values.

Method: Proposes an explicit exponential integrator with modified nonlinearity for Allen-Cahn SPDEs driven by space-time white noise. Uses refined regularity estimates for Kolmogorov equations with non-smooth test functions and introduces a new nonlinearity modification strategy to handle low regularity, super-linear drift growth, and the sharp interface limit.

Result: Achieves uniform-in-time and uniform-in-ε moment bounds, convergence in total variation distance of order O(T·Poly(ε⁻¹)τ^γ) with γ<½, polynomial dependence on ε⁻¹ and T (vs exponential), mild ε-independent time-step restriction, and uniform-in-time error bound O(τ^γ) for fixed ε=1 that matches classical weak convergence rates.

Conclusion: The proposed scheme overcomes severe limitations of existing methods for Allen-Cahn SPDEs near sharp interface limits, providing practical numerical methods with polynomial error dependence and mild time-step restrictions, confirmed by numerical experiments.

Abstract: This article investigates time-discrete approximations of Allen-Cahn type SPDEs driven by space-time white noise near the sharp interface limit $ε\to 0$, where the small parameter $ε$ is the diffuse interface thickness. We propose an explicit and easily implementable exponential integrator with a modified nonlinearity for the considered problem. Uniform-in-time and uniform-in-$ε$ moment bounds of the scheme are established and the convergence in total variation distance of order $O(T\cdot\text{Poly}(ε^{-1})τ^γ),γ<\tfrac12$ is established, between the law of the numerical scheme and that of the SPDE over $[0,T]$. In contrast to the exponential dependence due to standard arguments, the obtained error bound depends on $ε^{-1}$ and $T$ polynomially. By incorporating carefully chosen method parameters, we only require a mild and $ε$-independent restriction on the time step-size $τ$, getting rid of the severe restriction $τ=O(ε^σ),σ\geq1$ in the literature. Also, a uniform-in-time error bound of order $O(τ^γ),γ<\tfrac12$, is obtained for a fixed $ε=1$, which improves the existing ones in the literature and matches the classical weak convergence rate in the globally Lipschitz setting. The error analysis is highly nontrivial due to the low regularity of the considered problem, the super-linear growth of the drift, the non-smooth observables inherent in the total variation metric and the presence of the small interface parameter $ε\to0$. These difficulties are addressed by introducing a new strategy of nonlinearity modification and establishing refined regularity estimates for the associated Kolmogorov equation to an auxiliary process with non-smooth test functions. Numerical experiments confirm the theoretical convergence and the ability of interface-capturing for the proposed scheme.

</details>


### [15] [An unstructured second-order subgrid method for the shallow water equations](https://arxiv.org/abs/2601.01895)
*Max Ebstrup Bitsch,Irene Torpe Heilmann,Allan Peter Engsig-Karup,Ole Rene Sørensen,Jesper Grooss*

Main category: math.NA

TL;DR: A subgrid method for shallow water equations using unstructured triangular meshes with local refinement for bathymetry resolution, maintaining computational efficiency while capturing small-scale features.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient method for solving shallow water equations that can incorporate fine-scale bathymetry features without requiring a globally fine mesh, which would be computationally expensive.

Method: Uses unstructured triangular mesh with local subgrid refinement for bathymetry representation while keeping velocities on coarse mesh. Employs second-order WENO method for spatial discretization, explicit second-order Runge-Kutta for temporal integration, novel subgrid face value reconstruction for partially wet cells, and new gravity source term discretization.

Result: The scheme is demonstrated to be well-balanced on subgrid level, achieves second-order accuracy, handles moving flood and dry boundaries effectively, and shows benefits of increasing subgrid cells for improved finite volume method results.

Conclusion: The proposed subgrid method successfully balances computational efficiency with accurate representation of fine-scale bathymetry features, providing a practical approach for shallow water simulations with complex topography.

Abstract: We propose a new unstructured numerical subgrid method for solving the shallow water equations using a finite volume method with enhanced bathymetry resolution. The method employs an unstructured triangular mesh with support for triangulation of elements to form finer subgrids locally. The bathymetry is represented on the fine mesh, allowing the incorporation of small-scale features, while velocities are defined on the coarse mesh. The governing equations are solved numerically on the coarse mesh, making the method computationally cheaper compared to a traditional fine mesh computation. To accurately represent the velocities, we employ a second-order accurate WENO method and for temporal integration an explicit second-order accurate Runge-Kutta method. Furthermore, we present a novel subgrid face value reconstruction that accounts for partially wet cells, where only some of the subgrid cells are wet. Together with a newly developed gravity source term discretization, we demonstrate that the scheme is well-balanced on the subgrid level. Finally, the subgrid method is validated on several test cases to show: i) that the scheme is well-balanced, ii) confirm the order of the accuracy of the scheme, and iii) demonstrate the capability of handling moving flood and dry boundaries. We also highlight when it is beneficial to increase the number of subgrid cells to improve the results of the finite volume method.

</details>


### [16] [Energy Conserving Data Driven Discretizations for Maxwells Equations](https://arxiv.org/abs/2601.01902)
*Victory Obieke*

Main category: math.NA

TL;DR: Learning energy-preserving convolution stencils for Maxwell's equations from spectral data via convex optimization with skew-symmetry constraints.


<details>
  <summary>Details</summary>
Motivation: To develop data-driven spatial discretizations for Maxwell's equations that preserve important physical properties like energy conservation while maintaining accuracy comparable to traditional methods.

Method: Learn linear convolution stencils from high-fidelity spectral data by solving convex quadratic optimization with linear constraints enforcing skew-adjointness of discrete derivative operators, ensuring semi-discrete energy identity.

Result: Energy-constrained learned stencils achieve accuracy comparable to standard central differences while exactly preserving discrete electromagnetic energy under Crank-Nicolson time-stepping; ADMM and interior-point methods produce nearly identical operators with ADMM offering favorable tradeoff.

Conclusion: The proposed framework successfully learns energy-preserving spatial discretizations for Maxwell's equations from data, with ADMM being an efficient solver for the constrained optimization problem, enabling accurate simulations that maintain important physical properties.

Abstract: We study data-driven construction of spatial discretizations for the one-dimensional Maxwell system. Given high-fidelity training data generated by a spectral discretization, we learn a linear convolution stencil that approximates the spatial derivative operator appearing in Maxwell's equations. The stencil is obtained by solving a convex quadratic optimization problem, subject to linear constraints that enforce skew-adjointness of the discrete derivative. These constraints guarantee a semi-discrete energy identity for the resulting Maxwell system. We prove that our constraints characterize the class of skew-symmetric convolution operators and express the associated numerical wave speed and CFL restriction for the classical leapfrog scheme in terms of the learned stencil's Fourier symbol. We then compare several convex solvers for the resulting quadratic program -- projected gradient, Nesterov-accelerated gradient, ADMM, and an interior-point reference implemented in CVXPY -- and evaluate the learned schemes in time-dependent one-dimensional Maxwell simulations using a Crank--Nicolson (CN) time discretization. Our numerical experiments show that (i) energy-constrained learned stencils achieve accuracy comparable to standard central differences while exactly preserving the discrete electromagnetic energy under CN time-stepping, and (ii) ADMM and interior-point methods produce nearly identical operators, with ADMM offering a favorable tradeoff between accuracy, constraint satisfaction, and runtime.

</details>


### [17] [Multigoal-oriented adaptive finite element method with convergence rates](https://arxiv.org/abs/2601.01965)
*Roland Becker,Maximilian Brunner,Paula Hilbert,Michael Innerberger,Dirk Praetorius*

Main category: math.NA

TL;DR: Goal-oriented adaptive FEM for elliptic PDEs with multiple linear goal functionals, solving only two linear systems per iteration while adapting meshes to handle all singularities simultaneously.


<details>
  <summary>Details</summary>
Motivation: Need efficient adaptive finite element methods that can handle multiple goal functionals simultaneously without requiring separate computations for each functional, while maintaining optimal convergence rates.

Method: Goal-oriented adaptive finite element method for symmetric linear elliptic PDEs that solves only two linear systems per iteration, adapts triangulations to resolve all singularities simultaneously, and uses the same discrete space for all finite element solutions.

Result: The algorithm guarantees optimal convergence rates in an appropriate sense, with numerical experiments validating the theoretical results.

Conclusion: The proposed method efficiently handles multiple linear goal functionals with guaranteed optimal convergence, offering computational advantages over existing approaches.

Abstract: We formulate and analyze a goal-oriented adaptive finite element method for a symmetric linear elliptic partial differential equation (PDE) that can simultaneously deal with multiple linear goal functionals. In each step of the algorithm, only two linear finite element systems have to be solved. Moreover, all finite element solutions are computed with respect to the same discrete space, while the underlying triangulations are adapted to resolve all inherent singularities simultaneously. Unlike available results for such a setting in the literature, we give a thorough convergence analysis and verify that our algorithm guarantees, in an appropriate sense, even optimal convergence rates. Numerical experiments underline the derived theoretical results.

</details>


### [18] [Locally-averaged McCormick relaxations for discretization-regularized inverse problems](https://arxiv.org/abs/2601.01995)
*Barbara Kaltenbacher,Paul Manns*

Main category: math.NA

TL;DR: A method for computing approximate dual bounds in PDE coefficient identification using McCormick relaxation, local averaging, and optimization-based bound tightening, with proven regularization and convergence.


<details>
  <summary>Details</summary>
Motivation: To develop a global optimization approach for coefficient identification in PDEs that provides reliable dual bounds for branch-and-bound methods, addressing the challenges of bilinear terms and discretization errors in inverse problems.

Method: Uses McCormick relaxation to linearize bilinear PDE terms, local averaging to reduce inequality constraints, optimization-based bound tightening to tighten relaxations, and combines discretization error quantification with noise propagation analysis.

Result: The approach proves that the resulting discretization regularizes the inverse problem, leading to an overall convergent scheme, with numerical experiments validating the theoretical findings.

Conclusion: The proposed framework provides a rigorous method for computing dual bounds in PDE coefficient identification that ensures convergence and regularization, making it suitable for global optimization via branch-and-bound methods.

Abstract: In this paper, by means of a standard model problem, we devise an approach to computing approximate dual bounds for use in global optimization of coefficient identification in partial differential equations (PDEs) by, e.g., (spatial) branch-and-bound methods. Linearization is achieved by a McCormick relaxation (that is, replacing the bilinear PDE term by a linear one and adding inequality constraints), combined with local averaging to reduce the number of inequalities. Optimization-based bound tightening allows us to tighten the relaxation and thus reduce the induced error. Combining this with a quantification of the discretization error and the propagated noise, we prove that the resulting discretization regularizes the inverse problem, thus leading to an overall convergent scheme. Numerical experiments illustrate the theoretical findings.

</details>


### [19] [Asymptotic condition numbers for linear ordinary differential equations: the generic real case](https://arxiv.org/abs/2601.02079)
*Stefano Maset*

Main category: math.NA

TL;DR: This paper analyzes asymptotic behaviors of condition numbers for real linear ODEs in generic cases, extending previous work on complex linear ODEs.


<details>
  <summary>Details</summary>
Motivation: Previous work studied condition numbers for complex linear ODEs with relative error measurements. This paper extends that analysis to real linear ODEs in generic cases to understand how perturbations propagate in real systems.

Method: The paper analyzes asymptotic behaviors of two types of condition numbers for real linear ODEs: directional pointwise condition numbers (for specific perturbation directions) and pointwise condition numbers (for worst-case perturbation directions).

Result: The paper determines the asymptotic (long-time) behaviors of these condition numbers for real linear ordinary differential equations in generic cases.

Conclusion: The analysis provides deeper understanding of condition number behaviors for real linear ODEs, extending previous complex case results to real systems with practical applications.

Abstract: The paper \cite{M0} studied, for a \emph{complex} linear ordinary differential equation $y^\prime(t)=Ay(t)$, the long-time propagation to the solution $y(t)$ of a perturbation of the initial value. By measuring the perturbations with relative errors, this paper introduced a directional pointwise condition number, defined for a specific initial value and for a specific direction of perturbation of this initial value, and a pointwise condition number, defined for a specific initial value and the worst-case scenario for the direction of perturbation. The asymptotic (long-time) behaviors of these two condition numbers were determined. The present paper analyzes such asymptotic behaviors in depth, for a \emph{real} linear ordinary differential equation in a generic case.

</details>


### [20] [A quasi-orthogonal iterative method for eigenvalue problems](https://arxiv.org/abs/2601.02108)
*Shengyue Wang,Aihui zhou*

Main category: math.NA

TL;DR: Proposes a quasi-orthogonal iterative method for large-scale eigenvalue problems that eliminates explicit orthogonalization, reduces computational/communication costs, and improves parallel scalability while maintaining numerical orthogonality.


<details>
  <summary>Details</summary>
Motivation: Traditional eigenvalue methods for large-scale problems requiring many orthogonal eigenvectors suffer from high computational/communication costs and limited parallel scalability due to explicit orthogonalization requirements.

Method: A quasi-orthogonal iterative method that dispenses with explicit orthogonalization and orthogonal initial data. The method inherently preserves quasi-orthogonality (iterates asymptotically tend to be orthogonal) and enhances robustness against numerical perturbations.

Result: Rigorous analysis confirms energy-decay property and convergence of energy, gradient, and iterate. Numerical experiments validate theoretical results, demonstrate strong robustness and high-precision numerical orthogonality preservation.

Conclusion: The proposed iterative method serves as an efficient, stable alternative for large-scale eigenvalue computations, addressing scalability limitations of traditional methods while maintaining numerical orthogonality.

Abstract: For large-scale eigenvalue problems requiring many mutually orthogonal eigenvectors, traditional numerical methods suffer substantial computational and communication costs with limited parallel scalability, primarily due to explicit orthogonalization. To address these challenges, we propose a quasi-orthogonal iterative method that dispenses with explicit orthogonalization and orthogonal initial data. It inherently preserves quasi-orthogonality (the iterates asymptotically tend to be orthogonal) and enhances robustness against numerical perturbations. Rigorous analysis confirms its energy-decay property and convergence of energy, gradient, and iterate. Numerical experiments validate the theoretical results, demonstrate key advantages of strong robustness and high-precision numerical orthogonality preservation, and thereby position our iterative method as an efficient, stable alternative for large-scale eigenvalue computations.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [21] [A new partial differential nonlinear system containing quasivariational and parabolic variational inequalities and its application](https://arxiv.org/abs/2601.00934)
*Wei Li,Zhenghui Tang,Zengbao Wu,Chunyan Yang*

Main category: math.AP

TL;DR: Study of a complex nonlinear system with PDE, quasivariational inequality, and parabolic variational inequality in Banach spaces, applied to viscoelastic frictional contact problems with memory, wear, and damage.


<details>
  <summary>Details</summary>
Motivation: To analyze complex mechanical contact problems involving multiple physical phenomena (viscoelasticity, friction, wear, damage) that require coupled mathematical formulations beyond standard PDEs.

Method: Use Banach's fixed point theorem to prove unique solvability of the coupled system (PDE + quasivariational inequality + parabolic variational inequality) under moderate conditions.

Result: Established unique solvability of the nonlinear system and applied results to analyze viscoelastic frictional contact problems with long-memory effects, wear processes, and damage phenomenon.

Conclusion: The mathematical framework successfully models complex contact mechanics with multiple coupled phenomena, providing rigorous existence and uniqueness results for practical engineering applications.

Abstract: We study a new nonlinear system which contains a partial differential equation, a quasivariational inequality and a parabolic variational inequality in Banach spaces. We obtain the unique solvability of the coupled system under moderate conditions by using the Banach's fixed point theorem. We employ the main results to investigate a viscoelastic frictional contact problem with long-memory effects, wear processes, and damage phenomenon.

</details>


### [22] [Dimension reduction for gradient damage models in slender rods](https://arxiv.org/abs/2601.01001)
*E. Bonnetier,D. Henao,V. Ramos*

Main category: math.AP

TL;DR: A 3D gradient damage model is reduced to a 1D model for slender rods using Γ-convergence, showing that as the radius-to-length ratio approaches zero, the 3D energy functional converges to a 1D functional with uniaxial deformation.


<details>
  <summary>Details</summary>
Motivation: To develop a simplified 1D damage model for slender rods from a more complex 3D gradient damage formulation, enabling easier analysis and computation for structures with small radius-to-length ratios.

Method: Non-dimensionalization and rescaling of the 3D problem onto a unit cylinder, followed by analysis using Γ-convergence as the radius-to-length ratio δ→0. Compactness results and lower/upper bound inequalities establish convergence properties.

Result: The sequence of 3D energy functionals Γ-converges to a 1D functional. Minimizers of the 3D energy converge to minimizers of the 1D energy, with strains approaching diagonal form indicating uniaxial deformation.

Conclusion: The reduction from 3D to 1D is mathematically rigorous via Γ-convergence, providing a simplified model for slender rods that captures essential damage mechanics while reducing computational complexity.

Abstract: This paper presents a method for reducing a three-dimensional gradient damage model to a one-dimensional model for slender rods (with a small radius-to-length ratio, $δ= R/L \to 0$). The 3D model minimizes an energy functional that includes elastic strain energy, a damage-dependent degradation function $a_η(α)$, a damage energy term $w(α)$, and a gradient term penalizing abrupt damage variations. After non-dimensionalizing and rescaling, the problem is reformulated on a unit cylinder, and the behaviour of the energy functional is analyzed as $δ$ approaches zero. Using $Γ$-convergence, we show that the sequence of 3D energy functionals converges to a 1D functional, defined over displacement and damage fields that are independent of transverse coordinates. Compactness results guarantee the weak convergence of strains and damage gradients, while lower and upper bound inequalities confirm the energy limit. Minimizers of the 3D energy are proven to converge to the minimizers of the 1D energy, with strains approaching a diagonal form indicative of uniaxial deformation.

</details>


### [23] [Irregular Diffusions and Loss of Regularity in Polyconvex Gradient Flows](https://arxiv.org/abs/2601.01035)
*Bin Guo,Seonghak Kim,Baisheng Yan*

Main category: math.AP

TL;DR: The paper develops a convex integration method for constructing infinitely many Lipschitz weak solutions to irregular diffusion-type PDEs that are nowhere smooth, even with smooth data.


<details>
  <summary>Details</summary>
Motivation: To understand and construct solutions for diffusion-type PDEs that exhibit irregular behavior, where weak solutions are nowhere smooth despite smooth initial/boundary data, challenging classical regularity expectations.

Method: Reformulate diffusion equations as first-order PDE relations, adapt convex integration method, introduce new geometric structures called $\mathcal{T}_N$-configurations, and develop simplified structural hypothesis (Condition $O_N$) on diffusion functions.

Result: Under Condition $O_N$, initial-boundary value problems with certain smooth data admit infinitely many Lipschitz weak solutions that are nowhere $C^1$. Specific $\mathcal{T}_N$-configurations are analyzed with nondegeneracy conditions essential for verifying Condition $O_N$.

Conclusion: The construction reveals failure of regularity and uniqueness even for polyconvex gradient flows, demonstrating that strongly polyconvex energy functionals can generate irregular diffusion equations with non-unique, non-smooth solutions.

Abstract: We investigate diffusion-type partial differential equations that are irregular in the sense that they admit weak solutions which are nowhere smooth, even for prescribed smooth data. By reformulating these equations as first-order partial differential relations and adapting the method of convex integration, we develop a construction scheme based on new geometric structures, referred to as $\mathcal{T}_N$-configurations, together with a simplified structural hypothesis on the diffusion functions, termed Condition $O_N$. Under this condition, we show that the associated initial and boundary value problems with certain smooth initial-boundary data admit infinitely many Lipschitz weak solutions that are nowhere $C^1$. We further analyze specific $\mathcal{T}_N$-configurations and establish nondegeneracy conditions that are essential for verifying Condition $O_N$. As an application, we construct examples of strongly polyconvex energy functionals whose gradient flows generate irregular diffusion equations, thereby revealing a failure of regularity and uniqueness even within the class of polyconvex gradient flows.

</details>


### [24] [Normalized Solutions for Schrödinger-Bopp-Podolsky Systems with Critical Choquard-Type Nonlinearity on Bounded Domains](https://arxiv.org/abs/2601.01098)
*Li Chen,Li Wang*

Main category: math.AP

TL;DR: The paper studies normalized solutions for a critical Schrödinger-Bopp-Podolsky system with Riesz potential nonlinearity on bounded domains, proving existence of multiple normalized solutions for small mass constraints.


<details>
  <summary>Details</summary>
Motivation: To investigate normalized solutions (solutions with prescribed L²-norm) for a coupled Schrödinger-Bopp-Podolsky system with critical nonlinearities involving Riesz potentials, which arises in quantum physics and models interactions between charged particles.

Method: Uses a special minimax principle combined with a truncation technique to handle the critical nonlinearity and mass constraint, working under Navier boundary conditions on smooth bounded domains in ℝ³.

Result: Proves existence of a threshold b* > 0 such that for all b ∈ (0, b*), the system admits multiple normalized solutions, establishing multiplicity results for small mass constraints.

Conclusion: The Schrödinger-Bopp-Podolsky system with critical Riesz potential nonlinearity possesses multiple normalized solutions for sufficiently small prescribed mass, demonstrating rich solution structure despite the critical nature of the problem.

Abstract: In this paper, we study normalized solutions for the following critical Schrödinger-Bopp-Podolsky system: $$-Δu + q(x)φu = λu + |u|^{p-2}u + \bigl(I_α* |u|^{3+α}\bigr)|u|^{1+α}u,\quad \text{in } Ω_r,$$ $$-Δφ+ Δ^2φ= q(x)u^2, \ \qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \text{ in } Ω_r,$$ where $Ω_r \subset \mathbb R^3$ is a smooth bounded domain, $p \in \left(2, \frac{8}{3}\right)$, $q(x) \in C(\barΩ_r) \backslash \{0\}$ and $λ\in \mathbb R$ is the Lagrange multiplier associated with the constraint $\int_{Ω_r} |u|^2\, \mathrm d x = b^2$ for some $b > 0$. Here $α> 0$, $I_α$ denotes the Riesz potential, and the domain parameter $r$ reflects the size of $Ω_r$ whose precise definition will be given in Section 3. By applying a special minimax principle together with a truncation technique, we prove that there exists $b^* > 0$ such that the system admits multiple normalized solutions whenever $b \in (0, b^*)$ under Navier boundary conditions.

</details>


### [25] [Asymptotic stability of steady states for the compressible Navier-Stokes-Riesz system in the presence of vacuum](https://arxiv.org/abs/2601.01161)
*José A. Carrillo,Renjun Duan,Aneta Wróblewska-Kamińska,Junhao Zhang*

Main category: math.AP

TL;DR: Global existence and stability of strong solutions for a 1D compressible Navier-Stokes-Riesz system with vacuum free boundary and attractive Riesz potential.


<details>
  <summary>Details</summary>
Motivation: Study physical vacuum free boundary problems for compressible fluids with non-local attractive Riesz potentials, addressing challenges from both vacuum degeneracy and non-locality.

Method: Analyze the system in Lagrangian coordinates, establish Lyapunov-type stability of compactly supported steady states, and use weighted Sobolev spaces to handle vacuum boundary behavior.

Result: Prove unique global-in-time strong solution existence for adiabatic constant γ satisfying 2(1-s)<γ<1+2s/3 with 3/8<s<1/2, and obtain time convergence rates to steady states.

Conclusion: Successfully overcome combined challenges of vacuum free boundary degeneracy and non-local Riesz potential, establishing global well-posedness and stability for this class of problems.

Abstract: We consider a one-dimensional physical vacuum free boundary problem on the compressible Navier-Stokes-Riesz system for an attractive Riesz potential $|x|^{2s-1}/(2s-1)$ with $0<s<1/2$. It is proved that for the adiabatic constant $γ$ satisfying $2(1-s)<γ<1+2s/3$ under the additional condition that $3/8<s<1/2$, there exists a unique global-in-time strong solution. Specifically, we establish the Lyapunov-type stability of the compactly supported steady states in the Lagrangian coordinates and we also obtain the time rate of convergence for the strong solution to steady states with the same mass in weighted Sobolev spaces where the weights indicate the behavior of solutions near the vacuum free boundary. The difficulties and challenges in the proof are caused not only by the degeneracy due to the vacuum free boundary but also by the non-local feature of the Riesz potential.

</details>


### [26] [On the stability of degenerate Schrödinger equation under boundary fractional damping](https://arxiv.org/abs/2601.01286)
*Fatiha Chouaou,Abbes Benaissa*

Main category: math.AP

TL;DR: Study of degenerate Schrödinger equation with fractional boundary damping: well-posedness and stability analysis with exponential/polynomial decay rates.


<details>
  <summary>Details</summary>
Motivation: To analyze the mathematical properties of degenerate Schrödinger equations with fractional boundary damping, which have applications in quantum mechanics and wave propagation in non-uniform media.

Method: First establish well-posedness of the degenerate problem with Dirichlet-Neumann boundary conditions, then use multiplier method to prove stability and decay rates.

Result: Proved well-posedness of the degenerate Schrödinger equation and established both exponential and polynomial decay rates for the solution using multiplier techniques.

Conclusion: The degenerate Schrödinger equation with fractional boundary damping is well-posed and exhibits both exponential and polynomial stability, with decay rates established through multiplier method analysis.

Abstract: In this paper we study the well-posedness and stability of degenerate Schrödinger equation with a fractional boundary damping. First, we establish the well-posedness of the degenerate problem $ψ_t(x,t)-\imath(τ(x) ψ_x(x,t))_x=0, \hbox{ with } x \in (0,1)$, controlled by Dirichlet-Neumann conditions. Then, exponential and polynomial decay rate of the solution are established using multiplier method.

</details>


### [27] [Positive weak solutions of a double-phase variable exponent problem with a fractional-Hardy-type singular potential and superlinear nonlinearity](https://arxiv.org/abs/2601.01346)
*Mustafa Avci*

Main category: math.AP

TL;DR: Existence of nontrivial positive weak solutions for a double-phase variable exponent problem with singular fractional-Hardy potential using variational methods.


<details>
  <summary>Details</summary>
Motivation: To study complex nonlinear problems involving double-phase operators with variable exponents and singular potentials, which arise in various physical and engineering applications where material properties change with location or conditions.

Method: Variational framework approach using Mountain-Pass theorem and strong minimum principle to establish existence of solutions.

Result: Proved existence of at least one nontrivial positive weak solution for the double-phase variable exponent problem with singular fractional-Hardy potential.

Conclusion: The variational methods successfully establish solution existence for this complex nonlinear problem, demonstrating the effectiveness of combining Mountain-Pass theorem with strong minimum principle for such singular potential problems.

Abstract: In the present paper, we study a double-phase variable exponent problem which is set up within a variational framework including a singular potential of fractional-Hardy-type. We employ the Mountain-Pass theorem and the strong minimum principle to obtain the existence of at least one nontrivial positive weak solution.

</details>


### [28] [The unique limit of the Glimm-Lax construction for Sobolev data and obstructions to 1-d convex integration](https://arxiv.org/abs/2601.01349)
*Jeffrey Cheng,Cooper Faile,Sam G. Krupa*

Main category: math.AP

TL;DR: The paper establishes uniqueness for Glimm-Lax solutions to 1D hyperbolic conservation laws with initial data in Sobolev spaces, and shows limitations on non-uniqueness results for Hölder continuous solutions.


<details>
  <summary>Details</summary>
Motivation: While Glimm & Lax (1970) proved existence of global weak entropy solutions for 1D hyperbolic conservation laws with small initial data, and recent work by Bressan, Marconi & Vaidya (2025) provided partial uniqueness and stability results, there remains a need to establish stronger uniqueness properties for solutions with Sobolev regularity and understand limitations of non-uniqueness phenomena.

Method: The authors combine recent advances in L¹-stability theory (Bressan, Marconi & Vaidya, 2025) with L²-theory developments. They develop a weighted relative entropy contraction for perturbations of rarefaction waves as an auxiliary tool, and apply these techniques to analyze solution uniqueness in various function spaces.

Result: 1) Solutions with initial data in H^s (s>0) are unique within the full class of Glimm-Lax solutions that decay in total variation at rate 1/t. 2) The non-uniqueness result of Chen, Vasseur & Yu for continuous solutions cannot extend to C^α solutions for α > 1/2 or to certain fractional Sobolev spaces W^{s,p}. 3) Development of weighted relative entropy contraction for rarefaction wave perturbations.

Conclusion: The paper strengthens uniqueness theory for 1D hyperbolic conservation laws by establishing uniqueness for Sobolev-regular solutions and showing limitations on potential non-uniqueness in Hölder and fractional Sobolev spaces, while providing new analytical tools for studying solution stability.

Abstract: We consider a genuinely nonlinear $1$-d system of hyperbolic conservation laws with two unknowns. A famous construction of Glimm & Lax shows that global-in-time "Glimm-Lax" weak entropy solutions exist in this setting for any initial data with small $L^\infty$ norm [Mem. Amer. Math. Soc. (1970), no. 101]. Recent work in the $L^1$-stability theory by Bressan, Marconi & Vaidya has given the first partial uniqueness and stability results for these solutions [Arch. Ration. Mech. Anal. (2025), vol. 249]. In this paper, we build on these results by combining them with recent advances in the $L^2$-theory. We show that solutions with initial data in the Sobolev space $H^s$ for $s>0$ are unique in the full class of Glimm--Lax solutions that decay in total variation at a rate of $1/t$. As a secondary result, our techniques are also used to show the recent non-uniqueness result of Chen, Vasseur & Yu for continuous solutions (arxiv:2407.02927) cannot extend to $C^α$ solutions for $α> 1/2$, alongside some appropriate fractional Sobolev spaces $W^{s,p}$. An auxiliary result of independent interest is the development of a weighted relative entropy contraction for perturbations of rarefaction waves.

</details>


### [29] [On the well-posedness of two-dimensional Muskat problem with an elastic interface](https://arxiv.org/abs/2601.01374)
*Lizhe Wan,Jiaqi Yang*

Main category: math.AP

TL;DR: The paper analyzes the 2D Muskat problem with nonlinear elastic interface, proving local well-posedness for arbitrary initial data and global well-posedness for small initial data in specific cases.


<details>
  <summary>Details</summary>
Motivation: To study the mathematical properties of the Muskat problem (fluid interface dynamics) when coupled with nonlinear elastic effects, extending previous work by Nguyen on this interface problem.

Method: Following Nguyen's framework [35,36], the authors analyze both one-phase and two-phase scenarios using functional analysis in Sobolev spaces H^s, establishing well-posedness through rigorous mathematical proofs.

Result: 1) Local well-posedness in H^s for s≥2 for arbitrary initial data. 2) Global well-posedness for small initial data in H^s when s>3/2 for one-phase case and stable two-phase case (ρ⁺ ≤ ρ⁻).

Conclusion: The Muskat problem with nonlinear elastic interface is mathematically well-posed both locally and globally under appropriate conditions, providing a solid theoretical foundation for studying these coupled fluid-elastic interface dynamics.

Abstract: We investigate the two-dimensional Muskat problem with a nonlinear elastic interface, for both one-phase and two-phase scenarios. Following the framework developed by Nguyen [35,36], we demonstrate that the problem is locally well-posed in $H^s$ for $s\geq 2$ for arbitrary initial data. Furthermore, for the one-phase case and the stable two-phase case $(ρ^+ \leq ρ^-)$, we establish global well-posedness for small initial data in $H^s$ when $s> \frac{3}{2}$.

</details>


### [30] [Construction of Solutions with Extraordinary Gradient Amplification and Localization for Schrödinger Equations](https://arxiv.org/abs/2601.01389)
*Huaian Diao,Xieling Fan,Hongyu Liu*

Main category: math.AP

TL;DR: Constructs solutions to Schrödinger-type equations with prescribed extreme gradient amplification localized near specific boundary points, demonstrating deterministic localization phenomena analogous to quantum effects.


<details>
  <summary>Details</summary>
Motivation: To understand and mathematically demonstrate deterministic analogues of quantum localization phenomena (like Anderson localization) in Schrödinger dynamics, showing how extreme gradient amplification can be highly localized through carefully designed input profiles.

Method: Design smooth initial and/or boundary data for linear and nonlinear Schrödinger-type equations in 2D and 3D that create prescribed gradient amplification near specific points on the boundary of the domain's support. The approach leverages the structure of the equations combined with carefully engineered input profiles.

Result: For any finite time interval, any collection of distinct boundary points, and any amplitude threshold, one can design data such that spatial gradients exceed the threshold near these points outside the support region. The amplification is highly localized, with the spatial measure of high-gradient regions tending to zero as the threshold increases.

Conclusion: The results provide a deterministic mathematical manifestation of localization phenomena in quantum systems, showing a trade-off between extreme spatial localization and large gradient amplification that complements the Heisenberg uncertainty principle in a deterministic, localized spatial gradient framework.

Abstract: This paper constructs solutions to linear and nonlinear Schrödinger-type equations in two and three spatial dimensions that exhibit prescribed, extraordinary gradient amplification and localization. For any finite time interval $[0,T]$, any prescribed collection of $n\in\mathbb{N}$ distinct points on $\partial D$, where $D$ is the compact support of the anisotropic coefficients, lower-order terms, or nonlinearities, and any amplitude threshold $\mathcal{M}>0$, we show that one can design smooth initial and/or boundary data such that the spatial gradients of the resulting solutions exceed $\mathcal{M}$ in neighborhoods of these points outside $D$ for almost every $t\in[0,T]$. Moreover, the ratio between the local $C^{1,\frac12}$-norm of the solution near each prescribed point outside $D$ and the $C^{1,\frac12}$-norm inside $D$ is bounded from below by $\mathcal{M}/2$ for almost every $t\in[0,T]$.
  We further prove that the spatial measure of the regions where the gradient magnitude exceeds $\mathcal{M}$ tends to zero as $\mathcal{M}\to\infty$, demonstrating that the amplification phenomenon is highly localized. This effect arises from the structure of the
  Schrödinger-type equation combined with carefully designed input profiles. From a physical perspective, the results provide a deterministic analogue of localization phenomena observed in quantum scattering and Anderson localization. In addition, the observed trade-off between extreme spatial localization and large gradient amplification is fully consistent with the spirit of the Heisenberg uncertainty principle: while the latter is traditionally formulated in a global $L^2$ space--frequency framework, our results offer a complementary deterministic manifestation at the level of localized spatial gradients in Schrödinger dynamics.

</details>


### [31] [Fragmentation-coagulation processes with advection or diffusion in space](https://arxiv.org/abs/2601.01453)
*Jacek Banasiak,Nduduzo Majozi*

Main category: math.AP

TL;DR: The paper develops mathematical theory for advection/diffusion-fragmentation-coagulation equations with unbounded kernels, proving existence of positive semigroups and classical solvability in weighted L1 spaces.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for continuous fragmentation-coagulation models where particles are transported through advection or diffusion, particularly for systems with unbounded coagulation kernels that appear in physical applications.

Method: Proves new results on generation of C0-semigroups with parameter and applies them to the Abstract Cauchy Problem associated with advection/diffusion-fragmentation models. Uses weighted L1 spaces with respect to particle mass and spatial variables.

Result: Shows that the advection/diffusion-fragmentation problem generates a positive C0-semigroup in spaces L1(R+, X_x, (1+m^r)dm) for sufficiently large weight exponent r, enabling classical solvability for equations with unbounded coagulation kernels.

Conclusion: The developed semigroup theory provides a rigorous mathematical framework for analyzing a wide range of advection/diffusion-fragmentation-coagulation equations with physically relevant unbounded kernels, establishing classical solvability in appropriate function spaces.

Abstract: In this paper, we consider a continuous fragmentation--coagulation model in which the reacting particles can be transported in physical space through either advection or diffusion. We prove new results on the generation of $C_0$-semigroups with parameter and use them to show that the Abstract Cauchy Problem associated with a more general version of the advection/diffusion--fragmentation problem generates a positive $C_0$-semigroup in spaces $L_1(\mathbb R_+, X_x, (1+m^r)dm),$ where $m$ is the particle mass, $X_x$ is either the space of integrable or continuous functions with respect to the spatial variable, and the weight exponent $r$ is sufficiently large. These results enable us to prove the classical solvability of a wide range of advection/diffusion--fragmentation--coagulation equations with unbounded coagulation kernels.

</details>


### [32] [Interpolative Refinement of Gap Bound Conditions for Singular Parabolic Double Phase Problems](https://arxiv.org/abs/2601.01571)
*Bogi Kim,Jehan Oh*

Main category: math.AP

TL;DR: The paper establishes gradient higher integrability results for weak solutions to singular parabolic double phase equations under specific boundedness or continuity assumptions, refining gap bounds in this setting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study gradient regularity for weak solutions to inhomogeneous singular parabolic double phase equations, which combine p-growth and q-growth terms with a variable coefficient a(x,t). These equations arise in various applications and understanding their regularity properties is fundamental to PDE theory.

Method: The authors analyze weak solutions to the singular parabolic double phase equation using PDE techniques. They establish gradient higher integrability results under two alternative assumptions: either the solution is bounded (u∈L∞) with a specific bound on q relative to p and α, or the solution has certain continuity in time (u∈C(0,T;L^s(Ω))) with a different bound on q.

Result: The main results are gradient higher integrability theorems for weak solutions. Under either boundedness assumption with q ≤ p + α(p(n+2)-2n)/4, or continuity assumption with q ≤ p + αμ_s/(n+s) where μ_s = (p(n+2)-2n)s/4, the authors prove improved integrability properties for the gradient of solutions.

Conclusion: The paper provides refined gap bounds for singular parabolic double phase equations through interpolation techniques. These results advance the understanding of regularity properties for solutions to these important classes of partial differential equations and establish connections between solution boundedness/continuity and gradient integrability.

Abstract: We consider inhomogeneous singular parabolic double phase equations of type $$ u_t-\operatorname{div}(|Du|^{p-2}Du + a(x,t)|Du|^{q-2}Du)=-\operatorname{div} (|F|^{p-2}F + a(x,t)|F|^{q-2}F) $$ in $Ω_T := Ω\times (0,T)\subset \mathbb{R}^n\times \mathbb{R}$, where $\frac{2n}{n+2}<p\leq 2$, $p<q$ and $0\leq a(\cdot)\in C^{α,\fracα{2}}(Ω_T)$. We establish gradient higher integrability results for weak solutions to the above problems under one of the following two assumptions: $$ u\in L^\infty (Ω_T) \quad\text{and}\quad q\leq p +\frac{α(p(n+2)-2n)}{4}, $$ or $$ u\in C(0,T;L^s(Ω)),\quad s\geq 2 \quad\text{and}\quad q\leq p+\frac{αμ_s}{n+s}, $$ where $μ_s := \frac{(p(n+2)-2n)s}{4}$. These results yield an interpolation refinement of gap bounds in the singular parabolic double phase setting.

</details>


### [33] [Free boundary problem for two-dimensional ElectroHydroDynamic Equations with a gravity field](https://arxiv.org/abs/2601.01717)
*Lili Du,Yuanhong Zhao*

Main category: math.AP

TL;DR: Analysis of singular profiles in two-phase electrohydrodynamic free boundary problems near stagnation points, showing how electric field decay rate determines whether Stokes corners, asymmetric corners, or cusps form.


<details>
  <summary>Details</summary>
Motivation: To extend the Stokes conjecture (proved by Varvaruca and Weiss for one-phase fluids) to two-phase electrohydrodynamic flows, characterizing how electric fields affect singularity formation at stagnation points where fluid velocity vanishes.

Method: Variational principles and geometric methods using key technical tools: Weiss-type monotonicity formula, frequency formula, and concentration-compactness arguments.

Result: Discovery of critical decay rate for electric field near stagnation points that classifies singular profiles: faster decay yields Stokes corners; critical decay yields either Stokes or asymmetric corners; slower decay yields cusp singularities.

Conclusion: Electric field decay rate crucially determines singularity type in two-phase electrohydrodynamic free boundary problems, with three distinct regimes identified based on field strength relative to critical threshold.

Abstract: This paper studies a two-phase free boundary problem governed by the ElectroHydroDynamic equations, which describes a perfectly conducting, incompressible, irrotational fluid with gravity, surrounded by a dielectric gas. The interface separating fluid and gas is referred to as the free boundary. 
It is known that the free surface remains smooth away from the stagnation points, where the relative velocity of the incompressible fluid vanishes. In the presence of gravity, the Stokes conjecture, proved by Varvaruca and Weiss [Acta. Math. 206, 363-403, (2011)], implies that the corner type singularity will occur in the one-phase incompressible fluid. It is natural to ask whether this conjecture still holds in the two-phase flow problem. As a consequence, the primary objective of this work is to characterize the possible singular profiles of the free interface near the stagnation points in the presence of an electric field. 
Our main result is the discovery of a critical decay rate of the electric field near the stagnation points which indicates the classification of the singular profiles of the free surface. More precisely, we showed that when the decay rate of the electric field is faster than the critical decay rate, its negligible effect implies that the singular profile must be the well-known Stokes corner. When the electric field decays as the critical decay rate, the symmetry of the corner region may be broken, giving rise to either a Stokes corner or an asymmetric corner as the possible singular profile. If the decay rate is slower than the critical decay rate, the electric field dominates and completely destroys the corner structure, resulting in a cusp singularity. The analysis of these singularities relies on variational principles and geometric methods. Key technical tools include a Weiss-type monotonicity formula, a frequency formula, and a concentration-compactness argument.

</details>


### [34] [Energy decay of a viscoelastic wave equation with variable exponent logarithmic nonlinearity and weak damping](https://arxiv.org/abs/2601.01752)
*Qingqing Peng,Yikan Liu*

Main category: math.AP

TL;DR: Study of energy decay for viscoelastic wave equation with variable exponents logarithmic nonlinearity and weak damping, establishing explicit general decay results under mild conditions on relaxation function.


<details>
  <summary>Details</summary>
Motivation: To investigate energy decay properties of solutions to viscoelastic wave equations with complex nonlinearities (variable exponents logarithmic nonlinearity and weak damping), extending existing decay results under more general conditions on the relaxation function.

Method: Mathematical analysis of viscoelastic wave equation with variable exponents logarithmic nonlinearity and weak damping in bounded domains. Uses relaxation function conditions g'(t)≤-ζ(t)G(g(t)) and special case g'(t)≤-ξ(t)g^q(t) with 1≤q<2 to derive decay estimates.

Result: Established explicit general decay result under mild conditions on relaxation function g. Derived refined decay estimate improving existing results under general assumption g'(t)≤-ζ(t)G(g(t)). Obtained uniform exponential and polynomial decay rates under special case g'(t)≤-ξ(t)g^q(t) with 1≤q<2, extending earlier studies restricted to 1≤q<3/2.

Conclusion: The paper successfully extends decay analysis for viscoelastic wave equations with complex nonlinearities, providing improved decay estimates under more general conditions on relaxation functions and expanding the range of admissible q values for polynomial decay results.

Abstract: In this paper, we investigate the energy decay of the solution to a viscoelastic wave equation with variable exponents logarithmic nonlinearity and weak damping in a bounded domain. We establish an explicit general decay result under mild conditions on the relaxation function $g$. Furthermore, under the general assumption $g'(t)\leq-ζ(t)G(g(t))$ with some suitably given $ζ$ and $G$, we derive a refined decay estimate improving existing results. In particular, uniform exponential and polynomial decay rates are obtained under a further special situation $g'(t)\leq-ξ(t)g^q(t)$ with $1\leq q<2$, extending earlier studies that were restricted to the case $1\leq q<\frac{3}{2}$.

</details>


### [35] [Self-Similar Solutions and Global Existence for Nonlinear Reaction-Diffusion Systems in Industrial Ammonia Synthesis](https://arxiv.org/abs/2601.01799)
*Jamshid Khasanov,Sokhibjan Muminov,Sardor Jumaniyozov,Oybek Djabborov,Khudayberganov Shuhrat*

Main category: math.AP

TL;DR: Lie group analysis applied to ammonia synthesis reaction-diffusion equations yields self-similar solutions, reduced ODEs, and conditions for global existence in slow/fast diffusion regimes with asymptotic power-law behavior.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical foundations for predicting and optimizing ammonia production in catalytic reactors by analyzing nonlinear reaction-diffusion equations modeling industrial ammonia synthesis.

Method: Applied Lie group analysis to construct self-similar solutions, derived reduced ODE systems, used comparison principles and barrier techniques to establish existence conditions, performed asymptotic analysis near diffusion fronts, and conducted numerical simulations.

Result: Established sufficient conditions for global-in-time solutions in both slow-diffusion (γ_i > 0) and fast-diffusion (γ_i < 0) regimes, derived explicit power-law decay exponents for concentration profiles, and demonstrated spatio-temporal evolution through numerical simulations.

Conclusion: The study provides rigorous mathematical tools for ammonia production optimization with potential extensions to other chemically reacting systems, offering both theoretical foundations and practical insights for industrial applications.

Abstract: This paper investigates a system of nonlinear reaction-diffusion equations modeling the industrial synthesis of ammonia. By applying Lie group analysis, we construct self-similar solutions and derive a reduced system of ordinary differential equations. Using comparison principles and barrier techniques, we establish sufficient conditions for the existence of global-in-time solutions in both slow-diffusion ($γ_i > 0$) and fast-diffusion ($γ_i < 0$) regimes. Detailed asymptotic analysis near the diffusion front reveals power-law behavior of concentration profiles, with explicit expressions for the decay exponents. The theoretical results are illustrated by numerical simulations, demonstrating the spatio-temporal evolution of reactant concentrations under realistic parameter values. The study provides rigorous mathematical foundations for predicting and optimizing ammonia production in catalytic reactors, with potential extensions to other chemically reacting systems.

</details>


### [36] [Liouville type theorems for some $(p,q)$-Laplace equations with gradient dependent reaction on Riemannian manifolds](https://arxiv.org/abs/2601.01899)
*Youde Wang,Liqin Zhang*

Main category: math.AP

TL;DR: The paper studies gradient estimates and Liouville theorems for solutions to nonlinear elliptic equations involving p-Laplacian and q-Laplacian operators on Riemannian manifolds, using Bochner formula, Sobolev inequalities, and Nash-Moser iteration.


<details>
  <summary>Details</summary>
Motivation: To understand the local and global behaviors of solutions to combined p-Laplacian and q-Laplacian equations on complete Riemannian manifolds, particularly establishing gradient estimates and proving Liouville-type results for non-negative solutions.

Method: Combines Bochner formula, Saloff-Coste's Sobolev inequality, and Nash-Moser iteration method to derive gradient estimates. Uses integral estimate method specifically for proving Liouville theorems for non-negative entire solutions.

Result: Obtains concise gradient estimates for solutions to the nonlinear elliptic equation. Proves Liouville-type theorems, including that non-negative entire solutions to Δ_p u + Δ_q u = 0 on complete non-compact Riemannian manifolds with non-negative Ricci curvature must be constant when n ≤ p ≤ q.

Conclusion: The combined analytical techniques successfully establish gradient estimates and Liouville results for mixed p-q Laplacian equations on Riemannian manifolds, providing important classification results for solutions under geometric curvature conditions.

Abstract: In this paper, we combine Bochner formula, Saloff-Coste's Sobolev inequality and the Nash-Moser iteration method to study the local and global behaviors of solutions to the nonlinear elliptic equation $Δ_pu+Δ_qu+h(u,|\nabla u|^2)=0$ defined on a complete Riemannian manifold $\left(M,g\right)$, where $q\ge p>1$, $h\in C^1(\mathbb{R}\times\mathbb{R}^{+})$ and $Δ_z u=\mathrm{div}\left(\left|\nabla u\right|^{z-2}\nabla u\right)$, with $z\in\{ p,~q\}$, is the usual $z$-Laplace operator. Under some assumptions on $p$, $q$ and $h(x,y)$, we derive concise gradient estimates for solutions to the above equation and then obtain some Liouville type theorems. In particular, we use integral estimate method to show that, if $u$ is a non-negative entire solution to $Δ_p u +Δ_q u=0$ ($n\le p\le q$) on a complete non-compact Riemannian manifold $M$ with non-negative Ricci curvature and $\dim M = n\ge2$, then $u$ is a trivial constant solution.

</details>


### [37] [Qualitative Aspects of Periodic Traveling Waves for the Sinh-Gordon equation](https://arxiv.org/abs/2601.01923)
*Beatriz Signori Lonardoni,Fabio Natali*

Main category: math.AP

TL;DR: The paper analyzes periodic solutions of the sinh-Gordon equation, establishing existence via mountain pass theorem, constructing families via implicit function theorem, and analyzing spectral stability using Morse index theory and Hamiltonian-Krein index.


<details>
  <summary>Details</summary>
Motivation: To comprehensively analyze periodic solutions of the sinh-Gordon equation, including their existence, construction, spectral stability, and related Cauchy problem properties.

Method: Three-stage approach: 1) Existence of periodic solutions via mountain pass theorem for fixed wave speed; 2) Construction of solution families via implicit function theorem for fixed period; 3) Spectral stability analysis using monotonicity of period map, Morse index theory, and Hamiltonian-Krein index analysis.

Result: Established existence of periodic solutions, constructed families of solutions depending smoothly on wave speed, fully characterized non-positive spectrum of linearized operator, and determined spectral stability/instability of constructed waves.

Conclusion: The paper provides a complete framework for analyzing periodic sinh-Gordon solutions, connecting existence results with stability analysis, and linking spectral instability to blow-up phenomena in the Cauchy problem.

Abstract: This paper presents a comprehensive analysis of several aspects of the sinh-Gordon equation within a periodic setting. Our investigation proceeds in three main stages. First we establish the existence of periodic solutions for a fixed wave speed and varying periods by applying the mountain pass theorem. Subsequently, for a fixed period, we construct a family of periodic solutions that depend smoothly on the wave speed; this is achieved via the implicit function theorem. The spectral stability of these waves is then rigorously addressed. We perform a detailed spectral analysis of the linearized operator around the wave of fixed period. A key element in this analysis is the monotonicity of the period map, which, when combined with Morse index theory, enables us to fully characterize the non-positive spectrum of the projected operator in the space of zero-mean periodic functions. Finally, by employing the Hamiltonian-Krein index analysis, we determine the spectral stability and instability of the constructed waves. We additionally discuss qualitative aspects of the Cauchy problem associated with the sinh-Gordon equation, including local well-posedness and blow-up phenomena. The former supports a new linearization of the problem, while the latter predicts the spectral instability of the wave.

</details>


### [38] [A linear model of separation for western boundary currents with bathymetry](https://arxiv.org/abs/2601.01986)
*Anne-Laure Dalibard,Corentin Gentil*

Main category: math.AP

TL;DR: This paper analyzes strongly rotating/stratified fluids under β-plane approximation with strong topography, proposing a linear model that captures western boundary current separation through explicit asymptotic expansions with interior and boundary layer components.


<details>
  <summary>Details</summary>
Motivation: To develop a simplified linear model that captures the key feature of western boundary currents - separation from the coast - despite its simplicity, allowing for explicit computations to understand the interplay between rotation, stratification, and bathymetry.

Method: Uses asymptotic analysis of strongly rotating and stratified fluids under β-plane approximation in a 3D domain with strong topography. Constructs approximate solutions through asymptotic expansions with interior parts and two boundary layer types: Munk-type (quasi-geostrophic) and Ekman-type (non-geostrophic).

Result: Successfully constructs approximate solutions at any order and justifies their validity. The asymptotic expansion reveals intricate links between rotation, stratification and bathymetry. The Ekman part analysis is novel with properties differing from classical Ekman layers. Numerical illustrations confirm the desired separation behavior.

Conclusion: The proposed linear idealized model effectively captures western boundary current separation through explicit asymptotic analysis, revealing new insights about Ekman boundary layers and demonstrating the complex interplay between rotation, stratification, and topography in ocean dynamics.

Abstract: This paper is devoted to the asymptotic analysis of strongly rotating and stratified fluids, under a $β$-plane approximation, and within a three-dimensional spatial domain with strong topography. Our purpose is to propose a linear idealized model, which is able to capture one of the key features of western boundary currents, in spite of its simplicity: the separation of the currents from the coast. Our simplified framework allows us to perform explicit computations, and to highlight the intricate links between rotation, stratification and bathymetry. In fact, we are able to construct approximate solutions at any order for our system, and to justify their validity. Each term in the asymptotic expansion is the sum of an interior part and of two boundary layer parts: a ``Munk'' type boundary layer, which is quasi-geostrophic, and an ``Ekman part'', which is not. Even though the Munk part of the approximation bears some similarity with previously studied 2D models, the analysis of the Ekman part is completely new, and several of its properties differ strongly from the ones of classical Ekman layers. Our theoretical analysis is supplemented with numerical illustrations, which exhibit the desired separation behavior.

</details>


### [39] [Global Hilbert expansion for the ionic Vlasov-Poisson-Boltzmann system](https://arxiv.org/abs/2601.02006)
*Fucai Li,Yichun Wang*

Main category: math.AP

TL;DR: Global-in-time validity of Hilbert expansion for ionic Vlasov-Poisson-Boltzmann system, deriving compressible Euler-Poisson system as Knudsen number → 0.


<details>
  <summary>Details</summary>
Motivation: To rigorously justify the Hilbert expansion method for the ionic Vlasov-Poisson-Boltzmann system, which describes ion dynamics in dilute collisional plasmas, and to derive the limiting compressible Euler-Poisson system as the Knudsen number approaches zero.

Method: Hilbert expansion approach with multi-layered mathematical structure: expansion coefficients satisfy linear hyperbolic systems, while remainder equation couples with nonlinear Poisson equation for electrostatic potential. Uses refined elliptic estimates for exponential nonlinearities and new enclosed L²∩W¹,∞ estimates for potential-dependent terms.

Result: Successfully proves global-in-time validity of Hilbert expansion for the ionic Vlasov-Poisson-Boltzmann system in ℝ³, rigorously deriving the compressible Euler-Poisson system governing global smooth irrotational ion flows as Knudsen number → 0.

Conclusion: The Hilbert expansion method is globally valid for the ionic Vlasov-Poisson-Boltzmann system, providing rigorous mathematical foundation for deriving the compressible Euler-Poisson limit from kinetic theory in plasma physics.

Abstract: We justify the global-in-time validity of Hilbert expansion for the ionic Vlasov-Poisson-Boltzmann system in $\mathbb{R}^3$, a fundamental model describing ion dynamics in dilute collisional plasmas. As the Knudsen number approaches zero, we rigorously derive the compressible Euler-Poisson system governing global smooth irrotational ion flows. The truncated Hilbert expansion exhibits a multi-layered mathematical structure: the expansion coefficients satisfy linear hyperbolic systems, while the remainder equation couples with a nonlinear Poisson equation for the electrostatic potential. This requires refined elliptic estimates addressing the exponential nonlinearities and some new enclosed $L^2\cap W^{1,\infty}$ estimates for the potential-dependent terms.

</details>


### [40] [Dissipative solutions to a Beris-Edwards type model for compressible active nematic liquid crystals](https://arxiv.org/abs/2601.02051)
*Kuntal Bhandari,Apala Majumdar,Šárka Nečasová*

Main category: math.AP

TL;DR: Existence of dissipative solutions for compressible active nematic liquid crystals in 3D bounded domains with nonlinear viscosity and nonhomogeneous boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for compressible active nematic liquid crystal hydrodynamics in realistic settings with nonlinear viscosity, nonhomogeneous boundaries, and complex pressure potentials.

Method: Three-level approximation scheme: Galerkin approximation, parabolic regularization of continuity equation, and convex regularization of viscous stress potential. Uses Beris-Edwards type model in Landau-de Gennes framework.

Result: Proved existence of dissipative solutions (weak solutions with defect measure) for compressible active nematodynamics with nonlinear viscosity and nonhomogeneous boundary data.

Conclusion: Developed new techniques to handle non-Newtonian stress tensors, broader pressure potentials, and nonhomogeneous boundaries, establishing mathematical existence results for complex active nematic systems.

Abstract: We study the hydrodynamics of compressible active nematic liquid crystals in a three-dimensional and bounded domain, with a nonlinear viscosity tensor and nonhomogeneous boundary data, in a Landau-de Gennes framework. We prove the existence of dissipative solutions within a Beris-Edwards type model for active nematodynamics, which are weak solutions satisfying the underlying equations modulo a defect measure. The proof follows from a three level approximation scheme -- the Galerkin approximation, the classical parabolic regularization of the continuity equation, and the convex regularization of the potential generating the viscous stress. New techniques are required to deal with non-Newtonian stress tensor, larger classes of admissible pressure potentials and nonhomogeneous boundary conditions.

</details>


### [41] [Optimal Spectral Inequality for the Higher-Dimensional Landau Operator](https://arxiv.org/abs/2601.02093)
*Sedef Özcan,Matthias Täufer*

Main category: math.AP

TL;DR: Proves optimal spectral inequalities for Landau operators (Schrödinger operators with constant magnetic fields) in arbitrary dimensions, generalizing previous 2D results to higher dimensions with applications to control theory, spectral theory, and mathematical physics.


<details>
  <summary>Details</summary>
Motivation: Previous spectral inequality results for Landau operators were only known in dimension d=2. The paper aims to extend these results to arbitrary dimensions (d≥3), addressing the additional complications that arise in higher dimensions due to more complex magnetic Bernstein inequalities.

Method: Uses magnetic Bernstein estimates and analyticity, adapting Kovrijkine's approach from the Logvinenko-Sereda theorem. The strategy involves handling the more delicate estimates required for magnetic Bernstein inequalities in dimensions d≥3 compared to the two-dimensional case.

Result: Proves optimal spectral inequalities for Landau operators in full space and arbitrary dimension. These inequalities provide lower bounds on the L²-mass of functions in spectral subspaces when integrated over sampling sets S ⊂ ℝ^d.

Conclusion: Successfully generalizes spectral inequality results from 2D to arbitrary dimensions, overcoming the challenges posed by magnetic Bernstein inequalities in higher dimensions. The results have immediate applications in control theory, spectral theory, and mathematical physics.

Abstract: We prove optimal spectral inequalities for Landau operators in full space and in arbitrary dimension. Spectral inequalities are lower bounds on the L 2 -mass of functions in spectral subspaces of finite energy when integrated over a sampling set S $\subset$ R d . Landau operators are Schr{ö}dinger operators associated with a constant magnetic field of the form (-$\nabla$ + A(x)) 2 where A is a -in case of non-vanishing magnetic field -unbounded vector potential. Our strategy relies on so-called magnetic Bernstein estimates and analyticity, adapting an approach used by Kovrijkine in the context of the Logvinenko-Sereda theorem. We generalize results previously only known in dimension d = 2. The main difficulty in dimension d $\ge$ 3 are the magnetic Bernstein inequalities which, in comparison to the twodimensional case, lead to additional complications and require more delicate estimates. Our results have immediate consequences for control theory, spectral theory and mathematical physics which we comment on.

</details>


### [42] [Long time dynamics of the Nernst-Planck-Darcy System on $\mathbb{R}^3$](https://arxiv.org/abs/2601.02208)
*Elie Abdo,Joe Germany,Mohammad Khalil Hamdan,Kifah Kontar*

Main category: math.AP

TL;DR: Study of ionic electrodiffusion in porous media using Nernst-Planck equations, proving sharp decay rates for ionic concentrations and logarithmic growth of relative entropy over time.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time dynamics of ionic electrodiffusion in three-dimensional porous media, specifically how ionic concentrations evolve and dissipate over time in an incompressible fluid flow.

Method: Analysis of the Nernst-Planck equations describing N ionic species in 3D incompressible fluid through porous media, focusing on spatial derivatives decay in L^2 norm and relative entropy behavior.

Result: Proved that k-th spatial derivatives of each ionic concentration decay to zero in L^2 with sharp rate t^{-3/4 - k/2}, and relative entropy grows with sharp rate log t.

Conclusion: The system exhibits precise quantitative long-time behavior: ionic concentrations dissipate with polynomial decay rates while relative entropy exhibits logarithmic growth, providing complete characterization of electrodiffusion dynamics in porous media.

Abstract: We study ionic electrodiffusion modeled by the Nernst--Planck equations describing the evolution of $N$ ionic species in a three-dimensional incompressible fluid flowing through a porous medium. We address the long-time dynamics of the resulting system in the three-dimensional whole space $\mathbb{R}^3$. We prove that the $k$-th spatial derivatives of each ionic concentration decays to zero in $L^2$ with a sharp rate of order $t^{-\frac{3}{4}-\frac{k}{2}}$. Moreover, we investigate the behavior of the relative entropy associated with the model and show that it blows up in time with a sharp growth rate of order $\log t$.

</details>


### [43] [Semi-Classical Localization of the Schrödinger Resolvent on Closed Riemann Surfaces](https://arxiv.org/abs/2601.02274)
*Sébastien Campagne*

Main category: math.AP

TL;DR: The paper studies localization of semi-classical Schrödinger equation solutions on closed Riemann surfaces with irregular bounded potentials, using regularization to establish local-to-global estimates.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how irregular (merely bounded) potentials affect solution localization in semi-classical analysis, moving beyond classical studies that assume smooth potentials.

Method: Employ regularization techniques to handle the potential's lack of smoothness and establish a local-to-global estimate connecting local potential regularity to global solution concentration.

Result: Quantitative measure showing how local regularity of bounded potentials influences global concentration of Schrödinger equation solutions on closed Riemann surfaces.

Conclusion: The work bridges smooth and non-continuous regimes in semi-classical analysis by providing tools to study localization with irregular potentials.

Abstract: This paper investigates the localization properties of solutions to the semi-classical Schrödinger equation on closed Riemann surfaces. Unlike classical studies that assume a smooth potential, our work addresses the challenges arising from irregular potentials, specifically those that are merely bounded. We employ a regularization technique to manage the potential's lack of smoothness and establish a local-to-global estimate. This result provides a quantitative measure of how the local regularity of the potential influences the global concentration of the solution, thereby bridging the gap between smooth and non-continuous regimes in semi-classical analysis.

</details>


### [44] [Another look at regularity in transport-commutator estimates](https://arxiv.org/abs/2601.02326)
*Elias Hess-Childs,Matthew Rosenzweig,Sylvia Serfaty*

Main category: math.AP

TL;DR: The paper analyzes regularity requirements for transport velocity fields to control Riesz-type commutators, with applications to mean-field limits of particle systems with Riesz interactions.


<details>
  <summary>Details</summary>
Motivation: Understanding the regularity needed for velocity fields to control commutators is crucial for analyzing mean-field limits and fluctuations in particle systems with pairwise Riesz interactions, which have applications in physics and mathematics.

Method: The paper uses counterexample construction to show limitations of relaxing regularity assumptions, identifies trade-offs between interaction singularity and velocity regularity requirements, and employs defective commutator estimates via Brezis-Wainger-Hansson inequality.

Result: 1) L∞ gradient assumption cannot be relaxed to BMO in general (except 1D logarithmic endpoint). 2) Trade-off exists: smoother interactions require stronger velocity control. 3) Defective commutator estimates hold for almost-Lipschitz fields, enabling convergence rates in scaling-critical Sobolev spaces.

Conclusion: The paper establishes precise regularity requirements for velocity fields in commutator estimates for Riesz interactions, revealing fundamental limitations while providing positive results for almost-Lipschitz fields that enable practical applications in mean-field theory.

Abstract: We are interested in how regular a transport velocity field must be in order to control Riesz-type commutators. Estimates for these commutators play a central role in the analysis of the mean-field limit and fluctuations for systems of particles with pairwise Riesz interactions, which we start by reviewing. Our first new result shows that the usual $L^\infty$ assumption on the gradient of the velocity field cannot, in general, be relaxed to a BMO assumption. We construct counterexamples in all dimensions and all Riesz singularities $-2< s<d$, except for the one-dimensional logarithmic endpoint $s=0$. At this exceptional endpoint, such a relaxation is possible, a fact related to the classical Coifman-Rochberg-Weiss commutator bound for the Hilbert transform. Our second result identifies a trade-off between the singularity of the interaction potential and the required regularity of the velocity field. Roughly speaking, smoother (less singular) interactions require stronger velocity control if one wants a commutator estimate in the natural energy seminorm determined by the potential. We formulate this principle for a broad class of potentials and show that, in the sub-Coulomb Riesz regime, the velocity regularity appearing in the known commutator inequality is sharp. Despite these negative findings, we show as our third result that a defective commutator estimate holds for almost-Lipschitz transport fields. Such a defective estimate, which is a consequence of the celebrated Brezis-Wainger-Hansson inequality, allows us to prove rates of convergence when the mean-field density belongs to the scaling-critical Sobolev space.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [45] [New RVE concept and FFT methods in micromechanics of composites subjected to body force with compact support](https://arxiv.org/abs/2601.00822)
*Valeriy A. Buryachenko*

Main category: physics.comp-ph

TL;DR: A novel methodology for homogenization of periodic composite materials using specially designed body force fields to create boundary-effect-free reduced representative volume elements, enabling accurate machine learning surrogate models.


<details>
  <summary>Details</summary>
Motivation: To develop a physically grounded framework for homogenization of complex periodic composite materials that eliminates boundary artifacts from finite samples while capturing microstructural features and nonlocal interactions.

Method: Generate novel dataset using specially designed body force fields with compact support (BFCS) to create functionally reduced RVE without boundary artifacts. Use FFT-based solver for BFCS loading, perform translated averaging of DNS results, and train ML/NN models to learn effective nonlocal surrogate operators.

Result: The method produces accurate surrogate operators that predict macroscopic responses while reflecting microstructural features and nonlocal interactions, eliminating influences from finite sample size and boundary effects.

Conclusion: The proposed framework provides a physically grounded, data-driven approach for constructing accurate surrogate models for homogenization of complex periodic composite materials by accounting for field localization while removing boundary artifacts.

Abstract: We consider static linear elastic composite materials (CMs) with periodic structure. The core of the proposed methodology is the generation of a novel dataset using specially designed body force fields with compact support (BFCS), enabling a new RVE concept that reduces the infinite periodic medium to a finite domain without boundary artifacts. This functionally reduced RVE is used for translated averaging of direct numerical simulations (DNS) results, efficiently computed via a newly developed FFT-based solver for BFCS loading. The resulting dataset captures localized field responses and is used to train machine learning (ML) and neural networks (NN) models to learn effective nonlocal surrogate operators. These operators accurately predict macroscopic responses while reflecting microstructural features and nonlocal interactions. By accounting for field localization while simultaneously eliminating influences from finite sample size and boundary effects, it provides a physically grounded and data-driven framework for constructing accurate surrogate models for the homogenization of complex materials.

</details>


### [46] [AutoPot: Automated and massively parallelized construction of Machine-Learning Potentials](https://arxiv.org/abs/2601.01185)
*Max Hodapp,Guillaume Anciaux*

Main category: physics.comp-ph

TL;DR: AutoPot is a software framework that automates the construction and archiving of machine-learning interatomic potentials (MLIPs) to address the complexity of current MLIP training protocols.


<details>
  <summary>Details</summary>
Motivation: Current MLIP training requires complex protocols with active learning or fine-tuning, which are difficult to implement efficiently, interpret, analyze, and reproduce. There's a need for automated solutions to simplify MLIP development.

Method: AutoPot builds on BlackDynamite (for parallel parametric tasks) and Motoko (event-based workflow manager). It supports training configuration selection from large candidate sets and on-the-fly selection from MD simulations, using Moment Tensor Potentials (MLIP-2) and VASP for ab initio calculations.

Result: The software provides an automated framework for MLIP construction with flexibility to add other MLIP and ab initio codes, and to customize training protocols through Python functions without complex parsers.

Conclusion: AutoPot addresses the complexity of MLIP training by providing an automated, flexible, and extensible software framework that simplifies MLIP development while maintaining reproducibility and ease of customization.

Abstract: Machine-learning potentials (MLIPs) have been a breakthrough for computational physics in bringing the accuracy of quantum mechanics to atomistic modeling. To achieve near-quantum accuracy, it is necessary that neighborhoods contained in the training set are rather close to the ones encountered during a simulation. Yet, constructing a single training set that works well for all applications is, and likely will remain, infeasible, so, one strategy is to supplement training protocols for MLIPs with additional learning methods, such as active learning, or fine-tuning. This strategy, however, yields very complex training protocols that are difficult to implement efficiently, and cumbersome to interpret, analyze, and reproduce.
  To address the above difficulties, we propose AutoPot, a software for automating the construction and archiving of MLIPs. AutoPot is based on BlackDynamite, a software operating parametric tasks, e.g., running simulations, or single-point ab initio calculations, in a highly-parallelized fashion, and Motoko, an event-based workflow manager for orchestrating interactions between the tasks. The initial version of AutoPot supports selection of training configurations from large training candidate sets, and on-the-fly selection from molecular dynamics simulations, using Moment Tensor Potentials as implemented in MLIP-2, and single-point calculations of the selected training configurations using VASP. Another strength of AutoPot is its flexibility: BlackDynamite tasks and orchestrators are Python functions to which own existing code can be easily added and manipulated without writing complex parsers. Therefore, it will be straightforward to add other MLIP and ab initio codes, and manipulate the Motoko orchestrators to implement other training protocols.

</details>


### [47] [From Fermat's Principle to Physics-Informed Neural Networks: A Unified Computational Approach to Variational Physics](https://arxiv.org/abs/2601.01262)
*Aman Razdan,Aditya Shankar Mazumdar,Amit Tanwar,Pragati Ashdhir*

Main category: physics.comp-ph

TL;DR: This paper presents a computational approach to teaching variational principles in physics using modern tools like gradient descent, automatic differentiation, and PINNs, applied to classical and quantum problems.


<details>
  <summary>Details</summary>
Motivation: Traditional undergraduate instruction of variational principles remains primarily analytical, lacking integration with modern computational tools. The authors aim to bridge this gap by enhancing conceptual understanding while introducing students to contemporary research methodologies.

Method: Reformulate classical variational problems as optimization tasks and implement them using Python libraries (NumPy, Matplotlib, PyTorch, JAX). Use gradient descent, automatic differentiation, and Physics-Informed Neural Networks (PINNs) to solve problems across physics domains.

Result: Demonstrated successful application to a progression of problems: Snell's law from Fermat's principle, projectile motion, harmonic motion, nonlinear pendulum, heat conduction, double pendulum, vibrating strings, and quantum systems (hydrogen, helium atoms, silicon nucleus).

Conclusion: The computational approach enhances understanding of variational principles while equipping students with modern research skills applicable across classical, quantum, and nuclear physics domains.

Abstract: Variational principles are a unifying mathematical framework across many areas of physics, yet their instruction at the undergraduate level remains primarily analytical. This work presents a pedagogically oriented and computationally enhanced approach to variational modeling that integrates contemporary tools including gradient descent, automatic differentiation, and Physics-Informed Neural Networks (PINNs). Classical variational problems are reformulated as optimization tasks and implemented using open-source Python libraries such as NumPy, Matplotlib, PyTorch, and JAX. The proposed approach is demonstrated through a progression of problems drawn from standard undergraduate curricula, including the derivation of Snell's law from Fermat's principle, projectile motion with and without viscous drag, simple harmonic motion, nonlinear pendulum with damping, steady-state heat conduction governed by the Laplace and Poisson equations with nonlinear temperature-dependent internal heat generation, the double pendulum via the principle of least action, and variational treatments of vibrating strings. In addition, quantum mechanical applications are presented through variational solutions of the hydrogen atom, helium atom, and a schematic nuclear model of the silicon nucleus, illustrating the breadth of the framework across classical, quantum, and nuclear physics. The approach aims to enhance conceptual understanding while simultaneously introducing students to modern computational research methodologies.

</details>


### [48] [Learning Stiff Dynamical Operators: Scaling, Fast-Slow Excitation, and Eigen-Consistent Neural Models](https://arxiv.org/abs/2601.01632)
*Mauro Valorani*

Main category: physics.comp-ph

TL;DR: A new neural operator learning approach for stiff dynamical systems that preserves spectral fidelity through stiffness-aware scaling, fast direction excitation, and Jacobian diagnostics.


<details>
  <summary>Details</summary>
Motivation: Stiff dynamical systems pose significant challenges for neural operator learning due to training errors concentrating on slow manifold states, collapse of fast dynamics, and failure to reproduce true eigenstructure. Existing methods struggle with stiffness in multi-scale modeling applications like combustion and chemical kinetics.

Method: Three key techniques: (1) stiffness-aware scaling of time derivatives, (2) fast direction excitation via local trajectory cloud bursts, and (3) autograd-based Jacobian diagnostics to ensure eigenstructure fidelity.

Result: Applied to the Davis-Skodje system, the approach successfully recovers both slow and fast modes across stiffness regimes, reducing fast eigenvalue error by an order of magnitude while improving rollout fidelity.

Conclusion: Spectral fidelity - not just trajectory accuracy - should be a primary target in data-driven learning of stiff operators, as demonstrated by the proposed approach's success in preserving eigenstructure.

Abstract: Stiff dynamical systems represent a central challenge in multi scale modeling across combustion, chemical kinetics, and nonlinear dynamical systems. Neural operator learning has recently emerged as a promising approach to approximate dynamical generators from data, yet stiffness imposes severe obstacles: training errors concentrate on slow manifold states, collapse of fast dynamics occurs, and the learned operator may fail to reproduce the true eigenstructure.
  We demonstrate three key advances enabling accurate learning of stiff operators and preserving spectral fidelity: (i) stiffness aware scaling of time derivatives, (ii) fast direction excitation via local trajectory cloud bursts, and (iii) autograd-based Jacobian diagnostics ensuring eigenstructure fidelity. Applied to the Davis-Skodje system, the approach recovers both slow and fast modes across stiffness regimes, reducing fast eigenvalue error by an order of magnitude while improving rollout fidelity. These results argue that spectral fidelity - not trajectory accuracy alone - should be a first-class target in data driven learning of stiff operators.

</details>


### [49] [Ab initio quantum embedding at finite temperature with density matrix embedding theory](https://arxiv.org/abs/2601.01641)
*Laurence Giordano,Y. Stanley Tan,Zhi-Hao Cui,Chong Sun*

Main category: physics.comp-ph

TL;DR: FT-DMET: finite-temperature extension of density matrix embedding theory for crystalline systems with practical implementation strategies and applications to characterize finite-temperature phases.


<details>
  <summary>Details</summary>
Motivation: To develop a practical finite-temperature extension of density matrix embedding theory (DMET) for realistic crystalline systems, enabling characterization of finite-temperature phases in quantum materials.

Method: Developed FT-DMET framework with extended bath orbitals, DMET self-consistency at finite temperature, computational strategies including mutual-information-guided bath truncation, controlled thermal electron number treatment, and low-temperature impurity solvers.

Result: Applied to periodic hydrogen chains and square lattices, observed Pomeranchuk-like effect in 1D and enhanced stability of long-range order in 2D, demonstrating method's capability to characterize finite-temperature phases.

Conclusion: FT-DMET provides a practical framework for studying finite-temperature properties of crystalline systems, with computational strategies enabling efficient calculations and revealing interesting finite-temperature phenomena in model systems.

Abstract: We present a finite-temperature extension of density matrix embedding theory (FT-DMET) for realistic crystalline systems. We describe a practical framework for constructing extended bath orbitals, solving the embedding problem, and performing DMET self-consistency at finite temperature. To reduce computational cost, we introduce strategies based on mutual-information-guided bath truncation, controlled treatment of the thermal electron number without explicit optimization, and the use of low-temperature impurity solvers and one-shot FT-DMET in the low-temperature regime. We apply this approach to periodic hydrogen chains and square lattices to characterize their finite-temperature phases. We observe the Pomeranchuk-like effect in one dimension and enhanced stability of long-range order in two dimensions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [50] [Plasma-Activated Water (PAW) for the Degradation of Organic Pollutants in Diluted Industrial Effluents](https://arxiv.org/abs/2601.01270)
*Punit Kumar,Priti Saxena*

Main category: physics.plasm-ph

TL;DR: PAW generated by gliding arc plasma effectively degrades persistent organic pollutants in wastewater with high efficiency (up to 90% for dyes, 85% for pesticides, 80% for pharmaceuticals) through radical-driven pseudo-first-order kinetics.


<details>
  <summary>Details</summary>
Motivation: Need for sustainable, nonthermal solutions to degrade persistent organic pollutants (dyes, pesticides, pharmaceuticals) in industrial effluents while minimizing chemical use and supporting water reuse.

Method: Used gliding arc plasma system to generate PAW; varied exposure time, dilution ratio, and pollutant concentration; analyzed degradation using UV-Vis spectroscopy, HPLC, TOC, and COD measurements.

Result: High degradation efficiencies achieved: up to 90% for dyes, 85% for pesticides, and 80% for pharmaceuticals following pseudo-first-order kinetics driven by hydroxyl and nitrate/nitrite radicals.

Conclusion: PAW demonstrates potential as a green, scalable wastewater treatment strategy that minimizes chemical use, supports water reuse, enhances environmental safety, and has promising future for pilot-scale applications.

Abstract: Plasma activated water (PAW) offers a sustainable, nonthermal solution for degrading persistent organic pollutants in industrial effluents. This study employed a gliding arc plasma system to generate PAW for treating diluted waste water containing dyes, pesticides, and pharmaceuticals. Experimental parameters such as exposure time, dilution ratio, and pollutant concentration were varied, with analysis conducted using UV Vis spectroscopy, HPLC, TOC, and COD. Results showed high degradation efficiencies, up to 90% for dyes, 85% for pesticides, and 80% for pharmaceuticals following pseudo first order kinetics driven by hydroxyl and nitrate or nitrite radicals. The findings demonstrate PAWs potential as a green, scalable wastewater treatment strategy that minimizes chemical use, supports water reuse, and enhances environmental safety, with future scope for pilot scale applications.

</details>


### [51] [Overcoming the space-charge dilemma in low-energy heavy ion beams via a multistage acceleration lens system](https://arxiv.org/abs/2601.01367)
*M. Nishiura,T. Ido,M. Okamura,K. Ueda,A. Shimizu,H. Takubo*

Main category: physics.plasm-ph

TL;DR: Researchers overcome space-charge limits in heavy-ion accelerators by reshaping electrostatic potentials to create combined acceleration-focusing columns, enabling 100+ μA gold ion transport (10x improvement).


<details>
  <summary>Details</summary>
Motivation: Low-energy heavy-ion beams face severe space-charge divergence that limits transportable current to just a few microamperes, especially problematic for high-mass ions where low velocity increases perveance. This limitation restricts applications in fusion diagnostics, ion implantation, and molecular ion research.

Method: Transform existing multistage accelerators by shaping electrostatic potential configurations to create combined acceleration-focusing columns. Optimize interstage voltage distributions to superimpose strong electrostatic lens effects that counteract space-charge expansion. Develop generalized design framework mapping transport 'design window' for beam current, ion mass, and acceleration voltage.

Result: For gold ions at 64 keV, achieved stable transport of beam currents exceeding 100 μA - more than 10x higher than conventional limits. Numerical phase-space analysis shows improvement comes from prioritizing envelope control over emittance preservation, a necessary trade-off in space-charge-dominated regimes.

Conclusion: Established universal, practical guideline for high-current heavy-ion beam transport applicable to fusion plasma diagnostics, ion implantation, and massive molecular ion applications. The approach fundamentally changes how electrostatic accelerators can be designed to overcome intrinsic space-charge limitations.

Abstract: Low-energy heavy-ion beams are fundamentally limited by severe space-charge divergence, which constrains the transportable beam current to a few microamperes in conventional electrostatic accelerators. This limitation is particularly critical for high-mass ions, where the generalized perveance increases rapidly because of their low velocity. Here, we demonstrate that this apparent space-charge limit can be overcome by shaping the electrostatic potential configuration of an existing multistage accelerator, thereby transforming the acceleration column itself into a combined acceleration-focusing column. By optimizing the interstage voltage configuration, a strong electrostatic lens effect is superimposed on the accelerating field to counteract space-charge-driven expansion. We formulate a generalized design framework that quantitatively maps the transport 'design window' in terms of beam current, ion mass, and acceleration voltage. For gold ions at 64 keV, this approach enables stable transport of beam currents exceeding 100 microA, more than an order of magnitude higher than the conventional limit. Numerical phase-space analysis shows that this improvement is achieved by prioritizing envelope control over emittance preservation, a trade-off intrinsic to space-charge-dominated regimes. Our results establish a universal and practical guideline for high-current heavy-ion beam transport, relevant to fusion plasma diagnostics, ion implantation, and massive molecular ion applications.

</details>


### [52] [Hybrid PIC-fluid model for numerical simulation of laser-plasma interaction](https://arxiv.org/abs/2601.01633)
*Andrey Sladkov*

Main category: physics.plasm-ph

TL;DR: A hybrid PIC-fluid model for 3D laser-plasma simulation with kinetic ions, ten-moment fluid electrons, and laser-envelope approach, implemented in AKAM code for scalable HED plasma applications.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable numerical framework that bridges fully kinetic and fluid approaches for simulating laser-plasma interactions in high-energy-density and laboratory plasma applications, capturing essential physics while maintaining computational efficiency.

Method: Hybrid PIC-fluid model with ions treated kinetically (PIC), electrons as ten-moment fluid, laser-envelope model for energy deposition without resolving optical oscillations, and inclusion of collisional and ionization processes for self-consistent evolution.

Result: The model captures ion-scale dynamics, pressure anisotropy, and non-Maxwellian distributions efficiently, and is implemented in the scalable AKAM code framework.

Conclusion: The hybrid model provides a practical computational framework that balances physical fidelity with efficiency for laser-plasma interaction studies, bridging the gap between fully kinetic and fluid approaches.

Abstract: A hybrid PIC-fluid model is proposed for three dimensional numerical simulation of laser-plasma interaction. Ions are treated kinetically, electrons as a ten-moment fluid, capturing ion-scale dynamics, pressure anisotropy, and non-Maxwellian distributions efficiently. A laser-envelope model handles energy deposition and ponderomotive heating without resolving optical oscillations. Collisional and ionisation processes ensure self-consistent evolution of energy and charge states. The model is implemented in the AKAM code, providing a scalable framework that bridges fully kinetic and fluid approaches for high-energy-density and laboratory plasma applications.

</details>


### [53] [Excess energy of strongly coupled one-component plasma from variational approach](https://arxiv.org/abs/2601.01659)
*S. A. Khrapak,A. G. Khrapak*

Main category: physics.plasm-ph

TL;DR: Variational approach using hard-sphere reference entropy accurately calculates one-component plasma excess energy; Percus-Yevick virial entropy identified as most accurate, validating Rosenfeld-Tarazon scaling.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate variational method for calculating the excess energy of one-component plasma fluids by using different hard-sphere fluid entropy variants as reference systems, and to identify the most accurate entropy formulation.

Method: Variational approach combined with different variants of excess entropy from hard-sphere fluid as reference system; comparison with recent Monte Carlo simulation results to validate accuracy.

Result: Percus-Yevick virial entropy identified as the most accurate entropy for variational calculations of one-component plasma excess energy; Rosenfeld-Tarazon scaling of thermal component shows excellent agreement with Monte Carlo results.

Conclusion: The variational approach using Percus-Yevick virial entropy provides accurate calculations of one-component plasma excess energy, validating the Rosenfeld-Tarazon scaling and suggesting potential for further development in this analytical framework.

Abstract: The excess energy of the one-component plasma fluid is calculated using the variational approach combined with different variants of the excess entropy of the hard-sphere fluid, which is used as a reference system. Our comparison with recent Monte Carlo results for the excess energy of the one-component plasma identifies the Percus-Yevick virial entropy as the most accurate entropy to be used in the variational calculation of this kind. The reason for this and potential developments of the present analysis are briefly discussed. We demonstrate that the original Rosenfeld-Tarazons scaling of the thermal component of the excess energy of the one-component plasma fluid is in excellent agreement with recent Monte Carlo results.

</details>


### [54] [Partially Ionized Plasma Physics and Technological Applications](https://arxiv.org/abs/2601.01671)
*Igor Kaganovich,Michael Tendler*

Main category: physics.plasm-ph

TL;DR: Review of partially ionized plasma physics focusing on non-Maxwellian electron energy distributions, complex nonlinear phenomena, and advanced computational modeling for technological applications.


<details>
  <summary>Details</summary>
Motivation: Partially ionized plasmas have gained attention due to technological applications in manufacturing (especially computer components), advances in computer modeling, and theoretical analysis capabilities.

Method: Theoretical analysis using kinetic Boltzmann equation to obtain non-Maxwellian electron energy distribution functions (EEDF), with detailed study of discharges in inert and molecular gases to understand complex nonlinear plasma self-organization phenomena.

Result: Identified various nonlinear phenomena including non-monotonic EEDFs in argon-NF3 mixture afterglow, explosive generation of cold electron populations in capacitive discharges, and EEDF hysteresis in inductively coupled plasmas.

Conclusion: Recent development of highly advanced computer codes addresses outstanding plasma technology problems, highlighting the importance of partially ionized plasma physics for technological applications and manufacturing.

Abstract: Partially ionized plasma physics has attracted a lot of attention recently due to numerous technological applications made possible by the increased sophistication of computer modelling, the depth of the theoretical analysis, and the technological applications to a vast field of the manufacturing for computer components. The partially ionized plasma is characterized by a significant presence of neutral particles in contrast to fully ionized plasma. The theoretical analysis is based upon solutions of the kinetic Boltzmann equation yielding the non-Maxwellian electron energy distribution function (EEDF) thereby emphasizing the difference with a fully ionized plasma. The impact of the effect on discharges in inert and molecular gases is described in detail yielding the complex nonlinear phenomena in plasma self-organization. A few examples of such phenomena are given including the non-monotonic EEDFs in the discharge afterglow in mixture of argon with the molecular gas NF3; the explosive generation of cold electron populations in capacitive discharges, hysteresis of EEDF in inductively coupled plasmas. Recently, highly advanced computer codes were developed in order to address the outstanding problems of plasma technology. These developments are briefly described in general terms.

</details>


### [55] [Photonic Interactions with Semiconducting Barrier Discharges](https://arxiv.org/abs/2601.01994)
*Ayah Soundous Taihi,David Z. Pai*

Main category: physics.plasm-ph

TL;DR: Photoexcitation of silicon enhances plasma emission and electric field in semiconductor barrier discharges without changing electrical energy, with effects depending on wavelength and absorption depth.


<details>
  <summary>Details</summary>
Motivation: To investigate how external irradiation synchronized with discharge can mimic photoconductive coupling between plasma and semiconductor surface, focusing on microscopic processes governing plasma-semiconductor interaction.

Method: Used nanosecond pulsed irradiation (532-1064 nm) on Si-SiO₂ interface, combined with fast imaging, optical emission spectroscopy, and current-voltage measurements; compared SeBD to MOS photodetector.

Result: Photoexcitation enhances plasma emission and increases reduced electric field without detectable change in electrical energy; magnitude and thresholds depend on wavelength and absorption length determining carrier generation location.

Conclusion: The optoelectronic properties of silicon influence surface ionization waves through absorption length-dependent carrier generation and separation mechanisms at the semiconductor interface.

Abstract: Semiconducting Barrier Discharges (SeBDs) generate uniform ionization waves in air at atmospheric pressure. In this work, we investigate how externally applied irradiation synchronized with the discharge can mimic photoconductive-type coupling between the plasma and the semiconductor surface. By illuminating the Si-SiO$_2$ interface with nanosecond pulsed irradiation at wavelengths from 532 nm to 1064 nm, and using fast imaging, optical emission spectroscopy, and current-voltage measurements, we demonstrate that the photoexcitation of charge carriers in silicon enhances the plasma emission and increases the reduced electric field, with no detectable change in the electrical energy. The magnitude and thresholds of these responses depend on wavelength. By comparing the SeBD to a MOS photodetector, this behaviour can be explained by the absorption length. This length determines whether carriers are photogenerated inside the depletion region at the SiO$_2$-Si interface, where they are efficiently separated and undergo impact-ionization amplification, or deeper in the silicon bulk where carrier separation is weaker and free-carrier absorption diminishes the quantum efficiency. These results focus on the microscopic processes governing the plasma-semiconductor coupling and demonstrate how the optoelectronic properties of silicon can influence surface ionization waves.

</details>


### [56] [Towards solving the ICRH wave and Fokker-Planck equations self-consistently](https://arxiv.org/abs/2601.02130)
*Dirk Van Eester,Vincent Maquet,Fabrice Louche,Bernard Reman*

Main category: physics.plasm-ph

TL;DR: Framework for self-consistent solution of wave and Fokker-Planck equations in ion cyclotron resonance frequency domain using building blocks approach.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive framework for solving wave-particle interaction problems in ion cyclotron resonance heating (ICRH) self-consistently, addressing the need to account for wave coupling in plasmas with non-Maxwellian distributions.

Method: Constructs "building blocks" common to both wave and Fokker-Planck equations, uses known expressions and methods for wave-particle interaction, presents two cases: guiding centre motion limited to magnetic field lines, and extended case accounting for drifts with axisymmetry assumption.

Result: Demonstrates feasibility of self-consistent solution approach, includes limited analytical results, and illustrates computation of dielectric response for arbitrary distribution functions as key component of combining wave and Fokker-Planck solving.

Conclusion: Proposes a practical framework for fully self-consistent treatment of wave and Fokker-Planck equations in ICRH domain, enabling better modeling of wave-particle interactions in non-Maxwellian plasmas through modular building blocks approach.

Abstract: The present paper sketches a framework for solving the wave and Fokker-Planck equations in the ion cyclotron resonance frequency domain fully selfconsistently. It illustrates this can be done by first constructing "building blocks" that are commonly needed by the wave and Fokker-Planck equations, allowing e.g. to account for wave coupling in plasmas containing non-Maxwellian distributions. Up to details, the paper exploits known expressions and methods to solve the two intimately connected aspects of the description of the wave-particle interaction underlying ion cyclotron resonance heating. Two cases are presented: the case where the guiding centre motion is limited to just following magnetic field lines, and the extended case accounting for drifts away from magnetic surfaces but assuming axisymmetry. A limited set of analytical results is included. As combining wave and Fokker-Planck solving is the focus, the computation of the dielectric response for arbitrary distribution functions is illustrated as well.

</details>


### [57] [Variability of MHD Instabilities in Benign Termination of High-Current Runaway Electron Beams in the JET and DIII-D Tokamaks](https://arxiv.org/abs/2601.02262)
*C. F. B. Zimmermann,C. Paz-Soldan,G. Su,C. Reux,A. F. Battey,O. Ficker,S. N. Gerasimov,C. J. Hansen,S. Jachmich,A. Lvovskiy,J. Puchmayr,N. Schoonheere,U. Sheikh,I. G. Stewart,G. Szepesi,JET Contributors,the EUROfusion Tokamak Exploitation Team*

Main category: physics.plasm-ph

TL;DR: Benign termination of runaway electron beams via MHD instabilities is challenging at high plasma currents (Ip ≥ 2.5 MA). RE current profile peaking determines which MHD instability boundary is encountered, with non-benign cases having more peaked profiles and lower MHD perturbation amplitudes.


<details>
  <summary>Details</summary>
Motivation: To understand why benign termination of runaway electrons via MHD instabilities becomes difficult at high plasma currents (≥2.5 MA) in tokamaks, and to identify the key factors determining successful vs. unsuccessful terminations.

Method: Analysis of ~40 JET and ~20 DIII-D discharges using fast magnetic sensor measurements and EFIT equilibrium reconstructions. Linear resistive MHD modeling with CASTOR3D code to confirm findings.

Result: Non-benign terminations occur at low edge safety factor (q_edge ≈ 2) with peaked RE current profiles, while benign terminations occur at higher q_edge (≥3) with less peaked profiles. RE current peaking determines which MHD instability boundary is encountered. Non-benign cases have lower MHD perturbation amplitudes despite similar growth rates.

Conclusion: The interplay between ideal and resistive MHD dynamics governs termination success, not just ideal MHD timescales. RE current profile peaking is the key factor determining whether benign or non-benign termination occurs.

Abstract: Benign termination, in which magnetohydrodynamic (MHD) instabilities deconfine runaway electrons (REs) following hydrogenic injections, is a promising strategy for mitigating dangerous RE loads after disruptions. Recent experiments on the Joint European Torus (JET) have explored this scenario at higher pre-disruptive plasma currents than are achievable on other devices, revealing challenges in obtaining benign terminations at $I_p \geq 2.5$ MA. This work analyzes the evolution of these high-current RE beams and their terminating MHD events using fast magnetic sensor measurements and EFIT equilibrium reconstructions for approximately $40$ JET and $20$ DIII-D tokamak discharges. On JET, unsuccessful non-benign terminations occur at low edge safety factor ($q_{\text{edge}} \approx 2$), and are preceded by intermittent, non-terminating MHD events at higher rational $q_{\text{edge}}$. Trends in the internal inductance $l_i$ indicate more peaked RE current profiles in the high-$I_p$ non-benign population, which may hinder successful recombination through re-ionization. In contrast, benign terminations on JET typically occur at higher $q_{\text{edge}} \geq 3$ and exhibit less peaked RE current profiles. DIII-D displays a range of terminating edge safety factors, correlated with the measured $l_i$ values. Across both tokamaks, the RE current peaking is therefore found to determine which MHD instability boundary is encountered, confirmed by linear resistive MHD modeling with the CASTOR3D code. Measured growth rates are similar for benign and non-benign cases, indicating that ideal MHD timescales at low density after hydrogenic injection do not alone explain efficient RE deconfinement. Instead, non-benign cases are characterized by their lower MHD perturbation amplitudes $δB$. These observations suggest that the interplay between ideal and resistive dynamics governs the termination process.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [58] [Probabilistic modeling of Cherenkov emission from particle showers](https://arxiv.org/abs/2601.00944)
*Ian Crawshaw,Tianlu Yuan,Emre Yildici,Lu Lu,Anatoli Fedynitch*

Main category: astro-ph.HE

TL;DR: The paper develops parameterized distributions to efficiently simulate Cherenkov light from particle showers in ice for neutrino telescopes, improving event fluctuation modeling.


<details>
  <summary>Details</summary>
Motivation: Full Monte Carlo simulation of particle showers is computationally expensive at scale, forcing experiments to use parametrized approximations. Current approximations lack accurate modeling of event-to-event fluctuations in Cherenkov light yield from showers in ice.

Method: Construct parameter distributions that describe Cherenkov light yield from particle showers in ice, making them extensible to similar media. Use sampling from these distributions to capture fluctuations in amplitude and shape along the shower axis.

Result: The method provides much improved description of event-to-event fluctuations compared to previous parametrized approximations, enabling more accurate simulation of both signal and background events.

Conclusion: Including these fluctuation effects is essential for accurate simulation in current and next-generation neutrino telescopes, where computational efficiency must be balanced with realistic shower modeling.

Abstract: Subatomic particles can interact with target nuclei in matter or decay in flight, and an individual high-energy particle can induce a particle shower composed of numerous, lower-energy secondaries. These particle showers broadly exhibit universality across diverse media, including air, water, ice, and other materials, with their development governed by the Standard Model. Full Monte Carlo simulation of particle showers, where each secondary is individually tracked and propagated, can be a computational challenge to perform at scale. Experiments thus resort to parametrized approximations when efficient simulation becomes necessary. Here, we construct distributions of parameters capable of describing the Cherenkov light yield from particle showers in ice, and extensible to other, similar media. Sampling from the distributions allows for a much improved description of event-to-event fluctuations, in amplitude and shape, along the shower axis. Including these effects is essential for a more accurate simulation of signal and background events in current and next-generation neutrino telescopes.

</details>


### [59] [One-dimensional PIC Simulation of Induced Compton Scattering in Magnetized Electron-Positron Pair Plasma](https://arxiv.org/abs/2601.01169)
*Shoma F. Kamijima,Rei Nishiura,Masanori Iwamoto,Kunihito Ioka*

Main category: astro-ph.HE

TL;DR: PIC simulations confirm two modes of density fluctuations in induced Compton scattering of circularly polarized Alfvén waves in magnetized pair plasmas: charged (Langmuir-like) and neutral (acoustic-like) modes.


<details>
  <summary>Details</summary>
Motivation: To understand induced Compton scattering processes in magnetized electron-positron pair plasmas, which has implications for astrophysical phenomena like fast radio bursts and laser-plasma experiments.

Method: One-dimensional Particle-in-Cell (PIC) simulations of circularly polarized Alfvén wave propagation in magnetized electron-positron pair plasma.

Result: Simulations confirm theoretical predictions of two distinct density fluctuation modes: charged mode (electron-positron densities fluctuate oppositely) and neutral mode (both species fluctuate in phase). Linear growth rates match analytical estimates, and sometimes saturation occurs before full scattering.

Conclusion: The study establishes that induced Compton scattering can grow linearly in magnetized pair plasmas, providing a foundation for understanding fast radio bursts and laser-plasma experiments.

Abstract: We investigate induced Compton scattering of a circularly polarized Alfvén wave propagating in a magnetized electron-positron pair plasma using one-dimensional Particle-in-Cell (PIC) simulations. In this system, two distinct modes of density fluctuations, referred to as the charged mode and the neutral mode, are theoretically expected to arise through parametric instabilities. Our simulations confirm these predictions: in the charged mode, the electron and positron densities fluctuate oppositely (Langmuir-like), while in the neutral mode, the charge is Debye-screened and both species fluctuate in phase (acoustic-like). The linear growth rates obtained from the simulations are in good agreement with analytical estimates for both modes. We also find that, in some cases, the linear growth saturates before full scattering occurs, allowing the incident wave to propagate without significant attenuation. Our results allow us to determine whether induced Compton scattering grows linearly in magnetized pair plasmas, offering a foundation for studies of fast radio bursts and laser-plasma experiments.

</details>


### [60] [Proton Acceleration by Collisionless Shocks in Supermassive Black Hole Coronae: Implications for High-Energy Neutrinos](https://arxiv.org/abs/2601.01999)
*Minh Nhat Ly,Yoshiyuki Inoue,Yasuhiko Sentoku,Takayoshi Sano*

Main category: astro-ph.HE

TL;DR: PIC simulations show diffusive shock acceleration in AGN coronae efficiently accelerates protons (10% efficiency) but not electrons (<1%), supporting hadronic models for neutrino production in Seyfert galaxies like NGC 1068.


<details>
  <summary>Details</summary>
Motivation: IceCube observations show high-energy neutrino excess from Seyfert galaxies without corresponding gamma-rays, suggesting hadronic interactions in gamma-ray opaque regions like AGN coronae. The mechanism for accelerating protons to ~100 TeV energies remains unknown.

Method: Used one-dimensional Particle-in-cell (PIC) simulations spanning broad plasma parameters to investigate diffusive shock acceleration (DSA) in active galactic nucleus coronae.

Result: DSA is robust and efficient for proton acceleration, consistently channeling ~10% of shock kinetic energy into non-thermal ions, even for low Mach number shocks (Ms ≈ 2). Electron acceleration is highly variable and inefficient (<1%).

Conclusion: Findings provide first-principles support for hadronic models of neutrino production in AGN and offer quantitative constraints that can explain the observed gamma-ray deficit from Seyfert galaxies.

Abstract: Recent observations by the IceCube Neutrino Observatory have revealed a significant excess of high-energy neutrinos from nearby Seyfert galaxies, such as NGC~1068, without a corresponding flux of high-energy gamma-rays. This suggests that neutrinos are produced via hadronic interactions in a region opaque to gamma-rays, likely a hot corona surrounding the central supermassive black hole. However, the mechanism responsible for accelerating the parent protons to the required energies ($\sim 100$ TeV) remains an open question. In this study, we investigate diffusive shock acceleration (DSA) in active galactic nucleus (AGN) coronae using a suite of one-dimensional Particle-in-cell (PIC) simulations spanning a broad range of plasma parameters. We find that DSA is a robust and efficient mechanism for proton acceleration, consistently channeling approximately 10\% of the shock's kinetic energy into non-thermal ions, even for shocks with sonic Mach number as low as $ M_s \approx 2$. In contrast, the efficiency of electron acceleration is highly variable and less efficient ($<1\%$) in our parameter survey. These findings provide strong, first-principles support for the hadronic models of neutrino production in AGN and offer quantitative constraints that can explain the observed gamma-ray deficit.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [61] [Strain-triggered high-temperature superconducting transition in two-dimensional carbon allotrope](https://arxiv.org/abs/2601.01100)
*Tian Yan,Ru Zheng,Jin-Hua Sun,Fengjie Ma,Xun-Wang Yan,Miao Gao,Tian Cui,Zhong-Yi Lu*

Main category: cond-mat.supr-con

TL;DR: THO-graphene, a 2D carbon allotrope, becomes superconducting under biaxial tensile strain with Tc up to 45K, setting a record for 2D elemental superconductors.


<details>
  <summary>Details</summary>
Motivation: To demonstrate purely strain-induced superconductivity in 2D materials, which remains exceedingly scarce, and to control metal-superconductor transitions in 2D systems via strain engineering.

Method: First-principles calculations to study THO-graphene (composed of triangles, hexagons, and octagons) under biaxial tensile strain, analyzing electron-phonon coupling enhancement.

Result: Strain-induced superconductivity in THO-graphene with Tc up to 45K, achieved solely through biaxial tensile strain without other modifications.

Conclusion: This work provides a notable example of purely strain-induced superconductivity in 2D systems and sets a new record for superconducting transition temperature in 2D elemental superconductors.

Abstract: Driving non-superconducting materials into a superconducting state through specific modulation is a key focus in the field of superconductivity. Pressure is a powerful method that can switch a three-dimensional (3D) material between non-superconducting and superconducting states. In the two-dimensional (2D) case, strain engineering plays a similar role to pressure. However, purely strain-induced superconductivity in 2D systems remains exceedingly scarce. Using first-principles calculations, we demonstrate that a superconducting transition can be induced solely by applying biaxial tensile strain in a 2D carbon allotrope, THO-graphene, which is composed of triangles, hexagons, and octagons. Free-standing THO-graphene is non-superconducting. Surprisingly, the electron-phonon coupling in strained THO-graphene is enhanced strong enough to pair electrons and realize superconductivity, with the highest superconducting transition temperature reaching 45 K. This work not only provides a notable example of controlling metal-superconductor transition in 2D system just via strain, but also sets a new record of superconducting transition temperature for 2D elemental superconductors.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [62] [The inverse eigenvalue problems for perturbed Bessel operator with mixed data](https://arxiv.org/abs/2601.01093)
*Zeguang Liu,Xin-Jian Xu*

Main category: math.SP

TL;DR: This paper establishes uniqueness results for inverse eigenvalue problems of perturbed Bessel operators, extending existing results from ℓ=0 to ℓ≥-1/2 using spectral data and norming constants.


<details>
  <summary>Details</summary>
Motivation: To extend existing inverse spectral theory results for Bessel operators from the specific case ℓ=0 to the more general case ℓ≥-1/2, providing broader applicability in quantum mechanics and spectral analysis.

Method: (1) For ℓ∈ℕ∪{0}, uses closedness condition of function systems constructed from eigenvalues and norming constants. (2) For ℓ≥-1/2, uses density condition of eigenvalues and norming constants, possibly with additional smoothness conditions on the potential. (3) Extends existing corollaries from ℓ=0 to ℓ≥-1/2.

Result: Established uniqueness results for inverse spectral problems: (1) For ℓ∈ℕ∪{0} using closedness conditions. (2) For ℓ≥-1/2 using density conditions. Successfully extended known results from ℓ=0 to the general case ℓ≥-1/2.

Conclusion: The paper successfully generalizes inverse spectral theory for perturbed Bessel operators from the ℓ=0 case to the broader ℓ≥-1/2 case, providing new uniqueness results and extending existing corollaries to more general angular-momentum quantum numbers.

Abstract: We consider inverse eigenvalue problems for the perturbed Bessel operator in $L^{2}(0,1)$. (1) For the case where the angular-momentum quantum number $\ell\in\mathbb{N}\cup\{0\}$, we establish a uniqueness result for the inverse spectral problem by utilizing the closedness condition of a certain function system constructed based on the eigenvalues and the norming constants. (2) For the broader case where $\ell \geq -1/2$, we provide a uniqueness result for the inverse problem by using the density condition satisfied by the eigenvalues and the norming constants, where an additional smoothness condition may be imposed on the potential. (3) In the last section of this article, we present some corollaries based on (2). The results in these corollaries have already been established for the case $\ell=0$ by Gesztesy, Simon, Wei, Xu, Hatinoǧlu, et al., and we extend these results to the general case $\ell \geq -1/2$.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [63] [HyDRA: Hybrid Denoising Regularization for Measurement-Only DEQ Training](https://arxiv.org/abs/2601.01228)
*Markus Haltmeier,Lukas Neumann,Nadja Gruber,Johannes Schwab,Gyeongha Hwang*

Main category: cs.CV

TL;DR: HyDRA enables training Deep Equilibrium models using only measurement data (no ground truth pairs needed) by combining measurement consistency with adaptive denoising regularization and early stopping.


<details>
  <summary>Details</summary>
Motivation: Image reconstruction problems are challenging due to ill-posedness and lack of supervised datasets. Existing DEQ models require supervised pairs (x,y), but in practice often only measurements y are available.

Method: HyDRA combines measurement consistency with adaptive denoising regularization term, plus a data-driven early stopping criterion. This enables DEQ training using only measurement data without ground truth.

Result: Experiments on sparse-view CT show competitive reconstruction quality and fast inference compared to supervised methods.

Conclusion: HyDRA provides an effective measurement-only framework for training DEQ models, addressing practical limitations where only measurements are available.

Abstract: Solving image reconstruction problems of the form \(\mathbf{A} \mathbf{x} = \mathbf{y}\) remains challenging due to ill-posedness and the lack of large-scale supervised datasets. Deep Equilibrium (DEQ) models have been used successfully but typically require supervised pairs \((\mathbf{x},\mathbf{y})\). In many practical settings, only measurements \(\mathbf{y}\) are available. We introduce HyDRA (Hybrid Denoising Regularization Adaptation), a measurement-only framework for DEQ training that combines measurement consistency with an adaptive denoising regularization term, together with a data-driven early stopping criterion. Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [64] [Latent Space Element Method](https://arxiv.org/abs/2601.01741)
*Seung Whan Chung,Youngsoo Choi,Christopher Miller,H. Keo Springer,Kyle T. Sullivan*

Main category: math.DS

TL;DR: LSEM is a scalable latent surrogate assembly method that tiles learned subdomain models to solve PDEs on larger domains than seen in training, using latent ODE surrogates with learned coupling and smooth blending.


<details>
  <summary>Details</summary>
Motivation: To build surrogate solvers that can scale to larger domains without requiring access to PDE operators or extensive training data on large domains, enabling modular and reusable local models.

Method: Latent Space Element Method (LSEM) uses element-based latent surrogate assembly: each element is a LaSDI latent ODE surrogate trained on local patches, coupled through learned directional interaction terms in latent space, with smooth window-based blending for global field reconstruction.

Result: Experiments on 1D Burgers and Korteweg-de Vries equations show LSEM maintains predictive accuracy while scaling to spatial domains larger than those seen in training, avoiding Schwarz iterations and interface residual evaluations.

Conclusion: LSEM provides an interpretable and extensible approach toward foundation-model surrogate solvers built from reusable local models, enabling scalable PDE solving beyond training domain sizes.

Abstract: How can we build surrogate solvers that train on small domains but scale to larger ones without intrusive access to PDE operators? Inspired by the Data-Driven Finite Element Method (DD-FEM) framework for modular data-driven solvers, we propose the Latent Space Element Method (LSEM), an element-based latent surrogate assembly approach in which a learned subdomain ("element") model can be tiled and coupled to form a larger computational domain. Each element is a LaSDI latent ODE surrogate trained from snapshots on a local patch, and neighboring elements are coupled through learned directional interaction terms in latent space, avoiding Schwarz iterations and interface residual evaluations. A smooth window-based blending reconstructs a global field from overlapping element predictions, yielding a scalable assembled latent dynamical system. Experiments on the 1D Burgers and Korteweg-de Vries equations show that LSEM maintains predictive accuracy while scaling to spatial domains larger than those seen in training. LSEM offers an interpretable and extensible route toward foundation-model surrogate solvers built from reusable local models.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [65] [All-Optical Deep Learning with Quantum Nonlinearity](https://arxiv.org/abs/2601.01690)
*Qingyi Zhou,Jungmin Kim,Yutian Tao,Guoming Huang,Ming Zhou,Zewei Shao,Zongfu Yu*

Main category: physics.optics

TL;DR: All-optical deep learning using quantum emitters in nanophotonic structures enables ultra-low-power AI with strong optical nonlinearities, reducing power consumption by 7 orders of magnitude.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks consume unsustainable power, and existing optical neural networks lack efficient optical nonlinearities needed for complex tasks.

Method: Embed quantum emitters in inverse-designed nanophotonic structures, leveraging their saturability for strong nonlinearity, and use physics-aware training for complex tasks.

Result: Demonstrated nonlinear classification and reinforcement learning in all-optical networks, with quantum activation operating below nW/μm², reducing power budget by 7 orders of magnitude. Large language models can operate at watt-level with sublinear power scaling.

Conclusion: Quantum nanophotonics provides a viable route toward sustainable AI inference by enabling ultra-low-power all-optical deep learning.

Abstract: The rapid scaling of deep neural networks comes at the cost of unsustainable power consumption. While optical neural networks offer an alternative, their capabilities remain constrained by the lack of efficient optical nonlinearities. To address this, we propose an all-optical deep learning architecture by embedding quantum emitters in inverse-designed nanophotonic structures. Due to their saturability, quantum emitters exhibit exceptionally strong nonlinearity compared with conventional materials. Using physics-aware training, we demonstrate that the proposed architecture can solve complex tasks, including nonlinear classification and reinforcement learning, which have not been realized in all-optical neural networks. To enable fair comparison across different platforms, we introduce a framework that quantitatively links nonlinearity to a network's expressive power. Analysis shows that our quantum activation, operating below nW/μm^2 intensity, reduces the power budget by seven orders of magnitude. System-level estimates show that the optical power required for large language models scales sublinearly with model size, enabling watt-level operation. Our results indicate that quantum nanophotonics provides a route toward sustainable AI inference.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [66] [PauliEngine: High-Performant Symbolic Arithmetic for Quantum Operations](https://arxiv.org/abs/2601.02233)
*Leon Müller,Adelina Bärligea,Alexander Knapp,Jakob S. Kottmann*

Main category: quant-ph

TL;DR: PauliEngine is a high-performance C++ framework for efficient Pauli string operations with Python interface, offering significant speedups over existing implementations.


<details>
  <summary>Details</summary>
Motivation: Quantum computation requires fast classical manipulation of qubit operators for scalability. Current implementations lack the performance needed for large-scale quantum software tools.

Method: Built on binary symplectic representation with optimized bit-wise operations. Supports both numerical and symbolic coefficients and provides Python interface for accessibility.

Result: Runtime benchmarks show substantial speedups over state-of-the-art implementations. The framework provides efficient primitives for Pauli string operations.

Conclusion: PauliEngine offers a scalable backend for operator-based quantum software tools and simulations, addressing performance bottlenecks in quantum software development.

Abstract: Quantum computation is inherently hybrid, and fast classical manipulation of qubit operators is necessary to ensure scalability in quantum software. We introduce PauliEngine, a high-performance C++ framework that provides efficient primitives for Pauli string multiplication, commutators, symbolic phase tracking, and structural transformations. Built on a binary symplectic representation and optimized bit-wise operations, PauliEngine supports both numerical and symbolic coefficients and is accessible through a Python interface. Runtime benchmarks demonstrate substantial speedups over state-of-the-art implementations. PauliEngine provides a scalable backend for operator-based quantum software tools and simulations.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [67] [Approximation of generalized Wiener classes of functions of several variables in different metrics](https://arxiv.org/abs/2601.01182)
*Andrii Shidlich*

Main category: math.CA

TL;DR: Analysis of approximation characteristics for generalized Wiener function classes in various metrics


<details>
  <summary>Details</summary>
Motivation: To study and provide estimates for linear and nonlinear approximation characteristics of generalized Wiener classes of multivariate functions across different metric spaces

Method: Mathematical analysis of generalized Wiener function classes, examining both linear and nonlinear approximation methods in various metrics (likely Lp, Sobolev, etc.)

Result: Presents new results alongside known findings on approximation estimates for multivariate functions in generalized Wiener classes

Conclusion: Advances understanding of approximation theory for multivariate functions in generalized Wiener classes across different metric spaces

Abstract: The paper presents new and known results on estimates of important linear and nonlinear approximation characteristics of generalized Wiener classes of functions of several variables in different metrics.

</details>


### [68] [Optimal Hardy Inequality for Fractional Laplacians on the Lattice](https://arxiv.org/abs/2601.00902)
*Philipp Hake,Matthias Keller,Felix Pogorzelski*

Main category: math.CA

TL;DR: The paper proves optimality of fractional Hardy inequality constants on integer lattices by showing null-criticality of Hardy weights and establishing a threshold parameter where weights transition from positive critical to subcritical.


<details>
  <summary>Details</summary>
Motivation: To establish optimal constants for fractional Hardy inequalities on discrete structures (integer lattices) and understand the critical behavior of Hardy weights with respect to a parameter.

Method: Uses asymptotic expansion of fractional discrete Riesz kernel to analyze Hardy weights, proves null-criticality, and establishes parameter threshold separating positive critical and subcritical regimes.

Result: Shows Hardy weight at threshold is optimal (any larger weight fails to be Hardy weight), inequality has no minimizer, and provides precise characterization of critical behavior based on parameter values.

Conclusion: The paper establishes complete characterization of fractional Hardy inequality on integer lattices with optimal constants and critical behavior, using discrete Riesz kernel analysis as key technical tool.

Abstract: We study the fractional Hardy inequality on the integer lattice. We prove null-criticality of the Hardy weight and hence optimality of the constant. More specifically, we present a family of Hardy weights with respect to a parameter and show that below a certain threshold the Hardy weight is positive critical while above the threshold it is subcritical. In particular, the Hardy weight at the threshold is optimal in the sense that any larger weight would fail to be a Hardy weight and the Hardy inequality does not allow for a minimizer. A crucial ingredient in our proof is an asymptotic expansion of the fractional discrete Riesz kernel.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [69] [Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches](https://arxiv.org/abs/2601.01774)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.AI

TL;DR: LLMs perform poorly at direct numerical solution of transcendental equations but excel when used as symbolic interfaces to classical iterative solvers, achieving 68-82% error reduction.


<details>
  <summary>Details</summary>
Motivation: Transcendental equations requiring iterative numerical solutions are common in engineering, but it's unclear whether LLMs can solve them directly or need to be combined with classical solvers.

Method: Tested 6 state-of-the-art LLMs on 100 problems across 7 engineering domains, comparing direct numerical prediction against hybrid approach where LLMs formulate equations and provide initial conditions for Newton-Raphson iteration.

Result: Direct prediction errors: 0.765-1.262 mean relative error. Solver-assisted errors: 0.225-0.301 (68-82% reduction). Electronics showed 93.1% improvement while Fluid Mechanics only 7.2%.

Conclusion: LLMs excel at symbolic manipulation and domain knowledge but struggle with precision-critical iterative arithmetic. Optimal deployment is as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

Abstract: Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [70] [A quadratic-scaling algorithm with guaranteed convergence for quantum coupled-channel calculations](https://arxiv.org/abs/2601.01159)
*Hubert J. Jóźwiak,Md Muktadir Rahman,Timur V. Tscherbul*

Main category: physics.chem-ph

TL;DR: WISE algorithm achieves O(N²) scaling for quantum scattering calculations, breaking the O(N³) bottleneck of traditional coupled-channel methods.


<details>
  <summary>Details</summary>
Motivation: The cubic scaling [O(N³)] of traditional quantum coupled-channel methods severely limits applications in atomic/molecular physics, chemical reaction dynamics, and astrochemistry, preventing rigorous calculations for complex systems.

Method: Weinberg-regularized Iterative Series Expansion (WISE) algorithm solves time-independent Schrödinger equation for scattering S-matrix with quadratic scaling. It applies regularization to Lippmann-Schwinger integral equation kernel to resolve divergence issues in iterative techniques and explicitly incorporates closed-channel effects including Feshbach resonances.

Result: Demonstrated exact quantum results with quadratic scaling for He + CO and CO + N₂ collisions, achieving rigorous calculations previously limited by computational cost.

Conclusion: WISE establishes a new computational paradigm enabling state-to-state quantum scattering computations for complex molecular systems, providing novel insights into multichannel molecular collision dynamics.

Abstract: Rigorous quantum dynamics calculations provide essential insights into complex scattering phenomena across atomic and molecular physics, chemical reaction dynamics, and astrochemistry. However, the application of the gold-standard quantum coupled-channel (CC) method has been fundamentally constrained by a steep cubic scaling of computational cost [${O}(N^3)$]. Here, we develop a general, rigorous, and robust method for solving the time-independent Schrödinger equation for a single column of the scattering S-matrix with quadratic scaling [${O}(N^2)$] in the number of channels. The Weinberg-regularized Iterative Series Expansion (WISE) algorithm resolves the divergence issues affecting iterative techniques by applying a regularization procedure to the kernel of the multichannel Lippmann-Schwinger integral equation. The method also explicitly incorporates closed-channel effects, including those responsible for multichannel Feshbach resonances. We demonstrate the power of this approach by performing rigorous calculations on He + CO and CO + N$_2$ collisions, achieving exact quantum results with demonstrably quadratic scaling. Our results establish a new computational paradigm, enabling state-to-state quantum scattering computations for complex molecular systems and providing a novel window onto the intricate multichannel molecular collision dynamics.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [71] [An enumerative min-max theorem for minimal surfaces](https://arxiv.org/abs/2601.01736)
*Adrian Chun-Pong Chu,Yangyang Li,Zhihan Wang*

Main category: math.DG

TL;DR: Proves an enumerative min-max theorem connecting genus g minimal surfaces in positive Ricci curvature 3-manifolds to topological properties of embedded surfaces, with application showing every positive Ricci curvature 3-sphere contains at least 4 genus 2 minimal surfaces.


<details>
  <summary>Details</summary>
Motivation: To develop topological methods for enumerating minimal surfaces with prescribed genus in 3-manifolds, particularly in positive Ricci curvature settings where geometric analysis can be combined with topological techniques.

Method: Proves an enumerative min-max theorem that relates the number of genus g minimal surfaces to topological properties of the space of embedded surfaces (possibly with finitely many singularities) of genus ≤ g. Uses topological methods and min-max theory in geometric analysis.

Result: Completes a central component of the program for enumerating minimal surfaces with prescribed genus. As a specific application, proves that every 3-sphere of positive Ricci curvature contains at least 4 embedded minimal surfaces of genus 2.

Conclusion: The enumerative min-max theorem provides a powerful tool for counting minimal surfaces using topological methods, with concrete applications to existence results for minimal surfaces in positive Ricci curvature manifolds.

Abstract: We prove an enumerative min-max theorem that relates the number of genus g minimal surfaces in 3-manifolds of positive Ricci curvature to topological properties of the set of embedded surfaces of genus $\leq g$, possibly with finitely many singularities. This completes a central component of our program of using topological methods to enumerating minimal surfaces with prescribed genus.
  As an application, we show that every 3-sphere of positive Ricci curvature contains at least 4 embedded minimal surfaces of genus 2.

</details>


### [72] [Quasi-linear equation $Δ_pv+av^q=0$ on manifolds with integral bounded Ricci curvature and geometric applications](https://arxiv.org/abs/2601.01837)
*Youde Wang,Guodong Wei,Liqin Zhang*

Main category: math.DG

TL;DR: The paper establishes Liouville theorems and gradient estimates for solutions to Δ_pv + av^q = 0 on complete Riemannian manifolds with χ-type Sobolev inequalities, with topological applications to uniqueness of ends.


<details>
  <summary>Details</summary>
Motivation: To extend and improve existing results on nonexistence and gradient estimates for p-Laplace type equations on Riemannian manifolds, moving beyond the "P-function" method used in previous work by Ciraolo, Farina and Polvara.

Method: Uses χ-type Sobolev inequalities and curvature conditions to establish Liouville theorems and gradient estimates. The approach differs from previous "P-function" methods, relying instead on Sobolev inequalities and curvature bounds.

Result: 1) Liouville theorem under χ-type Sobolev inequality with bounded L^{χ/(χ-1)} norm of negative part of Ricci curvature. 2) Lower estimate of volume growth for geodesic balls. 3) Local logarithm gradient estimate for positive solutions when χ ≤ n/(n-2) with L^γ-integrability of negative Ricci curvature. 4) Topological application: uniqueness of ends under Sobolev inequality and curvature conditions.

Conclusion: The paper provides new analytical tools for studying p-Laplace equations on Riemannian manifolds with Sobolev inequalities, offering improved Liouville theorems and gradient estimates with significant topological consequences for manifold structure.

Abstract: We consider nonexistence and gradient estimate for solutions to $Δ_pv +av^{q}=0$ defined on a complete Riemannian manifold with {\it $χ$-type Sobolev inequality}. A Liouville theorem on this equation is established if the lying manifold $(M, g)$ supports a {\it $χ$-type Sobolev inequality} and the $L^{\fracχ{χ-1}}$ norm of $\ric_-(x)$ of $(M, g)$ is bounded from upper by some constant depending on $\dim(M)$, Sobolev constant $\mathbb{S}_χ(M)$ and volume growth order of geodesic ball $B_r\subset M$. This extends and improves some conclusions obtained recently by Ciraolo, Farina and Polvara \cite{CFP}, but our method employed in this paper is different from their ``P-function" method. In particular, for such manifold with a {\it $χ$-type Sobolev inequality}, we give the lower estimate of volume growth of geodesic ball. If $χ\leq n/(n-2)$, we also establish the local logarithm gradient estimate for positive solutions to this equation under the condition $\ric_-(x)$ is $L^γ$-integrable where $γ>{\fracχ{χ-1}}$.
  As topological applications of main results(see \corref{main5}) we show that for a complete noncompact Riemannian manifold on which the Sobolev inequality \eqref{chi-n} holds true, $\dim(M)=n\geq 3$ and $\ric(x)\geq 0$ outside some geodesic ball $B(o,R_0)$, there exists a positive constant $C(n)$ depending only on $n$ such that, if $$\|\ric_-\|_{L^{\frac{n}{2}}}\leq C(n)\mathbb{S}_{\frac{n}{n-2}}(M),$$ then $(M, g)$ is of a unique end.

</details>


### [73] [Sub-Laplacian generalized curvature dimension inequalities on Riemannian foliations](https://arxiv.org/abs/2601.02035)
*Fabrice Baudoin,Guang Yang*

Main category: math.DG

TL;DR: Develops Bochner theory and Bakry-Emery calculus for horizontal Laplacians on Riemannian foliations without bundle-like or minimality assumptions, with applications to comparison theorems, compactness results, and heat semigroup estimates.


<details>
  <summary>Details</summary>
Motivation: To extend Bochner theory and Bakry-Emery calculus to horizontal Laplacians associated with general Riemannian foliations, removing restrictive assumptions like bundle-like metrics or minimal/totally geodesic leaves that limit previous approaches.

Method: Uses a metric connection adapted to the horizontal-vertical splitting to derive explicit Bochner formulas for horizontal Laplacians acting on horizontal and vertical gradients, involving horizontal Ricci curvature, torsion, and vertical mean curvature terms.

Result: Establishes generalized curvature dimension inequalities extending sub-Riemannian results, obtains horizontal Laplacian comparison theorems, Bonnet-Myers type compactness with diameter bounds, stochastic completeness, eigenvalue estimates, and gradient/regularization estimates for horizontal heat semigroup.

Conclusion: Develops a comprehensive Bochner theory framework for horizontal Laplacians on general Riemannian foliations with broad applications including contact manifolds and Carnot groups, providing powerful analytical tools without restrictive geometric assumptions.

Abstract: We develop a Bochner theory and Bakry-Emery calculus for horizontal Laplacians associated with general Riemannian foliations. No bundle-like assumption on the metric, nor any total geodesicity or minimality condition on the leaves is imposed. Using a metric connection adapted to the horizontal-vertical splitting, we derive explicit Bochner formulas for the horizontal Laplacian acting on horizontal and vertical gradients, as well as a unified identity for the full gradient. These formulas involve horizontal Ricci curvature, torsion and vertical mean curvature terms intrinsic to the foliated structure. From these identities, we establish generalized curvature dimension inequalities, extending earlier results in sub-Riemannian geometry. As applications, we obtain horizontal Laplacian comparison theorems, Bonnet-Myers type compactness results with explicit diameter bounds, stochastic completeness, first eigenvalue estimates and gradient and regularization estimates for the horizontal heat semigroup. The framework applies, in particular, to contact manifolds and Carnot groups of arbitrary step.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [74] [Low energy resolvent estimates for slowly decaying attractive potentials](https://arxiv.org/abs/2601.01102)
*Kenichi Ito,Tomoya Tagawa*

Main category: math-ph

TL;DR: Elementary commutator method proves low energy resolvent estimates for Schrödinger operators with slowly decaying attractive potentials, establishing Rellich's theorem, limiting absorption principle, and Sommerfeld's uniqueness theorem without microlocal or functional-analytic techniques.


<details>
  <summary>Details</summary>
Motivation: To establish fundamental spectral and scattering properties for Schrödinger operators with slowly decaying attractive potentials at low energies, which are important in quantum mechanics and scattering theory.

Method: Uses an elementary commutator method developed by Ito and Skibsted that avoids both microlocal analysis and advanced functional-analytic techniques, making the approach more accessible.

Result: Proves three key results: Rellich's theorem (absence of embedded eigenvalues), the limiting absorption principle (continuity of resolvent to real axis), and Sommerfeld's uniqueness theorem (uniqueness of outgoing solutions).

Conclusion: The elementary commutator method provides a powerful yet accessible approach to establishing fundamental resolvent estimates for Schrödinger operators with slowly decaying attractive potentials at low energies.

Abstract: We discuss the low energy resolvent estimates for the Schrödinger operator with slowly decaying attractive potential. The main results are Rellich's theorem, the limiting absorption principle and Sommerfeld's uniqueness theorem. For the proofs we employ an elementary commutator method due to Ito--Skibsted, for which neither of microlocal or functional-analytic techniques is required.

</details>


### [75] [Broadband quasistatic passive cloaking: bounds and limitations in the near-field regime](https://arxiv.org/abs/2601.02169)
*Maxence Cassier,Graeme W. Milton,Aaron Welters*

Main category: math-ph

TL;DR: The paper proves fundamental limitations on passive cloaking devices, showing they cannot make dielectric inclusions invisible over finite frequency intervals in the quasistatic regime, and provides quantitative bounds on cloaking effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental question of whether passive cloaks can make dielectric inclusions invisible over finite frequency intervals in the quasistatic regime of Maxwell's equations, and to establish theoretical limits on cloaking capabilities.

Method: Uses Dirichlet-to-Neumann (DtN) map analysis with two key approaches: 1) variational principles from composite theory via a new representation theorem for DtN maps, and 2) analytic methods connecting DtN maps to Herglotz and Stieltjes functions using passivity assumptions and sum rules.

Result: Provides negative answer to the possibility of passive cloaking over finite frequency intervals, establishes quantitative bounds on DtN maps involving physical parameters (frequency interval, device/obstacle volumes, permittivity), and shows fundamental limitations for both exact and approximate cloaking in lossy and lossless cases.

Conclusion: Passive cloaking devices have fundamental theoretical limitations that prevent them from achieving perfect invisibility over finite frequency intervals, with derived bounds providing quantitative constraints on cloaking performance based on physical parameters.

Abstract: We consider here several aspects of the following challenging question: is it possible to use a passive cloak to make invisible a dielectric inclusion on a finite frequency interval in the quasistatic regime of Maxwell's equations for an observer close to the object? In this work, by considering the Dirichlet-to-Neumann (DtN) map, we not only answer negatively this question, but we go further and provide some quantitative bounds on this map that provide fundamental limits to both cloaking as well as approximate cloaking. These bounds involve the following physical parameters: the length and center of the frequency interval, the volume of the cloaking device, the volume of the obstacle, and the relative permittivity of the object. Our approach is based on two key tools: i) variational principles from the abstract theory of composites and ii) the analytic approach to deriving bounds from sum rules for passive systems. To use i), we prove a new representation theorem for the DtN map which allows us to interpret this map as an effective operator in the abstract theory of composites. One important consequence of this representation is that it allows one to incorporate the broad and deep results from the theory of composites, such as variational principles, and to apply the bounds derived from them to the DtN map. These results could be useful in other contexts other than cloaking. Next, to use ii), we show that the passivity assumption allows us to connect the DtN map (as function of the frequency) with two important classes of analytic functions, namely, Herglotz and Stieltjes functions. The sum rules for these functions, combined with the variational approach, allows us to derive new inequalities on the DtN map which impose fundamental limitations on passive cloaking, both exact and approximate, over a frequency interval. We consider both cases of lossy and lossless cloaks.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [76] [Stochastic Control Methods for Optimization](https://arxiv.org/abs/2601.01248)
*Jinniao Qiu*

Main category: math.OC

TL;DR: Stochastic control framework for global optimization in Euclidean spaces and Wasserstein space, using regularized control problems, dynamic programming, and probabilistic representations with Monte Carlo schemes.


<details>
  <summary>Details</summary>
Motivation: To develop a unified stochastic control approach for global optimization problems in both finite-dimensional Euclidean spaces and the space of probability measures (Wasserstein space), addressing the challenge of finding global minima in complex optimization landscapes.

Method: 1) For Euclidean optimization: Approximate minimization problems with regularized stochastic control problems, analyze Hamilton-Jacobi-Bellman equations via dynamic programming, use Cole-Hopf transform and Feynman-Kac formula for tractable representations. 2) For probability measure optimization: Formulate regularized mean-field control problem with master equation, approximate with controlled N-particle systems. 3) Use Monte Carlo-based numerical schemes derived from probabilistic representations.

Result: Theoretical convergence: As regularization parameter → 0 (and particle number → ∞ for probability measure optimization), the control problem value converges to the global minimum of the original objective. Numerical experiments demonstrate practical performance and support theoretical convergence rates.

Conclusion: The proposed stochastic control framework provides a unified approach for global optimization in both Euclidean and Wasserstein spaces, with theoretical convergence guarantees and practical Monte Carlo-based numerical methods that perform well in experiments.

Abstract: In this work, we investigate a stochastic control framework for global optimization over both finite-dimensional Euclidean spaces and the Wasserstein space of probability measures. In the Euclidean setting, the original minimization problem is approximated by a family of regularized stochastic control problems; using dynamic programming, we analyze the associated Hamilton--Jacobi--Bellman equations and obtain tractable representations via the Cole--Hopf transform and the Feynman--Kac formula. For optimization over probability measures, we formulate a regularized mean-field control problem characterized by a master equation, and further approximate it by controlled $N$-particle systems. We establish that, as the regularization parameter tends to zero (and as the particle number tends to infinity for the optimization over probability measures), the value of the control problem converges to the global minimum of the original objective. Building on the resulting probabilistic representations, Monte Carlo-based numerical schemes are proposed and numerical experiments are reported to illustrate the practical performance of the methods and to support the theoretical convergence rates.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [LLMize: A Framework for Large Language Model-Based Numerical Optimization](https://arxiv.org/abs/2601.00874)
*M. Rizki Oktavian*

Main category: cs.LG

TL;DR: LLMize is an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning, treating optimization as a black-box process where solutions are generated in natural language and refined using feedback.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. There's a need for accessible optimization approaches that can handle complex, domain-specific tasks where constraints and heuristics are difficult to formalize mathematically.

Method: LLMize formulates optimization as a black-box process: candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. It supports multiple strategies including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing.

Result: Evaluated on convex optimization, linear programming, Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. While LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks.

Conclusion: LLMize enables practitioners to define complex optimization problems through natural language descriptions without requiring expertise in mathematical programming or metaheuristic design, offering a novel approach to optimization that leverages LLMs' reasoning capabilities for domain-specific applications.

Abstract: Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.

</details>


### [78] [Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space](https://arxiv.org/abs/2601.01295)
*Changhoon Song,Seungchan Ko,Youngjoon Hong*

Main category: cs.LG

TL;DR: The paper introduces log-weighted Barron spaces that require weaker regularity assumptions than classical Barron spaces, enabling better approximation bounds for deep ReLU networks with explicit depth dependence.


<details>
  <summary>Details</summary>
Motivation: Classical Barron spaces require stronger regularity than Sobolev spaces, and existing depth-sensitive results often impose restrictive constraints like sL ≤ 1/2. There's a need for function spaces with weaker assumptions to better explain the practical success of deep models on high-dimensional data.

Method: Introduce log-weighted Barron space ℬ^log with strictly weaker assumptions than classical ℬ^s spaces. Study embedding properties, conduct statistical analysis via Rademacher complexity, prove approximation bounds for deep ReLU networks with explicit depth dependence, define family ℬ^{s,log}, establish H^1 norm approximation bounds, and identify maximal depth scales.

Result: Functions in ℬ^log can be approximated by deep ReLU networks with explicit depth dependence. The new space requires weaker regularity than classical Barron spaces, and the paper identifies depth scales that preserve approximation rates, showing how depth reduces regularity requirements.

Conclusion: The log-weighted Barron space framework provides a more precise explanation for deep architecture performance beyond classical Barron settings, clarifying how depth reduces regularity requirements for efficient representation in high-dimensional problems.

Abstract: Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \le 1/2$. In this paper, we introduce a log-weighted Barron space $\mathscr{B}^{\log}$, which requires a strictly weaker assumption than $\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\mathscr{B}^{\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\mathscr{B}^{s,\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [79] [Data-driven sparse modeling and decomposition for superspreading-wetting dynamics of a droplet](https://arxiv.org/abs/2601.01776)
*Kai Fukami,Eita Shoji*

Main category: physics.flu-dyn

TL;DR: Data-driven modeling reveals a nanoparticle-induced bias flux term in nanofluid superspreading that explains surfactant-free wetting dynamics beyond classical lubrication theory.


<details>
  <summary>Details</summary>
Motivation: Superspreading in surfactant-free nanofluids defies traditional surfactant-driven explanations, requiring new theoretical frameworks to understand the complex wetting dynamics observed at nanometer scales.

Method: Combined high-precision phase-shifting imaging ellipsometry to measure nanometer-scale film thickness profiles with data-driven modeling to derive a compact partial differential equation governing droplet dynamics.

Result: For pure solvents, the model recovers classical lubrication physics. For nanofluids, a unique transport term scaling with the gradient of inverse film thickness emerges, representing nanoparticle-induced bias flux consistent with capillary wicking in precursor films.

Conclusion: Integration of high-precision measurements with data-driven modeling effectively unravels complex wetting dynamics, identifying nanofluid-specific mechanisms that explain superspreading without surfactants.

Abstract: Superspreading wetting is traditionally attributed to surfactant-driven mechanisms. However, recent observations of superspreading in surfactant-free nanofluids defy standard theoretical explanations. This study considers a data-driven approach to model droplet dynamics with the thickness of liquid films on the nanometer-micrometer scale in a compact form of a partial differential equation. We examine spatiotemporal film-thickness profiles resolved at the nanometer scale via phase-shifting imaging ellipsometry. For a pure solvent, the present governing equation recovers the classical lubrication physics driven by disjoining pressure and evaporation. In contrast, the nanofluid dynamics necessitates a unique transport term scaling with the gradient of the inverse film thickness. Theoretical analysis suggests this term represents a nanoparticle-induced bias flux, consistent with a hypothesized capillary wicking mechanism within the precursor film. The identification of the current nanofluid-specific term underscores the efficacy of integrating high-precision experimental measurements with data-driven modeling to unravel complex wetting dynamics.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [80] [A stable and accurate X-FFT solver for linear elastic homogenization problems in 3D](https://arxiv.org/abs/2601.02172)
*Flavia Gehrig,Matti Schneider*

Main category: cs.CE

TL;DR: Novel X-FFT solver combines X-FEM with FFT methods to achieve interface-conforming accuracy for 3D mechanical problems with smooth material interfaces.


<details>
  <summary>Details</summary>
Motivation: Traditional FFT-based methods fail to accurately capture material interfaces not aligned with the computational grid, leading to suboptimal accuracy in mechanical simulations.

Method: Integrates extended finite element (X-FEM) discretization into FFT-based framework, uses modified absolute enrichment, and develops preconditioner based on strongly stable GFEM to address conditioning issues.

Result: The X-FFT solver achieves interface-conforming accuracy, numerical efficiency, and stability when solving 3D linear elastic homogenization problems with smooth material interfaces.

Conclusion: The proposed X-FFT solver successfully bridges the gap between FFT efficiency and interface-conforming accuracy, offering a robust solution for 3D mechanical problems with complex material interfaces.

Abstract: Although FFT-based methods are renowned for their numerical efficiency and stability, traditional discretizations fail to capture material interfaces that are not aligned with the grid, resulting in suboptimal accuracy. To address this issue, the work at hand introduces a novel FFT-based solver that achieves interface-conforming accuracy for three-dimensional mechanical problems. More precisely, we integrate the extended finite element (X-FEM) discretization into the FFT-based framework, leveraging its ability to resolve discontinuities via additional shape functions. We employ the modified abs(olute) enrichment and develop a preconditioner based on the concept of strongly stable GFEM, which mitigates the conditioning issues observed in traditional X-FEM implementations. Our computational studies demonstrate that the developed X-FFT solver achieves interface-conforming accuracy, numerical efficiency, and stability when solving three-dimensional linear elastic homogenization problems with smooth material interfaces.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [81] [thornado+Flash-X: A Hybrid DG-IMEX and Finite-Volume Framework for Neutrino-Radiation Hydrodynamics in Core-Collapse Supernovae](https://arxiv.org/abs/2601.00976)
*Eirik Endeve,Vassilios Mewes,J. Austin Harris,M. Paul Laiu,Ran Chu,Steven A. Fromm,Anthony Mezzacappa,O. E. Bronson Messer,W. Raphael Hix,Stephen W. Bruenn,Eric J. Lentz,Klaus Weide,Christian Y. Cardall,Ann S. Almgren,Anshu Dubey,Sean M. Couch,Philipp Moesta,Donald E. Willcox*

Main category: astro-ph.IM

TL;DR: Development of thornado toolkit for high-order neutrino-radiation hydrodynamics coupled to Flash-X framework, featuring GPU-enabled neutrino transport with DG methods and implicit neutrino-matter coupling for core-collapse supernova simulations.


<details>
  <summary>Details</summary>
Motivation: To enable increasingly realistic large-scale core-collapse supernova (CCSN) simulations by developing advanced neutrino-transport algorithms that can be coupled to existing multiphysics simulation frameworks like Flash-X.

Method: Implemented spectral six-species two-moment neutrino transport with algebraic closure and O(v/c) relativistic corrections using discontinuous Galerkin (DG) methods. Developed nonlinear neutrino-matter coupling with nested fixed-point iteration and Anderson acceleration for fully implicit treatment of collisional processes. Coupled to Flash-X's finite-volume hydrodynamics using hybrid DG-FV representation and operator-split evolution.

Result: Verified implementation with basic transport tests and relaxation/deleptonization problems. Demonstrated accuracy in spherically symmetric CCSN simulations with close agreement to Chimera code. Showed viability for multidimensional modeling through axisymmetric CCSN simulation. GPU-enabled implementation using OpenMP offloading/OpenACC.

Conclusion: The thornado toolkit establishes a foundation for future enhancements in physics fidelity, numerical algorithms, and computational performance, enabling increasingly realistic large-scale CCSN simulations within the Flash-X framework.

Abstract: We present neutrino-transport algorithms implemented in the toolkit for high-order neutrino-radiation hydrodynamics (thornado) and their coupling to self-gravitating hydrodynamics within the adaptive mesh refinement (AMR)-based multiphysics simulation framework Flash-X. thornado, developed primarily for simulations of core-collapse supernovae (CCSNe), employs a spectral, six-species two-moment formulation with algebraic closure and special-relativistic observer corrections accurate to $O(v/c)$, and uses discontinuous Galerkin (DG) methods for phase-space discretization combined with implicit-explicit time stepping. A key development is a nonlinear neutrino-matter coupling algorithm based on nested fixed-point iteration with Anderson acceleration, enabling fully implicit treatment of collisional processes, including energy-coupling interactions such as neutrino-electron scattering and pair production. Coupling to finite-volume (FV) hydrodynamics is achieved with a hybrid DG-FV representation of the fluid variables and operator-split evolution in Flash-X. The implementation is verified using basic transport tests with idealized opacities and relaxation and deleptonization problems with tabulated microphysics. Spherically symmetric CCSN simulations demonstrate accuracy and robustness of the coupled scheme, including close agreement with the CCSN simulation code Chimera. An axisymmetric CCSN simulation further demonstrates the viability of DG-based neutrino transport for multidimensional supernova modeling within Flash-X. thornado's neutrino-transport solver is GPU-enabled using OpenMP offloading or OpenACC, and all CCSN applications included in this work use the GPU implementation. Together, these results establish a foundation for future enhancements in physics fidelity, numerical algorithms, and computational performance, for increasingly realistic large-scale CCSN simulations.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [82] [Ion Temperature Inference from Neutron Counting in Maxwellian Deuterium Plasmas](https://arxiv.org/abs/2601.01566)
*Allison J. Radich,Vlad Grecu,Patrick J. F. Carle,Myles Hildebrand,Stephen J. Howard,Colin P. McNally,Meritt Reynolds,Akbar Rohollahi,Ryan E. Underwood,Sara Weinstein*

Main category: physics.ins-det

TL;DR: Method for inferring deuterium fuel ion temperature from neutron counts using fast liquid scintillators in Maxwellian velocity distribution conditions, applied to magnetized target fusion.


<details>
  <summary>Details</summary>
Motivation: Need for time-resolved ion temperature diagnostics in Magnetized Target Fusion applications that doesn't require direct line-of-sight to plasma or neutron collimation.

Method: Combine local neutron count rates from fast liquid scintillators to estimate total neutron yield, with absolute detection efficiency determined via MCNP neutron scattering simulation based on 3D experiment model. Uses pulse-shape discrimination, pile-up correction, detector calibration, and uncertainty characterization.

Result: Method successfully demonstrated on General Fusion's Plasma Injector 3 spherical tokamak, with results compared to Ion Doppler spectroscopy ion temperature diagnostic.

Conclusion: The neutron-based method provides a viable time-resolved ion temperature diagnostic for Magnetized Target Fusion that avoids line-of-sight requirements and neutron collimation, validated against established spectroscopy techniques.

Abstract: A method is presented for inferring the deuterium fuel ion temperature from neutron counts measured with fast liquid scintillators in conditions where the ion velocity distribution is Maxwellian. Local neutron count rates at each scintillator position are combined to estimate total neutron yield from the plasma, where absolute detection efficiency is determined via MCNP neutron scattering simulation based on a 3D model of the experiment structure. This method is particularly advantageous for Magnetized Target Fusion applications as it yields a time-resolved diagnostic and does not require direct line-of-sight to the plasma or collimation of the neutrons. The instrumentation configuration, pulse-shape discrimination and pile-up correction algorithms, detector calibration, and ion temperature calculation method with uncertainty characterization are discussed. An application of the method to General Fusion's Plasma Injector~3 (PI3) spherical tokamak device is demonstrated and the results are compared to an Ion Doppler spectroscopy ion temperature diagnostic.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [83] [Cauchy Data for Formation of Multiple Black Holes with Prescribed ADM Parameters](https://arxiv.org/abs/2601.01517)
*Dawei Shen,Jingbo Wan*

Main category: gr-qc

TL;DR: Simple construction of smooth, asymptotically flat vacuum initial data for relativistic N-body collapse with independently prescribed ADM energy, momentum, and angular momentum for each component, satisfying E>|P| condition.


<details>
  <summary>Details</summary>
Motivation: To model relativistic collapsing N-body systems with multiple black holes, allowing independent specification of physical parameters for each component while ensuring proper causal structure and dynamical formation of trapped regions.

Method: Simple construction of smooth, asymptotically flat vacuum initial data with prescribed ADM parameters for each collapsing component, ensuring timelike condition E>|P| and no initial trapped surfaces.

Result: Initial data contains no trapped surfaces initially, but future development contains multiple causally independent trapped regions that form dynamically from localized subsets. Well-separated collapsing components with relative motion yield spacetimes containing multiple black holes.

Conclusion: The construction provides a foundation for studying multiple black hole formation from collapsing N-body systems with independently specified physical parameters, enabling investigation of relativistic gravitational collapse dynamics and black hole formation.

Abstract: We give a simple construction of smooth, asymptotically flat vacuum initial data modeling a relativistic collapsing $N$--body system, with independently prescribed ADM energy, linear momentum, and angular momentum for each component, subject to the timelike condition $\E>|¶|$. The initial data contain no trapped surfaces, and the future development contains multiple causally independent trapped regions that dynamically form from localized subsets of the initial slice. In particular, the maximal development of data with well-separated collapsing components and relative motion is expected to yield spacetimes containing multiple black holes.

</details>
