<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 14]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Discontinuous Galerkin finite element operator network for solving non-smooth PDEs](https://arxiv.org/abs/2601.03668)
*Kapil Chawla,Youngjoon Hong,Jae Yong Lee,Sanghyun Lee*

Main category: math.NA

TL;DR: DG-FEONet is a data-free operator learning framework that combines discontinuous Galerkin methods with neural networks to solve parametric PDEs with discontinuous coefficients and non-smooth solutions without requiring training data.


<details>
  <summary>Details</summary>
Motivation: Traditional operator learning models like DeepONet and Fourier Neural Operator require large paired datasets and struggle with sharp features in PDEs with discontinuous coefficients and non-smooth solutions. There's a need for data-free methods that can handle discontinuities effectively.

Method: Combines discontinuous Galerkin (DG) method with neural networks using Symmetric Interior Penalty Galerkin (SIPG) scheme. Minimizes residual of DG-based weak formulation, predicts element-wise solution coefficients via neural network, enabling data-free training without precomputed input-output pairs.

Result: Validated on 1D and 2D PDE problems, demonstrating accurate recovery of discontinuities, strong generalization across parameter space, and reliable convergence rates. Shows potential for robust, singularity-aware operator approximation.

Conclusion: DG-FEONet successfully combines local discretization schemes with machine learning to achieve robust operator approximation for challenging PDEs with discontinuities, offering a data-free alternative to traditional operator learning approaches.

Abstract: We introduce Discontinuous Galerkin Finite Element Operator Network (DG--FEONet), a data-free operator learning framework that combines the strengths of the discontinuous Galerkin (DG) method with neural networks to solve parametric partial differential equations (PDEs) with discontinuous coefficients and non-smooth solutions. Unlike traditional operator learning models such as DeepONet and Fourier Neural Operator, which require large paired datasets and often struggle near sharp features, our approach minimizes the residual of a DG-based weak formulation using the Symmetric Interior Penalty Galerkin (SIPG) scheme. DG-FEONet predicts element-wise solution coefficients via a neural network, enabling data-free training without the need for precomputed input-output pairs. We provide theoretical justification through convergence analysis and validate the model's performance on a series of one- and two-dimensional PDE problems, demonstrating accurate recovery of discontinuities, strong generalization across parameter space, and reliable convergence rates. Our results highlight the potential of combining local discretization schemes with machine learning to achieve robust, singularity-aware operator approximation in challenging PDE settings.

</details>


### [2] [Local Interpolation via Low-Rank Tensor Trains](https://arxiv.org/abs/2601.03885)
*Siddhartha E. Guzman,Egor Tiunov,Leandro Aolita*

Main category: math.NA

TL;DR: A novel TT interpolation method that constructs finer-scale TT representations from coarse ones with guaranteed error bounds, exponential compression, and logarithmic complexity.


<details>
  <summary>Details</summary>
Motivation: Existing TT methods struggle with high computational costs and fail to recover low-rank structure for complex functions, limiting practical applications in high-dimensional computations.

Method: Low-rank TT interpolation framework that takes a TT on a coarse grid and constructs a finer-scale TT with additional cores while maintaining constant rank in the last m cores, ensuring ℓ²-norm error bounds.

Result: Achieves exponential compression at fixed accuracy, logarithmic complexity with respect to grid points, and successfully applies to 1D/2D/3D problems including airfoil embeddings, image super-resolution, and synthetic turbulence generation.

Conclusion: Enables scalable TT-native solvers for complex geometries and multiscale generative models, with broad implications for scientific simulation, imaging, and real-time graphics.

Abstract: Tensor Train (TT) decompositions provide a powerful framework to compress grid-structured data, such as sampled function values, on regular Cartesian grids. Such high compression, in turn, enables efficient high-dimensional computations. Exact TT representations are only available for simple analytic functions. Furthermore, global polynomial or Fourier expansions typically yield TT-ranks that grow proportionally with the number of basis terms. State-of-the-art methods are often prohibitively expensive or fail to recover the underlying low-rank structure. We propose a low-rank TT interpolation framework that, given a TT describing a discrete (scalar-, vector-, or tensor-valued) function on a coarse regular grid with $n$ cores, constructs a finer-scale version of the same function represented by a TT with $n+m$ cores, where the last $m$ cores maintain constant rank. Our method guarantees a $\ell^{2}$-norm error bound independent of the total number of cores, achieves exponential compression at fixed accuracy, and admits logarithmic complexity with respect of the number of grid points. We validate its performance through numerical experiments, including 1D, 2D, and 3D applications such as: 2D and 3D airfoil mask embeddings, image super-resolution, and synthetic noise fields such as 3D synthetic turbulence. In particular, we generate fractal noise fields directly in TT format with logarithmic complexity and memory. This work opens a path to scalable TT-native solvers with complex geometries and multiscale generative models, with implications from scientific simulation to imaging and real-time graphics.

</details>


### [3] [Constrained dynamics for searching saddle points on general Riemannian manifolds](https://arxiv.org/abs/2601.03931)
*Yukuan Hu,Laura Grazioli*

Main category: math.NA

TL;DR: Developed a constrained saddle dynamics framework for general Riemannian manifolds using Grassmann bundle geometry, with rigorous convergence guarantees for saddle-search algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing constrained saddle point methods are limited to special manifolds with global regular level-set representations, excluding important applications like electronic excited-state calculations. There's a need for a universal approach applicable to general Riemannian manifolds.

Method: Formulated constrained saddle dynamics over the Grassmann bundle of the tangent bundle, incorporating the second fundamental form to capture tangent space variations along trajectories. The approach respects intrinsic quotient structure and uses nonredundant parametrizations.

Result: Established local linear stability of the dynamics and local linear convergence of resulting algorithms, providing first convergence guarantees for discretized saddle-search algorithms in manifold settings. Numerical experiments on linear eigenvalue problems and electronic excited-state calculations demonstrate effectiveness.

Conclusion: The developed framework enables constrained saddle point search on general Riemannian manifolds, removing unnecessary nondegeneracy assumptions and addressing the ill-conditioning nature of saddle point location compared to finding local minimizers.

Abstract: Finding constrained saddle points on Riemannian manifolds is significant for analyzing energy landscapes arising in physics and chemistry. Existing works have been limited to special manifolds that admit global regular level-set representations, excluding applications such as electronic excited-state calculations. In this paper, we develop a constrained saddle dynamics applicable to smooth functions on general Riemannian manifolds. Our dynamics is formulated compactly over the Grassmann bundle of the tangent bundle. By analyzing the Grassmann bundle geometry, we achieve universality via incorporating the second fundamental form, which captures variations of tangent spaces along the trajectory. We rigorously establish the local linear stability of the dynamics and the local linear convergence of the resulting algorithms. Remarkably, our analysis provides the first convergence guarantees for discretized saddle-search algorithms in manifold settings. Moreover, by respecting the intrinsic quotient structure, we remove unnecessary nondegeneracy assumptions on the eigenvalues of the Riemannian Hessian that are present in existing works. We also point out that locating saddle points can be more ill-conditioning than finding local minimizers, and requires using nonredundant parametrizations. Finally, numerical experiments on linear eigenvalue problems and electronic excited-state calculations showcase the effectiveness of the proposed algorithms and corroborate the established local theory.

</details>


### [4] [On the importance of smoothness, interface resolution and numerical sensitivities in shape and topological sensitivity analysis](https://arxiv.org/abs/2601.03967)
*M. H. Gfrerer,P. Gangl*

Main category: math.NA

TL;DR: The paper analyzes how PDE discretization methods affect shape and topological derivatives, showing that enriched methods with interface capture are necessary for proper convergence.


<details>
  <summary>Details</summary>
Motivation: To understand how different discretization approaches for PDE constraints influence the computation and convergence of shape and topological derivatives, which are crucial for shape optimization problems.

Method: Two discretization methods are studied: 1) Standard method using splines of degree p (including linear hat functions for p=1) that ignore interface locations, and 2) Enriched method that incorporates interface locations via enrichment functions. Both methods are applied to a tracking-type functional and a two-material Poisson problem in 1D.

Result: Shape derivative regularity depends on basis function smoothness. Point-wise convergence of shape derivatives requires interface consideration in the ansatz space. Only the enriched method converges for topological derivatives.

Conclusion: For accurate computation of shape and topological derivatives in PDE-constrained optimization, discretization must properly account for interface locations through enrichment methods rather than using standard approaches that ignore interfaces.

Abstract: In this paper we investigate the influence of the discretization of PDE constraints on shape and topological derivatives. To this end, we study a tracking-type functional and a two-material Poisson problem in one spatial dimension. We consider the discretization by a standard method and an enriched method. In the standard method we use splines of degree $p$ such that we can control the smoothness of the basis functions easily, but do not take any interface location into consideration. This includes for p=1 the usual hat basis functions. In the enriched method we additionally capture the interface locations in the ansatz space by enrichment functions. For both discretization methods shape and topological sensitivity analysis is performed. It turns out that the regularity of the shape derivative depends on the regularity of the basis functions. Furthermore, for point-wise convergence of the shape derivative the interface has to be considered in the ansatz space. For the topological derivative we show that only the enriched method converges.

</details>


### [5] [Posterior error bounds for prior-driven balancing in linear Gaussian inverse problems](https://arxiv.org/abs/2601.03971)
*Josie König,Han Cheng Lie*

Main category: math.NA

TL;DR: The paper provides error bounds for approximate Bayesian inverse problems using model order reduction, specifically connecting systems theory with inverse problems to derive a priori error bounds for smoothing problems.


<details>
  <summary>Details</summary>
Motivation: In large-scale Bayesian inverse problems, exact forward model evaluations are computationally expensive. There's a need to use approximate forward models while controlling approximation quality, particularly for linear time-invariant dynamical systems where we want to infer initial conditions from partial observations.

Method: The paper uses perturbation theory for inverses to bound errors in approximate posterior distributions for linear Gaussian inverse problems. For smoothing problems in linear time-invariant systems, it establishes a connection between the prior-preconditioned Hessian and the impulse response of a prior-driven system, then applies this connection to prove error bounds for model order reduction methods (specifically balanced truncation).

Result: The paper proves the first a priori error bounds for system-theoretic model order reduction methods applied to smoothing problems. The bounds control approximation error of posterior mean and covariance in terms of truncated Hankel singular values of the underlying system, revealing a novel connection between systems theory and inverse problems.

Conclusion: The work establishes a theoretical foundation for using model order reduction in Bayesian inverse problems, providing rigorous error bounds that connect systems theory concepts (Hankel singular values) with statistical inference quality, enabling principled use of approximate forward models in large-scale inverse problems.

Abstract: In large-scale Bayesian inverse problems, it is often necessary to apply approximate forward models to reduce the cost of forward model evaluations, while controlling approximation quality. In the context of Bayesian inverse problems with linear forward models, Gaussian priors, and Gaussian noise, we use perturbation theory for inverses to bound the error in the approximate posterior mean and posterior covariance resulting from a linear approximate forward model. We then focus on the smoothing problem of inferring the initial condition of linear time-invariant dynamical systems, using finitely many partial state observations. For such problems, and for a specific model order reduction method based on balanced truncation, we show that the impulse response of a certain prior-driven system is closely related to the prior-preconditioned Hessian of the inverse problem. This reveals a novel connection between systems theory and inverse problems. We exploit this connection to prove the first a priori error bounds for system-theoretic model order reduction methods applied to smoothing problems. The bounds control the approximation error of the posterior mean and covariance in terms of the truncated Hankel singular values of the underlying system.

</details>


### [6] [A Bivariate Spline Construction of Orthonormal Polynomials over Polygonal Domains and Its Applications to Quadrature](https://arxiv.org/abs/2601.04022)
*Ming-Jun Lai*

Main category: math.NA

TL;DR: Methods for constructing orthogonal/orthonormal polynomials over arbitrary polygonal domains using bivariate spline functions, with applications to quadrature rules.


<details>
  <summary>Details</summary>
Motivation: Need for computational methods to construct orthogonal polynomials over arbitrary polygonal domains for numerical integration and analysis purposes.

Method: Uses bivariate spline functions with MATLAB implementation to generate spline spaces over triangulations. Develops two algorithms: one for orthonormal polynomials of degree d, and another for orthonormal polynomials of degree d+1 in the orthogonal complement of P_d. Also introduces polynomial reduction strategies based on odd- and even-degree orthogonal polynomials.

Result: Numerical examples for degrees d=1-5 illustrate polynomial structure and zero curves, providing evidence against Gauss quadrature existence on centrally symmetric domains. Introduces polynomial reduction strategies that lead to new quadrature schemes and efficient, high-precision quadrature rules for various polygonal domains.

Conclusion: The paper presents effective computational methods for constructing orthogonal polynomials over arbitrary polygonal domains, enabling new quadrature schemes and providing insights about Gauss quadrature limitations on certain domains.

Abstract: We present computational methods for constructing orthogonal/orthonormal polynomials over arbitrary polygonal domains in $\mathbb{R}^2$
  using bivariate spline functions. Leveraging a mature MATLAB implementation which generates spline spaces of any degree, any smoothness over any triangulation, we have exact polynomial representation over the polygonal domain of interest. Two algorithms are developed: one constructs orthonormal polynomials of degree $d>0$
  over a polygonal domain, and the other constructs orthonormal polynomials of degree $d+1$ in the orthogonal complement of $\mathbb{P}_d$. Numerical examples for degrees $d=1--5$ illustrate the structure and zero curves of these polynomials, providing evidence against the existence of Gauss quadrature on centrally symmetric domains. In addition, we introduce polynomial reduction strategies based on odd- and even-degree orthogonal polynomials, reducing the integration to the integration of its residual quadratic or linear polynomials. These reductions motivate new quadrature schemes, which we further extend through polynomial interpolation to obtain efficient, high-precision quadrature rules for various polygonal domains.

</details>


### [7] [A higher order sparse grid combination technique](https://arxiv.org/abs/2601.04075)
*Julia Muñoz-Echániz,Christoph Reisinger*

Main category: math.NA

TL;DR: Generalized sparse grid combination technique combines multivariate extrapolation with standard combination formula to boost second-order accurate finite difference schemes to fourth-order accuracy on sparse grids.


<details>
  <summary>Details</summary>
Motivation: To develop efficient high-order numerical methods for solving PDEs in high dimensions by leveraging sparse grid techniques and extrapolation to achieve higher accuracy without requiring dense grids.

Method: Combines multivariate extrapolation of finite difference solutions with standard sparse grid combination formula; analyzes error expansion terms as solutions of semi-discrete problems; verifies assumptions on Poisson problem with smooth data.

Result: Achieves fourth-order combined sparse grid solution from second-order scheme; demonstrates practical convergence in up to seven dimensions; validates theoretical framework on Poisson problem.

Conclusion: The generalized sparse grid combination technique successfully lifts second-order schemes to fourth-order accuracy, providing an efficient approach for high-dimensional PDE problems with verified theoretical foundations and practical performance.

Abstract: We show that a generalised sparse grid combination technique which combines multi-variate extrapolation of finite difference solutions with the standard combination formula lifts a second order accurate scheme on regular meshes to a fourth order combined sparse grid solution. In the analysis, working in a general dimension, we characterise all terms in a multivariate error expansion of the scheme as solutions of a sequence of semi-discrete problems. This is first carried out formally under suitable assumptions on the truncation error of the scheme, stability and regularity of solutions. We then verify the assumptions on the example of the Poisson problem with smooth data, and illustrate the practical convergence in up to seven dimensions.

</details>


### [8] [Algebraic Multigrid with Overlapping Schwarz Smoothers and Local Spectral Coarse Grids for Least Squares Problems](https://arxiv.org/abs/2601.04112)
*Ben S. Southworth,Hussam Al Daas,Golo A> Wimmer,Ed Threlfall*

Main category: math.NA

TL;DR: A new algebraic multigrid method for sparse least-squares systems that blends aggregation-based coarsening, overlapping Schwarz smoothers, and spectral coarse spaces to solve problems where classical AMG fails, particularly for anisotropic operators.


<details>
  <summary>Details</summary>
Motivation: Classical AMG methods fail on challenging scientific computing applications involving sparse least-squares systems and anisotropic problems, particularly in areas like magnetic confinement fusion where existing methods cannot reduce residuals.

Method: Blends aggregation-based coarsening, overlapping Schwarz smoothers, and locally constructed spectral coarse spaces. Exploits factorized structure of A to create inexpensive symmetric positive semidefinite splitting, yielding local generalized eigenproblems that define sparse, nonoverlapping coarse basis functions. Creates fully algebraic, recursive multilevel hierarchy that can coarsen slowly for AMG-like complexity or aggressively for robustness.

Result: Achieves convergence rates independent of anisotropy on rotated diffusion problems, remains scalable with problem size. For small anisotropy, comparable to classical AMG. Most notably, solves extremely anisotropic heat conduction operators in magnetic confinement fusion where AMG and smoothed aggregation fail completely.

Conclusion: Proposes a robust least-squares AMG-DD method that requires no geometric information, avoids global eigenvalue solves, maintains parallelizable setup, and provides efficient convergence across many orders of magnitude in anisotropy strength for problems unsolvable by existing AMG methods.

Abstract: This paper develops a new algebraic multigrid (AMG) method for sparse least-squares systems of the form $A=G^TG$ motivated by challenging applications in scientific computing where classical AMG methods fail. First we review and relate the use of local spectral problems in distinct fields of literature on AMG, domain decomposition (DD), and multiscale finite elements. We then propose a new approach blending aggregation-based coarsening, overlapping Schwarz smoothers, and locally constructed spectral coarse spaces. By exploiting the factorized structure of $A$, we construct an inexpensive symmetric positive semidefinite splitting that yields local generalized eigenproblems whose solutions define sparse, nonoverlapping coarse basis functions. This enables a fully algebraic and naturally recursive multilevel hierarchy that can either coarsen slowly to achieve AMG-like operator complexities, or coarsen aggressively-with correspondingly larger local spectral problems-to ensure robustness on problems that cannot be solved by existing AMG methods. The method requires no geometric information, avoids global eigenvalue solves, and maintains efficient parallelizable setup through localized operations. Numerical experiments demonstrate that the proposed least-squares AMG-DD method achieves convergence rates independent of anisotropy on rotated diffusion problems and remains scalable with problem size, while for small amounts of anisotropy we obtain convergence and operator complexities comparable with classical AMG methods. Most notably, for extremely anisotropic heat conduction operators arising in magnetic confinement fusion, where AMG and smoothed aggregation fail to reduce the residual even marginally, our method provides robust and efficient convergence across many orders of magnitude in anisotropy strength.

</details>


### [9] [Quantitative Constraints for Stable Sampling on the Sphere](https://arxiv.org/abs/2601.04119)
*Martin Ehler,Karlheinz Gröchenig*

Main category: math.NA

TL;DR: The paper derives explicit quantitative volume constraints for sampling measures on spheres that satisfy Marcinkiewicz-Zygmund inequalities, with emphasis on fully explicit constants.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explicit constants in existing literature on Marcinkiewicz-Zygmund inequalities for sampling measures on spheres, and to provide genuinely quantitative bounds that can be applied to practical problems.

Method: Using precise localization estimates for Jacobi polynomials to obtain explicit upper and lower bounds on the μ_t-mass of geodesic balls at scale t^{-1}, with special emphasis on fully explicit constants.

Result: Obtained explicit quantitative volume constraints for sampling measures satisfying Marcinkiewicz-Zygmund inequalities, including bounds for s-dimensional Hausdorff volume of sampling sets and optimal lower bounds for length of Marcinkiewicz-Zygmund curves.

Conclusion: The paper provides genuinely quantitative results with explicit constants for Marcinkiewicz-Zygmund sampling on spheres, addressing a gap in the literature where constants are typically left implicit.

Abstract: We derive quantitative volume constraints for sampling measures $μ_t$ on the unit sphere $\mathbb{S}^d$ that satisfy Marcinkiewicz-Zygmund inequalities of order $t$. Using precise localization estimates for Jacobi polynomials, we obtain explicit upper and lower bounds on the $μ_t$-mass of geodesic balls at the natural scale $t^{-1}$. Whereas constants are typically left implicit in the literature, we place special emphasis on fully explicit constants, and the results are genuinely quantitative. Moreover, these bounds yield quantitative constraints for the $s$-dimensional Hausdorff volume of Marcinkiewicz-Zygmund sampling sets and, in particular, optimal lower bounds for the length of Marcinkiewicz-Zygmund curves.

</details>


### [10] [Active subspace methods and derivative-based Shapley effects for functions with non-independent variables](https://arxiv.org/abs/2601.04132)
*Matieyendou Lamboni,Sergei Kucherenko*

Main category: math.NA

TL;DR: Extends active subspace methods to handle non-independent variables, introduces sensitivity-based active subspaces as alternative to derivative-based approaches, with theoretical guarantees and practical implementations.


<details>
  <summary>Details</summary>
Motivation: Existing derivative-based active subspace methods and Shapley effects assume independent variables, but many real-world problems involve non-independent variables. Need methods that can handle variable dependencies while identifying influential subspaces.

Method: Extends derivative-based active subspace methods to non-independent variables using gradient theory for dependent variables. Introduces sensitivity-based active subspaces that seek reduced variable sets to minimize function variance. Provides optimal gradient computations with dimension-free bias bounds and parametric convergence rates.

Result: Theoretical framework for handling non-independent variables established. Both derivative-based and sensitivity-based methods show varying relative performance across different functions in simulations. Methods provide practical implementations with theoretical guarantees.

Conclusion: Successfully extends active subspace methodology to non-independent variables, offering both derivative-based and sensitivity-based approaches. The choice between methods depends on the specific function characteristics, providing practitioners with flexible tools for uncertainty quantification with dependent variables.

Abstract: Lower-dimensional subspaces that impact estimates of uncertainty are often described by Linear combinations of input variables, leading to active variables. This paper extends the derivative-based active subspace methods and derivative-based Shapley effects to cope with functions with non-independent variables, and it introduces sensitivity-based active subspaces. While derivative-based subspace methods focus on directions along which the function exhibits significant variation, sensitivity-based subspace methods seek a reduced set of active variables that enables a reduction in the function's variance. We propose both theoretical results using the recent development of gradients of functions with non-independent variables and practical settings by making use of optimal computations of gradients, which admit dimension-free upper-bounds of the biases and the parametric rate of convergence. Simulations show that the relative performance of derivative-based and sensitivity-based active subspaces methods varies across different functions.

</details>


### [11] [Efficient third-order iterative algorithms for computing zeros of special functions](https://arxiv.org/abs/2601.04148)
*Dhivya Prabhu K,Sanjeev Singh,Antony Vijesh*

Main category: math.NA

TL;DR: A third-order iterative method for computing zeros of solutions to second-order ODEs, derived via trapezoidal rule approximation of Riccati equations, with proven convergence conditions and applications to special functions.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable and efficient numerical method for finding zeros of solutions to second-order ordinary differential equations, which is important for analyzing special functions like Legendre, Hermite, Bessel, and other mathematical functions that satisfy such ODEs.

Method: Derived a third-order iterative procedure by approximating the solution of the related Riccati differential equation using the trapezoidal rule. Established sufficient conditions for theoretical non-local convergence and provided suitable initial guesses for computing all zeros in a given interval.

Result: The method works for orthogonal polynomials (Legendre, Hermite) and special functions (Bessel, Coulomb wave, confluent hypergeometric, cylinder functions) that satisfy the convergence conditions. Numerical simulations demonstrate effectiveness, and comparative analysis shows advantages over recent studies.

Conclusion: The proposed third-order iterative method is reliable and effective for computing zeros of solutions to second-order ODEs, with proven convergence theory and practical applicability to important mathematical functions.

Abstract: This manuscript presents a novel and reliable third-order iterative procedure for computing the zeros of solutions to second-order ordinary differential equations. By approximating the solution of the related Riccati differential equation using the trapezoidal rule, this study has derived the proposed third-order method. This work establishes sufficient conditions to ensure the theoretical non-local convergence of the proposed method. This study provides suitable initial guesses for the proposed third-order iterative procedure to compute all zeros in a given interval of the solutions to second-order ordinary differential equations. The orthogonal polynomials like Legendre and Hermite, as well as the special functions like Bessel, Coulomb wave, confluent hypergeometric, and cylinder functions, satisfy the proposed conditions for convergence. Numerical simulations demonstrate the effectiveness of the proposed theory. This work also presents a comparative analysis with recent studies.

</details>


### [12] [From Penrose to Melrose: Computing Scattering Amplitudes at Infinity for Unbounded Media](https://arxiv.org/abs/2601.04167)
*Anıl Zenginoğlu*

Main category: math.NA

TL;DR: A geometric scattering method using conformal compactification to compute scattering amplitudes for Helmholtz equation in variable unbounded media with long-range asymptotics, providing far-field data without Green's functions.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework for computing scattering amplitudes in variable, unbounded media with potentially long-range asymptotics, avoiding reliance on explicit solutions or Green's function representations.

Method: Combines Penrose's conformal compactification with Melrose's geometric scattering theory to formulate the time-harmonic scattering problem on a compactified manifold with boundary. Constructs a two-step asymptotic solver for scattering amplitudes at infinity, designed to couple with interior solvers via domain decomposition.

Result: Numerical experiments for constant, short-range, and long-range media with single-mode and Gaussian beam incidence demonstrate spectral convergence of computed scattering amplitudes in all cases.

Conclusion: The method provides a unified geometric framework for scattering in variable media where both incident and scattered fields solve the same background Helmholtz operator, enabling far-field computation without explicit Green's functions.

Abstract: We develop a method to compute scattering amplitudes for the Helmholtz equation in variable, unbounded media with possibly long-range asymptotics. Combining Penrose's conformal compactification and Melrose's geometric scattering theory, we formulate the time-harmonic scattering problem on a compactified manifold with boundary and construct a two-step solver for scattering amplitudes at infinity. The construction is asymptotic: it treats a neighborhood of infinity, and is meant to couple to interior solvers via domain decomposition. The method provides far-field data without relying on explicit solutions or Green's function representation. Scattering in variable media is treated in a unified framework where both the incident and scattered fields solve the same background Helmholtz operator. Numerical experiments for constant, short-range, and long-range media with single-mode and Gaussian beam incidence demonstrate spectral convergence of the computed scattering amplitudes in all cases.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Existence, Uniqueness and Classification of Plane Waves](https://arxiv.org/abs/2601.03575)
*Robert Milton*

Main category: math.AP

TL;DR: Establishes existence, uniqueness, and classification of plane waves for irreversible reactions with smooth concentration dependence, using novel analytic methods.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical foundation for plane wave solutions in reaction systems, particularly for irreversible reactions that depend smoothly on reactant and product concentrations (or prey-predator populations).

Method: Uses rudimentary analytic techniques to prove existence and uniqueness of plane wave solutions at every wavespeed above a threshold value V*. The method extends to cutoff reactions (zero below threshold product concentration).

Result: Establishes existence, uniqueness, and classification of plane waves for irreversible reactions. Proves unique plane wave exists at every wavespeed V > V* above some threshold. Results extend to cutoff reactions.

Conclusion: While the results themselves are not novel, the method of proof represents a new contribution, providing alternative analytic techniques for establishing wave solutions in reaction systems.

Abstract: Existence, uniqueness and classification is established for plane waves supported by an irreversible reaction which is a smooth function of local reactant and product concentrations (or prey and predator populations). Rudimentary analytic techniques are used to guarantee a unique plane wave at every wavespeed $V>V_*$ above some threshold. The result readily extends to cutoff reactions, which are zero below some threshold product concentration. These results are not novel, but the method of proof is.

</details>


### [14] [Global well-posedness of non-integrable hyperbolic-ellptic Ishimori system in the critical Sobolev space](https://arxiv.org/abs/2601.03576)
*Zexian Zhang,Yi Zhou*

Main category: math.AP

TL;DR: Global well-posedness proved for hyperbolic-elliptic Ishimori system in critical Sobolev space for general decoupling constant κ, extending previous integrable case results.


<details>
  <summary>Details</summary>
Motivation: Extend previous results on the integrable case (κ=1) to general decoupling constant κ, and develop a unified framework applicable to hyperbolic and elliptic Schrödinger maps in higher dimensions.

Method: Combines caloric gauge technique with U^p-V^p type Strichartz estimates, using novel bilinear estimates established via a new div-curl lemma introduced by the second author.

Result: Proves global well-posedness in the critical Sobolev space for the hyperbolic-elliptic Ishimori system with general κ ∈ ℝ.

Conclusion: Successfully extends previous integrable case results to general decoupling constant, providing a unified framework that also applies to hyperbolic and elliptic Schrödinger maps in dimensions d ≥ 2.

Abstract: We consider the Cauchy problem for the hyperbolic-elliptic Ishimori system with general decoupling constant $κ\in \mathbb{R}$ and prove global well-posedness in the critical Sobolev space. The proof relies primarily on new bilinear estimates, which are established via a novel div-curl lemma first introduced by the second author in \cite{zhou_1+2dimensional_2022}. Our approach combines the caloric gauge technique with $U^p$-$V^p$ type Strichartz estimates to handle the hyperbolic structure of the equation. The results extend previous work on the integrable case $κ= 1$ to general $κ$ and provide a unified framework that also applies to hyperbolic and elliptic Schrödinger maps in dimensions $d \ge 2$.

</details>


### [15] [Liouville theorems and gradient estimates of a nonlinear elliptic equation for the V-Laplacian](https://arxiv.org/abs/2601.03721)
*Yike Jia*

Main category: math.AP

TL;DR: Gradient estimates for positive solutions to nonlinear elliptic equations on smooth metric measure spaces with curvature bounds, plus Liouville theorems and Harnack inequalities.


<details>
  <summary>Details</summary>
Motivation: To establish gradient estimates for positive solutions to nonlinear elliptic equations on smooth metric measure spaces with curvature conditions, extending previous work by Wang and others to more general settings.

Method: Using techniques from geometric analysis on smooth metric measure spaces, specifically working with the V-Laplacian (Δ_V) and establishing estimates under k-Bakry-Émery curvature bounds of -(k-1)K with K≥0.

Result: Obtained gradient estimates for positive solutions, related Liouville theorems, and Harnack inequalities for the nonlinear elliptic equation Δ_V u^m + μ(x)u + p(x)u^α = 0 on smooth metric measure spaces.

Conclusion: The paper successfully extends previous results to more general settings, providing comprehensive gradient estimates and related properties for solutions to nonlinear elliptic equations on metric measure spaces with curvature bounds.

Abstract: In this paper we establish gradient estimates for positive solutions to the nonlinear elliptic equation $$Δ_{V}u^{m}+μ(x)u+p(x)u^α=0 , \quad m>1$$on any smooth metric measure space whose $k$-Bakry-Émery curvature is bounded from below by $-(k-1)K$ with $K \geq 0$. Additionally, we obtain related Liouville theorems and Harnack inequalities. We partially extend conclusions of Wang, when $V=0$, $μ=0$ the equation becomes $Δu^{m}+p(x)u^α=0$. And $V=f$, $μ=c, p=0 $, the equation becomes $Δ_{f}u^{m}+cu=0 $.

</details>


### [16] [On the structure of entropy dissipation and regularity for quasi-entropy solutions to 1d scalar conservation laws and to isentropic Euler system with $γ=3$](https://arxiv.org/abs/2601.03739)
*Fabio Ancona,Elio Marconi,Luca Talamini*

Main category: math.AP

TL;DR: The paper studies quasi-entropy solutions to scalar conservation laws, introduces Lagrangian representations, and proves entropy dissipation measures concentrate on 1-rectifiable sets for 1D non-degenerate fluxes and isentropic Euler system with γ=3.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of entropy dissipation measures for quasi-entropy solutions to conservation laws, particularly their geometric concentration properties and regularity improvements.

Method: 1. Investigate quasi-entropy solutions in multiple dimensions with Lagrangian representation. 2. Prove concentration of entropy dissipation measures on 1-rectifiable sets for 1D non-degenerate fluxes. 3. Apply similar analysis to isentropic Euler system with γ=3, exploiting sign of kinetic measures for improved regularity.

Result: 1. Suitable Lagrangian representation for quasi-entropy solutions. 2. Entropy dissipation measures concentrate on 1-rectifiable sets for 1D scalar conservation laws with non-degenerate fluxes. 3. Same concentration result for isentropic Euler system with γ=3, with slight improvement in available fractional regularity.

Conclusion: The entropy dissipation of quasi-entropy solutions exhibits geometric concentration properties, with measures supported on lower-dimensional rectifiable sets, providing structural insights into dissipation mechanisms for conservation laws.

Abstract: In this paper, we first investigate quasi-entropy solutions to scalar conservation laws in several space dimensions. In this setting, we introduce a suitable Lagrangian representation for such solutions. Next, we prove that, in one space dimension and for fluxes $f$ satisfying a general non-degeneracy condition, the entropy dissipation measures of quasi-entropy solutions are concentrated on a 1-rectifiable set. The same result is obtained for the isentropic Euler system with $γ= 3$, for which we also slightly improve the available fractional regularity by exploiting the sign of the kinetic measures.

</details>


### [17] [Mean-field limits for interacting particles on general adaptive dynamical networks](https://arxiv.org/abs/2601.03742)
*Nathalie Ayi*

Main category: math.AP

TL;DR: The paper establishes a Vlasov-type equation as the large-population limit for interacting particle systems on adaptive dynamical networks, using both mean-field and graph limit approaches.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of interacting particle systems on adaptive dynamical networks, particularly motivated by opinion dynamics models where agents interact through evolving weighted graphs that create non-exchangeable dynamics.

Method: Two complementary approaches: 1) Mean-field methodology (following Sznitman) with assumption that weight dynamics is independent of one agent's state; 2) Graph limit framework in deterministic setting that removes the restriction and handles more general interaction structures.

Result: Established wellposedness and stability results for the limiting Vlasov-type equation, quantitative estimates for propagation of independence, and clarified relationship between continuum (graph limit) formulation and mean-field limit.

Conclusion: Provides a unified description of asymptotic dynamics for interacting particle systems on adaptive dynamical networks through Vlasov-type equations on extended phase spaces, bridging mean-field and graph limit perspectives.

Abstract: We study the large-population limit of interacting particle systems evolving on adaptive dynamical networks, motivated in particular by models of opinion dynamics. In such systems, agents interact through weighted graphs whose structure evolves over time in a coupled manner with the agents' states, leading to non-exchangeable dynamics. In the dense-graph regime, we show that the asymptotic behavior is described by a Vlasov-type equation posed on an extended phase space that includes both the agents' states and identities and the evolving interaction weights. We establish this limiting equation through two complementary approaches. The first follows the mean-field methodology in the spirit of Sznitman [28]. In this framework, we impose the additional assumption that the weight dynamics is independent of one of the agent's states, an assumption that remains well motivated from a modeling perspective and allows for a direct derivation of the mean-field limit. The second approach is based on the graph limit framework and is formulated in a deterministic setting. This perspective makes it possible to remove the aforementioned restriction on the weight dynamics and to handle more general interaction structures. Our analysis includes wellposedness and stability results for the limiting Vlasov-type equation, as well as quantitative estimates ensuring the propagation of independence. We further clarify the relationship between the continuum (graph limit) formulation and the mean-field limit, thereby providing a unified description of the asymptotic dynamics of interacting particle systems on adaptive dynamical networks.

</details>


### [18] [$BV$-Estimates for Non-Linear Parabolic PDE with Linear Drift](https://arxiv.org/abs/2601.03755)
*El Mahdi Erraji,Noureddine Igbida,Fahd Karami,Driss Meskine*

Main category: math.AP

TL;DR: Space BV regularity established for nonlinear parabolic PDEs with linear drift term in bounded domains with mixed Dirichlet-Neumann boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To establish bounded variation (BV) regularity for solutions of nonlinear parabolic PDEs with drift terms, which is important for understanding solution behavior and properties in bounded domains with mixed boundary conditions.

Method: Study nonlinear parabolic PDEs with linear drift term in bounded domains with mixed Dirichlet-Neumann boundary conditions, using general nonlinearity and reasonable assumptions on data.

Result: Successfully established space BV regularity of solutions for the nonlinear parabolic PDEs, with results covering linear transport equations in bounded domains with outward-pointing drift vector fields on boundaries as a special case.

Conclusion: The paper proves BV regularity for solutions of nonlinear parabolic PDEs with drift terms under mixed boundary conditions, extending results to include linear transport equations as a particular case.

Abstract: In the present work, we establish space Bounded Variation $(BV)$ regularity of the solution for a non-linear parabolic partial differential equations involving a linear drift term. We study the problem in a bounded domain with mixed Dirichlet-Neumann boundary conditions, a general non-linearity and reasonable assumptions on the data. Our results also cover, as a particular case, the linear transport equation in a bounded domain with an outward-pointing drift vector field on the boundary.

</details>


### [19] [On the existence of forward self-similar solutions to the two-dimensional Navier-Stokes equations](https://arxiv.org/abs/2601.03833)
*Changfeng Gui,Hao Liu,Chunjing Xie*

Main category: math.AP

TL;DR: Global existence of forward self-similar solutions to 2D Navier-Stokes equations for any divergence-free initial velocity homogeneous of degree -1, without smallness assumptions.


<details>
  <summary>Details</summary>
Motivation: The 2D Navier-Stokes problem is critical because (-1)-homogeneous initial data leads to locally infinite kinetic energy at the origin and non-integrable vorticity, making classical energy estimates unavailable. This contrasts with the 3D case where such vector fields are locally square-integrable.

Method: Decompose solution into linear part (solving heat equation) and finite-energy perturbation part. Exploit inherent cancellation between linear and perturbation parts to control interaction terms and establish H¹-estimates. Investigate corresponding Leray system in weighted Sobolev space to derive optimal pointwise estimates.

Result: Global existence of forward self-similar solutions for any divergence-free initial velocity homogeneous of degree -1, with no smallness assumption on initial data. The perturbation part decays faster at infinity.

Conclusion: The paper successfully constructs global-in-time self-similar solutions to 2D Navier-Stokes equations for critical (-1)-homogeneous initial data by overcoming the lack of classical energy estimates through decomposition techniques and exploiting cancellation properties.

Abstract: We establish the global existence of forward self-similar solutions to the two-dimensional incompressible Navier-Stokes equations for any divergence-free initial velocity $u_0$ that is homogeneous of degree $-1$ and locally Hölder continuous. This result requires no smallness assumption on the initial data. In sharp contrast to the three-dimensional case, where $(-1)$-homogeneous vector fields are locally square-integrable, the 2D problem is critical in the sense that the initial kinetic energy is locally infinite at the origin, and the initial vorticity fails to be locally integrable. Consequently, the classical local energy estimates are not available. To overcome this, we decompose the solution into a linear part solving the heat equation and a finite-energy perturbation part. By exploiting a kind of inherent cancellation relation between the linear part and the perturbation part, we can control interaction terms and establish the $H^1$-estimates for the perturbation part. Further investigating the corresponding Leray system in weighted Sobolev space, we can derive an optimal pointwise estimate. This gives the faster decay of the perturbation part at infinity and enables us to construct global-in-time self-similar solutions.

</details>


### [20] [On the Fučík spectrum of the Logarithmic Laplacian](https://arxiv.org/abs/2601.03865)
*Rakesh Arora,Tuhina Mukherjee*

Main category: math.AP

TL;DR: The paper investigates the Fučík spectrum of the logarithmic Laplacian operator, characterizing its structure including isolated lines, first nontrivial curve properties, variational characterization of second eigenvalue, and sign-changing nature of higher eigenfunctions.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral properties of the logarithmic Laplacian operator, particularly its Fučík spectrum, which describes parameter pairs where the nonlinear eigenvalue problem with sign-dependent coefficients admits nontrivial solutions.

Method: Mathematical analysis using variational methods and spectral theory to study the Fučík spectrum associated with the logarithmic Laplacian operator on bounded domains.

Result: Shows that lines through the first eigenvalue are isolated in the spectrum, establishes existence and properties of the first nontrivial curve (Lipschitz continuity, strict monotonicity, asymptotic behavior), provides variational characterization of second eigenvalue, proves all eigenfunctions for eigenvalues greater than first eigenvalue are sign-changing, and addresses nonresonance problems.

Conclusion: The Fučík spectrum of the logarithmic Laplacian has a rich structure with isolated lines and nontrivial curves, providing complete characterization of spectral properties and enabling analysis of nonlinear problems through variational methods despite challenges posed by the first eigenvalue's contrasting features.

Abstract: In this paper, we investigate the Fučík spectrum $Σ_L$ associated with the logarithmic Laplacian. This spectrum is defined as the set of all pairs $(α,β) \in \mathbb{R}^2$ for which the problem \[ L_Δu = αu^+-βu^- ~\text{in} ~ Ω\quad \text{and} \quad u=0 ~\text{in} ~\mathbb{R}^N\setminus Ω\] admits a nontrivial solution $u$. Here, $Ω\subset \mathbb{R}^N$ is a bounded domain with $C^{1,1}$ boundary, $u^\pm = \max\{\pm u,0\}$, and $u = u^+ - u^-$. We show that the lines $λ_1^L \times \mathbb{R}$ and $\mathbb{R} \times λ_1^L$, where $λ_1^L$ denotes the first eigenvalue of $L_Δ$, lies in the spectrum $Σ_L$ and are isolated within the spectrum. Furthermore, we establish the existence of the first nontrivial curve in $Σ_L$ and analyze its qualitative properties, including Lipschitz continuity, strict monotonicity, and asymptotic behavior. In addition, we obtain a variational characterization of the second eigenvalue of the logarithmic Laplacian and show that all eigenfunctions corresponding to eigenvalues $λ> λ_1^L$ are sign-changing. Finally, we address a nonresonance problem with respect to the Fučík spectrum $Σ_L$, employing variational methods and carefully overcoming the difficulties arising from the contrasting features of the first eigenvalue $λ_1^L$.

</details>


### [21] [The uniqueness and concentration behavior of solutions for a nonlinear fractional Schrödinger system](https://arxiv.org/abs/2601.03968)
*Chungen Liu,Zhigao Zhang,Jiabin Zuo*

Main category: math.AP

TL;DR: The paper studies a nonlinear system of two coupled fractional Schrödinger equations with attractive interactions, proving uniqueness of solutions and analyzing concentration behavior as interactions approach critical values.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of coupled fractional Schrödinger systems with attractive interactions, particularly focusing on solution uniqueness and concentration behavior near critical interaction strengths.

Method: Analyzed an associated L²-constrained minimization problem, used implicit function theorem for uniqueness proof, established delicate energy estimates, and studied concentration behavior as interaction strengths approach critical values.

Result: Proved uniqueness of solutions, showed that solutions concentrate at flattest common minimum points of trapping potentials when interactions approach critical values, and derived optimal blow-up rates.

Conclusion: The paper provides rigorous mathematical analysis of coupled fractional Schrödinger systems, establishing uniqueness results and characterizing concentration phenomena with precise blow-up rates near critical interaction strengths.

Abstract: The paper is concerned with a nonlinear system of two coupled fractional Schrödinger equations with both attractive intraspecies and attractive interspecies interactions in $\mathbb{R}$. By analyzing an associated $L^2$-constrained minimization problem, the uniqueness of solutions to this system is proved via the implicit function theorem. Under a certain type of trapping potential, by establishing some delicate energy estimates, we present a detailed analysis on the concentration behavior of the solutions as the total strength of intraspecies and interspecies interactions tends to a critical value, where each component of the solutions blows up and concentrates at a flattest common minimum point of the associated trapping potentials. An optimal blow-up rate of solutions to the system is also given.

</details>


### [22] [Global stability of vacuum for the relativistic Vlasov-Maxwell-Boltzmann system](https://arxiv.org/abs/2601.04018)
*Chuqi Cao,Xingyu Li*

Main category: math.AP

TL;DR: Global existence and nonlinear stability of vacuum for 3D relativistic Vlasov-Maxwell-Boltzmann system with uniform bounds in speed of light c, using vector field method and Glassey-Strauss decomposition.


<details>
  <summary>Details</summary>
Motivation: To establish global existence and stability results for the relativistic Vlasov-Maxwell-Boltzmann system that are uniform in the speed of light parameter c, without requiring compact support assumptions on initial data.

Method: Vector field method combined with Glassey-Strauss decomposition of electromagnetic field; derivation of chain rule for relativistic Boltzmann collision operator compatible with vector field commutation properties.

Result: Global existence and nonlinear stability of vacuum for small initial data with bounds uniform in c; control of coupled kinetic and electromagnetic equations.

Conclusion: The developed tools successfully establish global stability near vacuum for the relativistic Vlasov-Maxwell-Boltzmann system with uniform bounds in speed of light parameter.

Abstract: We consider the three-dimensional relativistic Vlasov-Maxwell-Boltzmann system, where the speed of light $c$ is an arbitrary constant no less than 1, and we establish global existence and nonlinear stability of the vacuum for small initial data, with bounds that are uniform in $c$. The analysis is based on the vector field method combined with the Glassey-Strauss decomposition of the electromagnetic field, and does not require any compact support assumption on the initial data. A key ingredient of the proof is the derivation of a chain rule for the relativistic Boltzmann collision operator that is compatible with the commutation properties of the vector fields. These tools allow us to control the coupled kinetic and electromagnetic equations and to obtain global stability near vacuum.

</details>


### [23] [Fractional heat content asymptotics for Carnot groups](https://arxiv.org/abs/2601.04088)
*Rohan Sarkar*

Main category: math.AP

TL;DR: The paper studies small-time asymptotics of fractional heat content for domains in Carnot groups, showing convergence to horizontal perimeter with explicit rate functions.


<details>
  <summary>Details</summary>
Motivation: To extend the study of heat content asymptotics to fractional sub-Laplacians in Carnot groups, which are important in sub-Riemannian geometry and analysis on stratified Lie groups.

Method: The authors analyze the fractional heat content Q_Ω^(α)(t) defined via solutions to heat equations with fractional sub-Laplacian operators L_α = L^{α/2} (where L is the sub-Laplacian) on bounded domains Ω with Dirichlet boundary conditions.

Result: For 1 ≤ α ≤ 2, there exists an explicit rate function μ_α such that lim_{t→0} (|Ω| - Q_Ω^(α)(t))/μ_α(t) = |∂Ω|_H, where |∂Ω|_H is the horizontal perimeter of Ω. The rate function μ_α coincides with the Euclidean case.

Conclusion: The fractional heat content asymptotics in Carnot groups behave similarly to the Euclidean case, with convergence to horizontal perimeter at small times, establishing connections between fractional operators and geometric invariants in sub-Riemannian settings.

Abstract: We propose a novel approach for studying small-time asymptotics of the fractional heat content of $C^2$ non-characteristic domains in Carnot groups. Denoting the sub-Laplacian operator by $\mathcal{L}$, the fractional heat content of a bounded domain $Ω$ is defined as $Q^{(α)}_Ω(t)=\int_Ω u_α(t,x)dx$, where $u_α$ is the solution to the heat equation corresponding to the fractional sub-Laplacian $\mathcal{L}_α:=\mathcal{L}^{α/2}$ with Dirichlet boundary condition on $Ω$. We prove that for $1\leα\le 2$, there exists explict rate function $μ_α: (0,\infty)\to (0,\infty)$ such that \[ \lim_{t\to 0}\frac{|Ω|-Q^{(α)}_Ω(t)}{μ_α(t)}=|\partialΩ|_H, \] where $|\partialΩ|_H$ is the horizontal perimeter of $Ω$. Moreover, the rate function $μ_α$ coincides with the same for the Euclidean case.

</details>


### [24] [Time Reparametrization and Chaotic Dynamics in Conformable $C_0$-Semigroups](https://arxiv.org/abs/2601.04105)
*Mohamed Khoulane,Aziz El Ghazouani,M'hamed El Omari*

Main category: math.AP

TL;DR: Conformable derivatives are shown to be equivalent to classical semigroups observed through a nonlinear time transformation, with dynamical properties preserved under this correspondence.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between conformable derivatives (which provide fractional-looking calculus while remaining local) and classical semigroup theory, and to determine which dynamical phenomena in conformable models are genuinely new versus inherited from classical dynamics.

Method: Develop an operator-theoretic perspective showing conformable time evolution is a classical C₀-semigroup observed through a nonlinear clock. Introduce conformable time map ψ(t)=t^α/α and prove every C₀-α-semigroup can be written as T_α(t)=T(ψ(t)) for a unique classical C₀-semigroup. Use functional analysis in conformable Lebesgue spaces L^{p,α} with explicit isometric identification to standard L^p spaces.

Result: Established one-to-one correspondence between conformable semigroups and classical semigroups via nonlinear time reparametrization. Showed that orbit-based linear dynamics are invariant under conformable reparametrization, and that α-hypercyclicity and α-chaos coincide with usual notions for associated classical semigroup. Obtained conformable version of Desch-Schappacher-Webb spectral criterion for chaos.

Conclusion: Conformable derivatives essentially represent classical semigroup dynamics observed through a nonlinear time transformation. The analysis clarifies which dynamical phenomena in conformable models are genuinely new (few) versus inherited from classical semigroup dynamics via nonlinear change of time.

Abstract: Conformable derivatives provide a fractional-looking calculus that remains local and admits a simple representation through classical derivatives with explicit weights. In this paper we develop a systematic operator-theoretic perspective showing that conformable time evolution is, in essence, a classical $C_0$-semigroup observed through a nonlinear clock. We introduce the conformable time map $ψ(t)=t^α/α$ and prove that every $C_0$--$α$-semigroup $\{T_α(t)\}_{t\ge0}$ can be written as $T_α(t)=T(ψ(t))$ for a uniquely determined classical $C_0$-semigroup $\{T(s)\}_{s\ge0}$, with generators agreeing on a common domain. This correspondence yields a one-to-one transfer of mild solutions and shows that orbit-based linear dynamics are invariant under conformable reparametrization. In particular, $α$-hypercyclicity and $α$--chaos coincide with the usual notions for the associated classical semigroup. As a consequence, we obtain a conformable version of the Desch--Schappacher--Webb spectral criterion for chaos. We also place the analysis in the natural functional setting provided by conformable Lebesgue spaces $L^{p,α}$ and their explicit isometric identification with standard $L^p$ spaces, which allows one to transport estimates and spectral arguments without loss. The results clarify which dynamical phenomena in conformable models are genuinely new and which are inherited from classical semigroup dynamics via a nonlinear change of time.

</details>


### [25] [Teukolsky on slowly-rotating Kerr-de Sitter in the vanishing $Λ$ limit](https://arxiv.org/abs/2601.04117)
*Allen Juntao Fang,Jérémie Szeftel,Arthur Touati*

Main category: math.AP

TL;DR: The paper proves uniform energy, Morawetz, and r^p-weighted estimates for the Teukolsky equation on slowly-rotating Kerr-de Sitter backgrounds, with uniformity in the cosmological constant Λ, and recovers Kerr results as Λ→0.


<details>
  <summary>Details</summary>
Motivation: To establish uniform estimates for the Teukolsky equation on Kerr-de Sitter backgrounds that remain valid as the cosmological constant Λ approaches zero, bridging the gap between Kerr-de Sitter and Kerr spacetime analyses.

Method: Proving energy, Morawetz, and r^p-weighted estimates for solutions to the Teukolsky equation on slowly-rotating Kerr-de Sitter backgrounds, with careful treatment of the cosmological constant dependence to ensure uniformity.

Result: Obtained uniform estimates that hold on the entire domain of outer communications (extending up to r ∼ Λ^{-1/2}) and remain valid as Λ→0, allowing recovery of known Kerr results in the limit.

Conclusion: The paper successfully establishes Λ-uniform estimates for the Teukolsky equation on Kerr-de Sitter backgrounds, providing a bridge to Kerr spacetime results and demonstrating the robustness of the analysis across different cosmological constant regimes.

Abstract: We prove energy, Morawetz and $r^p$-weighted estimates for solutions to the Teukolsky equation set on a slowly-rotating Kerr-de Sitter background. The main feature of our estimates is their uniformity with respect to the cosmological constant $Λ>0$ (thus allowed to tend to $0$), while they hold on the whole domain of outer communications, extending up to $r\sim Λ^{-\frac{1}{2}}$. As an application of our result, we recover well-known corresponding estimates for solutions to Teukolsky on a slowly-rotating Kerr background in the limit $Λ\to 0$.

</details>


### [26] [Trace regularity of solutions to the Navier equations](https://arxiv.org/abs/2601.04173)
*Jerin Tasnim Farin,Giusy Mazzone*

Main category: math.AP

TL;DR: Analysis of trace regularity for stress vector on elastic solid boundary under Navier equations with mixed boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties of stress vector traces on boundaries of elastic solids, similar to known "hidden trace regularity" results for wave equations.

Method: Mathematical analysis of time-dependent displacement-traction problem for Navier equations of linear elasticity in bounded 3D domains with mixed Dirichlet/Neumann boundary conditions.

Result: Obtained trace regularity results for stress vector on boundary, analogous to hidden trace regularity known for scalar wave equations.

Conclusion: Successfully extended hidden trace regularity concepts from wave equations to linear elasticity problems with mixed boundary conditions.

Abstract: We present results on the trace regularity of the stress vector on the boundary of an elastic solid satisfying the time-dependent, displacement-traction problem for the Navier equations of linear elasticity in a bounded domain of $\mathbb{R}^3$. Specifically, the solid's displacement is subject to Dirichlet- and Neumann-type conditions on different portions of its boundary and possibly non-zero body forces and initial data. Our regularity results are reminiscent of the so-called "hidden trace regularity" results for solutions to the scalar wave equation obtained in [12].

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [27] [Finding Graph Isomorphisms in Heated Spaces in Almost No Time](https://arxiv.org/abs/2601.03787)
*Sara Najem,Amer E. Mouawad*

Main category: physics.comp-ph

TL;DR: A new spectral-geometric algorithm for graph isomorphism that uses vertex curvatures to construct candidate correspondences, with deterministic polynomial-time performance on tested instances including difficult cases.


<details>
  <summary>Details</summary>
Motivation: Graph isomorphism is a fundamental problem with wide applications, but remains challenging for highly symmetric structures despite decades of study. Classical spectral methods have limitations, particularly for difficult cases.

Method: The approach combines spectral graph theory and geometry to construct candidate vertex correspondences using vertex curvatures. Any correspondence produced is explicitly verified to ensure correctness (no false positives).

Result: The algorithm correctly resolves every instance tested in deterministic polynomial time, including graphs known to be difficult for classical spectral techniques. It never incorrectly identifies non-isomorphic graphs as isomorphic.

Conclusion: Enriched spectral methods are more powerful than previously understood and represent a promising direction for practical resolution of the graph isomorphism problem's complexity.

Abstract: Determining whether two graphs are structurally identical is a fundamental problem with applications spanning mathematics, computer science, chemistry, and network science. Despite decades of study, graph isomorphism remains a challenging algorithmic task, particularly for highly symmetric structures. Here we introduce a new algorithmic approach based on ideas from spectral graph theory and geometry that constructs candidate correspondences between vertices using their curvatures. Any correspondence produced by the algorithm is explicitly verified, ensuring that non-isomorphic graphs are never incorrectly identified as isomorphic. Although the method does not yet guarantee success on all isomorphic inputs, we find that it correctly resolves every instance tested in deterministic polynomial time, including a broad collection of graphs known to be difficult for classical spectral techniques. These results demonstrate that enriched spectral methods can be far more powerful than previously understood, and suggest a promising direction for the practical resolution of the complexity of the graph isomorphism problem.

</details>


### [28] [Accelerated simulation of multiscale gas-radiation coupling flows via a general synthetic iterative scheme](https://arxiv.org/abs/2601.03935)
*Jianan Zeng,Qi Li,Yanbing Zhang,Wei Su,Lei Wu*

Main category: physics.comp-ph

TL;DR: GSIS (general synthetic iterative scheme) accelerates simulation of radiative gas flows in hypersonic reentry by coupling kinetic models with macroscopic synthetic equations, achieving orders-of-magnitude speedup while preserving accuracy across flow regimes.


<details>
  <summary>Details</summary>
Motivation: Gas-radiation coupling is critical for hypersonic reentry flows where extreme temperatures cause non-equilibrium gas and radiative heat transport. Accurate and efficient simulation is essential for designing reliable thermal protection systems for atmospheric entry vehicles.

Method: A Boltzmann-type kinetic model for radiative gas flows is solved using GSIS, which integrates an unstructured finite-volume discrete velocity method with macroscopic synthetic equations. The kinetic model provides high-order closures for constitutive relations, while synthetic equations accelerate convergence in near-continuum regimes.

Result: GSIS achieves orders-of-magnitude speedup over conventional iterative schemes in multiscale simulations. The algorithm is asymptotic-preserving, correctly recovering continuum and optically thick limits on coarse meshes independent of mean free path. Numerical simulations including 3D hypersonic flow over Apollo capsule demonstrate accurate capture of non-equilibrium effects and radiative heat transfer.

Conclusion: GSIS provides an efficient and accurate framework for simulating radiative gas dynamics in hypersonic environments, enabling reliable thermal protection system design through accelerated multiscale simulations while maintaining physical accuracy across different flow and radiation transport regimes.

Abstract: Gas-radiation coupling critically influences hypersonic reentry flows, where extreme temperatures induce pronounced non-equilibrium gas and radiative heat transport. Accurate and efficient simulation of radiative gas dynamics is therefore indispensable for reliable design of thermal protection systems for atmospheric entry vehicles. In this study, a Boltzmann-type kinetic model for radiative gas flows is solved across a broad spectrum of flow and radiation transport regimes using the general synthetic iterative scheme (GSIS). The approach integrates an unstructured finite-volume discrete velocity method with a set of macroscopic synthetic equations. Within this framework, the kinetic model provides high-order closures for the constitutive relations in the synthetic equations. Simultaneously, the macroscopic synthetic equations drive the evolution of the mesoscopic kinetic system, significantly accelerating steady-state convergence in near-continuum regimes, as substantiated by linear Fourier stability analysis. Crucially, the algorithm is proven to be asymptotic-preserving, correctly recovering the continuum and optically thick limits, represented by the radiative Navier-Stokes-Fourier equations governing distinct translational, rotational, vibrational, and radiative temperatures, on coarse meshes independent of the mean free path. Numerical simulations of challenging benchmarks, including three-dimensional hypersonic flow over an Apollo reentry capsule, demonstrate that GSIS achieves orders-of-magnitude speedup over conventional iterative schemes in multiscale simulations of radiative gas flows while accurately capturing non-equilibrium effects and radiative heat transfer in hypersonic environments.

</details>


### [29] [A constrained-transport embedded boundary method for compressible resistive magnetohydrodynamics](https://arxiv.org/abs/2601.04099)
*Samuel W. Jones,Colin P. McNally,Meritt Reynolds*

Main category: physics.comp-ph

TL;DR: A method for implementing arbitrarily shaped boundaries on Cartesian meshes for compressible resistive magnetohydrodynamics simulations, using finite volume formulation with Riemann solvers and constrained transport, avoiding cut cell time step issues.


<details>
  <summary>Details</summary>
Motivation: Increased interest in pulsed-power magneto-inertial fusion devices requires accurate simulation methods that can handle complex geometries while solving magnetohydrodynamics equations.

Method: Finite volume formulation with Riemann solver for flux computation on Cartesian mesh faces, face-centered constrained transport for induction equation, ghost-fluid approach for moving interfaces between materials, and avoidance of cut cell time step problems by computing fluxes on Cartesian mesh faces/edges.

Result: Method converges at second order without discontinuities and first order with material property discontinuities; verified with preliminary results including shock-wave-driven and magnetically-driven compressions of magnetohydrostatic equilibria.

Conclusion: The presented method successfully handles arbitrarily shaped boundaries on Cartesian meshes for magnetohydrodynamics simulations, providing accurate convergence properties suitable for pulsed-power fusion device modeling.

Abstract: Motivated by the increased interest in pulsed-power magneto-inertial fusion devices in recent years, we present a method for implementing an arbitrarily shaped embedded boundary on a Cartesian mesh while solving the equations of compressible resistive magnetohydrodynamics. The method is built around a finite volume formulation of the equations in which a Riemann solver is used to compute fluxes on the faces between grid cells, and a face-centered constrained transport formulation of the induction equation. The small time step problem associated with the cut cells is avoided by always computing fluxes on the faces and edges of the Cartesian mesh. We extend the method to model a moving interface between two materials with different properties using a ghost-fluid approach, and show some preliminary results including shock-wave-driven and magnetically-driven dynamical compressions of magnetohydrostatic equilibria. We present a thorough verification of the method and show that it converges at second order in the absence of discontinuities, and at first order with a discontinuity in material properties.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [30] [A laser plasma soliton fusion scheme](https://arxiv.org/abs/2601.03943)
*Pisin Chen,Yung-Kun Liu,Gerard Mourou*

Main category: physics.plasm-ph

TL;DR: Laser-plasma solitons enable efficient DT fusion by creating electron-free environment and enhancing fusion cross sections through intense electromagnetic fields.


<details>
  <summary>Details</summary>
Motivation: To overcome fundamental obstructions to reaching breakeven condition in fusion reactions by creating a novel fusion scheme using laser-plasma solitons.

Method: Use two consecutive lasers: first excites plasma solitons, second (more intense with matched lower frequency) resonantly fortifies soliton electromagnetic field. Plasma density gradient induces soliton motion, sweeping D/T ions through electron-free environment for fusion.

Result: Breakeven condition is attainable. Using fiber laser and iCAN laser technologies, gigawatt output may be conceivable with high repetition rate and high intensity operation.

Conclusion: Laser-plasma soliton fusion scheme provides a promising pathway to achieve breakeven fusion conditions by creating optimal electron-free environment and enhancing fusion cross sections through intense electromagnetic fields.

Abstract: We introduce a novel fusion scheme enabled by laser-plasma solitons, which promises to overcome several fundamental obstructions to reaching the breakeven condition. For concreteness, we invoke deuterium-tritium (DT) as fuels. The intense electromagnetic field trapped inside the soliton significantly enhances the DT-fusion cross section, its ponderomotive potential evacuates electrons, and it accelerates D/T to kinetic energies suitable for fusion reaction. While electrons are expelled almost instantly, the much heavier D/T moves at picosecond time scale. Such a difference in time scales renders a time window for DT fusion to occur efficiently in an electron-free environment. We inject two consecutive lasers, where the first would excite plasma solitons and the second, much more intense and with a matched lower frequency, would fortify the soliton electromagnetic field resonantly. We impose a plasma density gradient to induce soliton motion. All D/T inside the plasma column swept by the moving soliton during its lifetime would participate in this fusion mechanism. We show that the breakeven condition is attainable. Invoking fiber laser and the iCAN laser technologies for high repetition rate and high intensity operation, gigawatt output maybe conceivable.

</details>


### [31] [Quantum Monte Carlo Simulations for predicting electron-positron pair production via the linear Breit-Wheeler process](https://arxiv.org/abs/2601.03953)
*Lucas I. Iñigo Gamiz,Óscar Amaro,Efstratios Koukoutsis,Marija Vranić*

Main category: physics.plasm-ph

TL;DR: Quantum Monte Carlo integration for photon-photon pair production achieves high accuracy on current quantum hardware, demonstrating practical quantum-classical simulation integration.


<details>
  <summary>Details</summary>
Motivation: To harness quantum computing's potential for scientific simulations by integrating quantum modules into hybrid quantum-classical simulations, specifically for problems in high-energy physics and intense laser-matter interactions.

Method: Quantum Monte Carlo integration algorithm applied to predict photon-photon pair production, with approximations for polynomial embedding and quantum state initialization.

Result: Quantum simulations demonstrate high accuracy relative to theoretical predictions, achieving up to 90% accuracy on current quantum hardware.

Conclusion: Quantum Monte Carlo integration is viable for practical scientific simulations on current quantum hardware, with pathways identified for integration with classical simulation codes.

Abstract: Quantum computing (QC) has the potential to revolutionise the future of scientific simulations. To harness the capabilities that QC offers, we can integrate it into hybrid quantum-classical simulations, which can boost the capabilities of supercomputing by leveraging quantum modules that offer speedups over classical counterparts. One example is quantum Monte Carlo integration, which is theorised to achieve a quadratic speedup over classical Monte Carlo, making it suitable for high-energy physics, strong-field QED, and multiple scientific and industrial applications. In this paper, we demonstrate that quantum Monte Carlo can be used to predict the number of pairs created when two photon beams collide head-on, a problem relevant to high-energy physics and intense laser-matter interactions. The results from the quantum simulations demonstrate high accuracy relative to theoretical predictions. The accuracy of the simulations is only constrained by the approximations required to embed polynomials and to initialise the quantum state. We also demonstrate that our algorithm can be used in current quantum hardware, providing up to 90 % accuracy relative to theoretical predictions. Furthermore, we propose pathways towards integrations with classical simulation codes.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [32] [SAP-X2C: Optimally-Simple Two-Component Relativistic Hamiltonian With Size-Intensive Picture Change](https://arxiv.org/abs/2601.04174)
*Kshitijkumar A. Surjuse,Edward F. Valeev*

Main category: physics.chem-ph

TL;DR: SAP-X2C: A simple relativistic 2-component Hamiltonian that models two-electron picture-change effects using superposition of atomic potentials, maintaining low cost while improving accuracy over 1eX2C.


<details>
  <summary>Details</summary>
Motivation: To develop a simple yet accurate relativistic Hamiltonian that addresses two-electron picture-change effects while maintaining computational efficiency and applicability to extended systems like periodic crystals.

Method: Uses Lehtola's superposition of atomic potentials (SAP) to model two-electron picture-change effects within the exact 2-component (X2C) framework, creating SAP-X2C approach.

Result: SAP-X2C shows accuracy similar to more complex atomic mean-field (AMF) X2C methods in approximating 4-component Dirac-Hartree-Fock reference for total energies, spinor energies, spin-orbit splittings, bond distances, and vibrational frequencies.

Conclusion: SAP-X2C provides a simple, low-cost alternative to complex AMF-X2C methods while maintaining similar accuracy and having a well-defined thermodynamic limit suitable for extended systems.

Abstract: We present a simple relativistic exact 2-component (X2C) Hamiltonian that models two-electron picture-change effects using Lehtola's superposition of atomic potentials (SAP) [S. Lehtola, J. Chem. Theory Comput. 15, 1593-1604 (2019)]. The SAP-X2C approach keeps the low-cost and technical simplicity of the popular 1-electron X2C (1eX2C) predecessor, but is significantly more accurate and has a well-defined thermodynamic limit, making it applicable to extended systems (such as periodic crystals). The assessment of the SAP-X2C-based Hartree-Fock total and spinor energies, spin-orbit splittings, equilibrium bond distances, and harmonic vibrational frequencies suggests that SAP-X2C is similar to the more complex atomic mean-field (AMF) X2C counterparts in the ability to approximate the 4-component Dirac-Hartree-Fock reference.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [The Geometry of the Pivot: A Note on Lazy Pivoted Cholesky and Farthest Point Sampling](https://arxiv.org/abs/2601.03706)
*Gil Shabat*

Main category: cs.LG

TL;DR: Pivoted Cholesky decomposition for kernel matrices is geometrically equivalent to Farthest Point Sampling in RKHS, with Cholesky factor construction as implicit Gram-Schmidt orthogonalization.


<details>
  <summary>Details</summary>
Motivation: While Pivoted Cholesky decomposition is widely used for low-rank approximations of kernel matrices in Gaussian Processes, its geometric intuition within kernel methods remains unclear despite well-documented algebraic properties in numerical linear algebra.

Method: The authors provide a geometric interpretation of Pivoted Cholesky decomposition in Reproducing Kernel Hilbert Space (RKHS), showing that the pivotal selection step is mathematically equivalent to Farthest Point Sampling using the kernel metric, and that Cholesky factor construction corresponds to implicit Gram-Schmidt orthogonalization.

Result: The paper elucidates the geometric connection between Pivoted Cholesky decomposition and Farthest Point Sampling in RKHS, providing a clear mathematical equivalence between the algorithmic steps and geometric operations in kernel space.

Conclusion: The geometric interpretation bridges theory and practice, offering deeper insight into why Pivoted Cholesky works well for kernel matrix approximations, with practical implementation provided through a minimalist Python code.

Abstract: Low-rank approximations of large kernel matrices are ubiquitous in machine learning, particularly for scaling Gaussian Processes to massive datasets. The Pivoted Cholesky decomposition is a standard tool for this task, offering a computationally efficient, greedy low-rank approximation. While its algebraic properties are well-documented in numerical linear algebra, its geometric intuition within the context of kernel methods often remains obscure. In this note, we elucidate the geometric interpretation of the algorithm within the Reproducing Kernel Hilbert Space (RKHS). We demonstrate that the pivotal selection step is mathematically equivalent to Farthest Point Sampling (FPS) using the kernel metric, and that the Cholesky factor construction is an implicit Gram-Schmidt orthogonalization. We provide a concise derivation and a minimalist Python implementation to bridge the gap between theory and practice.

</details>


### [34] [Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation](https://arxiv.org/abs/2601.04176)
*Pietro de Oliveira Esteves*

Main category: cs.LG

TL;DR: PINNs with automatic differentiation recover NLSE parameters with <0.2% error from 500 noisy points (20% Gaussian noise), outperforming traditional methods that fail in such noisy regimes.


<details>
  <summary>Details</summary>
Motivation: Traditional finite difference methods fail to recover physical parameters from the Nonlinear Schrodinger Equation under severe noise conditions due to noise amplification in numerical derivatives. There's a need for robust methods that can handle scarce and noisy experimental data in spatiotemporal dynamics.

Method: Physics-Informed Neural Networks (PINNs) integrated with automatic differentiation to recover physical parameters from the NLSE. The approach uses physics-based regularization as a filter against measurement uncertainty, trained on sparse, randomly sampled data points corrupted by additive Gaussian noise.

Result: Achieved reconstruction of nonlinear coefficient beta with <0.2% relative error using only 500 data points with 20% noise. Demonstrated consistent sub-1% accuracy across beta values (0.5-2.0) and varying data availability (100-1000 points). Robustness confirmed with standard deviation <0.15% for beta=1.0. Complete pipeline runs in ~80 minutes on NVIDIA Tesla T4 GPU.

Conclusion: Physics-based regularization effectively filters high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics with scarce, noisy data. The approach is accessible for widespread adoption and all code is publicly available.

Abstract: We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [35] [Stochastic Path Compression for Spectral Tensor Networks on Cyclic Graphs](https://arxiv.org/abs/2601.04172)
*Ryan T. Grimm,Joel D. Eaves*

Main category: cond-mat.stat-mech

TL;DR: SPC is a new method to compress cyclic tensor networks using iterative importance sampling to target edges with large bond-dimensions, creating spatially localized compression pathways.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of compressing cyclic tensor networks with continuous degrees of freedom, which is difficult due to the cyclic structure and large bond-dimensions that need to be managed efficiently.

Method: Stochastic Path Compression (SPC) uses iterative importance sampling to identify edges with large bond-dimensions, creating compression pathways that localize these dimensions spatially. Combined with an integral decimation algorithm, it enables compression of cyclic tensor networks.

Result: The method successfully computes absolute thermodynamics of q-state clock models on 2D square lattices and an XY model on a Watts-Strogatz graph (small-world network), demonstrating accurate compression of cyclic tensor networks with continuous degrees of freedom.

Conclusion: SPC provides an effective approach for compressing cyclic tensor networks by spatially separating high and low bond-dimension regions, enabling accurate thermodynamic calculations on complex networks with continuous variables.

Abstract: We develop a new approach to compress cyclic tensor networks called stochastic path compression (SPC) that uses an iterative importance sampling procedure to target edges with large bond-dimensions. Closed random walks in SPC form compression pathways that spatially localize large bond-dimensions in the tensor network. Analogous to the phase separation of two immiscible liquids, SPC separates the graph of bond-dimensions into spatially distinct high and low density regions. When combined with our integral decimation algorithm, SPC facilitates the accurate compression of cyclic tensor networks with continuous degrees of freedom. To benchmark and illustrate the methods, we compute the absolute thermodynamics of $q$-state clock models on two-dimensional square lattices and an XY model on a Watts-Strogatz graph, which is a small-world network with random connectivity between spins.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [36] [Physics-Based Decline Curve Analysis and Machine Learning for Temperature Forecasting in Enhanced Geothermal Systems: Utah FORGE](https://arxiv.org/abs/2601.03283)
*Mina S. Khalaf*

Main category: physics.soc-ph

TL;DR: Physics-consistent framework combining extended Arps decline curves with machine learning for reliable EGS temperature forecasting, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for EGS temperature forecasting have limitations: petroleum-based decline curves don't enforce geothermal heat transfer, ML surrogates lack physics consistency, and THM simulations are computationally expensive.

Method: 1) Extended Arps decline curves with equilibrium-temperature term for geothermal applications; 2) Equation-informed neural network with modified decline equations as differentiable layers; 3) Probabilistic Gaussian Process Regression surrogate; 4) XGBoost baseline for comparison.

Result: Extended decline models achieve near-perfect fidelity (median RMSE = 0.071°C). Equation-informed network: MAE = 3.06°C, RMSE = 4.49°C. Gaussian Process surrogate performs best: RMSE = 3.39°C, MAE = 2.34°C with well-calibrated uncertainty. XGBoost baseline shows higher errors.

Conclusion: The physics-consistent framework successfully bridges decline-curve analysis and surrogate modeling for EGS temperature forecasting, with Gaussian Process Regression delivering the most accurate predictions with uncertainty quantification.

Abstract: Reliable temperature forecasting in Enhanced Geothermal Systems (EGS) is essential, yet petroleum-based decline curves and many machine-learning surrogates do not enforce geothermal heat transfer, while thermo-hydro-mechanical (THM) simulation remains computationally expensive. This study proposes a physics-consistent framework that advances both decline-curve analysis and surrogate modeling. The classical Arps decline family is generalized for geothermal use by introducing an equilibrium-temperature term motivated by Newton-type cooling, ensuring finite late-time temperature limits while reducing exactly to the conventional Arps forms when the equilibrium term is set to zero. The extended decline curves are validated against Utah FORGE downhole temperature measurements and then used to construct learning surrogates on a controlled THM dataset spanning fracture count, well spacing, fracture spacing, host-rock thermal conductivity, and circulation rate. An equation-informed neural network embeds the modified decline equations as differentiable internal computational layers to produce full 0-60 month temperature trajectories from design and operational inputs. A probabilistic Gaussian Process Regression surrogate is also developed for direct multi-horizon forecasting with calibrated uncertainty, while a direct XGBoost regression baseline provides a purely data-driven reference. Across the simulation dataset, the extended decline models reproduce temperature trajectories with near-perfect fidelity (median RMSE = 0.071 °C), and the equation-informed network achieves typical hold-out errors of MAE = 3.06 °C and RMSE = 4.49 °C. The Gaussian Process surrogate delivers the strongest predictive accuracy across 3-60 month horizons (RMSE = 3.39 °C; MAE = 2.34 °C) with well-calibrated uncertainty, whereas the XGBoost baseline exhibits higher errors.

</details>


### [37] [Multi-Dimensional Opinion Formation](https://arxiv.org/abs/2601.04074)
*Hanna Bartel,Martin Burger,Marie-Therese Wolfram*

Main category: physics.soc-ph

TL;DR: A multi-dimensional opinion dynamics model where people have opinions and importance weights; opinion changes depend on weighted similarity across all opinions, leading to complex stationary states determined by weights.


<details>
  <summary>Details</summary>
Motivation: To develop a more realistic opinion dynamics model that accounts for multi-dimensional opinions and varying importance weights across different topics, capturing how people's opinions on one topic can be influenced by their overall opinion profile.

Method: Propose a kinetic model with binary interactions where opinion changes depend on weighted similarity across full opinion vectors. Derive mean-field PDE from kinetic equation. Use analytical computations and numerical simulations.

Result: The model generates complex stationary states, and the final opinion structures are critically determined by the people's opinion weights, as confirmed by both analytical and numerical analysis.

Conclusion: Importance weights across opinions play a crucial role in shaping collective opinion dynamics, with the weighted similarity coupling mechanism leading to rich emergent patterns in multi-dimensional opinion spaces.

Abstract: In this paper we propose and investigate a multi-dimensional opinion dynamics model where people are characterised by both opinions and importance weights across these opinions. Opinion changes occur through binary interactions, with a novel coupling mechanism: the change in one topic depends on the weighted similarity across the full opinion vector. We state the kinetic equation for this process and derive its mean-field partial differential equation to describe the overall dynamics. Analytical computations and numerical simulations confirm that this model generates complex stationary states, and we demonstrate that the final opinion structures are critically determined by the peoples' opinion weights.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [38] [Assessing Meteo-HySEA Performance for Adriatic Meteotsunami Events](https://arxiv.org/abs/2601.03856)
*Alejandro González,Cléa Denamiel,Jorge Macías*

Main category: physics.geo-ph

TL;DR: GPU-based Meteo-HySEA model shows promising meteotsunami simulation capabilities in Adriatic Sea with significant computational speed advantages over CPU-based ADCIRC, though both models have limitations in capturing atmospheric forcing and wave periods.


<details>
  <summary>Details</summary>
Motivation: Meteotsunamis pose coastal flooding hazards, especially in resonant bays like the Adriatic Sea. There's a need for efficient, accurate simulation models for operational early warning systems.

Method: Benchmarked GPU-based Meteo-HySEA against CPU-based AdriSC-ADCIRC system. Simulated three documented meteotsunami events (2014, 2017, 2020) using WRF downscaling of ERA reanalyses, validated with tide-gauge and microbarograph observations.

Result: Both models limited by underestimation of mesoscale pressure disturbances. Meteo-HySEA reproduces timing and spatial variability well, often yields larger amplitudes than ADCIRC, but tends to overestimate dominant wave periods. GPU acceleration provides order-of-magnitude computational efficiency gains.

Conclusion: GPU-based Meteo-HySEA offers strong potential for operational early warning due to computational efficiency enabling rapid high-resolution simulations, though further validation needed to assess wave period and energy dissipation representation.

Abstract: Meteotsunamis are atmospherically driven sea-level oscillations that can trigger hazardous coastal flooding, particularly in resonant bays. This study assesses the GPU-based Meteo-HySEA model for meteotsunami simulation in the Adriatic Sea, benchmarking its performance against the CPU-based AdriSC-ADCIRC system. Three documented events (2014, 2017, 2020) were simulated using WRF downscaling of ERA reanalyses and validated with tide-gauge and microbarograph observations. Both models are limited by the underestimation of mesoscale pressure disturbances in the atmospheric forcing. Meteo-HySEA generally reproduces the timing and spatial variability of sea-level oscillations and often yields larger amplitudes than ADCIRC, but it tends to overestimate dominant wave periods, particularly in enclosed basins. Differences in oscillation persistence underscore the need for further validation against high-resolution tide-gauge data to assess whether Meteo-HySEA captures harbor seiches more realistically or ADCIRC better represents physical energy dissipation. Crucially, GPU acceleration provides order-of-magnitude gains in computational efficiency, enabling rapid high-resolution, multi-grid simulations including inundation, and thus offering strong potential for operational early warning.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [39] [Green function rigidity for two dimensional sphere](https://arxiv.org/abs/2601.03773)
*Mijia Lai,Chilin Zhang*

Main category: math.DG

TL;DR: The paper proves that if a closed C² surface in ℝ³ has a Green function with a specific logarithmic form, then the surface must be a round sphere.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from verifying a conjecture by X. Chen and Y. Shi about the Green function on spheres in Euclidean space. The conjecture arises from their study of Green functions on surfaces and their relationship to geometric properties.

Method: The method involves analyzing the Green function on a closed C² embedded surface in ℝ³. The proof likely uses techniques from potential theory, geometric analysis, and properties of Green functions on surfaces to show that the specific logarithmic form of the Green function forces the surface to be spherical.

Result: The main result proves that if a closed C² surface M ⊂ ℝ³ has a point p such that its Green function G satisfies G(p,q) = -1/(2π) ln d_{ℝ³}(p,q) + c for all q ≠ p, then M must be a round sphere.

Conclusion: The conjecture by Chen and Shi is verified: the specific logarithmic form of the Green function characterizes round spheres among closed C² surfaces in ℝ³, establishing a connection between analytic properties (Green function behavior) and geometric properties (sphericity).

Abstract: We verify a conjecture proposed by X. Chen and Y. Shi, which arises from their study of the Green function on spheres in Euclidean space. More precisely, let $M\subset \mathbb{R}^3$ be a closed $C^{2}$ embedded surface and suppose that there exists a point $p\in M$ so that its Green function $G$ is of the form $G(p,q)=-\frac{1}{2π} \ln d_{\mathbb{R}^3}(p,q)+c, \forall q\neq p$, then $M$ must be a round sphere.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [40] [On flying through the base of a pseudo-streamer](https://arxiv.org/abs/2601.03620)
*Forrest Mozer,Oleksiy Agapitov,Kyungeun Choi,Andrii Voshchepynets*

Main category: physics.space-ph

TL;DR: Parker Solar Probe observed anomalous plasma conditions near 10 solar radii - enhanced density, reduced velocity/temperature, and strong electric fields in a pseudo-streamer base.


<details>
  <summary>Details</summary>
Motivation: To investigate unusual plasma conditions encountered during Parker Solar Probe's close solar approach and understand the structure and properties of solar wind features at unprecedented proximity to the Sun.

Method: Analysis of in-situ measurements from Parker Solar Probe orbit 24 near 10 solar radii perihelion, including plasma density, velocity, temperature, magnetic field polarity, suprathermal electron strahl, and electric field measurements.

Result: Discovered a confined region with 25,000 particles/cm³ density (enhanced), 200 km/s velocity (reduced), 25 eV ion temperature (reduced), 400 mV/m electric field, on closed magnetic field lines within a pseudo-streamer base extending to 10 solar radii.

Conclusion: The observations reveal unexpectedly low flow velocities and extended pseudo-streamer structures at 10 solar radii, with significant resistive and pressure gradient effects in Ohm's law, challenging previous remote sensing and theoretical models.

Abstract: Near the 10 solar radius perihelion of Parker Solar Probe orbit 24, a confined region containing an enhanced plasma density of 25,000 particles per cubic centimeter and broadband electrostatic waves was encountered. The solar wind velocity of 200 kilometers per second and ion temperature of 25 eV were significantly reduced as compared to their values in the ambient solar wind. These anomalous plasma conditions were observed on closed magnetic field lines, as determined from observations of the suprathermal electron strahl. Because the polarity of the radial magnetic field did not change sign on the two sides of the crossing and the crossed region contained a double-peaked plasma structure, the spacecraft must have passed through the base of a pseudo-streamer whose structure extended out to 10 solar radii. In the plasma frame, an electric field as large as 400 millivolts per meter was detected during the crossing. The current associated with this electric field was less than one milliampere per square meter, corresponding to a drift velocity less than 2.5 kilometers per second. It also contained a turbulent plasma with density fluctuations divided by density as large as 0.3, suggesting that the resistive term in the generalized ohm's law was significant. Also, the density as a function of time had a non-zero slope when the electric field was non-zero, suggesting that the pressure gradient term also mattered. As compared to earlier remote sensing and theoretical results, it is surprising that the plasma in this pseudo-streamer had a remarkably low flow velocity and that the pseudo-streamer base extended out to 10 solar radii.

</details>


### [41] [Properties of Magnetic Switchbacks in the Near-Sun Solar Wind](https://arxiv.org/abs/2601.04165)
*Samuel T. Badman,Naïs Fargette,Lorenzo Matteini,Oleksiy V. Agapitov,Mojtaba Akhavan-Tafti,Stuart D. Bale,Srijan Bharati Das,Nina Bizien,Trevor A. Bowen,Thierry Dudok de Wit,Clara Froment,Timothy Horbury,Jia Huang,Vamsee Krishna Jagarlamudi,Andrea Larosa,Maria S. Madjarska,Olga Panasenco,Etienne Pariat,Nour E. Raouafi,Alexis P. Rouillard,David Ruffolo,Nikos Sioulas,Shirsh Lata Soni,Luca Sorriso-Valvo,Gabriel Ho Hin Suen,Marco Velli,Jaye Verniero*

Main category: physics.space-ph

TL;DR: Review paper on magnetic switchbacks - fluctuations in solar wind where magnetic field sharply deflects while maintaining constant magnitude, observed by Parker Solar Probe in inner heliosphere.


<details>
  <summary>Details</summary>
Motivation: Switchbacks are nearly ubiquitous in inner heliosphere, have substantial energy content, and may play fundamental role in corona and solar wind dynamics, requiring comprehensive understanding.

Method: Review of in situ measurements primarily from Parker Solar Probe, discussing identification methods, measurement techniques, and analysis of individual switchbacks and their collective "patches".

Result: Overview of primary observational properties, identification of consensus findings versus limited/qualified results, and collation of open questions requiring further investigation.

Conclusion: Comprehensive review of magnetic switchback research with identification of key open questions and recommendations for future studies to advance understanding of these fundamental solar wind structures.

Abstract: Magnetic switchbacks are fluctuations in the solar wind in which the interplanetary magnetic field sharply deflects away from its background direction so as to create folds in magnetic field lines while remaining of roughly constant magnitude. The magnetic field and velocity fluctuations are extremely well correlated in a way corresponding to Alfvénic fluctuations propagating away from the Sun. For a background field which is nearly radial this causes an outwardly propagating jet to form. Switchbacks and their characteristic velocity jets have recently been observed to be nearly ubiquitous by Parker Solar Probe with in situ measurements in the inner heliosphere within 0.3 AU. Their prevalence, substantial energy content, and potentially fundamental role in the dynamics of the outer corona and solar wind motivate the significant research efforts into their understanding. Here we review the in situ measurements of these structures (primarily by Parker Solar Probe). We discuss how they are identified and measured, and present an overview of the primary observational properties of these structures, both in terms of individual switchbacks and their collective arrangement into ``patches''. We identify both properties for which there is a strong consensus and those that have limited or qualified support and require further investigation. We identify and collate several open questions and recommendations for future studies.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [42] [Equivariant Neural Networks for Force-Field Models of Lattice Systems](https://arxiv.org/abs/2601.04104)
*Yunhao Fan,Gia-Wei Chern*

Main category: cond-mat.str-el

TL;DR: ENN-based ML force fields for lattice Hamiltonians preserve discrete symmetries, enabling large-scale simulations of coupled electron-lattice dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing ML force fields for lattice models rely on hand-crafted symmetry descriptors that are system-specific and hinder generality/transferability across different lattice Hamiltonians.

Method: Develop symmetry-preserving framework using equivariant neural networks (ENNs) that embed discrete point-group and internal symmetries of lattice models directly into neural-network force fields. Applied to Holstein Hamiltonian on square lattice as proof of principle.

Result: ENN-based force-field model successfully captures mesoscale evolution of symmetry-breaking phase in Holstein Hamiltonian, enabling ML-enabled large-scale dynamical simulations.

Conclusion: Lattice-equivariant architectures provide general, data-driven approach for linking microscopic electronic processes to emergent dynamical behavior in condensed-matter lattice systems.

Abstract: Machine-learning (ML) force fields enable large-scale simulations with near-first-principles accuracy at substantially reduced computational cost. Recent work has extended ML force-field approaches to adiabatic dynamical simulations of condensed-matter lattice models with coupled electronic and structural or magnetic degrees of freedom. However, most existing formulations rely on hand-crafted, symmetry-aware descriptors, whose construction is often system-specific and can hinder generality and transferability across different lattice Hamiltonians. Here we introduce a symmetry-preserving framework based on equivariant neural networks (ENNs) that provides a general, data-driven mapping from local configurations of dynamical variables to the associated on-site forces in a lattice Hamiltonian. In contrast to ENN architectures developed for molecular systems -- where continuous Euclidean symmetries dominate -- our approach aims to embed the discrete point-group and internal symmetries intrinsic to lattice models directly into the neural-network representation of the force field. As a proof of principle, we construct an ENN-based force-field model for the adiabatic dynamics of the Holstein Hamiltonian on a square lattice, a canonical system for electron-lattice physics. The resulting ML-enabled large-scale dynamical simulations faithfully capture mesoscale evolution of the symmetry-breaking phase, illustrating the utility of lattice-equivariant architectures for linking microscopic electronic processes to emergent dynamical behavior in condensed-matter lattice systems.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [43] [Modelling spacecraft-emitted electrons measured by SWA-EAS experiment on board Solar Orbiter mission](https://arxiv.org/abs/2601.03818)
*Š. Štverák,D. Herčík,P. Hellinger,M. Popďakunik,G. R. Lewis,G. Nicolaou,C. J. Owen,Yu. V. Khotyaintsev,M. Maksimovic*

Main category: astro-ph.IM

TL;DR: The paper analyzes spacecraft charging effects on low-energy electron measurements from Solar Orbiter's SWA-EAS instrument using numerical simulations to understand contamination from spacecraft-emitted electrons.


<details>
  <summary>Details</summary>
Motivation: Thermal electron measurements in space are contaminated by spacecraft emissions (photo- and secondary electrons) and spacecraft charging, which particularly affects low-energy electron data. Understanding these effects is crucial for accurate plasma measurements.

Method: Used Spacecraft Plasma Interaction Software to model Solar Orbiter's interaction with solar wind plasma at 0.3 AU. Implemented a virtual detector to simulate SWA-EAS electron energy spectra and compared simulations with real in-situ measurements.

Result: Found qualitative agreement between simulations and real data. Unlike other missions, contamination by cold spacecraft-emitted electrons extends well above the spacecraft potential threshold. Contamination comes from multiple sources with varying contributions depending on ambient plasma conditions.

Conclusion: Spacecraft charging significantly contaminates low-energy electron measurements, with emissions from distant spacecraft surfaces causing contamination above the potential threshold. The SWA-EAS detector's real potential may differ from measured spacecraft potential, affecting spectral break positions.

Abstract: Thermal electron measurements in space plasmas typically suffer at low energies from spacecraft emissions of photo- and secondary electrons and from charging of the spacecraft body. We examine these effects by use of numerical simulations in the context of electron measurements acquired by the Electron Analyser System (SWA-EAS) on board the Solar Orbiter mission. We employed the Spacecraft Plasma Interaction Software to model the interaction of the Solar Orbiter spacecraft with solar wind plasma and we implemented a virtual detector to simulate the measured electron energy spectra as observed in situ by the SWA-EAS experiment. Numerical simulations were set according to the measured plasma conditions at 0.3~AU. We derived the simulated electron energy spectra as detected by the virtual SWA-EAS experiment for different electron populations and compared these with both the initial plasma conditions and the corresponding real SWA-EAS data samples. We found qualitative agreement between the simulated and real data observed in situ by the SWA-EAS detector. Contrary to other space missions, the contamination by cold electrons emitted from the spacecraft is seen well above the spacecraft potential energy threshold. A detailed analysis of the simulated electron energy spectra demonstrates that contamination above the threshold is a result of cold electron fluxes emitted from distant spacecraft surfaces. The relative position of the break in the simulated spectrum with respect to the spacecraft potential slightly deviates from that in the real observations. This may indicate that the real potential of the SWA-EAS detector with respect to ambient plasma differs from the spacecraft potential value measured on board. The overall contamination is shown to be composed of emissions from a number of different sources and their relative contribution varies with the ambient plasma conditions.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [44] [Non-thermal particle acceleration in multi-species kinetic plasmas: universal power-law distribution functions and temperature inversion in the solar corona](https://arxiv.org/abs/2601.03344)
*Uddipan Banik,Amitava Bhattacharjee*

Main category: astro-ph.SR

TL;DR: The paper develops a quasilinear theory explaining the origin of non-thermal power-law distributions in plasmas and connects it to solar corona temperature inversion, showing both phenomena result from Debye screening effects.


<details>
  <summary>Details</summary>
Motivation: To understand two puzzling astrophysical phenomena: 1) the origin of ubiquitous non-thermal power-law distributions in plasmas (like solar wind), and 2) temperature inversion in the solar corona. The authors argue these issues are deeply connected.

Method: Developed a self-consistent quasilinear theory (QLT) for electromagnetically driven kinetic plasmas, deriving a Fokker-Planck equation with drive diffusion coefficients for heating and Balescu-Lenard coefficients for internal turbulence and Coulomb collisions.

Result: Both electron and ion distributions relax toward a universal attractor with v^{-5} (E^{-2}) tail (κ=1.5 distribution) under specific drive conditions. This results from Debye screening effects. The theory explains how suprathermal particles escape solar gravity via velocity filtration, causing temperature inversion to 10^6 K.

Conclusion: The universal power-law distributions and solar corona temperature inversion are fundamentally connected through Debye screening physics. Velocity filtration with κ≈1.5-2 distributions provides reasonable fit to spectroscopic data and explains the abrupt temperature rise in the corona.

Abstract: The origin of non-thermal power-law distribution functions ubiquitously observed in astrophysical/space (e.g., the solar wind) and laboratory kinetic plasmas, is not well understood. Another puzzling phenomenon is temperature inversion in the solar corona. These two issues are deeply connected. We develop a self-consistent quasilinear theory (QLT) for electromagnetically driven kinetic plasmas, deriving a Fokker-Planck equation for the simultaneous relaxation of multiple species, with (i) a drive diffusion coefficient for the heating of dressed particles directly by the drive and indirectly by waves, and (ii) Balescu-Lenard diffusion and drag coefficients for internal turbulence and Coulomb collisions. Both electron and ion distributions relax towards a universal attractor with a $v^{-5}$ $(E^{-2})$ tail, akin to a $κ= 1.5$ distribution, under a super-Debye (but sub-Larmor) drive with a steep power-spectrum. This is an outcome of Debye screening: large-scale fields accelerate the unscreened, fast particles but not the screened, slow ones. The universality may be broken by shallow power-spectra and incomplete relaxation. Collisions cannot decelerate suprathermal particles, rendering a high $v$ tail immune to Maxwellianization. Such a tail may be generated in the solar corona by chromospheric convection despite collisional losses. The suprathermal particles escape sun's gravity (velocity filtration), inverting the temperature profile and raising it to $10^6$ K. A proper analysis of velocity filtration with a $κ\approx 1.5-2$ distribution inspired by QLT provides a reasonable fit to the spectroscopic data of heavy ions and explains the abrupt temperature rise, a consequence of the divergence of pressure in the $κ\to 1.5$ limit.

</details>


### [45] [Extreme-ultraviolet synthesis of nanojet-like ejections due to coalescing flux ropes](https://arxiv.org/abs/2601.03354)
*Samrat Sen,A. Ramada C. Sukarmadji,D. Nóbrega-Siverio,F. Moreno-Insertis,J. Martínez-Sykora,Patrick Antolin*

Main category: astro-ph.SR

TL;DR: Synthetic observations of nanojet-like signatures from flux rope coalescence in MHD simulations, providing predictions for detectability with current/future instruments.


<details>
  <summary>Details</summary>
Motivation: Small-scale energetic events like nanoflares and nanojets are challenging to detect but important for understanding coronal heating. There's a gap between MHD models and observations due to lack of synthetic diagnostics.

Method: Used MPI-AMRVAC code to simulate coalescence of two flux ropes, generating synthetic observables in Extreme-ultraviolet lines compatible with SDO/AIA and upcoming MUSE mission.

Result: Synthetic diagnostics produce key observational properties matching nanojet characteristics, suggesting a plausible 3D scenario where tiny flux ropes reconnect within loops.

Conclusion: Results provide predictions for nanojet detectability with current/future spectroscopic facilities and establish a bridge between MHD modeling and observations.

Abstract: Detection and characterization of small-scale energetic events such as nanoflares and nanojets remain challenging owing to their short lifetimes, small spatial extent, and relatively low energy release, despite their potential role in coronal heating. Recent observations have identified nanojets as small-scale (length $\lesssim 6.6$~Mm, width $\lesssim 1$~Mm), fast ($\sim$~few 100 km s$^{-1}$), and short-lived ($\lesssim 30$~s) ejections associated with nanoflare-scale energies, providing evidence of magnetic reconnection at small spatial scales. However, the lack of synthetic diagnostics has limited the connection between magnetohydrodynamic (MHD) models and observations. In this Letter, we present synthetic observations of the coalescence of two flux ropes, leading to nanojet-like signatures from a numerical model obtained with the \texttt{MPI-AMRVAC} code. We report synthetic observables in Extreme-ultraviolet lines compatible with existing instruments such as SDO/AIA, and upcoming MUSE mission, and compare the synthetic observables with an existing observation of nanojets. The synthetic diagnostics of the emissivity maps, Doppler velocity, thermal, and non-thermal line broadening produce key observational properties, suggesting a plausible 3D scenario for nanojet generation where tiny flux ropes reconnect within loops. Our results provide predictions for the detectability of nanojets with current and future spectroscopic facilities, and establish a bridge between MHD modeling and observations.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [46] [Terahertz volume plasmon-polariton modulation in all-dielectric hyperbolic metamaterials](https://arxiv.org/abs/2601.04000)
*Stefano Campanaro,Luca Bussi,Stefano Curtarolo,Arrigo Calzolari*

Main category: physics.optics

TL;DR: Doped III-V semiconductor hyperbolic metamaterials enable tunable terahertz plasmonics by overcoming metal limitations through customizable doping and geometry.


<details>
  <summary>Details</summary>
Motivation: Standard metals have intrinsic high electron density that limits terahertz plasmonics development. All-dielectric systems with doping offer tunable optical responses as alternatives.

Method: Multi-physics multi-scale theoretical approach to analyze doped III-V semiconductor hyperbolic metamaterials, studying doping effects and geometric parameters (thickness, composition, grating) on plasmon-polariton modes.

Result: Unraveled how doping and geometric characteristics modulate high-k plasmon-polariton modes across the metamaterial in the terahertz spectral region.

Conclusion: Doped semiconductor hyperbolic metamaterials provide a tunable platform for terahertz plasmonics by overcoming metal limitations through customizable doping and structural design.

Abstract: The development of plasmonics and related applications in the terahertz range faces limitations due to the intrinsic high electron density of standard metals. All-dielectric systems are profitable alternatives, which allows for customized modulation of the optical response upon doping. Here we focus on plasmon-based hyperbolic metamaterials realized stacking doped III-V semiconductors that have been shown to be optically active in the terahertz spectral region. By using a multi-physics multi-scale theoretical approach, we unravel the role of doping and geometrical characteristics (e.g., thickness, composition, grating) in the modulation of high-k plasmon-polariton modes across the metamaterial.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [47] [On average population levels for models with directed diffusion in heterogeneous environments](https://arxiv.org/abs/2601.03473)
*André Rickes,Elena Braverman*

Main category: math.DS

TL;DR: This paper analyzes how total population size relates to carrying capacity in logistic models with heterogeneous environments, extending previous results from r=K¹ and r=K⁰ to r=K^λ for any real λ, and examining dispersal strategies beyond simple diffusion.


<details>
  <summary>Details</summary>
Motivation: Previous studies showed contradictory results: Lou (2006) found total population exceeds carrying capacity when r=K¹, while Guo et al (2020) found it's less than carrying capacity when r=K⁰. DeAngelis et al (2016) suggested positive correlation matters. This paper aims to resolve the gap for general λ and disprove the assumption of a critical λ* where the tendency changes.

Method: The authors extend the logistic model to r=K^λ for any real λ, analyzing population dynamics in heterogeneous environments. They also incorporate more general dispersal strategies with diffusion term dΔ(u/P) instead of just dΔu, where P represents dispersal strategy parameters.

Result: The paper disproves the assumption of a critical λ* ∈ (0,1) where the tendency changes from population exceeding carrying capacity to being less than it. Instead, the relationship is more complicated. When incorporating dispersal strategy parameter P, the dependency of total population on diffusion coefficient shows different profiles compared to random diffusion.

Conclusion: The relationship between total population and carrying capacity is more complex than previously assumed, with no simple critical λ* threshold. Dispersal strategies significantly affect how total population depends on diffusion coefficients, showing different patterns from simple random diffusion models.

Abstract: In 2006 (J. Differential Equ.), Lou proved that, once the intrinsic growth rate $r$ in the logistic model is proportional to the spatially heterogeneous carrying capacity $K$ ($r=K^1$), the total population under the regular diffusion exceeds the total of the carrying capacity. He also conjectured that the dependency of the total population on the diffusion coefficient is unimodal, increasing to its maximum and then decreasing to the asymptote which is the total of the carrying capacity. DeAngelis et al (J. Math. Biol. 2016) argued that the prevalence of the population over the carrying capacity is only observed when the growth rate and the carrying capacity are positively correlated, at least for slow dispersal. Guo et al (J. Math. Biol. 2020) justified that, once $r$ is constant ($r=K^0$), the total population is less than the cumulative carrying capacity. Our paper fills up the gap for when $r=K^λ$ for any real $λ$, disproving an assumption that there is a critical $λ^{\ast} \in (0,1)$ at which the tendency of the prevalence of the carrying capacity over the total population size changes, demonstrating instead that the relationship is more complicated. In addition, we explore the dependency of the total population size on the diffusion coefficient when the third parameter of the dispersal strategy $P$ is involved: the diffusion term is $d Δ(u/P)$, not just $d Δu$, for any $λ$. We outline some differences from the random diffusion case, in particular, concerning the profile of the total population as a function of the diffusion coefficient.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [48] [Quantum computing for multidimensional option pricing: End-to-end pipeline](https://arxiv.org/abs/2601.04049)
*Julien Hok,Álvaro Leitao*

Main category: q-fin.CP

TL;DR: End-to-end framework for multi-asset option pricing combining risk-neutral density recovery with quantum-accelerated numerical integration, achieving 10-100x query reduction over classical methods.


<details>
  <summary>Details</summary>
Motivation: Address computational bottleneck of high-dimensional integration in multi-asset option pricing by leveraging quantum computing advantages while maintaining market-consistent arbitrage-free modeling.

Method: 1) Calibrate arbitrage-free marginal distributions from European option quotes using Normal Inverse Gaussian (NIG) model; 2) Couple marginals via Gaussian copula; 3) Use Quantum Accelerated Monte Carlo (QAMC) with Quantum Amplitude Estimation (QAE) for numerical integration.

Result: High calibration accuracy on liquid equity entities (Credit Agricole, AXA, Michelin); QAMC requires 10-100 times fewer queries than classical Monte Carlo methods for comparable precision; Theoretical accuracy bounds and query complexity established.

Conclusion: Provides practical integration of arbitrage-aware modeling with quantum computing, demonstrating scalability advantages and potential for complex derivatives pricing.

Abstract: This work introduces an end-to-end framework for multi-asset option pricing that combines market-consistent risk-neutral density recovery with quantum-accelerated numerical integration. We first calibrate arbitrage-free marginal distributions from European option quotes using the Normal Inverse Gaussian (NIG) model, leveraging its analytical tractability and ability to capture skewness and fat tails. Marginals are coupled via a Gaussian copula to construct joint distributions. To address the computational bottleneck of the high-dimensional integration required to solve the option pricing formula, we employ Quantum Accelerated Monte Carlo (QAMC) techniques based on Quantum Amplitude Estimation (QAE), achieving quadratic convergence improvements over classical Monte Carlo (CMC) methods. Theoretical results establish accuracy bounds and query complexity for both marginal density estimation (via cosine-series expansions) and multidimensional pricing. Empirical tests on liquid equity entities (Credit Agricole, AXA, Michelin) confirm high calibration accuracy and demonstrate that QAMC requires 10-100 times fewer queries than classical methods for comparable precision. This study provides a practical route to integrate arbitrage-aware modelling with quantum computing, highlighting implications for scalability and future extensions to complex derivatives.

</details>
