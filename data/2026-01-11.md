<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 21]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [math.AG](#math.AG) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [math-ph](#math-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Toward genuine efficiency and cluster robustness of preconditioned CG-like eigensolvers](https://arxiv.org/abs/2601.04429)
*Ming Zhou,Klaus Neymeyr*

Main category: math.NA

TL;DR: The paper proposes a new cluster-robust vector iteration method that improves LOBPCG by replacing LOPCG with asymptotically equivalent two-term recurrences and using timely correction with far previous iterates, significantly reducing computational steps and time for clustered eigenvalues.


<details>
  <summary>Details</summary>
Motivation: Existing vectorwise eigensolvers like LOPCG and its alternatives suffer from frequent delays and staircase-shaped convergence behavior when computing clustered eigenvalues, which cannot be explained by existing estimates. This inefficiency affects the overall performance of blockwise implementations like LOBPCG.

Method: The authors construct a class of cluster robust vector iterations by: 1) replacing LOPCG with asymptotically equivalent two-term recurrences, and 2) timely correcting search directions by selecting a far previous iterate as augmentation. This approach addresses the convergence delays in clustered eigenvalue computations.

Result: The new approach significantly reduces the number of required steps and the total computational time compared to existing methods like LOPCG and its alternatives.

Conclusion: The proposed cluster-robust vector iteration method effectively addresses convergence delays in clustered eigenvalue problems, improving the efficiency of eigensolvers and providing a better foundation for blockwise implementations like LOBPCG.

Abstract: The performance of eigenvalue problem solvers (eigensolvers) depends on various factors such as preconditioning and eigenvalue distribution. Developing stable and rapidly converging vectorwise eigensolvers is a crucial step in improving the overall efficiency of their blockwise implementations. The present paper is concerned with the locally optimal block preconditioned conjugate gradient (LOBPCG) method for Hermitian eigenvalue problems, and motivated by two recently proposed alternatives for its single-vector version LOPCG. A common basis of these eigensolvers is the well-known CG method for linear systems. However, the optimality of CG search directions cannot perfectly be transferred to CG-like eigensolvers. In particular, while computing clustered eigenvalues, LOPCG and its alternatives suffer from frequent delays, leading to a staircase-shaped convergence behavior which cannot be explained by the existing estimates. Keeping this in mind, we construct a class of cluster robust vector iterations where LOPCG is replaced by asymptotically equivalent two-term recurrences and the search directions are timely corrected by selecting a far previous iterate as augmentation. The new approach significantly reduces the number of required steps and the total computational time.

</details>


### [2] [Approximations of Extremal Eigenspace and Orthonormal Polar Factor](https://arxiv.org/abs/2601.04479)
*Ren-Cang Li*

Main category: math.NA

TL;DR: Tight error bounds for approximating top eigenspaces of Hermitian matrices and orthonormal polar factors of general matrices


<details>
  <summary>Details</summary>
Motivation: The paper addresses fundamental problems in matrix analysis: how well can we approximate key matrix components (top eigenspaces and polar factors) with computational efficiency

Method: Mathematical analysis establishing tight error bounds for approximations of top eigenspaces of Hermitian matrices and orthonormal polar factors of general matrices

Result: Obtained tight error bounds on the quality of approximations for both problems, providing optimal theoretical guarantees

Conclusion: The paper provides definitive error bounds for two important matrix approximation problems, establishing theoretical limits on approximation quality

Abstract: This paper is concerned with two extremal problems from matrix analysis. One is about approximating the top eigenspaces of a Hermitian matrix and the other one about approximating the orthonormal polar factor of a general matrix. Tight error bounds on the quality of the approximations are obtained.

</details>


### [3] [Nonlinear parametrization solver for fractional Burgers equations](https://arxiv.org/abs/2601.04482)
*Haojun Qin,Zhiwei Gao,Jinye Shen,George Karniadakis*

Main category: math.NA

TL;DR: STNP method solves fractional Burgers equations with oscillation-free shock resolution using sequential nonlinear parametrization and tangent space projection.


<details>
  <summary>Details</summary>
Motivation: Classical methods struggle with fractional Burgers equations due to nonlocality and shock-forming dynamics, suffering from Gibbs oscillations and requiring ad hoc stabilization that degrades in transport-dominated regimes.

Method: Sequential-in-time nonlinear parametrization (STNP) represents solution via nonlinear parametric ansatz, projects governing dynamics onto tangent space of parameter manifold using regularized least-squares at each time step, preserving causality without global optimization.

Result: STNP achieves oscillation-free shock resolution, accurately captures long-time dynamics, outperforms high-order spectral schemes with spectral vanishing viscosity, requires fewer degrees of freedom, and avoids ad hoc stabilization.

Conclusion: STNP provides a stable, well-posed time-marching scheme for fractional Burgers equations with theoretical guarantees, effectively handling nonlocality and shock formation while eliminating oscillations and stabilization issues of classical methods.

Abstract: Fractional Burgers equations pose substantial challenges for classical numerical methods due to the combined effects of nonlocality and shock-forming nonlinear dynamics. In particular, linear approximation frameworks-such as spectral, finite-difference, or discontinuous Galerkin methods-often suffer from Gibbs-type oscillations or require carefully tuned stabilization mechanisms, whose effectiveness degrades in transport-dominated and long-time integration regimes. In this work, we introduce a sequential-in-time nonlinear parametrization (STNP) for solving fractional Burgers equations, including models with a fractional Laplacian or with nonlocal nonlinear fluxes. The solution is represented by a nonlinear parametric ansatz, and the parameter evolution is obtained by projecting the governing dynamics onto the tangent space of the parameter manifold through a regularized least-squares formulation at each time step. This yields a well-posed and stable time-marching scheme that preserves causality and avoids global-in-time optimization. We provide a theoretical analysis of the resulting projected dynamics, including a stability estimate and an a posteriori error bound that explicitly decomposes the total error into contributions from initial condition fitting, projection residuals, and discretization of fractional operators. Our analysis clarifies the stabilizing role of regularization and quantifies its interaction with the nonlocal discretization error. Numerical experiments for both fractional Burgers models demonstrate that STNP achieves oscillation-free shock resolution and accurately captures long-time dynamics. The method consistently outperforms high-order spectral schemes augmented with spectral vanishing viscosity, while requiring significantly fewer degrees of freedom and avoiding ad hoc stabilization.

</details>


### [4] [Adaptive Multi-Grade Deep Learning for Highly Oscillatory Fredholm Integral Equations of the Second Kind](https://arxiv.org/abs/2601.04496)
*Jie Jiang,Yuesheng Xu*

Main category: math.NA

TL;DR: MGDL for solving highly oscillatory Fredholm integral equations with rigorous error analysis and adaptive grade selection based on training performance.


<details>
  <summary>Details</summary>
Motivation: To develop an effective deep learning approach for solving challenging highly oscillatory Fredholm integral equations of the second kind, which are difficult to solve with traditional numerical methods due to their oscillatory nature and potential singularities.

Method: Multi-Grade Deep Learning (MGDL) with rigorous error analysis of both continuous and discrete models, plus a novel adaptive algorithm that selects the network grade based on training performance to minimize approximation error.

Result: The discrete MGDL model retains convergence and stability of the continuous model under sufficiently small quadrature error. DNN training error is identified as the primary source of approximation error. Numerical experiments with highly oscillatory (up to wavenumber 500) and singular solutions confirm accuracy, effectiveness, and robustness.

Conclusion: MGDL provides an accurate, effective, and robust approach for solving highly oscillatory Fredholm integral equations, with the adaptive grade selection algorithm addressing the key challenge of training error in deep learning approximations.

Abstract: This paper studies the use of Multi-Grade Deep Learning (MGDL) for solving highly oscillatory Fredholm integral equations of the second kind. We provide rigorous error analyses of continuous and discrete MGDL models, showing that the discrete model retains the convergence and stability of its continuous counterpart under sufficiently small quadrature error. We identify the DNN training error as the primary source of approximation error, motivating a novel adaptive MGDL algorithm that selects the network grade based on training performance. Numerical experiments with highly oscillatory (including wavenumber 500) and singular solutions confirm the accuracy, effectiveness and robustness of the proposed approach.

</details>


### [5] [The explicit constraint force method for optimal experimental design](https://arxiv.org/abs/2601.04557)
*Conor Rowan*

Main category: math.NA

TL;DR: The paper investigates optimal experimental design (OED) using the explicit constraint force method (ECFM) formulation, finds it impractical due to measurement noise issues, and concludes ECFM is not viable for OED.


<details>
  <summary>Details</summary>
Motivation: While ECFM has been extended to inverse problems, optimal experimental design (OED) has not been explored with this formulation. Experimentalists need to design experiments that yield the most robust parameter recovery, but existing OED techniques are based on standard approaches, not ECFM.

Method: The authors review traditional OED approaches based on Fisher information matrix, propose an analogous formulation based on constraint forces, compare interpretations of objectives from standard vs. constraint force-based inverse problems, and test the method on several example problems.

Result: Experiments show that constraint force-based OED tends to position measurements in the stiffest regions of the system where responses are small. This strategy becomes impractical in the presence of measurement noise or finite measurement precision.

Conclusion: ECFM is not a viable approach to optimal experimental design due to the impracticality of placing measurements in stiff regions where signal responses are small and vulnerable to noise.

Abstract: The explicit constraint force method (ECFM) was recently introduced as a novel formulation of the physics-informed solution reconstruction problem, and was subsequently extended to inverse problems. In both solution reconstruction and inverse problems, model parameters are estimated with the help of measurement data. In practice, experimentalists seek to design experiments such that the acquired data leads to the most robust recovery of the missing parameters in a subsequent inverse problem. While there are well-established techniques for designing experiments with standard approaches to the inverse problem, optimal experimental design (OED) has yet to be explored with the ECFM formulation. In this work, we investigate OED with a constraint force objective. First, we review traditional approaches to OED based on the Fisher information matrix, and propose an analogous formulation based on constraint forces. Next, we reflect on the different interpretations of the objective from standard and constraint force-based inverse problems. We then test our method on several example problems. These examples suggest that an experiment which is optimal in the sense of constraint forces tends to position measurements in the stiffest regions of the system. Because the responses -- and thus the measurements -- are small in these regions, this strategy is impractical in the presence of measurement noise and/or finite measurement precision. As such, our provisional conclusion is that ECFM is not a viable approach to OED.

</details>


### [6] [An HHT-$α$-based finite element framework for wave propagation in constitutively nonlinear elastic materials](https://arxiv.org/abs/2601.04628)
*S. M. Mallikarjunaiah*

Main category: math.NA

TL;DR: A computational framework for modeling wave propagation in geometrically linear elastic materials with algebraically nonlinear constitutive relations, featuring nonlinear wave equations with explicit nonlinearity in time-derivative terms, solved using FEM spatial discretization and HHT-α time integration with Newton's method.


<details>
  <summary>Details</summary>
Motivation: To develop a validated computational tool for analyzing shock formation in advanced nonlinear materials by modeling wave propagation in materials with nonlinear constitutive relations, where traditional linear models fail to capture important phenomena like wavefront steepening and shock discontinuities.

Method: Derived a specific form of nonlinear wave equation with explicit nonlinearity in time-derivative terms. Used fully discrete formulation combining standard finite element method for spatial discretization with implicit Hilber-Hughes-Taylor (HHT)-α scheme for time integration. Employed Newton's method to iteratively solve the linearized equations at each time step due to nonlinear nature of the discrete system.

Result: Framework demonstrated optimal convergence rates in both space and time through rigorous convergence analyses. Parametric study revealed: magnitude parameter of stress-dependent variation in wave speed leads to wavefront steepening and shock discontinuities; exponent parameter acts as nonlinearity filter - high values suppress nonlinear effects in small-strain regimes, while low values allow significant dispersive behavior.

Conclusion: The work provides a validated computational tool for analyzing shock formation in advanced nonlinear materials, successfully capturing important nonlinear wave phenomena and elucidating the roles of key constitutive parameters in governing wave propagation behavior in nonlinear elastic materials.

Abstract: This paper presents a computational framework for modeling wave propagation in geometrically linear elastic materials characterized by algebraically nonlinear constitutive relations. We derive a specific form of the nonlinear wave equation in which the nonlinearity explicitly appears in the time-derivative terms that govern the evolution of the mechanical fields. The numerical solution is established using a fully discrete formulation that combines the standard finite element method for spatial discretization with the implicit Hilber-Hughes-Taylor (HHT)-$α$ scheme for time integration. To address the nonlinear nature of the discrete system, we employ Newton's method to iteratively solve the linearized equations at each time step. The accuracy and robustness of the proposed framework are rigorously verified through convergence analyses, which demonstrate optimal convergence rates in both space and time. Furthermore, a detailed parametric study is conducted to elucidate the influence of the model's constitutive parameters. The results reveal that the magnitude parameter of the stress-dependent variation in wave speed leads to wavefront steepening and the formation of shock discontinuities. Conversely, the exponent parameter acts as a nonlinearity filter; high values suppress nonlinear effects in small-strain regimes, whereas low values allow significant dispersive behavior. This work provides a validated tool for analyzing shock formation in advanced nonlinear materials.

</details>


### [7] [On the role of weak Marcinkiewicz-Zygmund constants in polynomial approximation by orthogonal bases](https://arxiv.org/abs/2601.04708)
*Congpei An,Alvise Sommariva,Marco Vianello*

Main category: math.NA

TL;DR: Numerical computation of L² Marcinkiewicz-Zygmund constants for cubature rules and their role in polynomial approximation using orthogonal bases, with testing on various domains and comparison of least squares projection methods.


<details>
  <summary>Details</summary>
Motivation: To understand the role of Marcinkiewicz-Zygmund constants in polynomial approximation by orthogonal bases and to evaluate the approximation power of different projection methods across various domains.

Method: Numerically compute L² Marcinkiewicz-Zygmund constants for cubature rules, test relevant rules on multiple domains (interval, square, disk, triangle, cube, sphere), and compare least squares projection with standard hyperinterpolation and its "exactness-relaxed" version using open-source Matlab codes.

Result: The paper provides numerical computations of Marcinkiewicz-Zygmund constants for various cubature rules across different domains, with comparative analysis of approximation power between least squares projection, standard hyperinterpolation, and its relaxed version.

Conclusion: The study offers practical computational tools and insights into the performance of different polynomial approximation methods based on Marcinkiewicz-Zygmund constants, with open-source code availability for reproducibility and further research.

Abstract: We compute numerically the $L^2$ Marcinkiewicz-Zygmund constants of cubature rules, with a special attention to their role in polynomial approximation by orthogonal bases. We test some relevant rules on domains such as the interval, the square, the disk, the triangle, the cube and the sphere. The approximation power of the corresponding least squares (LS) projection is compared with standard hyperinterpolation and its recently proposed ``exactness-relaxed'' version. The Matlab codes used for these tests are available in open-source form.

</details>


### [8] [A finite element method preserving the eigenvalue range of symmetric tensor fields](https://arxiv.org/abs/2601.04839)
*Abdolreza Amiri,Gabriel R. Barrenechea,Tristan Pryer*

Main category: math.NA

TL;DR: A finite element method that preserves eigenvalue bounds for tensor-valued convection-diffusion equations by formulating the problem as a variational inequality on a convex set and using eigenvalue truncation at degrees of freedom.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method that maintains the eigenvalue range of solutions for tensor-valued time-dependent convection-diffusion equations, which is important for preserving physical properties and stability in convection-dominated regimes.

Method: Uses a high-order CIP-stabilized finite element baseline, formulates the fully discrete problem as a variational inequality on a closed convex set of tensor-valued functions with eigenvalue bounds at degrees of freedom, implements a projection that diagonalizes and truncates eigenvalues at each node, and employs implicit Euler temporal discretization.

Result: Proves unconditional stability and optimal-order error estimates for the method, with numerical experiments confirming theoretical findings and demonstrating the method's ability to maintain eigenvalue constraints while accurately approximating solutions in convection-dominated regimes.

Conclusion: The proposed method successfully preserves eigenvalue bounds for tensor-valued convection-diffusion equations while maintaining accuracy and stability, particularly in challenging convection-dominated scenarios.

Abstract: This paper presents a finite element method that preserves (at the degrees of freedom) the eigenvalue range of the solution of tensor-valued time-dependent convection--diffusion equations. Starting from a high-order spatial baseline discretisation (in this case, the CIP stabilised finite element method), our approach formulates the fully discrete problem as a variational inequality posed on a closed convex set of tensor-valued functions that respect the same eigenvalue bounds at their degrees of freedom. The numerical realisation of the scheme relies on the definition of a projection that, at each node, performs the diagonalisation of the tensor and then truncates the eigenvalues to lie within the prescribed bounds. The temporal discretisation is carried out using the implicit Euler method, and unconditional stability and optimal-order error estimates are proven for this choice. Numerical experiments confirm the theoretical findings and illustrate the method's ability to maintain eigenvalue constraints while accurately approximating solutions in the convection-dominated regime.

</details>


### [9] [Virtual Element methods for non-Newtonian shear-thickening fluid flow problems](https://arxiv.org/abs/2601.04866)
*Paola F. Antonietti,Lourenço Beirão da Veiga,Michele Botti,André Harnist,Giuseppe Vacca,Marco Verani*

Main category: math.NA

TL;DR: Theoretical analysis of Virtual Element Method for incompressible non-Newtonian Carreau-Yasuda flows in shear-thickening regime, featuring exactly divergence-free velocity and polygonal mesh compatibility.


<details>
  <summary>Details</summary>
Motivation: Extend previous work on shear-thinning flows to shear-thickening regime (r > 2) for Carreau-Yasuda constitutive law, requiring new analytical tools for theoretical analysis.

Method: Virtual Element Method with exactly divergence-free discrete velocity field, compatible with polygonal meshes. Novel tools include: inf-sup stability analysis in non-Hilbertian norms, specialized stabilization term for r > 2 nonlinear structure, and tailored discrete norm for nonlinear constitutive relation.

Result: Comprehensive theoretical analysis developed for shear-thickening regime covering both degenerate (δ = 0) and non-degenerate (δ > 0) cases. Numerical results demonstrate practical performance.

Conclusion: Successfully extends Virtual Element Method to shear-thickening non-Newtonian flows with rigorous theoretical foundation and practical numerical validation.

Abstract: In this work, we present a comprehensive theoretical analysis for Virtual Element discretizations of incompressible non-Newtonian flows governed by the Carreau-Yasuda constitutive law, in the shear-thickening regime (r > 2) including both degenerate (delta = 0) and non-degenerate (delta > 0) cases. The proposed Virtual Element method features two distinguishing advantages: the construction of an exactly divergence-free discrete velocity field and compatibility with general polygonal meshes. The analysis presented in this work extends a previous work, where only shear-thinning behavior (1 < r < 2) was considered. Indeed, the theoretical analysis of the shear-thickening setting requires several novel analytical tools, including: an inf-sup stability analysis of the discrete velocity-pressure coupling in non-Hilbertian norms, a stabilization term specifically designed to address the nonlinear structure as the exponent r > 2; and the introduction of a suitable discrete norm tailored to the underlying nonlinear constitutive relation. Numerical results demonstrate the practical performance of the proposed formulation.

</details>


### [10] [Guided Variational Network for Image Decomposition](https://arxiv.org/abs/2601.04999)
*Alessandro Lanza,Serena Morigi,Youwei Wen,Li Yang*

Main category: math.NA

TL;DR: GVD is a self-tuning image decomposition method that learns adaptive weights via statistics or neural networks, bridging classical variational models with modern data-driven approaches.


<details>
  <summary>Details</summary>
Motivation: Classical variational/optimization models for cartoon-texture decomposition are numerically intractable and require tedious manual tuning of global regularization parameters.

Method: Proposes Guided Variational Decomposition (GVD) with spatially adaptive quadratic norms whose pixel-wise weights are learned through local probabilistic statistics or lightweight neural networks within a bilevel framework.

Result: GVD provides a unified, interpretable, computationally efficient model that bridges classical variational ideas with modern adaptive/data-driven methods, with automatic parameter selection.

Conclusion: GVD emerges as a robust, self-tuning, superior solution for reliable image decomposition that eliminates manual parameter tuning while maintaining computational efficiency.

Abstract: Cartoon-texture image decomposition is a critical preprocessing problem bottlenecked by the numerical intractability of classical variational or optimization models and the tedious manual tuning of global regularization parameters.We propose a Guided Variational Decomposition (GVD) model which introduces spatially adaptive quadratic norms whose pixel-wise weights are learned either through local probabilistic statistics or via a lightweight neural network within a bilevel framework.This leads to a unified, interpretable, and computationally efficient model that bridges classical variational ideas with modern adaptive and data-driven methodologies. Numerical experiments on this framework, which inherently includes automatic parameter selection, delivers GVD as a robust, self-tuning, and superior solution for reliable image decomposition.

</details>


### [11] [A simple rigorous integrator for semilinear parabolic PDEs](https://arxiv.org/abs/2601.05146)
*Jan Bouwe van den Berg,Maxime Breden*

Main category: math.NA

TL;DR: A new method for rigorous integration of parabolic PDEs with explicit error bounds between numerical approximations and exact solutions, using a posteriori fixed-point analysis and adaptive time-stepping.


<details>
  <summary>Details</summary>
Motivation: Numerical simulations of PDEs are ubiquitous but lack correctness guarantees. The paper aims to provide rigorous error bounds for parabolic PDE solutions, ensuring mathematical correctness of numerical approximations.

Method: Uses a fixed-point reformulation based on piece-wise constant approximations of linearization around numerical solutions. Provides a posteriori error bounds with adaptive time-stepping strategy. Can prove convergence to stable hyperbolic equilibria with infinite final timestep.

Result: Demonstrates ability to integrate over long time intervals and capture non-trivial dynamics on Swift-Hohenberg, Ohta-Kawasaki, and Kuramoto-Sivashinsky equations. Method provides explicit, rigorous error bounds.

Conclusion: The approach is simple, efficient, and expected to generalize to other parabolic PDEs and boundary value problems, providing mathematically guaranteed correctness for numerical simulations.

Abstract: Simulations of the dynamics generated by partial differential equations (PDEs) provide approximate, numerical solutions to initial value problems. Such simulations are ubiquitous in scientific computing, but the correctness of the results is usually not guaranteed. We propose a new method for the rigorous integration of parabolic PDEs, i.e., the derivation of rigorous and explicit error bounds between the numerically obtained approximate solution and the exact one, which is then proven to exist over the entire time interval considered. These guaranteed error bounds are obtained a posteriori, using a fixed point reformulation based on a piece-wise in time constant approximation of the linearization around the numerical solution. Our setup leads to relatively simple-to-understand estimates, which has several advantages. Most critically, it allows us to optimize various aspects of the proof, and in particular to provide an adaptive time-stepping strategy. In case the solution converges to a stable hyperbolic equilibrium, we are also able to prove this convergence, applying our rigorous integrator with a final, infinitely long timestep. We showcase the ability of our method to rigorously integrate over relatively long time intervals, and to capture non-trivial dynamics, via examples on the Swift--Hohenberg equation, the Ohta--Kawasaki equation and the Kuramoto--Sivashinsky equation. We expect that the simplicity and efficiency of the approach will enable generalization to a wide variety of other parabolic PDEs, as well as applications to boundary value problems.

</details>


### [12] [Variable Projection Methods for Solving Regularized Separable Inverse Problems with Applications to Semi-Blind Image Deblurring](https://arxiv.org/abs/2601.05224)
*Delfina B. Comerso Salzer,Malena I. Español,Gabriela Jeronimo*

Main category: math.NA

TL;DR: Extends variable projection method to handle differentiable regularization on nonlinear parameters and general-form Tikhonov regularization on linear variables, with quasi-Newton solver and convergence analysis.


<details>
  <summary>Details</summary>
Motivation: Separable nonlinear least squares problems arise in many inverse problems like semi-blind image deblurring. Existing variable projection methods need extension to handle regularization on both linear and nonlinear parameters, and efficient solvers for large-scale problems.

Method: Extends variable projection (VarPro) to include differentiable regularization on nonlinear parameters and general-form Tikhonov regularization on linear variables. Develops quasi-Newton method for reduced problem with convergence analysis. Introduces inexact LSQR-based variant for large-scale problems with inner-solve and Hessian approximations.

Result: Provides local convergence analysis showing superlinear/quadratic convergence under standard assumptions. For inexact variant, proves local convergence despite approximations. Numerical experiments on semi-blind deblurring show parameter regularization prevents degenerate solutions and methods achieve accurate reconstructions with favorable accuracy-cost tradeoff.

Conclusion: Extended VarPro framework with regularization on both parameter types enables robust solutions to separable nonlinear least squares problems. The quasi-Newton solver and inexact LSQR variant provide efficient algorithms with proven convergence properties, making them suitable for practical inverse problems like semi-blind deblurring.

Abstract: Separable nonlinear least squares problems appear in many inverse problems, including semi-blind image deblurring. The variable projection (VarPro) method provides an efficient approach for solving such problems by eliminating linear variables and reducing the problem to a smaller, nonlinear one. In this work, we extend VarPro to solve minimization problems containing a differentiable regularization term on the nonlinear parameters, along with a general-form Tikhonov regularization term on the linear variables. Furthermore, we develop a quasi-Newton method for solving the resulting reduced problem, and provide a local convergence analysis under standard smoothness assumptions, establishing conditions for superlinear or quadratic convergence. For large-scale settings, we introduce an inexact LSQR-based variant and prove its local convergence despite inner-solve and Hessian approximations. Numerical experiments on semi-blind deblurring show that parameter regularization prevents degenerate no-blur solutions and that the proposed methods achieve accurate reconstructions, with the inexact variant offering a favorable accuracy-cost tradeoff consistent with the theory.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [On $W^{2,\varepsilon}$-estimates for a class of singular-degenerate parabolic equations](https://arxiv.org/abs/2601.04324)
*Junyuan Fang,Tuoc Phan*

Main category: math.AP

TL;DR: The paper establishes weighted W^{2,ε}-estimates for parabolic equations with singular/degenerate coefficients using A_{1+1/n} weights, via local quantitative lower estimates and perturbation methods.


<details>
  <summary>Details</summary>
Motivation: To develop foundational tools for studying fully nonlinear parabolic equations with singular and degenerate coefficients, which arise in various applications where coefficients can vanish or blow up.

Method: Introduces weighted parabolic cylinders intrinsically suitable for the equations, uses parabolic ABP estimates and perturbation methods, performs careful regularization and truncation of weights, and establishes local quantitative lower estimates of solutions (mean sojourn times).

Result: Proves F.-H. Lin type weighted W^{2,ε}-estimates under smallness assumptions on weighted mean oscillation of the weight, extending Evans' results for linear elliptic equations to the parabolic case with singular/degenerate coefficients.

Conclusion: The paper provides foundational ingredients and estimates that serve as building blocks for studying more complex fully nonlinear parabolic equations with singular-degenerate coefficients.

Abstract: We study a class of parabolic equations in non-divergence form with measurable coefficients that are singular, degenerate, or both singular and degenerate through a weight belonging to the $A_{1+\frac{1}{n}}$ -Muckenhoupt class of weights. Under some smallness assumption on a weighted mean oscillation of the weight, F.-H. Lin type weighted $W^{2,\varepsilon}$-estimates are proved. To prove the result, we establish a result on local quantitative lower estimates of solutions to the class of equations, which are known as the mean sojourn times of sample paths within sets. This type of estimate was proved by L. C. Evans for the class of linear elliptic equations in non-divergence form with uniformly elliptic and bounded measurable coefficients. A class of weighted parabolic cylinders intrinsically suitable for the class of equations is introduced. The parabolic ABP estimates, and a perturbation method are used to overcome the singularity and degeneracy of the coefficients. Careful analysis on regularization and truncation of the weights is performed. The paper provides foundational ingredients and estimates for the study of fully nonlinear parabolic equations with singular-degenerate coefficients.

</details>


### [14] [A fourth-order regularization of the curvature flow of immersed plane curves with Dirichlet boundary conditions](https://arxiv.org/abs/2601.04385)
*Giovanni Bellettini,Virginia Lorenzini,Matteo Novaga,Riccardo Scala*

Main category: math.AP

TL;DR: Fourth-order regularization of curvature flow for plane curves with fixed boundary converges to standard curvature flow as regularization parameter ε→0.


<details>
  <summary>Details</summary>
Motivation: To study curvature flow for immersed plane curves with fixed boundary by introducing a regularized version that avoids singularities and allows for rigorous analysis of convergence.

Method: Use an elastica-type functional with small parameter ε to create a fourth-order regularization of curvature flow. Analyze the approximating flow and prove its smooth convergence to the standard curvature flow as ε→0.

Result: The regularized flow converges smoothly to the curvature flow with Dirichlet boundary conditions for all times before the first singularity occurs in the limit flow.

Conclusion: The fourth-order regularization provides a valid approximation scheme for studying curvature flow with fixed boundary, with convergence guaranteed up to singularity formation in the limit flow.

Abstract: We consider a fourth-order regularization of the curvature flow for an immersed plane curve with fixed boundary, using an elastica-type functional depending on a small positive parameter $\varepsilon$. We show that the approximating flow smoothly converges, as $\varepsilon \to 0^+$, to the curvature flow of the curve with Dirichlet boundary conditions for all times before the first singularity of the limit flow.

</details>


### [15] [Uniform subelliptic estimates for degenerating Fokker-Planck equations](https://arxiv.org/abs/2601.04489)
*Hart F. Smith*

Main category: math.AP

TL;DR: Proves long-time bounds for Fokker-Planck equations with subelliptic diffusion using parametrix construction, extending previous elliptic results to subelliptic case.


<details>
  <summary>Details</summary>
Motivation: Extend recent work on Fokker-Planck equations from elliptic to subelliptic diffusion cases, particularly when jump operators are linear functions and Hörmander condition holds.

Method: Construct suitable parametrix for Fokker-Planck equation with subelliptic diffusion to prove semiclassical derivative estimates hold globally in L^p spaces.

Result: Shows semiclassical derivative estimates established for elliptic diffusion also hold for subelliptic diffusion, with global bounds in L^p for all 1 ≤ p ≤ ∞.

Conclusion: Successfully extends long-time bounds and derivative estimates from elliptic to subelliptic Fokker-Planck equations under appropriate assumptions.

Abstract: We expand upon recent work of Hernandez-Ranard-Riedel, Galkowski-Zworski, and Li, by proving long time bounds for solutions to certain Fokker-Planck equations with subelliptic diffusion term. We consider the case where the jump operators in the Lindbladian are linear functions of $x$, and place an assumption which implies that the Hörmander condition holds for the resulting Fokker-Planck equation. By constructing a suitable parametrix for this equation we show that semiclassical derivative estimates established in the above works for elliptic diffusion also hold for subelliptic diffusion, with global bounds in $L^p$ for all $1\le p\le \infty$.

</details>


### [16] [Hardy decomposition of first order Lipschitz functions by Lamé-Navier solutions](https://arxiv.org/abs/2601.04528)
*Daniel Alfonso Santiesteban,Ricardo Abreu Blaya,Daniel Alpay*

Main category: math.AP

TL;DR: The paper studies whether higher-order Lipschitz functions on a domain boundary can be decomposed into boundary values of Lamé-Navier system solutions with jump discontinuities, using Clifford algebra and Hardy projections.


<details>
  <summary>Details</summary>
Motivation: To investigate decomposition properties of boundary functions for solutions to the Lamé-Navier system (elasticity equations) using Clifford algebra framework.

Method: Uses Clifford algebra to rewrite Lamé-Navier system via Euclidean Dirac operator, employs Hardy projections related to singular integral operators from Clifford analysis, and utilizes an involution operator on first-order Lipschitz classes.

Result: The main result addresses whether higher-order Lipschitz boundary functions can be decomposed into sums of boundary values of Lamé-Navier solutions with jumps across the boundary.

Conclusion: Clifford algebra and Hardy projection techniques provide tools to study boundary value decompositions for Lamé-Navier system solutions, with implications for elasticity theory and Clifford analysis.

Abstract: The Clifford algebra language allows us to rewrite the Lamé-Navier system in terms of the Euclidean Dirac operator. In this paper, the main question we shall be concerned with is whether or not a higher order Lipschitz function on the boundary $Γ$ of a Jordan domain $Ω\subset\mathbb{R}^m$ can be decomposed into a sum of the two boundary values of a solution of the Lamé-Navier system with jump across $Γ$. Our main tool are the Hardy projections related to a singular integral operator arising in the context of Clifford analysis, which turns out to be an involution operator on the first order Lipschitz classes.

</details>


### [17] [On behavior of free boundaries to generalized two-phase Stefan problems for parabolic partial differential equation systems](https://arxiv.org/abs/2601.04617)
*Toyohiko Aiki,Hana Kakiuchi*

Main category: math.AP

TL;DR: The paper establishes existence and uniqueness of solutions for a free boundary problem modeling bread baking, overcoming difficulties with evaporation front growth and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve a free boundary problem representing bread baking in a hot oven, which involves complex interactions between evaporation front position, temperature field, and water content with coupled boundary conditions.

Method: By improving the regularity of solutions, the authors overcome difficulties where the evaporation front growth rate depends on water content and the water content boundary condition depends on temperature.

Result: Established existence of solutions locally in time and proved uniqueness. Also derived results on maximal interval of existence under certain sign conditions for initial data.

Conclusion: Successfully solved the bread baking free boundary problem by enhancing solution regularity, providing both local existence/uniqueness and maximal existence interval results under appropriate conditions.

Abstract: Recently, we have proposed a new free boundary problem representing the bread baking process in a hot oven. Unknown functions in this problem are the position of the evaporation front, the temperature field and the water content. For solving this problem we observed two difficulties that the growth rate of the free boundary depends on the water content and the boundary condition for the water content contains the temperature. In this paper, by improving the regularity of solutions, we overcome these difficulties and establish existence of a solution locally in time and its uniqueness. Moreover, under some sign conditions for initial data, we derive a result on the maximal interval of existence to solutions.

</details>


### [18] [Liouville-type theorems for the stationary non-Newtonian fluids in a slab](https://arxiv.org/abs/2601.04622)
*Jingwen Han,Han Li*

Main category: math.AP

TL;DR: Liouville-type theorems for stationary shear thickening fluid equations in a slab: axisymmetric solutions must be trivial if local L∞-norm grows mildly with radius; bounded general solutions trivial if ruʳ is bounded.


<details>
  <summary>Details</summary>
Motivation: To establish Liouville-type theorems (non-existence of nontrivial solutions) for stationary shear thickening fluid equations in slab domains, extending similar results from Navier-Stokes equations to more complex non-Newtonian fluids.

Method: Inspired by Bang, Gui, Wang, and Xie's work on Navier-Stokes; establishes Saint-Venant type estimate characterizing growth of local Dirichlet integral; develops new estimate for constant in Korn's inequality over different domains.

Result: Two main results: (1) Axisymmetric solutions must be trivial if local L∞-norm grows mildly as radius R grows; (2) Bounded general solutions must be trivial if ruʳ is bounded.

Conclusion: Successfully extends Liouville-type theorems from Navier-Stokes to shear thickening fluids using Saint-Venant estimates and improved Korn inequality analysis, providing conditions under which stationary solutions must be trivial.

Abstract: In this paper, we investigate Liouville-type theorems for stationary solutions to the shear thickening fluid equations in a slab. We show that the axisymmetric solution must be trivial if its local $L^\infty$-norm grows mildly as the radius $R$ grows. Also, a bounded general solution $u$ must be trivial if $ru^r$ is bounded. The proof is inspired by the work of Bang, Gui, Wang, and Xie [J. Fluid Mech. 1005 (2025)] for the Navier-Stokes equations, and the key point is to establish a Saint-Venant type estimate that characterizes the growth of the local Dirichlet integral of nontrivial solutions. One new ingredient is the estimate of the constant in Korn's inequality over different domains.

</details>


### [19] [The Kato square root estimate with Robin boundary conditions](https://arxiv.org/abs/2601.04678)
*Sebastian Bechtel,Andreas Rosén*

Main category: math.AP

TL;DR: The paper proves the Kato square root estimate for second-order elliptic operators with Robin boundary conditions on bounded, locally uniform domains, handling accretive coefficients and potentially unbounded boundary conductivity.


<details>
  <summary>Details</summary>
Motivation: Previous Kato square root estimates have been established for various boundary conditions, but Robin boundary conditions present unique challenges that cannot be addressed using first-order approaches that worked for other boundary conditions.

Method: The authors develop a new approach for proving the Kato square root estimate specifically tailored to handle Robin boundary conditions with accretive coefficients and potentially unbounded boundary conductivity, moving away from traditional first-order methods.

Result: Successfully proves the Kato square root estimate for second-order divergence form elliptic operators with Robin boundary conditions on bounded, locally uniform domains, even with accretive coefficients and unbounded boundary conductivity.

Conclusion: This work extends the theory of Kato square root estimates to the challenging case of Robin boundary conditions, requiring novel techniques beyond standard first-order approaches and opening new directions for analysis of boundary value problems.

Abstract: We prove the Kato square root estimate for second-order divergence form elliptic operators $-div(A\nabla)$ on a bounded, locally uniform domain $D \subseteq \mathbb{R}^n$, for accretive coefficients $A \in L^\infty(D; \mathbb{C}^n)$, under the Robin boundary condition $ν\cdot A\nabla u + bu = 0$ for a (possibly unbounded) boundary conductivity $b$. In contrast to essentially all previous estimates of Kato square root operators, no first-order approach seems possible for the Robin boundary conditions.

</details>


### [20] [Hyperbolic regularization effects for degenerate elliptic equations](https://arxiv.org/abs/2601.04753)
*Xavier Lamy,Riccardo Tione*

Main category: math.AP

TL;DR: The paper establishes regularity results for Lipschitz solutions to degenerate elliptic PDEs in 2D, showing singular sets are negligible and obtaining partial C¹ regularity when degeneracy occurs only on curves.


<details>
  <summary>Details</summary>
Motivation: Previous regularity results for degenerate elliptic equations typically assumed degeneracy sets were zero-dimensional (isolated points). The paper aims to extend these results to more general cases where degeneracy occurs on curves, which is more challenging and realistic.

Method: Uses differential inclusions viewpoint, establishes pointwise gradient localization theorem, exploits hyperbolic structure of the equation along degeneracy curves, and applies tools from Hamilton-Jacobi equations and scalar conservation laws to compensate for regularity loss.

Result: Proves that the singular set of nondifferentiability points is H¹-negligible, derives new sharp partial C¹ regularity results when G is degenerate only on curves, and recovers/extends all previously known results for zero-dimensional degeneracy sets.

Conclusion: The paper successfully extends regularity theory for degenerate elliptic equations from zero-dimensional degeneracy sets to one-dimensional curves, providing a more comprehensive understanding of Lipschitz solutions to such equations in two dimensions.

Abstract: This paper investigates the regularity of Lipschitz solutions $u$ to the general two-dimensional equation $\text{div}(G(Du))=0$ with highly degenerate ellipticity. Just assuming strict monotonicity of the field $G$ and heavily relying on the differential inclusions point of view, we establish a pointwise gradient localization theorem and we show that the singular set of nondifferentiability points of $u$ is $\mathcal{H}^1$-negligible. As a consequence, we derive new sharp partial $C^1$ regularity results under the assumption that $G$ is degenerate only on curves. This is done by exploiting the hyperbolic structure of the equation along these curves, where the loss of regularity is compensated using tools from the theories of Hamilton-Jacobi equations and scalar conservation laws. Our analysis recovers and extends all the previously known results, where the degeneracy set was required to be zero-dimensional.

</details>


### [21] [Hypocoercivity and metastability of degenerate KFP equations at low temperature](https://arxiv.org/abs/2601.04784)
*Loïs Delande*

Main category: math.AP

TL;DR: The paper proves semiclassical hypocoercivity estimates for degenerate Kramers-Fokker-Planck operators and derives Eyring-Kramers formulas for their spectrum, quantifying spectral gaps using WKB-based Gaussian quasimodes.


<details>
  <summary>Details</summary>
Motivation: To analyze Kramers-Fokker-Planck operators with general degenerate coefficients, which are important in statistical physics but challenging due to degeneracy, and to understand their spectral properties in the semiclassical regime.

Method: Proves semiclassical hypocoercivity estimates for a broad class of degenerate operators, then adapts the WKB method to construct sharp Gaussian quasimodes, enabling derivation of Eyring-Kramers formulas for the bottom eigenvalues.

Result: Establishes semiclassical hypocoercivity estimates for degenerate Kramers-Fokker-Planck operators, proves Eyring-Kramers formulas for their bottom eigenvalues, and quantifies the spectral gap separating these eigenvalues from the rest of the spectrum.

Conclusion: The WKB-based construction of Gaussian quasimodes provides a powerful tool for analyzing spectral properties of degenerate Kramers-Fokker-Planck operators in the semiclassical regime, yielding precise Eyring-Kramers formulas and spectral gap quantification.

Abstract: We consider Kramers-Fokker-Planck operators with general degenerate coefficients. We prove semiclassical hypocoercivity estimates for a large class of such operators. Then, we manage to prove Eyring-Kramers formulas for the bottom of the spectrum of some particular degenerate operators in the semiclassical regime, and quantify the spectral gap separating these eigenvalues from the rest of the spectrum. The main ingredient is the construction of sharp Gaussian quasimodes through an adaptation of the WKB method.

</details>


### [22] [The Semigeostrophic-Euler Limit: Lifespan Lower Bounds and $O(\varepsilon)$ Velocity Stability](https://arxiv.org/abs/2601.04797)
*Victor Armegioiu*

Main category: math.AP

TL;DR: The paper studies the 2D semigeostrophic system in small-amplitude scaling, showing it converges to incompressible Euler with improved lifespan (log-log gain) and strong velocity stability.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence of the semigeostrophic system to incompressible Euler equations in the dual formulation, particularly quantifying the persistence time and stability of this approximation.

Method: Uses a natural bootstrap regime for Poisson/Monge-Ampère coupling, combining incompressible transport structure with sharp elliptic control of velocity gradient and flow-based stability arguments.

Result: Two main results: 1) lifespan lower bound with log-log gain (persistence at scale ε⁻¹|log log ε|), 2) strong velocity-stability estimate with O(ε) rate in L² on any bootstrap window.

Conclusion: The results provide a clean quantitative bridge from semigeostrophic system to Euler that is both longer-lived (by log-log factor) and quantitatively stable in velocity, complementing previous convergence rates.

Abstract: We study the two-dimensional semigeostrophic (SG$^{\varepsilon}$) system on the torus in the small-amplitude scaling and its convergence to incompressible Euler in the dual (geostrophic) formulation. Within a natural bootstrap regime for the Poisson/Monge-Ampère coupling, we obtain two main results. First, we prove a lifespan lower bound in slow time with a \emph{log-log} gain; in physical time this yields persistence at least on the scale $\varepsilon^{-1}|\log\log \varepsilon|$. Second, on any bootstrap window we establish a strong velocity-stability estimate with rate $O(\varepsilon)$ in $L^2$, complementing Loeper's $O(1/\varepsilon)$ existence time and $\varepsilon^{2/3}$ weak convergence rate. The proofs combine the incompressible transport structure with a sharp elliptic control of the velocity gradient and a flow-based stability argument. Overall, the results give a clean quantitative bridge from SG$^{\varepsilon}$ to Euler that is both longer-lived (by a log-log factor) and quantitatively stable in velocity.

</details>


### [23] [Semiclassical analysis of the magnetic Laplacian on hyperbolic surfaces](https://arxiv.org/abs/2601.04804)
*Thibault Lefeuvre*

Main category: math.AP

TL;DR: Overview of magnetic Laplacian results on hyperbolic surfaces from arXiv:2505.08584 and ongoing work


<details>
  <summary>Details</summary>
Motivation: To provide a concise overview of quantum phenomena emerging from the magnetic Laplacian on hyperbolic surfaces, presenting results from existing research and ongoing collaborations

Method: Analytic framework using magnetic Laplacian on hyperbolic surfaces, building on previous work [arXiv:2505.08584] and collaborations with L. Charles and A. Chabert

Result: Main results from the referenced paper and work in preparation, though specific findings are not detailed in this abstract

Conclusion: The magnetic Laplacian provides a rich framework for studying quantum phenomena on hyperbolic surfaces, with ongoing research expanding these results

Abstract: The magnetic Laplacian on hyperbolic surfaces provides a rich analytic framework in which a variety of quantum phenomena emerge. The present note, written for the \emph{Proceedings of the Journées EDP 2025}, is a concise overview of the main results obtained in [arXiv:2505.08584] and work in preparation by the author with L. Charles and A. Chabert.

</details>


### [24] [Boundedness in a two-dimensional doubly degenerate nutrient taxis system with logistic source](https://arxiv.org/abs/2601.04845)
*Zhiguang Zhang,Yuxiang Li*

Main category: math.AP

TL;DR: The paper proves global existence and uniform boundedness of weak solutions for a doubly degenerate nutrient taxis system in 2D with arbitrary smooth initial data, using a weighted energy method.


<details>
  <summary>Details</summary>
Motivation: The system models Bacillus subtilis colony aggregation patterns on thin agar plates. Previous results required additional assumptions like small initial data or convex domains in 2D, so the authors aim to establish global boundedness for arbitrary smooth initial data.

Method: Uses a weighted energy method that leverages the quadratic degradation term in the logistic growth, which enhances the dissipative structure of the system.

Result: Proves that the problem admits a global weak solution that remains uniformly bounded in time for arbitrary smooth initial data in bounded smooth domains in ℝ².

Conclusion: The quadratic degradation term provides sufficient dissipation to establish global boundedness without restrictive assumptions, advancing the understanding of this biological pattern formation model.

Abstract: We are concerned with the following doubly degenerate nutrient taxis system \begin{align} \begin{cases}\tag{$\star$}\label{eq-0.1} u_t=\nabla\cdot(u v\nabla u)-\nabla\cdot(u^{2} v\nabla v)+u-u^2,\\[1mm] v_t=Δv-u v, \end{cases} \end{align} posed in a bounded smooth domain $Ω\subset\mathbb{R}^2$ under homogeneous Neumann boundary conditions. This model was introduced to describe the aggregation patterns of colonies of \emph{Bacillus subtilis} observed on thin agar plates. Previous results have established global boundedness in one space dimension and, in two dimensions, under additional assumptions such as small initial data or convex domains (see, e.g., M. Winkler, \textit{Trans. Amer. Math. Soc.}, 2021; M. Winkler, \textit{J. Differ. Equ.}, 2024). In the presence of the quadratic degradation term in the logistic growth, which markedly enhances the dissipative structure of the system, and by employing a weighted energy method, we prove that for arbitrary smooth initial data the problem \eqref{eq-0.1} admits a global weak solution that remains uniformly bounded in time.

</details>


### [25] [Improved convergence rates in the fast-reaction approximation of the triangular Shigesada-Kawasaki-Teramoto system](https://arxiv.org/abs/2601.04894)
*Hector Bouton*

Main category: math.AP

TL;DR: The paper analyzes convergence rates for the fast-reaction approximation to the triangular Shigesada-Kawasaki-Teramoto model on bounded domains in dimensions d≤3.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide rigorous quantitative convergence rates for the fast-reaction approximation of the triangular SKT model, which is important for understanding pattern formation in reaction-diffusion systems.

Method: The authors use analytical methods to establish explicit convergence rates in various function spaces (L∞L²∩L²H¹ on the whole domain, and L∞Hˡ in the interior for all l>0).

Result: The main results are explicit convergence rates: on the whole domain in L∞L²∩L²H¹ spaces, and in the interior with convergence in any L∞Hˡ space for all l>0.

Conclusion: The fast-reaction approximation to the triangular SKT model converges with explicit rates, providing quantitative justification for this approximation in pattern formation studies.

Abstract: We consider the fast-reaction approximation to the triangular Shigesada-Kawasaki-Teramoto model on a bounded domain in the physical dimension $d\le 3$. We provide explicit convergence rates on the whole domain in $\textnormal{L}^\infty\textnormal{L}^2\cap\textnormal{L}^2\textnormal{H}^1$ and in the interior we prove convergence with an explicit rate in any $\textnormal{L}^\infty\textnormal{H}^l$ for all $l > 0$.

</details>


### [26] [On Navier-Stokes equations arising from the rotation of an obstacle in a fluid](https://arxiv.org/abs/2601.04944)
*Tahar Zamène Boulmezaoud,Nabil Kerdid,Amel Kourta*

Main category: math.AP

TL;DR: Existence and regularity of solutions to modified Navier-Stokes equations with rotating rigid body in 3D using weighted Sobolev spaces


<details>
  <summary>Details</summary>
Motivation: To study fluid motion with rotating rigid bodies in unbounded domains (R3), addressing challenges of solution behavior at large distances where standard Sobolev spaces may not be adequate

Method: Use weighted Sobolev spaces to handle asymptotic behavior at infinity; analyze modified Navier-Stokes equations describing fluid-rigid body interaction; prove existence and regularity under suitable assumptions

Result: Proved existence and regularity of solutions satisfying appropriate conditions at infinity for the modified Navier-Stokes equations with rotating rigid body

Conclusion: Weighted Sobolev spaces provide effective framework for analyzing fluid-structure interaction problems in unbounded domains, enabling rigorous treatment of solutions' asymptotic behavior at large distances

Abstract: We consider the modified Navier-Stokes equations in R3 describing the motion of a fluid in the presence of a rotating rigid body. Weighted Sobolev spaces are used to describe the behavior of solutions at large distances. Under suitable assumptions, w e prove the existence and regularity of solutions satisfying appropriate conditions at infinity.

</details>


### [27] [Critical blow-up lines in a two-species quasilinear chemotaxis system with two chemicals](https://arxiv.org/abs/2601.04994)
*Ziyue Zeng,Yuxiang Li*

Main category: math.AP

TL;DR: This paper analyzes a quasilinear two-species chemotaxis system with two chemicals, establishing critical parameter regimes for finite-time blow-up, global boundedness, and global existence of solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the dynamics of a two-species chemotaxis system with two chemicals, where each species produces a chemical that attracts the other species. The system involves nonlinear diffusion and sensitivity functions, and the goal is to determine parameter regimes that lead to different solution behaviors (blow-up vs. global existence).

Method: The authors study a quasilinear chemotaxis system with two species (u and w) and two chemicals (v and z). They analyze the asymptotic behavior of diffusion function D(s) and sensitivity function S(s) with power-law asymptotics. Using mathematical analysis techniques for partial differential equations, they establish conditions on parameters p and q that determine solution behavior.

Result: Three main results: 1) In a ball domain, when q-p > 2-n/2 and q > 1-n/2, finite-time blow-up occurs for some radially symmetric initial data. 2) For any smooth bounded domain, when q-p < 2-n/2, all solutions are globally bounded. 3) For any smooth bounded domain, when q < 1-n/2, all solutions exist globally. The system has two critical lines: q-p = 2-n/2 and q = 1-n/2.

Conclusion: The study identifies precise parameter regimes that determine the dynamics of the two-species chemotaxis system. The critical lines q-p = 2-n/2 and q = 1-n/2 separate three distinct behaviors: finite-time blow-up, global boundedness, and global existence. These results provide a complete classification of solution behaviors based on the asymptotic properties of the diffusion and sensitivity functions.

Abstract: In this study, we explore the quasilinear two-species chemotaxis system with two chemicals \begin{align}\tag{$\star$} \begin{cases} u_t = \nabla \cdot(D(u)\nabla u) - \nabla \cdot \left(S(u) \nabla v\right), & x \in Ω, \ t > 0, \\ 0 = Δv - μ_w + w, \quad μ_w=\fint_Ωw, & x \in Ω, \ t > 0, \\ w_t = Δw - \nabla \cdot \left(w \nabla z\right), & x \in Ω, \ t > 0, \\ 0 = Δz - μ_u + u, \quad μ_u=\fint_Ωu, & x \in Ω, \ t > 0, \\ \frac{\partial u}{\partial ν} = \frac{\partial v}{\partial ν} = \frac{\partial w}{\partial ν} = \frac{\partial z}{\partial ν} = 0, & x \in \partial Ω, \ t > 0, \\ u(x, 0) = u_0(x), \quad w(x, 0) = w_0(x), & x \in Ω, \end{cases} \end{align} where $Ω\subset \mathbb{R}^n$ ($n \geq3$) is a smooth bounded domain. The functions $D(s)$ and $S(s)$ exhibit asymptotic behavior of the form \begin{align*} D(s) \simeq k_D s^p \ \text {and} \ S(s) \simeq k_S s^q, \quad s \gg 1 \end{align*} with $p,q \in \mathbb{R}$. We prove that \begin{itemize}
  \item when $Ω$ is a ball, if $q-p>2-\frac{n}{2}$ and $q>1-\frac{n}{2}$, there exist radially symmetric initial data $u_0$ and $w_0$, such that the corresponding solutions blow up in finite time;
  \item for any general smooth bounded domain $Ω\subset \mathbb{R}^n$, if $q-p<2-\frac{n}{2}$, all solutions are globally bounded;
  \item for any general smooth bounded domain $Ω\subset \mathbb{R}^n$, if $q<1-\frac{n}{2}$, all solutions are global. \end{itemize} We point out that our results implies that the system ($\star$) possess two critical lines $ q-p=2-\frac{n}{2}$ and $q=1-\frac{n}{2}$ to classify three dynamics among global boundedness, finite-time blow-up, and global existence of solutions to system ($\star$).

</details>


### [28] [Critical blow-up curve in a two-species chemotaxis system with two chemicals involving flux-limitation](https://arxiv.org/abs/2601.05008)
*Ziyue Zeng,Yuxiang Li*

Main category: math.AP

TL;DR: This paper studies a two-species chemotaxis system with flux-limitation and identifies a critical blow-up curve at p=q=(n-2)/(n-1) for n≥3. Below this curve, finite-time blow-up occurs; above it, solutions exist globally and remain bounded.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the blow-up behavior and global existence conditions for a two-species chemotaxis system with flux-limitation, which generalizes classical Keller-Segel models. The flux-limitation terms (1+|∇v|²)^{-p/2} and (1+|∇z|²)^{-q/2} introduce nonlinear damping effects that affect aggregation and blow-up phenomena.

Method: The authors analyze the PDE system using mathematical analysis techniques including energy methods, comparison principles, and construction of suitable test functions. For blow-up results, they likely use contradiction arguments and construct initial data leading to finite-time blow-up. For global existence, they employ a priori estimates and regularity arguments.

Result: The main result identifies a critical curve p=q=(n-2)/(n-1) in the parameter space (0,(n-2)/(n-1)]×(0,(n-2)/(n-1)] for n≥3. Below this curve (0<p,q<(n-2)/(n-1)), finite-time blow-up occurs for radially symmetric initial data in balls. Above this curve (p>(n-2)/(n-1) or q>(n-2)/(n-1)), solutions exist globally and remain bounded for any smooth bounded domain.

Conclusion: The flux-limitation parameters p and q critically determine the blow-up behavior of the two-species chemotaxis system. The threshold (n-2)/(n-1) separates regimes of finite-time blow-up and global bounded existence, providing a complete characterization of the system's qualitative behavior in different parameter regimes.

Abstract: We investigate the following two-species chemotaxis system with two chemicals involving flux-limitation \begin{align}\tag{$\star$} \begin{cases} u_t = Δu - \nabla \cdot \left(u(1+|\nabla v|^2)^{-\frac{p}{2}}\nabla v\right), & x \in Ω, \ t > 0, \\ 0 = Δv - μ_w + w, \quad μ_{w}=f_Ω w, & x \in Ω, \ t > 0, \\ w_t = Δw - \nabla \cdot \left(w (1+|\nabla z|^2)^{-\frac{q}{2}} \nabla z\right), & x \in Ω, \ t > 0, \\ 0 = Δz - μ_u + u, \quad μ_{u}=f_Ω u, & x \in Ω, \ t > 0, \\ \frac{\partial u}{\partial ν} = \frac{\partial v}{\partial ν} = \frac{\partial w}{\partial ν} = \frac{\partial z}{\partial ν} = 0, & x \in \partial Ω, \ t > 0, \\ u(x, 0) = u_0(x), \quad w(x, 0) = w_0(x), & x \in Ω, \end{cases} \end{align} where $p,q \in \mathbb{R}$ and $Ω\subset \mathbb{R}^n$ is a smooth bounded domain. In this paper, we identify a critical blow-up curve ( i.e $p=\frac{n-2}{n-1}$ and $q=\frac{n-2}{n-1}$ in the square $(0,\frac{n-2}{n-1}] \times (0,\frac{n-2}{n-1}]$) for system ($\star$) with $n\geq 3$ and $p,q>0$. Specifically, \begin{itemize}
  \item when $Ω=B_R(0) \subset \mathbb{R}^n$ with $n\geq 3$, if $0<p<\frac{n-2}{n-1}$ and $0<q<\frac{n-2}{n-1}$, there exist radially symmetric initial data such that the corresponding solution blows up in finite time;
  \item for any general smooth bounded domain, if either $n=1$ ( with $p,q \in \mathbb{R}$ arbitrary) or $n\geq 2$ with $p>\frac{n-2}{n-1}$ or $q>\frac{n-2}{n-1}$, then solutions exist globally and remain bounded. \end{itemize}

</details>


### [29] [Nodal set comparison for Allen--Cahn solutions with conical asymptotics](https://arxiv.org/abs/2601.05015)
*Sanghoon Lee,Taehun Lee*

Main category: math.AP

TL;DR: The paper establishes a comparison principle for entire solutions of the Allen-Cahn equation with nodal sets asymptotic to minimizing hypercones, showing that inclusion of positive phases enforces global ordering and determines solutions uniquely.


<details>
  <summary>Details</summary>
Motivation: To understand the global structure and ordering properties of entire solutions to the Allen-Cahn equation, particularly when nodal sets are asymptotic to regular minimizing hypercones, and to develop analogues of maximum principles from minimal surface theory for phase field equations.

Method: Develops a maximum principle for the linearized Allen-Cahn operator on unbounded domains that are not necessarily smooth, establishing a comparison principle that relates inclusion of positive phases to global ordering of solutions and disjointness of nodal sets.

Result: Proves that inclusion of positive phases enforces global ordering of solutions, the positive phase uniquely determines the solution, and strict phase inclusion implies corresponding nodal sets are disjoint - establishing an Allen-Cahn analogue of the strong maximum principle for minimal hypersurfaces.

Conclusion: The comparison principle provides fundamental ordering properties for entire Allen-Cahn solutions with conical asymptotics, establishing strong connections between phase inclusion, global solution structure, and nodal set geometry, analogous to classical results in minimal surface theory.

Abstract: We establish a comparison principle for entire solutions of the Allen--Cahn equation whose nodal sets, possibly singular, are asymptotic to a regular minimizing hypercone. We show that inclusion of the positive phases enforces a global ordering of the solutions. As a consequence, the positive phase uniquely determines the solution, and strict phase inclusion implies that the corresponding nodal sets are disjoint. Our analysis relies on a maximum principle for the linearized operator on unbounded domains that are not necessarily smooth, and yields an Allen--Cahn analogue of the strong maximum principle for minimal hypersurfaces.

</details>


### [30] [Finite-time blow-up in a quasilinear two-species chemotaxis system with two chemicals](https://arxiv.org/abs/2601.05023)
*Mingzhang Cai,Yuxiang Li,Ziyue Zeng*

Main category: math.AP

TL;DR: The paper proves finite-time blow-up solutions exist for a quasilinear two-species chemotaxis system with two chemicals when certain diffusion exponent conditions are met.


<details>
  <summary>Details</summary>
Motivation: To complement existing boundedness results for classical two-species chemotaxis systems by investigating blow-up phenomena in a modified system with different chemical equations and nonlinear diffusion.

Method: Analyzes a quasilinear chemotaxis system with two species (u,w) and two chemicals (v,z), where chemical equations are modified to 0 = Δv - μ₂ + w and 0 = Δz - μ₁ + u (with spatial averages μ₁, μ₂). Uses nonlinear diffusion functions D₁(s) ≃ s^{m₁-1} and D₂(s) ≃ s^{m₂-1} for m₁,m₂ > 1.

Result: Proves that when m₁ + m₂ > max{m₁m₂ + 2m₁/n, m₁m₂ + 2m₂/n} with n ≥ 3, the system admits solutions that blow up in finite time, complementing Zhong's boundedness result for the classical system.

Conclusion: The paper establishes a blow-up threshold condition for the modified quasilinear two-species chemotaxis system, showing that strong cross-interactions combined with weak diffusion can lead to finite-time blow-up in higher dimensions.

Abstract: This paper investigates the finite-time blow-up phenomena to a quasilinear two-species chemotaxis system with two chemicals \begin{align}\tag{$\star$}
  \begin{cases}
  u_t = \nabla \cdot \left(D_1(u) \nabla u\right) - \nabla \cdot \left(u \nabla v\right), & x \in Ω, \ t > 0,
  0 = Δv - μ_2 + w, \quad μ_2=\fint_Ωw, & x \in Ω, \ t > 0,
  w_t = \nabla \cdot \left(D_2(w) \nabla w\right) - \nabla \cdot \left(w \nabla z\right), & x \in Ω, \ t > 0,
  0 = Δz - μ_1 + u, \quad μ_1=\fint_Ωu, & x \in Ω, \ t > 0,
  \frac{\partial u}{\partial ν} = \frac{\partial v}{\partial ν} = \frac{\partial w}{\partial ν} = \frac{\partial z}{\partial ν} = 0, & x \in \partial Ω, \ t > 0,
  u(x, 0) = u_0(x), \quad w(x, 0) = w_0(x), & x \in Ω,
  \end{cases} \end{align} where $Ω\subset \mathbb{R}^n$ $(n \geqslant 3)$ is a smoothly bounded domain. The nonlinear diffusion functions \( D_1(s) \) and \( D_2(s) \) are of the following forms: \begin{align*}
  D_1(s)\simeq s^{m_1-1} \quad \text{and}\quad D_2(s) \simeq s^{m_2-1}, \quad m_1,m_2> 1 \end{align*} for $s\geqslant 1$.
  For the classical two-species chemotaxis system with two chemicals (i.e. the second and fourth equations are replaced by $0 = Δv - v + w$ and $0 = Δz - z + u$ ), Zhong [J. Math. Anal. Appl., 500 (2021), Paper No. 125130, pp. 22.] showed that the system possesses a globally bounded classical solution in the case that \[ m_1 + m_2 < \max\left\{m_1m_2 + \frac{2m_1}{ n},\ m_1m_2 + \frac{2m_2 }{ n}\right\}. \]
  Complementing the boundedness result, we prove that the system ($\star$) admits solutions that blow up in finite time, if \[ m_1 + m_2 > \max\left\{ m_1m_2 + \frac{2m_1}{ n},\ m_1m_2 + \frac{2m_2}{ n}\right\} \] with $n\geqslant 3$.

</details>


### [31] [On the effects of protection zone and directed population flux in prey-predator dynamics](https://arxiv.org/abs/2601.05054)
*Kousuke Kuto,Kazuhiro Oeda*

Main category: math.AP

TL;DR: Spatial predator-prey model with prey refuge and predator directed movement shows complex coexistence patterns through bifurcation analysis.


<details>
  <summary>Details</summary>
Motivation: To understand how spatial protection zones (refuges) for prey interact with predator movement behavior in ecological systems, particularly when predators exhibit far-sighted directed movement rather than near-sighted chemotaxis.

Method: Developed a spatial predator-prey model with prey refuge inaccessible to predators, using far-sighted population flux for predator movement. Analyzed both nonstationary (time-dependent) and stationary problems, establishing local well-posedness despite interface discontinuity and conducting bifurcation analysis of coexistence states.

Result: Established local-in-time well-posedness under Neumann boundary conditions despite refuge interface discontinuity. Identified bifurcation threshold for positive steady states from predator-only equilibria and described global continuation of resulting branches. Found that strong directed movement can induce turning-point structures and multiplicity of coexistence steady states.

Conclusion: The interplay between spatial protection (prey refuge) and predator movement behavior creates complex coexistence patterns, with far-sighted directed movement leading to nontrivial steady-state structures including multiplicity and turning points in parameter space.

Abstract: We study a spatial predator-prey model in which prey can enter a protection zone (refuge) inaccessible to predators, while predators exhibit directed movement toward prey-rich regions. The directed movement is modeled by a far-sighted population flux motivated by classical movement rules, in contrast to the more commonly analyzed near-sighted chemotaxis-type mechanisms. We first establish local-in-time well-posedness for the corresponding nonstationary problem under Neumann boundary conditions, despite the discontinuity induced by the refuge interface. We then investigate the stationary problem, focusing on how the coexistence states emerge and organize globally in parameter space. In particular, we identify the bifurcation threshold for positive steady states from semitrivial predator-only equilibria, and describe the global continuation of the resulting branches. Our analysis reveals that strong directed movement can induce turning-point structures and multiplicity of coexistence steady states, highlighting a nontrivial interplay between spatial protection and predator movement behavior.

</details>


### [32] [Non-linear parabolic PDEs with rough data and coefficients: existence, uniqueness and regularity of weak solutions in critical spaces](https://arxiv.org/abs/2601.05080)
*Pascal Auscher,Sebastian Bechtel*

Main category: math.AP

TL;DR: The paper establishes well-posedness of weak solutions for nonlinear parabolic PDEs with rough coefficients and initial data in critical Besov spaces, using weighted Z-spaces without smallness conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a robust theory for nonlinear parabolic PDEs with rough coefficients and rough initial data, overcoming limitations of existing methods that require smallness conditions or smoother data.

Method: Uses weighted Z-spaces and develops novel tools: hypercontractive singular integral operators on weighted Z-spaces and self-improving property for super-linear reverse Hölder inequalities.

Result: Establishes existence and uniqueness of maximal weak solutions for nonlinear parabolic PDEs with rough coefficients and initial data in critical homogeneous Besov spaces, demonstrated with rough reaction-diffusion equations.

Conclusion: The framework provides a powerful approach for handling rough parabolic problems without smallness conditions, with applications to various equation classes including Burgers-type and quasi-linear problems in subsequent work.

Abstract: This article investigates the well-posedness of weak solutions to non-linear parabolic PDEs driven by rough coefficients with rough initial data in critical homogeneous Besov spaces. Well-posedness is understood in the sense of existence and uniqueness of maximal weak solutions in suitable weighted $Z$-spaces in the absence of smallness conditions. We showcase our theory with an application to rough reaction--diffusion equations. Subsequent articles will treat further classes of equations, including equations of Burgers-type and quasi-linear problems, using the same approach. Our toolkit includes a novel theory of hypercontractive singular integral operators (SIOs) on weighted $Z$-spaces and a self-improving property for super-linear reverse Hölder inequalities.

</details>


### [33] [Sparsity and uniform regularity for regularised optimal transport](https://arxiv.org/abs/2601.05130)
*Rishabh S. Gvalani,Lukas Koch*

Main category: math.AP

TL;DR: The paper proves interior regularity estimates for regularized optimal transport with subquadratic polynomial or entropic regularization, showing uniform convergence to unregularized solutions.


<details>
  <summary>Details</summary>
Motivation: To establish interior regularity estimates for regularized optimal transport problems that are uniform in the regularization parameter, enabling convergence to unregularized solutions.

Method: The authors derive sharp local bounds on support size for regularized optimal transport with general convex, superlinear regularization terms. They prove interior Lipschitz estimates for transport-like maps and gradient Lipschitz estimates for potentials under bi-C¹,α regularity assumptions.

Result: For subquadratic and entropic regularization: interior Lipschitz estimates for transport-like maps and gradient Lipschitz estimates for potentials. For strictly subquadratic and entropic cases: improved interior C¹ and C² estimates. Estimates are uniform in regularization parameter, leading to convergence to unregularized solutions in appropriate function spaces.

Conclusion: The paper establishes uniform interior regularity estimates for regularized optimal transport, providing convergence guarantees to unregularized solutions and improving known global bias bounds for quadratic regularization.

Abstract: We consider regularised quadratic optimal transport with subquadratic polynomial or entropic regularisation. In both cases, we prove interior Lipschitz-estimates on a transport-like map and interior gradient Lipschitz-estimates on the potentials, under the assumption that the transport map solving the unregularised problem is bi-$C^{1,α}$-regular. For strictly subquadratic and entropic regularisation, the estimates improve to interior $C^1$ and $C^2$ estimates for the transport-like map and the potentials, respectively. Our estimates are uniform in the regularisation parameter. As a consequence of this, we obtain convergence of the transport-like map (resp. the potentials) to the unregularised transport map (resp. Kantorovich potentials) in $C^{0,1-}_{\mathrm{loc}}$ (resp. $C^{1,1-}_{\mathrm{loc}}$).
  Central to our approach are sharp local bounds on the size of the support for regularised optimal transport which we derive for a general convex, superlinear regularisation term. These bounds are of independent interest and imply global bias bounds for the regularised transport plans. Our global bounds, while not necessarily sharp, improve on the best known results in the literature for quadratic regularisation.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [34] [A joint voxel flow - phase field framework for ultra-long microstructure evolution prediction with physical regularization](https://arxiv.org/abs/2601.04898)
*Ao Zhou,Salma Zahran,Chi Chen,Zhengyang Zhang,Yanming Wang*

Main category: physics.comp-ph

TL;DR: A joint framework combining voxel-flow network with phase-field simulations enables long-horizon microstructure evolution prediction with high accuracy and physical consistency.


<details>
  <summary>Details</summary>
Motivation: Current machine learning methods for phase-field microstructure prediction suffer from low flexibility, poor generalization, and short prediction time lengths, despite attempts to reduce computational costs.

Method: Alternating framework coupling voxel-flow network (VFN) with periodic phase-field simulations. VFN predicts future evolution by learning pixel flow from past snapshots while preserving periodic boundaries. Periodic PF simulations suppress artifacts and reduce accumulated errors.

Result: VFN is ~1000x faster than PF simulation on GPU. For 18-frame prediction from 2 input frames: MSE 6.76%, SSIM 0.911. For ultra-long 82-frame prediction: grain number reduced from 600 to 29 with NMSE of average grain area at 1.64%.

Conclusion: The joint framework enables rapid, generalized, flexible, and physically consistent microstructure forecasting from image-based data for ultra-long time scales, outperforming existing methods.

Abstract: Phase-field (PF) modeling is a powerful tool for simulating microstructure evolution. To overcome the high computational cost of PF in solving complex PDEs, machine learning methods such as PINNs, convLSTM have been used to predict PF evolution. However, current methods still face shortages of low flexibility, poor generalization and short predicting time length. In this work, we present a joint framework coupling voxel-flow network (VFN) with PF simulations in an alternating manner for long-horizon temporal prediction of microstructure evolution. The VFN iteratively predicts future evolution by learning the flow of pixels from past snapshots, with periodic boundaries preserved in the process. Periodical PF simulations suppresses nonphysical artifacts, reduces accumulated error, and extends reliable prediction time length. The VFN is about 1,000 times faster than PF simulation on GPU. In validation using grain growth and spinodal decomposition, MSE and SSIM remain 6.76% and 0.911 when predicted 18 frames from only 2 input frames, outperforming similar predicting methods. For an ultra-long grain growth prediction for 82 frames from 2 input frames, grain number decreases from 600 to 29 with NMSE of average grain area remaining 1.64%. This joint framework enables rapid, generalized, flexible and physically consistent microstructure forecasting from image-based data for ultra-long time scales.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [A Comprehensive multi-species comparison of rotational temperature probes in a DC Ar/N$_2$ micro-hollow cathode discharge](https://arxiv.org/abs/2601.04967)
*Dimitrios Stefas,Belkacem Menacer,Alice Remigy,Nikolaos Chazapis,Guillaume Lombardi,Claudia Lazzaroni,Kristaq Gazeli*

Main category: physics.plasm-ph

TL;DR: Cross-comparison of rotational temperatures from multiple molecular species in microplasmas reveals N₂(C) is unreliable as a gas temperature probe due to Ar metastable interference, while OH(A) provides more straightforward thermometry.


<details>
  <summary>Details</summary>
Motivation: Accurate gas temperature determination in microplasmas is critical for applications, but isolated diagnostic approaches can yield misleading results under strong non-equilibrium conditions, necessitating cross-validation of multiple thermometric probes.

Method: High-resolution rotational spectra of N₂(C), OH(A), NH(A) and NO(A) generated in a DC Ar/N₂ microhollow cathode discharge plasma jet were recorded and their rotational temperatures were cross-compared through detailed experimental analysis and robust fitting.

Result: N₂(C) rotational temperature is significantly influenced by energy transfers from argon metastables and spectral interference from NH(A), making it unreliable as a thermometric probe in Ar-rich MHCD. OH(A) rotational population distribution appears less sensitive to Ar-induced perturbations across various discharge conditions, providing more straightforward results.

Conclusion: The study identifies conditions where measured rotational temperatures can be reliably considered in equilibrium with gas temperature, highlighting the importance of cross-validating multiple thermometric probes and investigating excitation kinetics when measuring rotational temperatures in reactive microplasmas.

Abstract: Accurate gas temperature ($T_{\rm Gas}$) determination in microplasmas is critical for optimizing their applications, yet isolated diagnostic approaches may yield misleading results, especially under strong non-equilibrium conditions. Here, high resolution rotational spectra of N$_2$(C), OH(A), NH(A) and NO(A), generated in the plasma jet of a DC Ar/N$_2$ microhollow cathode discharge (MHCD), are recorded and their associated rotational temperatures ($T_{\rm rot}$) are cross compared. A detailed experimental analysis and robust fitting of the rotational spectra are performed, achieving a reliable estimation of $T_{\rm Gas}$. The dominant formation mechanisms of these species and their corresponding impact on rotational population distributions are also interrogated. Particularly, our findings indicate that the $T_{\rm rot}$of N$_2$(C) is significantly influenced by energy transfers from argon metastables (Ar$^m$) and spectral interference from NH(A). This makes it unreliable as a thermometric probe in Ar-rich MHCD, unless complex analyses are employed. In contrast, OH(A) rotational population distribution appears to be less sensitive to Ar-induced perturbations across various discharge currents and pressures, providing more straightforward results. For all molecules considered, this study reveals the conditions under which all the measured $T_{\rm rot}$ can be reliably considered to be in equilibrium with $T_{\rm Gas}$. This highlights the importance of crossvalidating multiple thermometric probes and investigating relevant excitation kinetics when measuring $T_{\rm rot}$ in reactive microplasmas.

</details>


### [36] [Machine learning for radiative hydrodynamics in astrophysics](https://arxiv.org/abs/2601.05155)
*Gonzague Radureau*

Main category: physics.plasm-ph

TL;DR: AI accelerates radiation hydrodynamics: MLP approximates M1-multigroup closure (3000× speedup), PINNs solve equations for extrapolation.


<details>
  <summary>Details</summary>
Motivation: Radiation hydrodynamics simulations are computationally expensive due to M1-multigroup closure requiring numerical evaluation and CFL condition restricting time steps in explicit schemes.

Method: Two AI strategies: 1) Multi-Layer Perceptron approximates M1-multigroup closure relation; 2) Physics-Informed Neural Networks directly solve radiation hydrodynamics equations for time extrapolation.

Result: MLP achieves 3000× speedup with excellent accuracy, enabling high-fidelity radiative shock simulations showing spectral resolution effects. PINNs handle hydrodynamic shocks well but radiative shocks remain challenging.

Conclusion: AI methods significantly accelerate radiation hydrodynamics simulations, with MLP providing dramatic speedup for closure evaluation and PINNs showing promise for equation solving and extrapolation.

Abstract: Radiation hydrodynamics describes the interaction between high-temperature hypersonic plasmas and the radiation they emit or absorb, a coupling that plays a central role in many astrophysical phenomena related to accretion and ejection processes. The HADES code was developed to model such systems by coupling hydrodynamics with M1-gray or M1-multigroup radiative transfer models, which are well suited to optically intermediate media.
  Despite its accuracy, radiation hydrodynamics simulations remain extremely demanding in terms of computational cost. Two main limitations are responsible for this. First, the M1-multigroup model relies on a closure relation with no analytic expression, requiring expensive numerical evaluations. Second, the Courant-Friedrichs-Lewy condition strongly restricts the time step of the explicit schemes used in HADES. To overcome these difficulties, two complementary Artificial Intelligence based strategies were developed in this thesis.
  The first approach consists in training a Multi-Layer Perceptron to approximate the M1-multigroup closure relation. This method achieves excellent accuracy while reducing the computational cost by a factor of 3000, making it the most efficient approach currently available for this task. This performance gain enables high-fidelity simulations of radiative shocks, in which radiation directly influences the shock structure. In particular, increasing spectral resolution slows down the shock and enlarges the radiative precursor.
  The second approach explores the use of Physics-Informed Neural Networks to directly solve the radiation hydrodynamics equations and extrapolate simulations beyond their initial time range. Tests on purely hydrodynamic shocks show accurate handling of discontinuities, but application to radiative shocks remains challenging and requires further investigation.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [37] [Convergence Rates for Learning Pseudo-Differential Operators](https://arxiv.org/abs/2601.04473)
*Jiaheng Chen,Daniel Sanz-Alonso*

Main category: math.ST

TL;DR: This paper develops a wavelet-Galerkin framework for learning elliptic pseudo-differential operators, proposing a sparse estimator with convergence rates and showing the learned operator enables efficient Galerkin solvers.


<details>
  <summary>Details</summary>
Motivation: To bridge operator learning with scientific computing by developing efficient methods for learning fundamental elliptic pseudo-differential operators, which are crucial in PDEs and mathematical physics.

Method: A wavelet-Galerkin framework with structured infinite-dimensional regression using multiscale sparsity, featuring a sparse estimator with novel matrix compression and nested-support strategy to balance approximation and estimation errors.

Result: Established convergence rates for the proposed estimator and demonstrated that the learned operator induces an efficient and stable Galerkin solver whose numerical error matches its statistical accuracy.

Conclusion: The work successfully integrates operator learning, data-driven solvers, and wavelet methods in scientific computing, providing both theoretical guarantees and practical computational efficiency.

Abstract: This paper establishes convergence rates for learning elliptic pseudo-differential operators, a fundamental operator class in partial differential equations and mathematical physics. In a wavelet-Galerkin framework, we formulate learning over this class as a structured infinite-dimensional regression problem with multiscale sparsity. Building on this structure, we propose a sparse, data- and computation-efficient estimator, which leverages a novel matrix compression scheme tailored to the learning task and a nested-support strategy to balance approximation and estimation errors. In addition to obtaining convergence rates for the estimator, we show that the learned operator induces an efficient and stable Galerkin solver whose numerical error matches its statistical accuracy. Our results therefore contribute to bringing together operator learning, data-driven solvers, and wavelet methods in scientific computing.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [38] [Elimination Without Eliminating: Computing Complements of Real Hypersurfaces Using Pseudo-Witness Sets](https://arxiv.org/abs/2601.04383)
*Paul Breiding,John Cobb,Aviva K. Englander,Nayda Farnsworth,Jonathan D. Hauenstein,Oskar Henriksson,David K. Johnson,Jordy Lopez Garcia,Deepak Mundayur*

Main category: math.AG

TL;DR: A new method for computing regions in the real complement of hypersurfaces without requiring explicit equations, using pseudo-witness sets and univariate interpolation.


<details>
  <summary>Details</summary>
Motivation: Many important hypersurfaces in algebraic geometry arise as projections of other varieties, and computing their real complements partitions ambient space into regions. Existing methods require explicit equations, but computing these equations via elimination can be computationally demanding or infeasible.

Method: The approach uses univariate interpolation by computing the intersection of the hypersurface with a line. This is done using pseudo-witness sets without computing a defining equation for the hypersurface - effectively performing elimination without actually eliminating.

Result: The method is implemented in a forthcoming Julia package and demonstrated on several examples where it accurately recovers all regions of the real complement of hypersurfaces.

Conclusion: The proposed approach provides an efficient alternative to traditional elimination methods for computing regions of hypersurface complements, avoiding the computational challenges of explicit equation derivation while maintaining accuracy.

Abstract: Many hypersurfaces in algebraic geometry, such as discriminants, arise as the projection of another variety. The real complement of such a hypersurface partitions its ambient space into open regions. In this paper, we propose a new method for computing these regions. Existing methods for computing regions require the explicit equation of the hypersurface as input. However, computing this equation by elimination can be computationally demanding or even infeasible. Our approach instead derives from univariate interpolation by computing the intersection of the hypersurface with a line. Such an intersection can be done using so-called pseudo-witness sets without computing a defining equation for the hypersurface - we perform elimination without actually eliminating. We implement our approach in a forthcoming Julia package and demonstrate, on several examples, that the resulting algorithm accurately recovers all regions of the real complement of a hypersurface.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [39] [PINN-Based Solution for a Diffusion Controlled Droplet Growth](https://arxiv.org/abs/2601.05042)
*Pavel Gol'din,Gennady Y. Gor*

Main category: physics.flu-dyn

TL;DR: PINN approach solves moving-boundary diffusion problem for spherical droplet growth, accurately capturing self-similar growth law and concentration profiles.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible and computationally efficient framework for solving moving-boundary diffusion problems, specifically diffusion-controlled growth of spherical droplets, which is challenging with traditional methods.

Method: Physics-informed neural network (PINN) formulation that couples the governing diffusion equation with interfacial mass balance, treating droplet radius as an additional trainable function of time.

Result: PINN accurately reproduces the self-similar growth law and concentration profiles for wide range of initial droplet radii, demonstrating convergence toward asymptotic diffusive regime.

Conclusion: The proposed PINN approach provides an effective framework for moving-boundary diffusion problems and can be extended to include additional physical effects.

Abstract: We study diffusion-controlled growth of a spherical droplet with a moving boundary using a physics-informed neural network (PINN) formulation. The governing diffusion equation is coupled to the interfacial mass balance, with the droplet radius treated as an additional trainable function of time. The PINN accurately reproduces the self-similar growth law and concentration profiles for a wide range of initial droplet radii, demonstrating convergence toward the asymptotic diffusive regime. The proposed approach provides a flexible and computationally efficient framework for solving moving-boundary diffusion problems and can be readily extended to include additional physical effects.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [40] [Quantum Elastic Network Models and their Application to Graphene](https://arxiv.org/abs/2601.05161)
*Ioannis Kolotouros,Adithya Sireesh,Stuart Ferguson,Sean Thrasher,Petros Wallden,Julien Michel*

Main category: quant-ph

TL;DR: Quantum algorithm enables efficient simulation of macroscopic-scale materials using quantum elastic network models, showing exponential advantage for simulating coupled oscillators like graphene sheets.


<details>
  <summary>Details</summary>
Motivation: Classical molecular dynamics simulations are infeasible for simulating materials with atomic-level resolution on macroscopic scales due to massive memory and computational requirements, even with simple elastic network models.

Method: Introduces Quantum Elastic Network Models (QENMs) using Babbush et al.'s quantum algorithm that provides exponential advantage for simulating systems of coupled oscillators under specific conditions. Applies this to planar materials like graphene sheets.

Result: Demonstrates efficient simulation of 2D graphene sheets, analyzing complexity for initial-state preparation, Hamiltonian simulation, and measurement. Shows that centimeter-scale atomistic simulation of graphene requiring hundreds of petabytes classically could be encoded with ~160 logical qubits.

Conclusion: Quantum Elastic Network Models offer a promising approach to overcome classical limitations in materials simulation, enabling efficient macroscopic-scale simulations of materials like graphene with practical quantum resources.

Abstract: Molecular dynamics simulations are a central computational methodology in materials design for relating atomic composition to mechanical properties. However, simulating materials with atomic-level resolution on a macroscopic scale is infeasible on current classical hardware, even when using the simplest elastic network models (ENMs) that represent molecular vibrations as a network of coupled oscillators. To address this issue, we introduce Quantum Elastic Network Models (QENMs) and utilize the quantum algorithm of Babbush et al. (PRX, 2023), which offers an exponential advantage when simulating systems of coupled oscillators under some specific conditions and assumptions. Here, we demonstrate how our method enables the efficient simulation of planar materials. As an example, we apply our algorithm to the task of simulating a 2D graphene sheet. We analyze the exact complexity for initial-state preparation, Hamiltonian simulation, and measurement of this material, and provide two real-world applications: heat transfer and the out-of-plane rippling effect. We estimate that an atomistic simulation of a graphene sheet on the centimeter scale, classically requiring hundreds of petabytes of memory and prohibitive runtimes, could be encoded and simulated with as few as $\sim 160$ logical qubits.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [41] [Self-Organized Criticality from Protected Mean-Field Dynamics: Loop Stability and Internal Renormalization in Reflective Neural Systems](https://arxiv.org/abs/2601.04450)
*Byung Gyu Chae*

Main category: nlin.AO

TL;DR: The paper shows how reflective homeostatic dynamics enables self-organized criticality in neural systems through protected mean-field critical surfaces and intrinsic parameter flows.


<details>
  <summary>Details</summary>
Motivation: To understand how neural systems can achieve and maintain criticality without external fine-tuning, and to unify loop renormalization with adaptive response mechanisms.

Method: Uses MSRJD field-theoretic framework starting from reduced stochastic description to analyze fluctuation effects and loop corrections, showing how homeostatic curvature dynamically regularizes these effects.

Result: Fluctuation effects don't destabilize critical manifold; homeostatic curvature protects mean-field critical surface; response-driven structural adaptation generates intrinsic parameter flows that attract system toward critical surface without external tuning.

Conclusion: The framework unifies loop renormalization and adaptive response, establishing a concrete route to autonomous criticality in reentrant neural dynamics through reflective homeostatic mechanisms.

Abstract: The reflective homeostatic dynamics provides a minimal mechanism for self-organized criticality in neural systems. Starting from a reduced stochastic description, we demonstrate within the MSRJD field-theoretic framework that fluctuation effects do not destabilize the critical manifold. Instead, loop corrections are dynamically regularized by homeostatic curvature, yielding a protected mean-field critical surface that remains marginally stable under coarse-graining. Beyond robustness, we show that response-driven structural adaptation generates intrinsic parameter flows that attract the system toward this surface without external fine tuning. Together, these results unify loop renormalization and adaptive response in a single framework and establish a concrete route to autonomous criticality in reentrant neural dynamics.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [42] [Neumann series of Bessel functions for the solutions of the Sturm-Liouville equation in impedance form and related boundary value problems](https://arxiv.org/abs/2601.04513)
*Abigail G. Márquez-Hernández,Víctor A. Vicente-Benítez*

Main category: math.CA

TL;DR: The paper presents a Neumann series representation using spherical Bessel functions for solutions of impedance-form Sturm-Liouville equations, with explicit recursive construction of coefficients and uniform error bounds, enabling accurate eigenvalue computation.


<details>
  <summary>Details</summary>
Motivation: To develop accurate numerical methods for solving spectral problems of Sturm-Liouville equations in impedance form, particularly for computing eigenvalues with non-deteriorating accuracy, which is challenging with traditional approaches.

Method: Develops a Neumann series representation of solutions using spherical Bessel functions, with x-dependent coefficients constructed via recursive integration. Derives uniform truncation error bounds for spectral parameters satisfying |Im ρ| ≤ C, and implements this representation into a numerical method for eigenvalue computation.

Result: Successfully creates explicit recursive construction procedure for series coefficients, establishes uniform error bounds under specified conditions, and develops a numerical method that enables computation of eigenvalues with maintained accuracy (non-deteriorating).

Conclusion: The Neumann series representation with spherical Bessel functions provides an effective framework for solving impedance-form Sturm-Liouville spectral problems, offering explicit coefficient construction and uniform error control, leading to accurate eigenvalue computation methods.

Abstract: We present a Neumann series of spherical Bessel functions representation for solutions of the Sturm--Liouville equation in impedance form \[ (κ(x)u')' + λκ(x)u = 0,\quad 0 < x < L, \] in the case where $κ\in W^{1,2}(0,L)$ and has no zeros on the interval of interest. The $x$-dependent coefficients of this representation can be constructed explicitly by means of a simple recursive integration procedure. Moreover, we derive bounds for the truncation error, which are uniform whenever the spectral parameter $ρ=\sqrtλ$ satisfies a condition of the form $|\operatorname{Im}ρ|\leq C$. Based on these representations, we develop a numerical method for solving spectral problems that enables the computation of eigenvalues with non-deteriorating accuracy.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [43] [A Virtual Heat Flux Method for Simple and Accurate Neumann Thermal Boundary Imposition in the Material Point Method](https://arxiv.org/abs/2601.04570)
*Jidu Yu,Jidong Zhao*

Main category: math-ph

TL;DR: Novel Virtual Heat Flux Method (VHFM) for accurately imposing Neumann-type thermal boundary conditions in MPM without explicit boundary tracking or complex surface reconstruction.


<details>
  <summary>Details</summary>
Motivation: Accurately imposing Neumann-type thermal boundary conditions (especially convective heat flux) in MPM is challenging due to nonconformity between evolving material boundaries and fixed background grid.

Method: Constructs virtual flux field on auxiliary domain surrounding physical boundary to exactly satisfy prescribed boundary condition, transforming surface integral into easily computed volumetric integral.

Result: VHFM achieves accuracy comparable to conforming node-based imposition and significantly outperforms conventional particle-based approaches in 1D transient, 2D/3D curved boundaries, and complex moving geometries.

Conclusion: VHFM provides simple, computationally efficient, and robust solution for accurate thermal boundary conditions in thermo-mechanical and multiphysics MPM frameworks.

Abstract: In the Material Point Method (MPM), accurately imposing Neumann-type thermal boundary conditions, particularly convective heat flux boundaries, remains a significant challenge due to the inherent nonconformity between complex evolving material boundaries and the fixed background grid. This paper introduces a novel Virtual Heat Flux Method (VHFM) to overcome this limitation. The core idea is to construct a virtual flux field on an auxiliary domain surrounding the physical boundary, which exactly satisfies the prescribed boundary condition. This transforms the surface integral in the weak form into an equivalent, and easily computed, volumetric integral. Consequently, VHFM eliminates the need for explicit boundary tracking, specialized boundary particles, or complex surface reconstruction. A unified formulation is presented, demonstrating the method's straightforward extension to general scalar, vector, and tensor Neumann conditions. The accuracy, robustness, and convergence of VHFM are rigorously validated through a series of numerical benchmarks, including 1D transient analysis, 2D and 3D curved boundaries, and problems with large rotations and complex moving geometries. The results show that VHFM achieves accuracy comparable to conforming node-based imposition and significantly outperforms conventional particle-based approaches. Its simplicity, computational efficiency, and robustness make it an attractive solution for integrating accurate thermal boundary conditions into thermo-mechanical and other multiphysics MPM frameworks.

</details>


### [44] [A high order accurate and provably stable fully discrete continuous Galerkin framework on summation-by-parts form for advection-diffusion equations](https://arxiv.org/abs/2601.05071)
*Mrityunjoy Mandal,Jan Nordström,Arnaud G Malan*

Main category: math-ph

TL;DR: A high-order accurate fully discrete SBP-SAT finite element scheme for IBVPs that achieves super-convergence O(p+2) and maintains energy stability in both space and time.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical scheme for Initial Boundary Value Problems that combines high-order accuracy with energy stability, addressing the need for efficient and reliable computational methods for space-time variations.

Method: Continuous Galerkin finite element framework with Summation-By-Parts (SBP) spatial and temporal approximations, using Simultaneous Approximation Term (SAT) technique for weak imposition of initial and boundary conditions, resulting in an SBP-SAT formulation.

Result: The scheme achieves super-convergence with accuracy O(p+2) for p≥2 (where p is Lagrange basis degree), provides energy-stable discretization, and efficiently captures space-time variations even on coarse meshes as demonstrated through Method of Manufactured Solutions and application cases.

Conclusion: The proposed SBP-SAT finite element method provides a computationally effective, energy-stable, and high-order accurate approach for solving IBVPs, with demonstrated super-convergence properties and efficient performance on coarse meshes.

Abstract: We present a high-order accurate fully discrete numerical scheme for solving Initial Boundary Value Problems (IBVPs) within the Continuous Galerkin (CG)-based Finite Element framework. Both the spatial and time approximation in Summation-By-Parts (SBP) form are considered here. The initial and boundary conditions are imposed weakly using the Simultaneous Approximation Term (SAT) technique. The resulting SBP-SAT formulation yields an energy estimate in terms of the initial and external boundary data, leading to an energy-stable discretization in both space and time. The proposed method is evaluated numerically using the Method of Manufactured Solutions (MMS). The scheme achieves super-convergence in both spatial and temporal direction with accuracy $\mathcal{O}(p+2)$ for $p\geq 2$, where $p$ refers to the degree of the Lagrange basis. In an application case, we show that the fully discrete formulation efficiently captures space-time variations even on coarse meshes, demonstrating the method's computational effectiveness.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [45] [Stochastic convergence of a class of greedy-type algorithms for Configuration Optimization Problems](https://arxiv.org/abs/2601.05029)
*Evie Nielen,Oliver Tse*

Main category: math.OC

TL;DR: The paper proposes a stochastic framework for analyzing greedy sampling methods as Markov processes, enabling convergence analysis under mild assumptions, with applications to interpolation and a new randomized algorithm.


<details>
  <summary>Details</summary>
Motivation: Greedy sampling methods are widely used but have restrictive deterministic convergence analyses that are difficult to verify. The authors aim to develop a more flexible stochastic framework for analyzing these methods.

Method: Formulate greedy-type methods as continuous-time Markov processes on configuration spaces. Analyze convergence in expectation and probability under mild assumptions on error functional and transition kernel. Derive explicit convergence rates based on improvement conditions. Apply to 1D piecewise linear interpolation and introduce Randomized Polytope Division Method (R-PDM).

Result: Established convergence analysis framework for greedy methods with explicit convergence rates (logarithmic, polynomial, exponential decay). Proved exponential convergence of L^1-interpolation error for C^2 functions in 1D. Demonstrated effectiveness and variance reduction of R-PDM in numerical experiments.

Conclusion: The stochastic Markov process framework provides a flexible and rigorous approach to analyze greedy sampling methods, overcoming limitations of deterministic analyses. The approach enables convergence guarantees under mild assumptions and leads to practical randomized algorithms with improved performance.

Abstract: Greedy Sampling Methods (GSMs) are widely used to construct approximate solutions of Configuration Optimization Problems (COPs), where a loss functional is minimized over finite configurations of points in a compact domain. While effective in practice, deterministic convergence analyses of greedy-type algorithms are often restrictive and difficult to verify. We propose a stochastic framework in which greedy-type methods are formulated as continuous-time Markov processes on the space of configurations. This viewpoint enables convergence analysis in expectation and in probability under mild structural assumptions on the error functional and the transition kernel. For global error functionals, we derive explicit convergence rates, including logarithmic, polynomial, and exponential decay, depending on an abstract improvement condition. As a pedagogical example, we study stochastic greedy sampling for one-dimensional piecewise linear interpolation and prove exponential convergence of the $L^1$-interpolation error for $C^2$-functions. Motivated by this analysis, we introduce the Randomized Polytope Division Method (R-PDM), a randomized variant of the classical Polytope Division Method, and demonstrate its effectiveness and variance reduction in numerical experiments

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [46] [Multigroup Radiation Diffusion on a Moving Mesh: Implementation in RICH and Application to Tidal Disruption Events](https://arxiv.org/abs/2601.05120)
*Itamar Giron,Menahem Krief,Nicholas C. Stone,Elad Steinberg*

Main category: astro-ph.HE

TL;DR: The paper extends the RICH radiation-hydrodynamics code with multigroup flux-limited diffusion capability, making it the first multigroup RHD moving mesh code suitable for problems with extreme dynamic range and important radiation forces.


<details>
  <summary>Details</summary>
Motivation: Radiation-hydrodynamics (RHD) is crucial for understanding high-energy astrophysical phenomena, but existing codes have limitations. The grey flux-limited diffusion (FLD) approach in the original RICH code needed extension to multigroup FLD to better handle problems with extreme dynamic range and dynamically important radiation forces.

Method: Extended the publicly available RICH code (a semi-Lagrangian code solving RHD equations on an unstructured moving mesh) with a multigroup FLD solver. Implemented a novel scheme to accelerate convergence in optically thick cells and validated against multiple analytic benchmarks including a novel test of the RHD Doppler term.

Result: Successfully created the first multigroup RHD moving mesh code. Validated the multigroup module against analytic benchmarks. Applied the code to study a stellar tidal disruption event (TDE) with a 10^4 M⊙ intermediate-mass black hole, self-consistently producing a bright early-time X-ray flash prior to peak optical/UV light, consistent with observations of TDE AT 2022dsb.

Conclusion: The multigroup extension of RICH provides a powerful new tool for studying astrophysical phenomena with extreme dynamic range and important radiation forces, as demonstrated by its successful application to TDE simulations that reproduce observed X-ray features.

Abstract: Radiation-hydrodynamics (RHD) determines the bulk evolution and observable emission in a wide variety of high-energy astrophysical phenomena. Due to their complexity, RHD problems must usually be studied through numerical simulation. We have extended the publicly available RICH code, which previously solved the equations of RHD in the limit of grey flux-limited diffusion (FLD), to operate with a multigroup FLD solver. RICH is a semi-Lagrangian code that solves the equations of RHD on an unstructured moving mesh, and is the first multigroup RHD moving mesh code, making it uniquely applicable to problems with extreme dynamic range and dynamically important radiation forces. We validate our multigroup module against multiple analytic benchmarks, including a novel test of the RHD Doppler term. The computational efficiency of the code is aided by a novel scheme to accelerate convergence in optically thick cells. Finally, we apply multigroup RICH in a pilot study of a stellar tidal disruption event (TDE), using a $10^4 M_\odot$ intermediate-mass black hole. Our simulations self-consistently produce a bright early-time X-ray flash prior to peak optical/UV light, in qualitative agreement with post-processing of (grey) RICH simulations of supermassive black hole TDEs, as well as X-ray observations of the TDE AT 2022dsb.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [47] [sidmkit: A Reproducible Toolkit for SIDM Phenomenology and Galaxy Rotation-Curve Modeling](https://arxiv.org/abs/2601.04735)
*Nalin Dhiman*

Main category: astro-ph.IM

TL;DR: sidmkit is a Python package for self-interacting dark matter calculations and rotation curve fitting, implementing velocity-dependent cross sections and providing batch pipeline for SPARC data analysis.


<details>
  <summary>Details</summary>
Motivation: Self-interacting dark matter (SIDM) requires bridging multiple layers from microphysical scattering models to astrophysical constraints and data-driven halo fits, necessitating a transparent and reproducible computational framework.

Method: Developed sidmkit package implementing velocity-dependent momentum-transfer cross sections for Yukawa interactions using analytic approximations (Born, classical, Hulthén-based) with numerical partial-wave option. Provides velocity-moment averaging, scattering-rate utilities, and implements bounded non-linear least squares fits of NFW and Burkert halo models to SPARC data with mass-to-light priors and information-criterion summaries.

Result: Processed 191 rotmod galaxies (LTG+ETG bundles) with 382 total fits. Burkert model is preferred by ΔBIC > 0 for 65.4% of galaxies, with "strong" preference (ΔBIC > 6) in 32.5% of galaxies.

Conclusion: sidmkit provides a robust computational framework for SIDM micro-to-macro calculations and rotation curve analysis, demonstrating that Burkert halo models are statistically preferred over NFW models for a significant fraction of galaxies in the SPARC dataset.

Abstract: Self-interacting dark matter (SIDM) is a well-motivated extension of cold dark matter that can modify halo structure on galactic and group scales while remaining consistent with large-scale structure. However, practical SIDM work often requires bridging several layers, including microphysical scattering models, velocity-dependent effective cross sections, phenomenological astrophysical constraints, and (separately) data-driven halo fits, such as rotation curves. In this paper, we describe \texttt{sidmkit}, a transparent and reproducible Python package designed to support SIDM ``micro$\rightarrow$macro'' calculations and to provide a robust batch pipeline for fitting rotation curves in the SPARC data. On the SIDM side, \texttt{sidmkit} implements velocity-dependent momentum-transfer cross sections for a Yukawa interaction using standard analytic approximations (Born, classical, and Hulthén-based) with a numerical partial-wave option for spot checks. It also provides consistent velocity-moment averaging for Maxwellian relative speeds, scattering-rate utilities, and curated literature \emph{summary} constraints for regression tests and exploratory scans. On the rotation-curve side, we implement bounded non-linear least squares fits of NFW and Burkert halo models to SPARC baryonic decompositions, with optional mass-to-light priors and information-criterion summaries (AIC/BIC). For the demonstration dataset, we process 191 \texttt{rotmod} galaxies (LTG+ETG bundles) and fit both NFW and Burkert models (382 total fits). We find that Burkert is preferred by $Δ\mathrm{BIC} > 0$ for $65.4\%$ of galaxies, with ``strong'' preference ($Δ\mathrm{BIC}>6$) in $32.5\%$ of galaxies;

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [48] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: LLM representations encode syntax and semantics linearly; averaging sentence vectors creates syntactic/semantic centroids that capture significant linguistic information, with differential encoding profiles across layers.


<details>
  <summary>Details</summary>
Motivation: To understand how syntactic and semantic information is encoded in the inner layer representations of large language models, specifically examining whether these linguistic features are linearly encoded and how they differ across model layers.

Method: Analyze DeepSeek-V3 by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning to create syntactic and semantic "centroids," then examine how subtracting these centroids affects sentence similarity with syntactically/semantically matched sentences.

Result: Syntactic and semantic centroids capture significant proportions of linguistic information; subtracting them strongly affects similarity with matched sentences, suggesting linear encoding. Cross-layer encoding profiles differ for syntax vs. semantics, and the two signals can be partially decoupled.

Conclusion: Syntax and semantics are at least partially linearly encoded in LLM representations, with differential encoding patterns across layers, suggesting distinct processing mechanisms for these two types of linguistic information.

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [49] [Technical Note: Low-Dose Simulation for Grating-Based X-Ray Dark-Field Radiography Using a Virtually Decreased Irradiation Area](https://arxiv.org/abs/2508.15328)
*Henriette Bast,Rafael C. Schick,Thomas Koehler,Franz Pfeiffer*

Main category: physics.med-ph

TL;DR: Low-dose simulation algorithm for X-ray dark-field radiography creates stripe artifacts not present in actual low-dose measurements, reducing image quality at higher dose reductions.


<details>
  <summary>Details</summary>
Motivation: To validate a low-dose simulation algorithm for dark-field radiography and demonstrate that stripe artifacts in simulated images are algorithm limitations rather than real physical effects.

Method: Acquired dark-field radiographs of chest phantom at different radiation doses, then simulated low-dose images from high-dose measurements by virtually reducing irradiated area. Compared simulated vs. measured low-dose images using quantitative ROI analysis.

Result: Actual low-dose radiographs (quarter standard dose) were artifact-free, with dark-field signal differing from simulations by up to 10%. Simulated images showed algorithm-induced stripe artifacts that degraded image quality.

Conclusion: Virtual dose reduction is a simple simulation approach but creates stripe artifacts not present in real low-dose measurements, particularly at higher dose reductions, resulting in reduced image quality compared to actual low-dose images.

Abstract: Background: X-ray dark-field radiography uses small-angle scattering to visualize the structural integrity of lung alveoli. To study the influence of dose reduction on clinical dark-field radiographs, one can simulate low-dose images by virtually reducing the irradiated area. However, these simulations can exhibit stripe artifacts. Purpose: Validation of the low-dose simulation algorithm reported by Schick & Bast et al., PLoS ONE, 2024. Furthermore, we want to demonstrate that stripe artifacts observed in simulated images at very low-dose levels are introduced by limitations of the algorithm and would not appear in actual low-dose dark-field images. Methods: Dark-field radiographs of an anthropomorphic chest phantom were acquired at different tube currents equaling different radiation doses. Based on the measurement with a high radiation dose, dark-field radiographs corresponding to lower radiation doses were simulated by virtually reducing the irradiated area. The simulated low-dose radiographs were evaluated by a quantitative comparison of the dark-field signal using different regions of interests. Results: Dark-field radiographs acquired at one quarter of the standard dose were artifact-free. The dark-field signal differed from the simulated radiographs by up to 10%. Algorithm-induced stripe artifacts decrease the image quality of the simulated radiographs. Conclusions: Virtually reducing the irradiation area is a simple approach to generate low-dose radiographs based on images acquired with scanning-based dark-field radiography. However, as the algorithm creates stripe artifacts in the dark-field images, particularly at higher dose reductions, that are not present in measured low-dose images, simulated images have reduced image quality compared to their measured counterparts.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [50] [Towards Spatio-Temporal Extrapolation of Phase-Field Simulations with Convolution-Only Neural Networks](https://arxiv.org/abs/2601.04510)
*Christophe Bonneville,Nathan Bieberdorf,Pieterjan Robbe,Mark Asta,Habib Najm,Laurent Capolungo,Cosmin Safta*

Main category: cs.CE

TL;DR: A deep learning framework using conditional U-Nets and diffusion models to accelerate and extrapolate phase-field simulations of liquid metal dealloying, achieving 36,000× speedup with <15% error.


<details>
  <summary>Details</summary>
Motivation: Phase-field simulations of liquid metal dealloying are computationally expensive for large domains and long time horizons, limiting practical applications. There's a need for efficient surrogate models that can extrapolate beyond training data while maintaining physical accuracy.

Method: Developed a fully convolutional, conditionally parameterized U-Net surrogate with convolutional self-attention and physically informed padding. Coupled with a conditional diffusion model to generate synthetic initial conditions. Trained on small-domain, short-time simulations but can extrapolate to larger scales using U-Net's convolutional properties.

Result: Achieved relative errors below 5% in training regime and under 15% during large-scale extrapolation. Predicted key quantities and spatial statistics accurately across multiple alloy compositions. Delivered up to 36,000× speedup, reducing weeks-long simulations to seconds.

Conclusion: The framework enables high-fidelity extrapolation of phase-field simulations in both space and time for liquid metal dealloying, serving as a first step toward efficient large-scale computational materials science.

Abstract: Phase-field simulations of liquid metal dealloying (LMD) can capture complex microstructural evolutions but can be prohibitively expensive for large domains and long time horizons. In this paper, we introduce a fully convolutional, conditionally parameterized U-Net surrogate designed to extrapolate far beyond its training data in both space and time. The architecture integrates convolutional self-attention, physically informed padding, and a flood-fill corrector method to maintain accuracy under extreme extrapolation, while conditioning on simulation parameters allows for flexible time-step skipping and adaptation to varying alloy compositions. To remove the need for costly solver-based initialization, we couple the surrogate with a conditional diffusion model that generates synthetic, physically consistent initial conditions. We train our surrogate on simulations generated over small domain sizes and short time spans, but, by taking advantage of the convolutional nature of U-Nets, we are able to run and extrapolate surrogate simulations for longer time horizons than what would be achievable with classic numerical solvers. Across multiple alloy compositions, the framework is able to reproduce the LMD physics accurately. It predicts key quantities of interest and spatial statistics with relative errors typically below 5% in the training regime and under 15% during large-scale, long time-horizon extrapolations. Our framework can also deliver speed-ups of up to 36,000 times, bringing the time to run weeks-long simulations down to a few seconds. This work is a first stepping stone towards high-fidelity extrapolation in both space and time of phase-field simulation for LMD.

</details>
