<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 20]
- [math.AP](#math.AP) [Total: 23]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [math.OC](#math.OC) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [math.DG](#math.DG) [Total: 2]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [math.HO](#math.HO) [Total: 1]
- [math.RA](#math.RA) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [stat.ML](#stat.ML) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Structure-Preserving Numerical Scheme for Optimal Control and Design of Mixing in Incompressible Flows](https://arxiv.org/abs/2601.06294)
*Weiwei Hu,Ziqian Li,Yubiao Zhang,Enrique Zuazua*

Main category: math.NA

TL;DR: A structure-preserving computational framework for optimal mixing control in incompressible flows that exactly conserves mass and L²-energy while maintaining discrete state-adjoint duality.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical solver that faithfully represents continuous optimality conditions for flow mixing control while preserving key physical invariants, addressing the need for efficient mixing-enhancing controls in incompressible flows.

Method: Integrates centered finite-volume spatial discretization with time-symmetric Crank-Nicolson integrator for both forward advection and its adjoint, within a gradient-based optimization loop.

Result: Optimized time-dependent stirring produces nearly exponential decay of mix-norm, achieving orders-of-magnitude faster mixing than any single steady flow, with exact conservation of invariants.

Conclusion: Enforcing physical structure at discrete level leads to both exact conservation and highly effective mixing outcomes in optimal flow design, providing first evidence of this approach's effectiveness.

Abstract: We develop a structure-preserving computational framework for optimal mixing control in incompressible flows. Our approach exactly conserves the continuous system's key invariants (mass and $L^2$-energy), while also maintaining discrete state-adjoint duality at every time step. These properties are achieved by integrating a centered finite-volume discretization in space with a time-symmetric Crank-Nicolson integrator for both the forward advection and its adjoint, all inside a gradient-based optimization loop. The result is a numerical solver that is faithful to the continuous optimality conditions and efficiently computes mixing-enhancing controls. In our numerical tests, the optimized time-dependent stirring produces a nearly exponential decay of a chosen mix-norm, achieving orders-of-magnitude faster mixing than any single steady flow. To our knowledge, this work provides the first evidence that enforcing physical structure at the discrete level can lead to both exact conservation and highly effective mixing outcomes in optimal flow design.

</details>


### [2] [Supervised and Unsupervised Neural Network Solver for First Order Hyperbolic Nonlinear PDEs](https://arxiv.org/abs/2601.06388)
*Zakaria Baba,Alexandre M. Bayen,Alexi Canesse,Maria Laura Delle Monache,Martin Drieux,Zhe Fu,Nathan Lichtlé,Zihe Liu,Hossein Nick Zinat Matin,Benedetto Piccoli*

Main category: math.NA

TL;DR: Neural network replaces traditional numerical flux in finite volume schemes for learning hyperbolic conservation laws, preserving conservation structure and outperforming classical methods.


<details>
  <summary>Details</summary>
Motivation: To develop a data-driven approach for solving hyperbolic conservation laws that can outperform traditional numerical methods like Godunov's scheme, WENO, and Discontinuous Galerkin while maintaining the conservative structure essential for accurate physical modeling.

Method: Replace traditional numerical flux in finite volume schemes with a trainable neural network that preserves conservative structure. The model can be trained both supervised (with synthetic data) and unsupervised (using weak PDE formulation). Theoretical analysis provides neural network size bounds.

Result: The method often outperforms efficient classical schemes (Godunov, WENO, Discontinuous Galerkin) for comparable computational budgets. Demonstrated effectiveness on traffic prediction using real highway data from Berkeley DeepDrive drone dataset.

Conclusion: Neural network-based flux functions provide a powerful alternative to traditional numerical methods for hyperbolic conservation laws, offering better performance while maintaining conservation properties, with practical applications in real-world problems like traffic prediction.

Abstract: We present a neural network-based method for learning scalar hyperbolic conservation laws. Our method replaces the traditional numerical flux in finite volume schemes with a trainable neural network while preserving the conservative structure of the scheme. The model can be trained both in a supervised setting with efficiently generated synthetic data or in an unsupervised manner, leveraging the weak formulation of the partial differential equation. We provide theoretical results that our model can perform arbitrarily well, and provide associated upper bounds on neural network size. Extensive experiments demonstrate that our method often outperforms efficient schemes such as Godunov's scheme, WENO, and Discontinuous Galerkin for comparable computational budgets. Finally, we demonstrate the effectiveness of our method on a traffic prediction task, leveraging field experimental highway data from the Berkeley DeepDrive drone dataset.

</details>


### [3] [An NPDo Approach for Principal Joint Block Diagonalization](https://arxiv.org/abs/2601.06410)
*Ren-Cang Li,Ding Lu,Li Wang,Lei-Hong Zhang*

Main category: math.NA

TL;DR: Proposes NPDo approach for principal joint block-diagonalization (PJBD) to achieve partial diagonalization and identify dominant common block-diagonal parts, addressing limitations of Givens rotation methods for large matrices.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Givens rotation focus on full joint diagonalization and become impractical for matrices of moderate size (300×300 or larger). There's a need for methods that can handle partial joint block-diagonalization and identify dominant common block-diagonal structures.

Method: Proposes NPDo (Nonlinear Polar Decomposition with orthogonal polar factor dependency) approach. Uses an optimization problem designed for PJBD, with an associated SCF (Self-Consistent Field) iteration that is globally convergent to a stationary point while monotonically increasing the objective function.

Result: Numerical experiments demonstrate the effectiveness of NPDo approach and its superiority over Givens rotation-based methods for principal joint block-diagonalization problems.

Conclusion: NPDo provides a practical solution for PJBD problems, overcoming scalability limitations of traditional methods and enabling efficient identification of dominant common block-diagonal structures in large matrices.

Abstract: Matrix joint block-diagonalization (JBD) frequently arises from diverse applications such as independent component analysis, blind source separation, and common principal component analysis (CPCA), among others. Particularly, CPCA aims at joint diagonalization, i.e., each block size being $1$-by-$1$. This paper is concerned with {\em principal joint block-diagonalization\/} (\pjbd), which aim to achieve two goals: 1)~partial joint block-diagonalization, and 2)~identification of dominant common block-diagonal parts for all involved matrices. This is in contrast to most existing methods, especially the popular ones based on Givens rotation, which focus on full joint diagonalization and quickly become impractical for matrices of even moderate size ($300$-by-$300$ or larger). An NPDo approach is proposed and it is built on a {\em nonlinear polar decomposition with orthogonal polar factor dependency} that characterizes the solutions of the optimization problem designed to achieve \pjbd, and it is shown the associated SCF iteration is globally convergent to a stationary point while the objective function increases monotonically during the iterative process. Numerical experiments are presented to illustrate the effectiveness of the NPDo approach and its superiority to Givens rotation-based methods.

</details>


### [4] [On traces of the derivatives of the $L^2$-projection error](https://arxiv.org/abs/2601.06625)
*Torsten Linß,Christos Xenophontos*

Main category: math.NA

TL;DR: Explicit derivative bounds for L² projection of Hᵏ functions onto polynomial spaces of degree ≤ p


<details>
  <summary>Details</summary>
Motivation: Need explicit error estimates for polynomial approximations in numerical analysis, particularly for finite element methods where understanding approximation quality is crucial

Method: Analyze L² projection operator onto polynomial spaces, derive explicit bounds for derivatives of the projection error in terms of both differentiation order and polynomial degree

Result: Obtain explicit derivative estimates showing how projection error depends on both order of differentiation k and polynomial degree p

Conclusion: Provides explicit, computable bounds for polynomial approximation quality that are important for error analysis in numerical methods

Abstract: We provide derivative estimates for the $L^2$ projection of an $H^{k}$ function onto the space of polynomials of degree $\leq p$. The bounds are explicit in the order of differentiation and the polynomial degree $p$.

</details>


### [5] [The optimal error analysis of nonuniform L1 method for the variable-exponent subdiffusion model](https://arxiv.org/abs/2601.06773)
*Wenlin Qiu,Kexin Li,Yiqun Li,Hao Zhang*

Main category: math.NA

TL;DR: Improved error analysis for variable-exponent subdiffusion model using nonuniform temporal mesh, achieving better convergence rates than previous work.


<details>
  <summary>Details</summary>
Motivation: To improve the error estimates for fully discrete schemes of variable-exponent subdiffusion models, particularly under nonuniform temporal meshes, addressing limitations in existing convergence results.

Method: Applied perturbation method to reformulate the original model, then used L1 scheme for Caputo derivative discretization and interpolation quadrature rule for convolution term discretization on nonuniform temporal mesh.

Result: Proved temporal convergence rates of O(N^{-min{2-α(0), rα(0)}}) under nonuniform mesh, which improves existing convergence results for r ≥ (2-α(0))/α(0). Numerical experiments validate theoretical findings.

Conclusion: The proposed method provides improved error estimates for variable-exponent subdiffusion models, demonstrating better convergence rates than previous work under nonuniform temporal discretization.

Abstract: This work investigates the optimal error estimate of the fully discrete scheme for the variable-exponent subdiffusion model under the nonuniform temporal mesh. We apply the perturbation method to reformulate the original model into its equivalent form, and apply the L1 scheme as well as the interpolation quadrature rule to discretize the Caputo derivative term and the convolution term in the reformulated model, respectively. We then prove the temporal convergence rates $O(N^{-\min\{2-α(0), rα(0)\}})$ under the nonuniform mesh, which improves the existing convergence results in [Zheng, CSIAM T. Appl. Math. 2025] for $r\geq \frac{2-α(0)}{α(0)}$. Numerical results are presented to substantiate the theoretical findings.

</details>


### [6] [Analysis and Efficient Sylvester-Based Implementation of a Dimension-Split ETD2RK Scheme for Multidimensional Reaction-Diffusion Equations](https://arxiv.org/abs/2601.06849)
*Ibrahim O. Sarumi*

Main category: math.NA

TL;DR: Second-order exponential time differencing Runge-Kutta scheme with dimension splitting for reaction-diffusion equations, achieving computational cost reduction via tensor decomposition and Sylvester reformulation.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for multidimensional reaction-diffusion equations that overcome the high computational costs of traditional approaches, particularly the cubic and quintic scaling with grid points in 2D and 3D.

Method: ETD2RK-DS scheme with dimension splitting decomposes multidimensional problems into independent 1D systems, uses Padé approximations for matrix exponentials, and implements Sylvester-equation reformulation for spectral efficiency with reusable eigendecompositions.

Result: Proved uniform stability and second-order temporal convergence, achieved computational cost reduction from O(m³) to O(m²) in 2D and O(m⁵) to O(m³) in 3D, and demonstrated scalability and efficiency in numerical experiments including FitzHugh-Nagumo system.

Conclusion: The ETD2RK-DS framework provides a stable, second-order accurate, and computationally efficient approach for multidimensional reaction-diffusion equations with substantial advantages over classical LU-based solvers through dimension splitting and spectral implementation.

Abstract: We propose and analyze a second-order, dimension-split exponential time differencing Runge--Kutta scheme (ETD2RK-DS) for multidimensional reaction--diffusion equations in two and three spatial dimensions. Under mild assumptions on the nonlinear source term, we establish uniform stability bounds and prove second-order temporal convergence for the underlying dimension-split scheme.
  To enable efficient implementation, we employ Padé approximations of the matrix exponential, converting each required matrix-exponential--vector product into the solution of a shifted linear system. A convergence analysis of the resulting Padé-based ETD2RK-DS formulation is provided. We derive explicit and reproducible tensor-slicing and reshaping algorithms that realize the dimension-splitting strategy, decomposing multidimensional systems into collections of independent one-dimensional problems. This leads to a reduction of the dominant per-time-step computational cost from $\mathcal{O}(m^3)$ to $\mathcal{O}(m^2)$ in two dimensions and from $\mathcal{O}(m^5)$ to $\mathcal{O}(m^3)$ in three dimensions when compared with banded LU solvers for the unsplit problem, where $m$ denotes the number of grid points per spatial direction.
  Furthermore, we develop a Sylvester-equation reformulation of the resulting one-dimensional systems, enabling a highly efficient spectral implementation based on reusable eigendecompositions, matrix--vector multiplications, and Hadamard divisions. Numerical experiments in two and three dimensions, including a coupled FitzHugh--Nagumo system, confirm the second-order temporal accuracy, stability of the underlying scheme, and scalability of the proposed ETD2RK-DS framework, as well as the substantial computational advantages of the Sylvester-based implementation over classical LU-based solvers.

</details>


### [7] [The Ill-Posed Foundations of Physics-Informed Neural Networks and Their Finite-Difference Variants](https://arxiv.org/abs/2601.07017)
*Andreas Langer*

Main category: math.NA

TL;DR: The paper provides a unified mathematical foundation for AD-PINNs and FD-PINNs, proving both are ill-posed optimization problems with infinitely many minimizers, but establishing structural properties that explain FD-PINNs' superior stability.


<details>
  <summary>Details</summary>
Motivation: Physics-informed neural networks (PINNs) based on automatic differentiation (AD-PINNs) and finite differences (FD-PINNs) are widely used for solving PDEs, but their analytical properties remain poorly understood, creating a need for mathematical foundations to explain their behavior.

Method: The authors provide a unified mathematical analysis of both AD-PINNs and FD-PINNs, proving theoretical properties under mild regularity assumptions on activation functions and sufficiently wide neural networks. They establish ill-posedness results and structural properties through mathematical proofs, supported by numerical experiments on forward and inverse problems.

Result: Both AD-PINNs and FD-PINNs are ill-posed optimization problems with infinitely many minimizers when they exist. However, FD-PINNs are tightly coupled to the underlying finite-difference scheme, with every FD-PINN minimizer agreeing with a finite-difference minimizer on the grid. Numerical experiments show FD-PINNs remain stable in forward/inverse problems where AD-PINNs may fail.

Conclusion: The work clarifies analytical limitations of AD-PINNs and explains the structural reasons for FD-PINNs' more stable behavior, providing mathematical foundations that can guide practitioners in choosing appropriate PINN formulations for different PDE problems.

Abstract: Physics-informed neural networks based on automatic differentiation (AD-PINNs) and their finite-difference counterparts (FD-PINNs) are widely used for solving partial differential equations (PDEs), yet their analytical properties remain poorly understood. This work provides a unified mathematical foundation for both formulations. Under mild regularity assumptions on the activation function and for sufficiently wide neural networks of depth at least two, we prove that both the AD- and FD-PINN optimization problems are ill-posed: whenever a minimizer exists, there are in fact infinitely many, and uniqueness fails regardless of the choice of collocation points or finite-difference stencil. Nevertheless, we establish two structural properties. First, whenever the underlying PDE or its finite-difference discretization admits a solution, the corresponding AD-PINN or FD-PINN loss also admits a minimizer, realizable by a neural network of finite width. Second, FD-PINNs are tightly coupled to the underlying finite-difference scheme: every FD-PINN minimizer agrees with a finite-difference minimizer on the grid, and in regimes where the discrete PDE solution is unique, all zero-loss FD-PINN minimizers coincide with the discrete PDE solution on the stencil. Numerical experiments illustrate these theoretical insights: FD-PINNs remain stable in representative forward and inverse problems, including settings where AD-PINNs may fail to converge. We also include an inverse problem with noisy data, demonstrating that FD-PINNs retain robustness in this setting as well. Taken together, our results clarify the analytical limitations of AD-PINNs and explain the structural reasons for the more stable behavior observed in FD-PINNs.

</details>


### [8] [A Relaxed Direct-insertion Downscaling Method For Discrete-in-time Data Assimilation](https://arxiv.org/abs/2601.07025)
*Emine Celik,Eric Olson*

Main category: math.NA

TL;DR: Improved downscaling method with relaxation parameter overcomes observation frequency constraint, enabling convergence across wide frequency ranges and connecting to continuous nudging.


<details>
  <summary>Details</summary>
Motivation: The spectrally-filtered direct-insertion downscaling method for discrete-in-time data assimilation has a constraint on observation frequency that limits its practical application. The authors aim to overcome this limitation to make the method more flexible and applicable to real-world scenarios with varying observation frequencies.

Method: Introduces a relaxation parameter to the spectrally-filtered direct-insertion downscaling method. The key innovation is making this parameter proportional to the time between observations, which allows the method to handle varying observation frequencies while maintaining convergence properties.

Result: Numerical simulations show that with the relaxation parameter proportional to observation time intervals, the method maintains convergence of the approximating solution to the reference solution across a wide range of observation frequencies. Analytical proof demonstrates that as observation frequency approaches infinity, the method converges to the continuous-in-time nudging method.

Conclusion: The proposed relaxation parameter successfully overcomes the observation frequency constraint in discrete-in-time data assimilation, making the downscaling method more flexible and practical while maintaining theoretical convergence properties and connecting it to established continuous methods.

Abstract: This paper improves the spectrally-filtered direct-insertion downscaling method for discrete-in-time data assimilation by introducing a relaxation parameter that overcomes a constraint on the observation frequency. Numerical simulations demonstrate that taking the relaxation parameter proportional to the time between observations allows one to vary the observation frequency over a wide range while maintaining convergence of the approximating solution to the reference solution. Under the same assumptions we analytically prove that taking the observation frequency to infinity results in the continuous-in-time nudging method.

</details>


### [9] [An efficient hyper reduced-order model for segregated solvers for geometrical parametrization problems](https://arxiv.org/abs/2601.07082)
*Valentin Nkana Ngan,Giovanni Stabile,Andrea Mola,Gianluigi Rozza*

Main category: math.NA

TL;DR: Efficient hyper-reduced order model (HROM) for segregated finite-volume solvers in geometrically parametrized problems using discretize-then-project strategy with hyper-reduction techniques like DEIM.


<details>
  <summary>Details</summary>
Motivation: To enable fast and accurate CFD simulations for geometrically parametrized problems by reducing computational costs while maintaining accuracy, particularly for large-scale problems where full-order models are computationally expensive.

Method: Discretize-then-project strategy: first assemble full-order operators using finite volume/finite element discretizations, then project onto low-dimensional spaces using spatial sampling points selected via hyper-reduction techniques (DEIM). This removes online computational dependence on full mesh size.

Result: HROM closely matches full-order solutions while achieving substantial computational time reductions. The method is naturally parallelizable and scalable to very large meshes since only sparse subset of mesh cells is evaluated online.

Conclusion: Hyper-reduction can be effectively combined with segregated solvers and geometric parametrization to enable fast and accurate CFD simulations, demonstrating practical applicability for large-scale parametrized problems.

Abstract: We propose an efficient hyper-reduced order model (HROM) designed for segregated finite-volume solvers in geometrically parametrized problems. The method follows a discretize-then-project strategy: the full-order operators are first assembled using finite volume or finite element discretizations and then projected onto low-dimensional spaces using a small set of spatial sampling points, selected through hyper-reduction techniques such as DEIM. This approach removes the dependence of the online computational cost on the full mesh size. The method is assessed on three benchmark problems: a linear transport equation, a nonlinear Burgers equation, and the incompressible Navier--Stokes equations. The results show that the hyper-reduced models closely match full-order solutions while achieving substantial reductions in computational time. Since only a sparse subset of mesh cells is evaluated during the online phase, the method is naturally parallelizable and scalable to very large meshes. These findings demonstrate that hyper-reduction can be effectively combined with segregated solvers and geometric parametrization to enable fast and accurate CFD simulations.

</details>


### [10] [The MAC scheme for linear elasticity in displacement-stress formulation on non-uniform staggered grids](https://arxiv.org/abs/2601.07174)
*Hongxing Rui,Weijie Wang*

Main category: math.NA

TL;DR: A marker-and-cell finite difference method for 2D/3D linear elasticity on staggered grids with second-order accuracy and locking-free properties.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for linear elasticity problems that avoids spurious oscillations, maintains conservation properties, and works for both compressible and nearly incompressible materials.

Method: Marker-and-cell finite difference method using staggered grids: displacements on cell edge midpoints, normal stresses at cell centers, shear stresses at grid points. Mathematical analysis establishes stability and second-order L2-error super-convergence.

Result: The method demonstrates stability, second-order accuracy for both displacement and stress, locking-free behavior with respect to Lame constant, and excellent agreement between numerical experiments and theoretical predictions.

Conclusion: The proposed staggered grid finite difference method is efficient, robust, and suitable for linear elasticity problems across compressible and nearly incompressible materials, with proven mathematical properties and practical effectiveness.

Abstract: A marker-and-cell finite difference method is developed for solving the two dimensional and three dimensional linear elasticity in the displacement-stress formulation on staggered grids. The method employs a staggered grid arrangement, where the displacement components are approximated on the midpoints of cell edges, the normal stresses are defined at the cell centers, and the shear stresses are defined at the grid points. This structure ensures local conservation properties and avoids spurious oscillations in stress approximation. A rigorous mathematical analysis is presented, establishing the stability of the scheme and proving the second-order L2-error super-convergence for both displacement and stress. The proposed method is locking-free with respect to the Lame constant, making it suitable for both compressible and nearly incompressible elastic materials. Numerical experiments demonstrate the efficiency and robustness of the finite difference scheme, and the computed results show excellent agreement with the theoretical predictions.

</details>


### [11] [Parametric Probabilistic Manifold Decomposition for Nonlinear Model Reduction](https://arxiv.org/abs/2601.07278)
*Jiaming Guo,Dunhui Xiao*

Main category: math.NA

TL;DR: PPMD extends PMD to handle parametric problems, enabling continuous parametric surrogates with better accuracy and generalization than POD+GPR.


<details>
  <summary>Details</summary>
Motivation: PMD works well for temporal dynamics but cannot handle parametric variability needed for design optimization, control, and uncertainty quantification.

Method: Parametric Probabilistic Manifold Decomposition integrates probabilistic-manifold embeddings with parameter-aware latent learning using kernel ridge regression.

Result: PPMD achieves superior accuracy and generalization beyond training parameter range compared to POD+GPR in flow past cylinder and backward-facing step flow.

Conclusion: PPMD successfully extends PMD to parametric problems, enabling continuous high-fidelity parametric surrogates with proven convergence properties.

Abstract: Probabilistic Manifold Decomposition (PMD)\cite{doi:10.1137/25M1738863}, developed in our earlier work, provides a nonlinear model reduction by embedding high-dimensional dynamics onto low-dimensional probabilistic manifolds. The PMD has demonstrated strong performance for time-dependent systems. However, its formulation is for temporal dynamics and does not directly accommodate parametric variability, which limits its applicability to tasks such as design optimization, control, and uncertainty quantification. In order to address the limitations, a \emph{Parametric Probabilistic Manifold Decomposition} (PPMD) is presented to deal with parametric problems. The central advantage of PPMD is its ability to construct continuous, high-fidelity parametric surrogates while retaining the transparency and non-intrusive workflow of PMD. By integrating probabilistic-manifold embeddings with parameter-aware latent learning, PPMD enables smooth predictions across unseen parameter values (such as different boundary or initial conditions). To validate the proposed method, a comprehensive convergence analysis is established for PPMD, covering the approximation of the linear principal subspace, the geometric recovery of the nonlinear solution manifold, and the statistical consistency of the kernel ridge regression used for latent learning. The framework is then numerically demonstrated on two classic flow configurations: flow past a cylinder and backward-facing step flow. Results confirm that PPMD achieves superior accuracy and generalization beyond the training parameter range compared to the conventional proper orthogonal decomposition with Gaussian process regression (POD+GPR) method.

</details>


### [12] [TriCG with deflated restarting for symmetric quasi-definite linear systems](https://arxiv.org/abs/2601.07455)
*Kui Du,Jia-Jun Fan*

Main category: math.NA

TL;DR: TriCG-DR: Deflated restarting version of TriCG for symmetric quasi-definite systems, addressing slow convergence when off-diagonal blocks have large elliptic singular values.


<details>
  <summary>Details</summary>
Motivation: TriCG outperforms SYMMLQ for SQD systems but suffers slow convergence when off-diagonal blocks contain many large elliptic singular values, necessitating a deflation strategy.

Method: Developed generalized Saunders-Simon-Yip process with deflated restarting to construct deflation subspaces, creating TriCG with deflated restarting (TriCG-DR).

Result: Proposed TriCG-DR shows superior performance in numerical experiments, with deflation subspaces also applicable to multiple right-hand side problems.

Conclusion: Deflated restarting effectively addresses TriCG's convergence limitations for SQD systems with problematic off-diagonal blocks, offering improved performance.

Abstract: TriCG is a short-recurrence iterative method recently introduced by Montoison and Orban [SIAM J. Sci. Comput., 43 (2021), pp. A2502--A2525] for solving symmetric quasi-definite (SQD) linear systems. TriCG takes advantage of the inherent block structure of SQD linear systems and performs substantially better than SYMMLQ. However, numerical experiments have revealed that the convergence of TriCG can be notably slow when the off-diagonal block contains a substantial number of large elliptic singular values. To address this limitation, we introduce a deflation strategy tailored for TriCG to improve its convergence behavior. Specifically, we develop a generalized Saunders--Simon--Yip process with deflated restarting to construct the deflation subspaces. Building upon this process, we propose a novel method termed TriCG with deflated restarting. The deflation subspaces can also be utilized to solve SQD linear systems with multiple right-hand sides. Numerical experiments are provided to illustrate the superior performance of the proposed methods.

</details>


### [13] [Derivative-free discrete gradient methods](https://arxiv.org/abs/2601.07479)
*Håkon Noren Myhr,Sølve Eidnes*

Main category: math.NA

TL;DR: A derivative-free fourth-order discrete gradient method for preserving integrals in ODEs, using symmetrized Itoh-Abe discrete gradient and finite differences.


<details>
  <summary>Details</summary>
Motivation: To develop a high-order numerical integrator that preserves first integrals (like energy in Hamiltonian systems) without requiring derivative computations, making it suitable for problems where derivatives are expensive or unavailable.

Method: Combines order theory with symmetrized Itoh-Abe discrete gradient and finite differences to create a fourth-order implicit scheme. Uses Newton's method for solving with convergence analysis accounting for finite difference errors.

Result: Successfully constructs a derivative-free fourth-order method that preserves integrals. Numerical experiments confirm the order and show the method is faster than using automatic differentiation for derivatives. Also demonstrates application to Hamiltonian systems with data-based potentials.

Conclusion: The derivative-free discrete gradient method provides an efficient, high-order integrator for preserving integrals in ODEs, particularly useful for problems with expensive derivatives or when combining analytical and data-based potentials.

Abstract: Discrete gradient methods are a class of numerical integrators producing solutions with exact preservation of first integrals of ordinary differential equations. In this paper, we apply order theory combined with the symmetrized Itoh--Abe discrete gradient and finite differences to construct an integral-preserving fourth-order method that is derivative-free. The numerical scheme is implicit and a convergence result for Newton's iterations is provided, taking into account how the error due to the finite difference approximations affects the convergence rate. Numerical experiments verify the order and show that the derivative-free method is significantly faster than obtaining derivatives by automatic differentiation. Finally, an experiment using topographic data as the potential function of a Hamiltonian oscillator demonstrates how this method allows the simulation of discrete-time dynamics from a Hamiltonian that is a combination of data and analytical expressions.

</details>


### [14] [On spectral properties and fast initial convergence of the Kaczmarz method](https://arxiv.org/abs/2601.07498)
*Per Christian Hansen,Michiel E. Hochstenbach*

Main category: math.NA

TL;DR: Spectral and statistical analysis of Kaczmarz method (ART) explains its fast initial convergence and semi-convergence behavior in solving linear inverse problems.


<details>
  <summary>Details</summary>
Motivation: The Kaczmarz method (ART) shows fast initial convergence in practice, similar to simultaneous iterative methods, but its nonsymmetric iteration operator makes this behavior theoretically unexplained. Understanding this fast initial convergence and semi-convergence patterns has been an open question.

Method: Performed spectral analysis of Kaczmarz's method to understand its iteration operator properties, and conducted statistical analysis of how data noise propagates through iteration vectors.

Result: The analysis provides new insight into the fast initial convergence behavior of Kaczmarz method and sheds light on the semi-convergence phenomenon, explaining how data noise affects the iteration process.

Conclusion: The spectral and statistical analyses offer theoretical explanations for the observed practical behavior of Kaczmarz method, addressing the previously open question about its fast initial convergence and semi-convergence patterns.

Abstract: The Kaczmarz method is successfully used for solving discretizations of linear inverse problems, especially in computed tomography where it is known as ART. Practitioners often observe and appreciate its fast convergence in the first few iterations, leading to the same favorable semi-convergence that we observe for simultaneous iterative reconstruction methods. While the latter methods have symmetric and positive definite iteration operators that facilitate their analysis, the operator in Kaczmarz's method is nonsymmetric and it has been an open question so far to understand this fast initial convergence. We perform a spectral analysis of Kaczmarz's method that gives new insight into its (often fast) initial behavior. We also carry out a statistical analysis of how the data noise enters the iteration vectors, which sheds new light on the semi-convergence. Our results are illustrated with several numerical examples.

</details>


### [15] [Multiword matrix multiplication over large finite fields in floating-point arithmetic](https://arxiv.org/abs/2601.07508)
*Jérémy Berthomieu,Stef Graillat,Dimitri Lesnoff,Theo Mary*

Main category: math.NA

TL;DR: New multiword decomposition method enables efficient modular matrix multiplication with larger prime bitsizes using floating-point arithmetic, overcoming previous half-mantissa limitations.


<details>
  <summary>Details</summary>
Motivation: Existing modular matrix multiplication methods using floating-point arithmetic are limited to primes with bitsize at most half the mantissa size (e.g., 26 bits for double precision), becoming inefficient near this limit. There's a need to handle larger prime bitsizes efficiently for computer algebra applications.

Method: Proposes multiword decompositions that represent matrices A and B as scaled sums of u and v matrices (words) with smaller coefficients. Uses rigorous analysis to prove correctness for chosen scaling parameters and determines maximum bitsize of p that can be handled for given number of words.

Result: Two-word decompositions can handle prime bitsizes almost equal to full mantissa size (e.g., raising limit from 26 to 52 bits in double precision). (1,v) decompositions with v>1 handle intermediate bitsizes. Experimental benchmarks on CPU/GPU show the approach outperforms existing single-word method for bitsizes as low as 23 and handles bitsizes up to 52 with high performance.

Conclusion: The multiword decomposition approach successfully overcomes the half-mantissa limitation of existing modular matrix multiplication methods, enabling efficient computation with much larger prime bitsizes while maintaining high performance on modern hardware architectures.

Abstract: This article is concerned with the efficient computation of modular matrix multiplication C=AB mod p, a key kernel in computer algebra. We focus on floating-point arithmetic, which allows for using efficient matrix multiplication libraries. However, the existing approach is limited to primes p with bitsize at most half the mantissa size (e.g., 26 bits with double precision arithmetic), and becomes quite inefficient when p approaches this limit. We present a new approach that overcomes this limitation and can efficiently handle primes with larger bitsizes. The key idea is to use multiword decompositions, which represent A and B as scaled sums of u and v matrices (words) with smaller coefficients. We provide a rigorous analysis that proves the correctness of this approach for suitably chosen scaling parameters. Our analysis determines the maximum bitsize of p that can be handled for a given number of words; in particular, we show that decomposing in two words each input suffices to handle bitsizes almost equal to the full mantissa size (e.g., the 26 bits limit is raised to 52 bits in double precision arithmetic). Moreover, we show that (1,v) decompositions with v>1 are also of interest to handle intermediate bitsizes. We perform an extensive experimental analysis for various matrix shapes and prime bitsizes. Our performance benchmarks on both CPU and GPU architectures confirm the efficiency of the proposed approach, which can outperform the existing single word approach for bitsizes as low as 23, and can handle bitsizes as high as 52 while retaining high performance.

</details>


### [16] [A higher order polytopal method for contact mechanics with Tresca friction](https://arxiv.org/abs/2601.07586)
*Jerome Droniou,Raman Kumar,Roland Masson,Ritesh Singla*

Main category: math.NA

TL;DR: DDR scheme for contact mechanics with Tresca friction on fractures using mixed formulation with displacement field and Lagrange multiplier, proving discrete Korn inequality and inf-sup condition, robust in quasi-incompressible limit.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical scheme for contact mechanics problems involving fractures with Tresca friction, addressing limitations of usual low-order nodal-based schemes that lack robustness in quasi-incompressible limits.

Method: Discrete de Rham (DDR) scheme using mixed formulation with displacement field (piecewise quadratic reconstruction from vertex/edge/face/element values) and Lagrange multiplier (piecewise constant vectors on fracture faces) to enforce contact conditions.

Result: Proved discrete Korn inequality accounting for fractures and inf-sup condition in non-standard H^{-1/2}-norm; scheme is robust in quasi-incompressible limit for displacement variable; extensive numerical experiments confirm theoretical analysis.

Conclusion: The proposed DDR scheme provides an accurate and robust numerical method for contact mechanics with fractures and Tresca friction, overcoming limitations of conventional low-order schemes in quasi-incompressible regimes.

Abstract: In this work, we design and analyze a Discrete de Rham (DDR) scheme for a contact mechanics problem involving fractures along which a model of Tresca friction is considered. Our approach is based on a mixed formulation involving a displacement field and a Lagrange multiplier, enforcing the contact conditions, representing tractions at fractures. The approximation space for the displacement is made of vectors values attached to each vertex, edge, face, and element, while the Lagrange multiplier space is approximated by piecewise constant vectors on each fracture face. The displacement degrees of freedom allow reconstruct piecewise quadratic approximations of this field. We prove a discrete Korn inequality that account for the fractures, as well as an inf-sup condition (in a non-standard $H^{-1/2}$-norm) between the discrete Lagrange multiplier space and the discrete displacement space. We provide an in-depth error analysis of the scheme and show that, contrary to usual low-order nodal-based schemes, our method is robust in the quasi-incompressible limit for the primal variable~(displacement). An extensive set of numerical experiments confirms the theoretical analysis and demonstrate the practical accuracy and robustness of the scheme.

</details>


### [17] [TMATDG: applying TDG methods to multiple scattering via T-matrix approximation](https://arxiv.org/abs/2601.07704)
*Armando Maria Monforte*

Main category: math.NA

TL;DR: MATLAB package combining Trefftz Discontinuous Galerkin methods with T-matrix method for solving multiple scattering problems with polygonal obstacles


<details>
  <summary>Details</summary>
Motivation: To provide an efficient computational framework for solving multiple scattering problems involving polygonal obstacles, which are common in wave propagation scenarios but challenging to handle numerically

Method: Couples Trefftz Discontinuous Galerkin methods for Helmholtz scattering with T-matrix method, using TMATROM package for numerical T-matrix approximation, specifically designed for polygonal obstacles

Result: Developed a MATLAB package that provides a computational framework for solving multiple scattering problems with polygonal obstacles

Conclusion: The package successfully integrates Trefftz DG methods with T-matrix approach to handle complex multiple scattering problems involving polygonal geometries

Abstract: We present a MATLAB package for the solution of multiple scattering problems, coupling Trefftz Discontinuos Galerkin methods for Helmholtz scattering with the T-matrix method. We rely on the TMATROM package to numerically approximate the T-matrices and deal with multiple scattering problem, providing a framework to handle scattering by polygonal obstacles.

</details>


### [18] [Explicit complex time integrators for stiff problems](https://arxiv.org/abs/2601.07730)
*Jithin D. George,Julian Koellermeier,Samuel Y. Jung,Niall M. Mangan*

Main category: math.NA

TL;DR: Complex time steps in numerical integration provide expanded stability regions and computational advantages, especially for complex-valued systems like the Schrödinger equation, and can benefit real-valued stiff systems when combined with Projective Integration.


<details>
  <summary>Details</summary>
Motivation: Most time integration methods use real-valued time steps, but complex time steps offer an additional degree of freedom by allowing selection of time step magnitude in both real and imaginary directions, potentially expanding stability regions and providing computational advantages.

Method: The paper explores using complex time steps in numerical integration, analyzing specific paths in the complex time plane. For real-valued stiff systems, the method couples complex time steps with Projective Integration techniques.

Result: Complex time steps lead to expanded stability regions, providing clear computational advantages for complex-valued systems. They are uniquely optimal for the Schrödinger equation, and these benefits extend to certain classes of real-valued stiff systems when combined with Projective Integration.

Conclusion: Complex time integration offers significant advantages over traditional real-valued time steps, particularly for complex-valued systems like quantum mechanical equations, and can be effectively applied to stiff real-valued systems through appropriate coupling with existing integration methods.

Abstract: Most numerical methods for time integration use real-valued time steps. Complex time steps, however, can provide an additional degree of freedom, as we can select the magnitude of the time step in both the real and imaginary directions. We show that specific paths in the complex time plane lead to expanded stability regions, providing clear computational advantages for complex-valued systems. In particular, we highlight the Schrödinger equation, for which complex time integrators can be uniquely optimal. Furthermore, we demonstrate that these benefits extend to certain classes of real-valued stiff systems by coupling complex time steps with the Projective Integration method.

</details>


### [19] [On the Compact Discontinuous Galerkin method for polytopal meshes](https://arxiv.org/abs/2601.07757)
*Mattia Corti,Sergio Gómez*

Main category: math.NA

TL;DR: The paper presents stability and convergence analysis for the hp-version Compact Discontinuous Galerkin (CDG) method on polytopal meshes, introduces unified implementation algorithms for CDG/LDG/BR2 methods, and shows CDG's computational advantages through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical analysis for the CDG method (introduced by Peraire & Persson in 2008) applied to elliptic problems on polytopal meshes, develop practical implementation algorithms, and compare computational performance with other DG methods.

Method: hp-version Compact Discontinuous Galerkin (CDG) method for elliptic problems on polytopal meshes; stability and convergence analysis; unified implementation framework for CDG, LDG, and BR2 methods; numerical experiments studying coercivity dependence on parameters and flux directions.

Result: CDG yields compact stencil for stiffness matrix with faster assembly and solving times compared to LDG and BR2; numerical study shows how coercivity depends on method parameters and mesh element facets; demonstrates importance of correct numerical flux directions for variable polynomial degrees.

Conclusion: The CDG method is mathematically sound with proven stability and convergence properties, offers computational advantages through compact stencils and faster performance, and requires careful parameter selection for optimal results, particularly regarding flux directions with variable polynomial degrees.

Abstract: The Compact Discontinuous Galerkin method was introduced by Peraire and Persson in (SIAM J. Sci. Comput., 30, 1806--1824, 2008). In this work, we present the stability and convergence analysis for the $hp$-version of this method applied to elliptic problems on polytopal meshes. Moreover, we introduce fast and practical algorithms that allow the CDG, LDG, and BR2 methods to be implemented within a unified framework. Our numerical experiments show that the CDG method yields a compact stencil for the stiffness matrix, with faster assembly and solving times compared to the LDG and BR2 methods. We numerically study how coercivity depends on the method parameters for various mesh types, with particular focus on the number of facets per mesh element. Finally, we demonstrate the importance of choosing the correct directions for the numerical fluxes when using variable polynomial degrees.

</details>


### [20] [Necessary and Sufficient Conditions for the Existence of an LU Factorization for General Rank Deficient Matrices](https://arxiv.org/abs/2601.07791)
*Eric Darve*

Main category: math.NA

TL;DR: The paper establishes necessary and sufficient conditions for LU factorization without permutations for arbitrary square matrices, including singular cases, and extends to rank-revealing factorizations with sparsity bounds.


<details>
  <summary>Details</summary>
Motivation: To provide a complete characterization of when LU factorization exists without permutations for all square matrices, including singular and rank-deficient cases, which has practical implications for numerical linear algebra and matrix computations.

Method: The authors develop theoretical conditions based on nullity relationships between leading principal submatrices and corresponding column/row blocks. They build upon Okunev and Johnson's work but provide simpler, constructive proofs. They also extend the analysis to rank-revealing factorizations and derive sparsity bounds.

Result: Proved that LU factorization without permutations exists if and only if the nullity of every leading principal submatrix is bounded by the sum of nullities of corresponding leading column and row blocks. Extended results to rank-revealing factorizations with explicit sparsity bounds, and derived conditions for factorizations with unit triangular factors.

Conclusion: The paper provides a complete theoretical framework for understanding when LU factorization without permutations is possible for arbitrary square matrices, offering constructive proofs and practical extensions to rank-revealing factorizations with computational implications.

Abstract: We establish necessary and sufficient conditions for the existence of an LU factorization $A=LU$ for an arbitrary square matrix $A$, including singular and rank-deficient cases, without the use of row or column permutations. We prove that such a factorization exists if and only if the nullity of every leading principal submatrix is bounded by the sum of the nullities of the corresponding leading column and row blocks. While building upon the work of Okunev and Johnson, we present simpler, constructive proofs. Furthermore, we extend these results to characterize rank-revealing factorizations, providing explicit sparsity bounds for the factors $L$ and $U$. Finally, we derive analogous necessary and sufficient conditions for the existence of factorizations constrained to have unit lower or unit upper triangular factors.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [21] [Dynamics for a viscoelastic beam equation with past history and nonlocal boundary dissipation](https://arxiv.org/abs/2601.06414)
*Linfang Liu,Vando Narciso,Zhijian Yang*

Main category: math.AP

TL;DR: Study of long-time dynamics for viscoelastic plate equation with nonlinear/nonlocal boundary conditions, proving existence of compact global attractor and fractal exponential attractor with finite dimension.


<details>
  <summary>Details</summary>
Motivation: Extend previous work by Cavalcanti (2002) on viscoelastic plate equation from τ=0 to τ=-∞ case, analyze autonomous equivalent problem to understand long-term behavior and attractor properties of the dynamical system.

Method: Consider autonomous equivalent problem with τ=-∞, analyze dynamical system generated by weak solutions, prove existence of compact global attractor in weak phase space topology, and establish finite dimension and smoothness in subcritical case. For Hook Law force, prove existence of fractal exponential attractor with finite dimension in extended space.

Result: Proved that dynamical system has compact global attractor in weak phase space topology, which has finite dimension and smoothness in subcritical case. Additionally, when force follows Hook Law, system possesses fractal exponential attractor with finite dimension in extended space.

Conclusion: The viscoelastic plate equation with nonlinear/nonlocal boundary conditions exhibits rich long-time dynamics with compact attractors having finite dimension properties, extending previous results and providing deeper understanding of the system's asymptotic behavior.

Abstract: This article aims to study the long-time dynamics of the linear viscoelastic plate equation $\displaystyle{u_{tt}+Δ^2 u-\int_τ^tg(t-s)Δ^2u(s)ds=0}$ subject to nonlinear and nonlocal boundary conditions. This model, with $τ=0$, was first considered by Cavalcanti (Discrete Contin. Dyn. Syst., 8(3), 675-695, 2002), where results of global existence and uniform decay rates of energy have been established. In this work, by taking $τ=-\infty$, and considering the autonomous equivalent problem we prove that the dynamical system $(\mathcal{H},S_t)$ generated by the weak solutions has a compact global attractor $\mathfrak{A}$ (in the topology of the weak phase space $\mathcal{H}$), which in subcritical case has finite dimension and smoothness. Furthermore, when the force follows the {\it Hook Law}, we prove that $(\mathcal{H},S_t)$ possesses a (generalized) fractal exponential attractor $\mathfrak{A}_{\exp}$ with finite dimension in a space $\widetilde{\mathcal{H}}\supset\mathcal{H}$.

</details>


### [22] [Plastic limit of a viscoplastic Burgers equation -- A toy model for sea-ice dynamics](https://arxiv.org/abs/2601.06489)
*Xin Liu,Marita Thomas,Edriss S. Titi*

Main category: math.AP

TL;DR: The paper studies the plastic Burgers equation with 1-Laplacian regularization, analyzes its viscoplastic approximation, proves existence of BV-solutions, and connects it to sea-ice dynamics modeling.


<details>
  <summary>Details</summary>
Motivation: The plastic Burgers equation is interesting both as a mathematical model itself and as a one-dimensional version of Hibler's sea-ice dynamics model, where the 1-Laplacian captures plastic effects in ice behavior.

Method: Start from a viscoplastic Burgers equation (regularized version with parameter ε>0), construct global BV-solutions for the regularized problem, then take the singular limit ε→0 to obtain solutions for the plastic Burgers equation.

Result: Proved existence of BV-solutions for both the viscoplastic Burgers equation and the plastic Burgers equation in the singular limit, showing the limiting stress term belongs to the subdifferential of the total variation functional.

Conclusion: Successfully analyzed the plastic Burgers equation using regularization and singular limit techniques, establishing mathematical foundations for this model relevant to sea-ice dynamics.

Abstract: We study the plastic Burgers equation in one space dimension, i.e., the Burgers equation featuring an additional term formally given by the p-Laplacian with p=1, or rather, by the multivalued subdifferential of the total variation functional. Our study highlights that the interplay of the advection term with the stresses given by the multivalued 1-Laplacian is a crucial feature of this model. Eventhough it is an interesting model in itsef, it can also be regarded as a one-dimensional version of the momentum balance of Hibler's model for sea-ice dynamics. Therein, the stress tensor is given by a term with similar properties as the 1-Laplacian in order to account for plastic effects of the ice. For our analysis we start out from a viscoplastic Burgers equation, i.e., a suitably regularized version of the plastic Burgers equation with a small regularization parameter $\varepsilon>0$. For the viscoplastic Burgers equation, we construct a global BV-solution. In the singular limit $\varepsilon\to0$ we deduce the existence of a BV-solution for the plastic Burgers equation. In addition we show that the term arising as the limit of the regularized stresses is indeed related to an element of the subdifferential of the total variation functional.

</details>


### [23] [Minimality of free-boundary axial hyperplanes in high dimensional circular cones via calibration](https://arxiv.org/abs/2601.06601)
*Giacomo Vianello*

Main category: math.AP

TL;DR: For n≥4 and sufficiently large aperture, the intersection of an (n+1)-dimensional circular cone with an axial hyperplane is area-minimizing with respect to free-boundary variations inside the cone.


<details>
  <summary>Details</summary>
Motivation: To understand the minimal surface properties of intersections between high-dimensional cones and hyperplanes, particularly under free-boundary conditions within the cone.

Method: Uses a calibration argument to prove area-minimization properties for the intersection of an (n+1)-dimensional circular cone with an axial hyperplane.

Result: Proves that when n≥4 and the cone's aperture is sufficiently large, the intersection is area-minimizing with respect to free-boundary variations inside the cone.

Conclusion: The intersection of high-dimensional cones with axial hyperplanes exhibits area-minimizing properties under certain geometric conditions, providing insights into minimal surface theory in higher dimensions.

Abstract: Consider an $(n+1)$-dimensional circular cone. Using a calibration argument, we prove that if $n \geq 4$ and the aperture of the cone is sufficiently large, the intersection of the cone with an axial hyperplane is area-minimizing with respect to free-boundary variations inside the cone.

</details>


### [24] [Global Well-Posedness of the Vacuum Free Boundary Problem for the Degenerate Compressible Navier-Stokes Equations With Large Data of Spherical Symmetry](https://arxiv.org/abs/2601.06620)
*Gui-Qiang G. Chen,Jiawen Zhang,Shengguo Zhu*

Main category: math.AP

TL;DR: Global well-posedness of classical solutions for barotropic compressible Navier-Stokes equations with degenerate viscosity in spherical symmetry, proving no vacuum forms inside fluid for large initial data.


<details>
  <summary>Details</summary>
Motivation: The study of global-in-time dynamics of vacuum is crucial for viscous flows, but large-data problems for multidimensional spherically symmetric flows have remained open due to coordinate singularity at origin and strong degeneracy on moving boundary.

Method: Region-segmentation method: near origin, develop interior BD entropy estimate for flow-map-weighted density estimates; near boundary, construct ρ₀-weighted estimates for effective velocity, yielding novel flow-map-weighted estimates for both fluid and effective velocities.

Result: Prove that for spherically symmetric initial density ρ₀^β ∈ H³ with β ∈ (1/3, γ-1] vanishing on moving boundary as distance function, no vacuum forms inside fluid in finite time, and establish global well-posedness of classical solutions with large initial data.

Conclusion: The methodology developed solves long-standing open problem and should be useful for other related nonlinear equations with similar difficulties, particularly handling physical vacuum case when β=γ-1 which fails BD entropy condition when γ≥2.

Abstract: The study of global-in-time dynamics of vacuum is crucial for understanding viscous flows. However, the corresponding large-data problems for multidimensional spherically symmetric flows have remained open, due to the coordinate singularity at the origin and the strong degeneracy on the moving boundary. In this paper, we analyze the vacuum free boundary problem for the barotropic compressible Navier-Stokes equations with degenerate density-dependent viscosity coefficients (as in the shallow water equations) in two and three spatial dimensions. We prove that, for a general class of spherically symmetric initial density: $ρ_0^β\in H^3$ with $β\in (\frac{1}{3},γ-1]$ ($γ$: adiabatic exponent) vanishing on the moving boundary in the form of a distance function, no vacuum forms inside the fluid in finite time, and we establish the global well-posedness of classical solutions with large initial data. Particularly, when $β=γ-1$, the initial density contains a physical vacuum, but fails to satisfy the condition required for the Bresch-Desjardins (BD) entropy estimate when $γ\ge 2$. Our analysis is mainly based on a region-segmentation method: near the origin, we develop an interior BD entropy estimate, thereby obtaining flow-map-weighted estimates for the density; while, near the boundary, we construct $ρ_0$-weighted estimates for the effective velocity, which differ fundamentally from the classical BD entropy estimates and yield novel flow-map-weighted estimates for both the fluid and the effective velocities. These estimates enable us to obtain the uniform upper bound for the density and show that no cavitation occurs inside the fluid. The methodology developed here should also be useful for solving other related nonlinear equations involving similar difficulties.

</details>


### [25] [Necessary and sufficient condition for existence at resonance for eigenvalues of multiplicity two](https://arxiv.org/abs/2601.06623)
*Philip Korman*

Main category: math.AP

TL;DR: Establishes existence conditions for semilinear Dirichlet problems with linear part at resonance at double eigenvalues, and applies this to determine unboundedness of all solutions for corresponding semilinear heat equations.


<details>
  <summary>Details</summary>
Motivation: To analyze semilinear Dirichlet problems where the linear operator is at resonance (eigenvalue problems), particularly focusing on cases where eigenvalues have multiplicity two, which presents additional mathematical challenges compared to simple eigenvalues.

Method: Develops necessary and sufficient conditions for solution existence using analytical methods for semilinear elliptic problems with resonance at double eigenvalues, then applies these results to study the corresponding parabolic (heat) equation.

Result: Obtains complete characterization (necessary and sufficient conditions) for existence of solutions to the semilinear Dirichlet problem, and derives conditions under which all solutions of the corresponding semilinear heat equation become unbounded.

Conclusion: The paper provides rigorous mathematical criteria for solution existence in resonant semilinear problems with double eigenvalues, with important implications for understanding solution behavior in related parabolic equations, particularly unboundedness phenomena.

Abstract: We establish necessary and sufficient condition for existence of solutions for a class of semilinear Dirichlet problems with the linear part at resonance at eigenvalues of multiplicity two. The result is applied to give a condition for unboundness of all solutions of the corresponding semilinear heat equation.

</details>


### [26] [Extinction and persistence criteria in non-local Klausmeier model of vegetation dynamics on flat landscapes](https://arxiv.org/abs/2601.06681)
*Maciej Tadej,Ricardo Martinez-Garcia,Michael Hecht*

Main category: math.AP

TL;DR: Non-local plant dispersal in water-limited ecosystems enables vegetation persistence in smaller habitats than classical diffusion models predict, with fat-tailed dispersal kernels enhancing resilience.


<details>
  <summary>Details</summary>
Motivation: To understand how non-local plant dispersal affects vegetation pattern dynamics and survival thresholds in water-limited ecosystems, particularly in fragmented habitats where classical diffusion models may overestimate extinction risks.

Method: Developed a generalized Klausmeier model with non-local plant dispersal in finite habitats, established well-posedness, derived analytical conditions for vegetation survival, determined stability criteria for stationary solutions, and conducted numerical experiments comparing sub-Gaussian and super-Gaussian dispersal kernels.

Result: Identified a critical patch size threshold for vegetation survival (trade-off between local growth and boundary losses), derived a critical maximal biomass density for collapse to desert state, established stability criteria for stationary solutions, and found that non-local dispersal (especially fat-tailed kernels) enhances ecosystem resilience by allowing persistence in smaller habitats.

Conclusion: Non-local dispersal mechanisms significantly improve vegetation resilience in water-limited ecosystems, enabling persistence in smaller, fragmented habitats than predicted by classical local diffusion models, with fat-tailed dispersal kernels being particularly effective.

Abstract: This paper investigates the dynamics of vegetation patterns in water-limited ecosystems using a generalized Klausmeier model that incorporates non-local plant dispersal within a finite habitat. We establish the well-posedness of the system and provide a rigorous analysis of the conditions required for vegetation survival. Our results identify a critical patch size governed by the trade-off between local growth and boundary losses; habitats smaller than this threshold lead to inevitable extinction. Furthermore, we derive a critical maximal biomass density below which the population collapses to a desert state, regardless of the domain size. We determine stability criteria for stationary solutions and describe the emergence of stable, non-trivial biomass distributions. Numerical experiments comparing sub-Gaussian and super-Gaussian kernels confirm that non-local dispersal mechanisms, particularly those with fat tails, enhance ecosystem resilience by allowing vegetation to persist in smaller, fragmented habitats than predicted by classical local diffusion models.

</details>


### [27] [Homogenization of Lévy-type operators: operator estimates with correctors](https://arxiv.org/abs/2601.06832)
*Andrey Piatnitski,Vladimir Sloushch,Tatiana Suslina,Elena Zhizhina*

Main category: math.AP

TL;DR: The paper studies homogenization of a nonlocal operator with periodic coefficients, obtaining higher-order approximations of the resolvent using correctors.


<details>
  <summary>Details</summary>
Motivation: To improve previous homogenization results for nonlocal operators by obtaining more accurate approximations that include corrector terms, going beyond the first-order convergence rate.

Method: Analyzes a self-adjoint nonlocal operator with periodic coefficients using quadratic form techniques, develops corrector expansions based on the parameter α, and proves operator norm convergence estimates.

Result: Achieves O(ε) approximation error for the resolvent by including corrector terms up to order N, where N depends on α, significantly improving the previous O(ε^{2-α}) bound.

Conclusion: The paper successfully develops a higher-order homogenization theory for nonlocal operators with periodic coefficients, providing refined approximations with correctors that yield substantially improved convergence rates.

Abstract: The goal of the paper is to study in $L_2(\R^d)$ a self-adjoint operator ${\mathbb A}_\eps$, $\eps >0$, of the form $$ ({\mathbb A}_\eps u) (\x) = \int_{\R^d} μ(\x/\eps, \y/\eps) \frac{\left( u(\x) - u(\y) \right)}{|\x - \y|^{d+α}}\,d\y $$ with $1< α< 2$;
  here the function
  $μ(\x,\y)$ is $\Z^d$-periodic in the both variables, satisfies the symmetry relation $μ(\x,\y) = μ(\y,\x)$ and
  the estimates $0< μ_- \leqslant μ(\x,\y) \leqslant μ_+< \infty$. The rigorous definition of the operator ${\mathbb A}_\eps$ is given in terms of the corresponding quadratic form. In the previous work of the authors it was shown that the resolvent $({\mathbb A}_\eps + I)^{-1}$ converges, as $\eps\to0$, in the operator norm in $L_2(\mathbb R^d)$ to the resolvent of the effective operator $A^0$, and the estimate $\|({\mathbb A}_\eps + I)^{-1} - (\A^0 + I)^{-1} \| = O(\eps^{2-α})$ holds. In the present work we achieve a more accurate approximation of the resolvent of ${\mathbb A}_\eps$ which takes into account the correctors. Namely, for $N\in\mathbb N$ such that $2-1/N < α\le 2-1/(N+1)$, we obtain $$ \bigl\|({\mathbb A}_\eps + I)^{-1} - (\A^0 + I)^{-1} - \sum_{m=1}^N \eps^{m(2-α)} \mathbb{K}_m \bigr\| = O(\eps). $$

</details>


### [28] [Global regularity and sharp decay to the 2D Hypo-Viscous compressible Navier-Stokes equations](https://arxiv.org/abs/2601.06889)
*Chen Liang,Zhaonan Luo,Zhaoyang Yin*

Main category: math.AP

TL;DR: Global regularity and optimal time decay for 2D isentropic hypo-viscous compressible Navier-Stokes equations with small initial data near equilibrium.


<details>
  <summary>Details</summary>
Motivation: Study the long-time behavior and regularity properties of 2D compressible fluid equations with hypo-viscous dissipation, which is important for understanding turbulent flows and energy dissipation mechanisms.

Method: 1. Prove global existence of strong solutions with small initial data in H^s (s>1) framework near equilibrium. 2. Use improved Fourier splitting method and Littlewood-Paley decomposition to establish optimal time decay rates for low regularity data.

Result: 1. Existence of global strong solutions for small initial data near equilibrium. 2. Optimal time decay rates established for solutions with low regularity initial data.

Conclusion: The paper successfully establishes both global regularity and optimal time decay properties for 2D isentropic hypo-viscous compressible Navier-Stokes equations, providing important insights into the long-time behavior of such fluid systems.

Abstract: In this paper, we consider the global regularity and the optimal time decay rate for the 2D isentropic hypo-viscous compressible Navier-Stokes equations. Firstly, we prove that there exists a global strong solution with the small initial data are close to the constant equilibrium state in $H^s$ framework with $s>1$. Furthermore, by virtue of improved Fourier splitting method and the Littlewood-Paley decomposition theory, we then establish the optimal time decay rate for low regularity data.

</details>


### [29] [Existence results for a non-relativistic Chern-Simons model with purely mutual interaction](https://arxiv.org/abs/2601.06901)
*Aleks Jevnikar,Sang-Hyuck Moon*

Main category: math.AP

TL;DR: Existence and multiplicity results for a skew-symmetric singular Liouville system from Chern-Simons theory using constrained variational methods and Morse theory.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a skew-symmetric singular Liouville system that arises in non-relativistic Chern-Simons theory. Such systems are important in theoretical physics but present mathematical challenges due to their indefinite energy functionals.

Method: The authors overcome the difficulty of indefinite energy functionals by introducing a suitable constrained variational problem and implementing Morse-theoretical arguments. This approach allows them to bypass the limitations of standard variational methods.

Result: The paper establishes existence and multiplicity results for solutions to the skew-symmetric singular Liouville system, proving that multiple solutions exist under appropriate conditions.

Conclusion: The constrained variational approach combined with Morse theory provides an effective framework for studying indefinite energy functionals in singular Liouville systems from Chern-Simons theory, yielding both existence and multiplicity of solutions.

Abstract: We are concerned with a skew-symmetric singular Liouville system arising in non-relativistic Chern-Simons theory. Based on its variational structure, we establish existence and multiplicity results. Since the energy functional is indefinite, standard variational approaches do not apply directly. We overcome this difficulty by introducing a suitable constrained problem and implementing a Morse-theoretical argument

</details>


### [30] [Asymptotic formulas for phase recovering from phaseless data of biharmonic waves at a fixed frequency](https://arxiv.org/abs/2601.06917)
*Yuxiang Cheng,Xiaoxu Xu,Haiwen Zhang*

Main category: math.AP

TL;DR: Phase retrieval from phaseless total-field data in biharmonic scattering problems: uniqueness and asymptotic formulas.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of phase retrieval in biharmonic scattering problems where only phaseless total-field data (modulus measurements) are available, which is common in practical applications where phase information is difficult to measure.

Method: The authors prove uniqueness results showing that a phased biharmonic wave can be uniquely determined by its modulus within a nonempty domain. They use mathematical analysis techniques and derive explicit asymptotic formulas for phase retrieval using Atkinson-type asymptotic expansion.

Result: Two main results: (1) Uniqueness theorem establishing that phased biharmonic waves can be uniquely determined from phaseless total-field data; (2) Explicit asymptotic formulas for phase retrieval derived from Atkinson-type expansion, providing practical reconstruction methods.

Conclusion: The paper establishes theoretical foundations for phase retrieval in biharmonic scattering, proving uniqueness and providing explicit reconstruction formulas, which advances the mathematical understanding of inverse scattering problems with phaseless data.

Abstract: This paper focuses on phase retrieval from phaseless total-field data in biharmonic scattering problems. We prove that a phased biharmonic wave can be uniquely determined by the modulus of the total biharmonic wave within a nonempty domain. As a direct corollary, the uniqueness for the inverse biharmonic scattering problem with phaseless total-field data is established. Moreover, using the Atkinson-type asymptotic expansion, we derive explicit asymptotic formulas for the problem of phase retrieval.

</details>


### [31] [Lebesgue points of measures and non tangential convergence of Poisson-Hermite integrals](https://arxiv.org/abs/2601.07063)
*Guillermo Flores,Gustavo Garrigós,Beatriz Viviani*

Main category: math.AP

TL;DR: The paper studies differentiability conditions for complex measures in relation to boundary convergence of Poisson-type integrals associated with the Hermite operator, establishing equivalence between Lebesgue points and non-tangential convergence.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between differentiability conditions on complex measures and boundary convergence properties of Poisson-type integrals associated with the Hermite operator, particularly extending classical results to this setting.

Method: Analyzes differentiability conditions (Lebesgue points, σ-points) for complex measures and studies the boundary convergence of the Poisson-type integral P_tν = e^{-t√L}ν, where L = -Δ + |x|^2 is the Hermite operator, using non-tangential convergence methods.

Result: Shows that x_0 is a Lebesgue point for ν if and only if a slightly stronger notion than non-tangential convergence holds for P_tν at x_0. Also proves non-tangential convergence when x_0 is a σ-point of ν, which for d=1 coincides with the classical Fatou condition.

Conclusion: Establishes precise connections between measure differentiability conditions and boundary convergence properties for Hermite operator Poisson integrals, extending classical Fatou-type theorems to this context with both equivalence results and weaker sufficient conditions.

Abstract: We study differentiability conditions on a complex measure $ν$ at a point $x_0\in\mathbb{R}^d$, in relation with the boundary convergence at that point of the Poisson-type integral $P_tν=e^{-t\sqrt L}ν$, where $L=-Δ+|x|^2$ is the Hermite operator. In particular, we show that $x_0$ is a Lebesgue point for $ν$ iff a slightly stronger notion than non-tangential convergence holds for $P_tν$ at $x_0$. We also show non-tangential convergence when $x_0$ is a $σ$-point of $ν$, a weaker notion than Lebesgue point, which for $d=1$ coincides with the classical Fatou condition.

</details>


### [32] [An Inverse Almost Periodic Problem for a Semilinear Strongly Damped Wave Equation](https://arxiv.org/abs/2601.07081)
*Irina Kmit,Nataliya Protsakh,Viktor Tkachenko*

Main category: math.AP

TL;DR: Inverse boundary value problem for semilinear strongly damped wave equation with Dirichlet boundary conditions in Sobolev spaces, determining both weak solution and source coefficient using integral-type overdetermination.


<details>
  <summary>Details</summary>
Motivation: To solve inverse problems for semilinear strongly damped wave equations where both the solution and an unknown source coefficient need to be determined simultaneously, particularly in the context of bounded functions including almost periodic and periodic cases.

Method: Reduces inverse problem to direct problem, then solves in steps: 1) existence/uniqueness of weak solution on finite time interval, 2) extension to semiaxis t≥0, 3) further extension to all t∈R, 4) establishing almost periodic/periodic properties when data have those properties.

Result: Proves existence and uniqueness of weak solution, successful extension to entire real line, and preservation of almost periodic/periodic properties when initial data possess those characteristics.

Conclusion: The inverse boundary value problem for semilinear strongly damped wave equations is well-posed with integral-type overdetermination, and solutions inherit almost periodic/periodic properties from the data, providing comprehensive existence and regularity results.

Abstract: The paper investigates an inverse boundary value problem for a semilinear strongly damped wave equation with the Dirichlet boundary condition in Sobolev spaces of bounded (in particular, almost periodic and periodic) functions. In addition to finding a weak solution, we also determine a source coefficient in the right-hand side of the differential equation. To make the problem well-posed, an integral-type overdetermination condition is imposed. After reducing the inverse problem to a direct one, we solve the latter in several steps. First, we prove the existence and uniqueness of a weak solution to the corresponding initial-boundary value problem on a finite time interval. Next, we show that this solution can be extended in a bounded way to the semiaxis $t\ge 0$. In the following step, we further extend this bounded solution to all $t\in R$. Finally, we establish that if the data of the original problem are almost periodic (or periodic), then the resulting bounded weak solution is itself almost periodic (or periodic).

</details>


### [33] [Local and global $C^{1,β}$-regularity for uniformly elliptic quasilinear equations of $p$-Laplace and Orlicz-Laplace type](https://arxiv.org/abs/2601.07140)
*Carlo Alberto Antonini*

Main category: math.AP

TL;DR: The paper establishes gradient Hölder continuity for solutions to quasilinear elliptic equations including p-Laplace and Orlicz-Laplace operators, improving existing results for both interior and boundary regularity.


<details>
  <summary>Details</summary>
Motivation: To improve gradient regularity results for quasilinear elliptic equations, addressing limitations in existing literature and extending regularity to boundary conditions.

Method: Revisits and improves existing approaches for gradient Hölder continuity, applying techniques to p-Laplace and Orlicz-Laplace type operators with Dirichlet or Neumann boundary conditions.

Result: Establishes gradient Hölder continuity for solutions to quasilinear uniformly elliptic equations, achieving both interior and boundary regularity under various boundary conditions.

Conclusion: The paper provides enhanced gradient regularity results for important classes of quasilinear elliptic operators, extending previous work and covering both interior and boundary cases.

Abstract: We establish gradient Hölder continuity for solutions to quasilinear, uniformly elliptic equations, including $p$-Laplace and Orlicz-Laplace type operators. We revisit and improve upon the results existing in the literature, proving gradient regularity both in the interior and up to the boundary, under Dirichlet or Neumann boundary conditions.

</details>


### [34] [Uniform bounds for Neumann heat kernels and their traces in convex sets](https://arxiv.org/abs/2601.07341)
*Rupert L. Frank,Simon Larson*

Main category: math.AP

TL;DR: The paper proves uniform bounds on the heat trace for Neumann Laplacian on convex domains that capture first two small-time asymptotic terms, valid for all times with simple geometric dependence.


<details>
  <summary>Details</summary>
Motivation: To establish uniform bounds on heat trace asymptotics that work for all time scales (not just small-time) and depend only on simple geometric characteristics of the domain, providing more robust results than traditional asymptotic expansions.

Method: Proves bounds via precise and uniform expansion of the on-diagonal heat kernel near the boundary, using analysis of Neumann Laplacian properties. Methods extend to non-convex domains and Lipschitz domains for two-term asymptotics.

Result: Obtains bounds capturing first two terms in small-time expansion of heat trace for Neumann Laplacian on convex domains, valid for all times with dependence only on simple geometric characteristics. Results extend to non-convex and Lipschitz domains.

Conclusion: The paper provides uniform bounds on heat trace asymptotics that bridge small-time expansions with all-time validity, offering simpler geometric dependence and broader applicability beyond convex domains to Lipschitz domains.

Abstract: We prove a bound on the heat trace of the Neumann Laplacian on a convex domain that captures the first two terms in its small-time expansion, but is valid for all times and depends on the underlying domain only through very simple geometric characteristics. This is proved via a precise and uniform expansion of the on-diagonal heat kernel close to the boundary. Most of our results are valid without the convexity assumption and we also consider two-term asymptotics for the heat trace for Lipschitz domains.

</details>


### [35] [Stationary internal waves in a two-dimensional aquarium at low viscosity](https://arxiv.org/abs/2601.07391)
*Malo Jézéquel,Jian Wang*

Main category: math.AP

TL;DR: Proves uniform solvability of stationary internal waves with small viscosity in 2D analytic aquariums under Morse-Smale conditions using complex deformations.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical results for stationary internal wave problems with viscosity in realistic physical settings (aquariums with analytic boundaries).

Method: Uses complex deformations of the aquarium domain to make the inviscid stationary internal wave operator invertible, enabling analysis of the viscous problem.

Result: Proves uniform solvability (existence and uniqueness) of the stationary problem with small viscosity under Morse-Smale dynamical assumptions.

Conclusion: The complex deformation technique successfully establishes rigorous mathematical foundations for internal wave problems in physically relevant domains with viscosity.

Abstract: We prove the uniform solvability of a stationary problem associated to internal waves equation with small viscosity in a two dimensional aquarium with real-analytic boundary, under a Morse--Smale dynamical assumption. This is achieved by using complex deformations of the aquarium, on which the inviscid stationary internal wave operator is invertible.

</details>


### [36] [Classification of single-bubble blow-up solutions for Calogero--Moser derivative nonlinear Schrödinger equation](https://arxiv.org/abs/2601.07410)
*Uihyeon Jeong,Kihyun Kim,Taegyu Kim,Soonsik Kwon*

Main category: math.AP

TL;DR: The paper provides a sharp classification of finite-time blow-up dynamics for the Calogero-Moser derivative nonlinear Schrödinger equation, identifying quantized and exotic blow-up regimes based on initial data regularity.


<details>
  <summary>Details</summary>
Motivation: Previous work established finite-time blow-up constructions and soliton resolution for CM-DNLS, but lacked a complete classification of blow-up dynamics. This paper aims to go beyond soliton resolution to provide a sharp classification of all possible blow-up rates in the single-bubble regime.

Method: The authors use modulation analysis combined with the hierarchy of conservation laws from the complete integrability of CM-DNLS. They avoid more refined integrability-based techniques (inverse scattering, commuting flows, explicit formulas) to make the analysis applicable beyond chiral solutions.

Result: For initial data in H^{2L+1}(R) with L≥1, they prove a dichotomy: either the solution lies in a quantized regime where the scaling parameter satisfies λ(t)∼(T-t)^{2k} (1≤k≤L) with convergent phase/translation parameters, or in an exotic regime where λ(t)≲(T-t)^{2L+3/2}. This is the first classification of quantized blow-up dynamics in dispersive models.

Conclusion: The paper establishes a complete classification of blow-up dynamics for CM-DNLS in the single-bubble regime, introducing the concept of quantized blow-up rates and providing a framework for identifying such rates in classification problems for dispersive models.

Abstract: We study the Calogero--Moser derivative nonlinear Schrödinger equation (CM-DNLS), a mass-critical and completely integrable dispersive model. Recent works established finite-time blow-up constructions and soliton resolution, describing the asymptotic behaviors of blow-up solutions.
  In this paper, we go beyond soliton resolution and provide a sharp classification of finite-time blow-up dynamics in the \textit{single-bubble} regime. Assuming that a solution blows up at time $0<T<\infty$ with a single-soliton profile, we determine all possible blow-up rates. For initial data in $H^{2L+1}(\mathbb{R})$ with $L\ge1$, we prove a dichotomy: either the solution lies in a \emph{quantized regime}, where the scaling parameter satisfies \[
  λ(t)\sim (T-t)^{2k},\qquad 1\le k\le L, \] with convergent phase and translation parameters, or it lies in an \emph{exotic regime}, where the blow-up rate satisfies $λ(t)\lesssim (T-t)^{2L+\frac 32}$. To our knowledge, this is the first classification result for quantized blow-up dynamics in the class of dispersive models. We provide a framework for identifying the quantized blow-up rates in classification problems.
  The proof relies on a modulation analysis combined with the hierarchy of conservation laws provided by the complete integrability of (CM-DNLS). However, it does not use \emph{more refined integrability-based techniques}, such as the inverse scattering method, the method of commuting flows, or the explicit formula. As a result, our analysis applies beyond the chiral solutions.

</details>


### [37] [Critical points of solutions of elliptic equations in divergence form in planar non simply connected domains with smooth or nonsmooth boundary](https://arxiv.org/abs/2601.07412)
*Rolando Magnanini,Serge Nicaise,Madeline Chauvier*

Main category: math.AP

TL;DR: This paper studies critical points of solutions to second-order elliptic equations in divergence/diagonal form with bounded positive definite coefficients, assuming Hopf lemma conditions on boundary derivatives. It combines argument principle techniques with quasi-conformal mapping representations for divergence form operators in simply connected domains.


<details>
  <summary>Details</summary>
Motivation: To analyze critical points (points where gradient vanishes) of solutions to elliptic equations, particularly understanding their behavior and distribution under specific boundary conditions (Hopf lemma assumptions). This has implications for understanding solution structure and geometric properties of elliptic PDE solutions.

Method: Combines the argument principle (from previous work by Alessandrini and Magnanini) with representation formulas using quasi-conformal mappings for divergence form operators in simply connected domains. For degenerate coefficients, combines level lines technique and maximum principle with argument principle. Includes numerical experiments on illustrative examples.

Result: Develops theoretical framework for analyzing critical points of elliptic equation solutions under Hopf lemma boundary conditions. Extends previous results to both non-degenerate and degenerate coefficient cases. Provides numerical validation through experiments on illustrative examples.

Conclusion: The paper establishes a comprehensive approach to studying critical points of elliptic equation solutions by integrating argument principle methods with geometric representation techniques, handling both regular and degenerate coefficient cases, with numerical verification supporting the theoretical developments.

Abstract: We study the critical points of the solution of second elliptic equations in divergence and diagonal form with a bounded and positive definite coefficient, under the assumption that the statement of the Hopf lemma holds (sign assumptions on its normal derivatives) along the boundary. The proof combines the argument principle introduced in [1] for elliptic equations with the representation formula (using quasi-conformal mappings) for operators in divergence form in simply connected domains [2]. The case of a degenerate coefficient is also treated where we combine the level lines technique and the maximum principle with the argument principle. Finally, some numerical experiments on illustrative examples are presented.
  [1] G. Alessandrini and R. Magnanini. The index of isolated critical points and solutions of elliptic equations in the plane. Ann. Scuola Norm. Sup. Pisa Cl. Sci. (4), 19(4):567-589, 1992 [2] G. Alessandrini and R. Magnanini. Elliptic equations in divergence form, geometric critical points of solutions, and Stekloff eigenfunctions. SIAM J. Math. Anal., 25(5):1259-1268, 1994

</details>


### [38] [Log-concavity of solutions of parabolic equations related to the Ornstein-Uhlenbeck operator and applications](https://arxiv.org/abs/2601.07426)
*Andrea Colesanti,Lei Qin,Paolo Salani*

Main category: math.AP

TL;DR: The paper proves log-concavity preservation for parabolic Ornstein-Uhlenbeck flows in convex domains, leading to new proofs of Brunn-Minkowski type inequalities and eigenfunction properties.


<details>
  <summary>Details</summary>
Motivation: To investigate the log-concavity properties of solutions to parabolic Ornstein-Uhlenbeck equations in bounded convex domains, and to provide alternative proofs for known results about eigenvalues and eigenfunctions.

Method: Analyzes the log-concavity of the kernel for the parabolic Ornstein-Uhlenbeck operator in bounded, convex domains, showing that this property is preserved by the flow.

Result: Proves that log-concavity of initial data is preserved by the Ornstein-Uhlenbeck flow, and provides alternative proofs for: (1) a Brunn-Minkowski type inequality for the first eigenvalue, and (2) log-concavity of the first eigenfunction.

Conclusion: The log-concavity structure of the Ornstein-Uhlenbeck operator's kernel leads to preservation properties that yield new insights into eigenvalue inequalities and eigenfunction properties in convex domains.

Abstract: In this paper, we investigate the log-concavity of the kernel for the parabolic Ornstein-Uhlenbeck operator in a bounded, convex domain. Consequently, we get the preservation of the log-concavity of the initial datum by the related flow. As an application, we give another proof of a Brunn-Minkowski type inequality for the first eigenvalue of the Ornstein-Uhlenbeck operator and of the log-concavity of the related first eigenfunction (both results have been proved in [9], by different methods).

</details>


### [39] [Global renormalized solutions to Boltzmann systems modeling mixture gases of monatomic and polyatomic species](https://arxiv.org/abs/2601.07480)
*Yi-Long Luo,Jing-Xin Nie*

Main category: math.AP

TL;DR: The paper establishes existence of renormalized solutions for large-data Cauchy problems of Boltzmann systems modeling mixture gases with monatomic and polyatomic species, where polyatomic species have continuous internal energy variables.


<details>
  <summary>Details</summary>
Motivation: Inspired by DiPerna-Lions' work on renormalized solutions for Boltzmann equations, the authors aim to extend this framework to more complex gas mixtures containing both monatomic and polyatomic species, where polyatomic species have continuous internal energy variables.

Method: 1. Construct smooth approximated problems with uniform physically natural bounds. 2. Use the averaged velocity-internal energy lemma to show weak L¹ limits of approximated solutions yield renormalized solutions. 3. Verify the constructed solutions satisfy entropy inequality.

Result: Successfully proved existence of renormalized solutions for large-data Cauchy problems of Boltzmann systems modeling mixture gases with monatomic and polyatomic species, and showed these solutions satisfy the entropy inequality.

Conclusion: The paper extends DiPerna-Lions' renormalized solution framework to Boltzmann systems for gas mixtures with polyatomic species having continuous internal energy, establishing rigorous mathematical foundations for such complex physical systems.

Abstract: Inspired by DiPerna-Lions' work \cite{Diperna-Lions}, we study the renormalized solutions to the large-data Cauchy problem of the Boltzmann systems modeling mixture gases of monatomic and polyatomic species, in which the distribution functions $f_α$ characterized the polyatomic species contain the continuous internal energy variable $I \in \mathbb{R}_+$. We first construct the smooth approximated problem and establish the corresponding uniform and physically natural bounds. Then, by employing the averaged velocity (-internal energy) lemma, we can show that the weak $L^1$ limit of the approximated solution is exactly a renormalized solution what we required. Moreover, we also justify that the constructed renormalized solution subjects to the entropy inequality.

</details>


### [40] [On a Sobolev critical problem for the superposition of a local and nonlocal operator with the "wrong sign''](https://arxiv.org/abs/2601.07521)
*Stefano Biagi,Serena Dipierro,Enrico Valdinoci,Eugenio Vecchi*

Main category: math.AP

TL;DR: Study of critical problem for mixed-order operator combining Laplacian with fractional Laplacian of "wrong sign" as nonlocal perturbation.


<details>
  <summary>Details</summary>
Motivation: To investigate critical problems involving mixed-order operators where the fractional Laplacian has opposite sign, creating nontrivial solutions that wouldn't exist in purely local case.

Method: Consider mixed operator of form -Δ - γ(-Δ)^s, where fractional Laplacian has "wrong sign" and acts as nonlocal perturbation to local Laplacian.

Result: The "wrong sign" fractional perturbation enables existence of nontrivial solutions to critical problem that wouldn't be possible with purely local operator.

Conclusion: Nonlocal fractional perturbation with opposite sign is essential for producing nontrivial solutions in critical mixed-order operator problems.

Abstract: We study a critical problem for an operator of mixed order obtained by the superposition of a Laplacian with a fractional Laplacian. The main novelty is that we consider a mixed operator of the form $-Δ- γ(-Δ)^s$, namely we suppose that the fractional Laplacian has the ``wrong sign'' and can be seen as a nonlocal perturbation of the purely local case, which is needed to produce a nontrivial solution of the critical problem.

</details>


### [41] [Backward Reconstruction of the Chafee--Infante Equation via Physics-Informed WGAN-GP](https://arxiv.org/abs/2601.07733)
*Joseph L. Shomberg*

Main category: math.AP

TL;DR: Physics-informed WGAN-GP solves ill-posed inverse Chafee-Infante problem by reconstructing initial conditions from near-equilibrium states using U-Net generator with forward-simulation penalty.


<details>
  <summary>Details</summary>
Motivation: The inverse Chafee-Infante problem is severely ill-posed due to strong damping of high-frequency content in the reaction-diffusion equation, making reconstruction of unknown initial conditions from near-equilibrium states challenging and sensitive to noise.

Method: Physics-informed Wasserstein GAN with gradient penalty (WGAN-GP) using U-Net generator, PatchGAN critic with spectral normalization, Wasserstein loss with gradient penalty, and physics-informed terms including Lyapunov energy matching, distributional statistics, and crucial forward-simulation penalty that enforces consistency between predicted initial condition and its forward evolution.

Result: On 128×128 grid dataset (50k training, 10k testing pairs), best model achieves mean absolute error of ~0.2399 with sample-wise standard deviation of ~0.0027, demonstrating stable inversion, accurate interfacial structure recovery, and robustness to high-frequency noise.

Conclusion: The physics-informed WGAN-GP framework successfully solves the ill-posed inverse Chafee-Infante problem by integrating forward-simulation consistency, achieving accurate initial condition reconstruction with noise robustness.

Abstract: We present a physics-informed Wasserstein GAN with gradient penalty (WGAN-GP) for solving the inverse Chafee--Infante problem on two-dimensional domains with Dirichlet boundary conditions. The objective is to reconstruct an unknown initial condition from a near-equilibrium state obtained after 100 explicit forward Euler iterations of the reaction-diffusion equation \[ u_t - γΔu + κ\left(u^3 - u\right)=0. \] Because this mapping strongly damps high-frequency content, the inverse problem is severely ill-posed and sensitive to noise.
  Our approach integrates a U-Net generator, a PatchGAN critic with spectral normalization, Wasserstein loss with gradient penalty, and several physics-informed auxiliary terms, including Lyapunov energy matching, distributional statistics, and a crucial forward-simulation penalty. This penalty enforces consistency between the predicted initial condition and its forward evolution under the \emph{same} forward Euler discretization used for dataset generation. Earlier experiments employing an Eyre-type semi-implicit solver were not compatible with this residual mechanism due to the cost and instability of Newton iterations within batched GPU training.
  On a dataset of 50k training and 10k testing pairs on $128\times128$ grids (with natural $[-1,1]$ amplitude scaling), the best trained model attains a mean absolute error (MAE) of approximately \textbf{0.23988159} on the full test set, with a sample-wise standard deviation of about \textbf{0.00266345}. The results demonstrate stable inversion, accurate recovery of interfacial structure, and robustness to high-frequency noise in the initial data.

</details>


### [42] [Subprincipal Control of Pseudospectral Quasimodes, II](https://arxiv.org/abs/2601.07743)
*Pelle Brook Borgeke*

Main category: math.AP

TL;DR: Analysis of semiclassical subprincipal controlled quasimodes for pseudodifferential operators with double characteristics, focusing on tangential intersections of bicharacteristics and spectral instability.


<details>
  <summary>Details</summary>
Motivation: To extend previous work on spectral instability (pseudospectrum) from operators with transversal intersections of bicharacteristics to those with tangential intersections and double characteristics, where the principal symbol p has double zeros.

Method: Put pseudodifferential operator on normal form microlocally, use model operator P(h) to test for quasimodes, and analyze cases where operators are factorable as P(h) = P2(h)P1(h,B) to annihilate subprincipal control.

Result: Demonstrated two cases where spectral instability occurs with tangential intersections of bicharacteristics and double characteristics, showing how subprincipal symbol b affects quasimodes.

Conclusion: The paper extends spectral instability analysis to more complex operator structures with tangential intersections and double characteristics, providing insights into how subprincipal control affects quasimodes and pseudospectrum behavior.

Abstract: In this paper, we continue the analysis of the effects of semiclassical sub principal controlled quasimodes, approximate solutions to P(h)u(h,b), depending on the subprincipal symbol b, which can give spectral insta bility (pseudospectrum). We consider a pseudodifferential operator, which has double zeros for the principal symbol, p. This means that p = dp = 0 in a small neighborhood. In the first paper in this series, we considered operators with transversal inter sections of bicharacteristics. Now we study operators with tangential in tersections of bicharacteristics, as well as with double characteristics for p. We put the pseudodifferential operator on normal form microlocally, and use a model operator, P(h) to test for quasimodes. We demonstrate two cases where this happens. We shall also continue with more advanced cases, when the operators are factorable to P(h) = P2(h)P1(h,B), thus annihilating the subprincipal control over the quasimodes.

</details>


### [43] [On the well-posedness of the initial value problem for the MMT model](https://arxiv.org/abs/2601.07771)
*Mahendra Panthee,James Patterson,Yuzhao Wang*

Main category: math.AP

TL;DR: This paper establishes sharp well-posedness theory for the Majda-McLaughlin-Tabak (MMT) model, a two-parameter family of dispersive wave equations from weak turbulence theory, and identifies critical thresholds for derivative nonlinearity relative to dispersion.


<details>
  <summary>Details</summary>
Motivation: The MMT model arises in weak turbulence theory of random waves and can be viewed as a derivative nonlinear Schrödinger equation with nonlocal fractional derivatives. There is a need to establish a sharp well-posedness theory and identify critical thresholds for the derivative nonlinearity relative to dispersive order.

Method: The study investigates the initial value problem for the MMT model, analyzing the relationship between the nonlinearity and dispersion involving nonlocal fractional derivatives. The approach establishes well-posedness theory through mathematical analysis of the dispersive wave equations.

Result: The paper establishes sharp well-posedness for the MMT model and identifies the critical threshold for the derivative in the nonlinearity relative to the dispersive order required for well-posedness. As a byproduct, it also establishes sharp well-posedness for non-local fractional dNLS equations, resolving the regularity endpoint left open in previous work.

Conclusion: The study successfully develops a complete well-posedness theory for the MMT model, determining precise conditions for existence and uniqueness of solutions, while also advancing the understanding of nonlocal fractional derivative nonlinear Schrödinger equations.

Abstract: This work investigates the initial value problem (IVP) for the two-parameter family of dispersive wave equations known as the Majda-McLaughlin-Tabak (MMT) model, which arises in the weak turbulence theory of random waves. The MMT model can be viewed as a derivative nonlinear Schrödinger (dNLS) equation where both the nonlinearity and dispersion involve nonlocal fractional derivatives. The purpose of this study is twofold: first, to establish a sharp well-posedness theory for the MMT model; and second, to identify the critical threshold for the derivative in the nonlinearity relative to the dispersive order required to ensure well-posedness. As a by- product, we establish sharp well-posedness for non-local fractional dNLS equations; notably, our results resolve the regularity endpoint left open in https://www.aimsciences.org/article/doi/10.3934/dcdsb.2022039 .

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [44] [Efficient GPU-computing simulation platform JAX-PF for differentiable phase field model](https://arxiv.org/abs/2601.06079)
*Fanglei Hu,Jiachen Guo,Stephen Niezgoda,Wing Kam Liu,Jian Cao*

Main category: physics.comp-ph

TL;DR: JAX-PF is a GPU-accelerated, differentiable Phase Field software package built on JAX that offers ~5x speedup over existing tools, supports both explicit/implicit schemes, and enables inverse design through automatic differentiation.


<details>
  <summary>Details</summary>
Motivation: The need for efficient, flexible Phase Field simulation tools that can handle both forward simulations and inverse design problems, particularly for co-designing materials and manufacturing processes in line with the Materials Genome Initiative.

Method: Built on JAX framework for GPU acceleration and automatic differentiation, supporting both explicit and implicit time stepping schemes, eliminating manual derivation of free-energy functionals and Jacobians through AD.

Result: Achieves ~5x speedup over PRISMS-PF with MPI (24 CPU cores) for systems with ~4.19 million degrees of freedom using explicit schemes, scales efficiently with implicit schemes, and enables calibration of PF material parameters using AD-based sensitivities.

Conclusion: JAX-PF provides a fast, practical, integrated tool for forward simulation and inverse design, advancing material and manufacturing co-design and supporting the Materials Genome Initiative goals through its efficiency, flexibility, and full differentiability.

Abstract: We present JAX-PF, an open-source, GPU-accelerated, and differentiable Phase Field (PF) software package, supporting both explicit and implicit time stepping schemes. Leveraging the modern computing architecture JAX, JAX-PF achieves high performance through array programming and GPU acceleration, delivering ~5x speedup over PRISMS-PF with MPI (24 CPU cores) for systems with ~4.19 million degrees of freedom using explicit schemes, and scaling efficiently with implicit schemes for large-size problems. Furthermore, a key feature of JAX-PF is automatic differentiation (AD), eliminating manual derivations of free-energy functionals and Jacobians. Beyond forward simulations, JAX-PF demonstrates its potential in inverse design by providing sensitivities for gradient-based optimization. We demonstrate, for the first time, the calibration of PF material parameters using AD-based sensitivities, highlighting its capability for high-dimensional inverse problems. By combining efficiency, flexibility, and full differentiability, JAX-PF offers a fast, practical, and integrated tool for forward simulation and inverse design, advancing co-designing of material and manufacturing processes and supporting the goals of the Materials Genome Initiative.

</details>


### [45] [Slowdown and saturation of internal time according to the statistics of information input: a minimal model of response systems](https://arxiv.org/abs/2601.06433)
*Tatsuaki Tsuruyama*

Main category: physics.comp-ph

TL;DR: The paper analyzes how a system's internal time (count of distinct observed codes) advances based on input statistics, showing it slows relative to physical time and eventually saturates for finite code sets.


<details>
  <summary>Details</summary>
Motivation: To understand how information processing systems measure time internally based on the diversity of incoming information, and how this internal time progression relates to the statistical properties of the input stream.

Method: Define internal time as the number of distinct codes observed, analyze its advancement under Poisson arrival processes with various code distributions, derive closed-form expressions for uniform distributions, and quantify uncertainty using conditional entropy measures.

Result: Internal time advancement slows over time, eventually saturating for finite code sets; physical time needed to advance internal time increases in later stages; derived closed-form relationships for uniform distributions and bounds for non-uniform cases.

Conclusion: Internal time based on information diversity progresses non-linearly relative to physical time, with diminishing returns as more distinct codes are observed, providing insights into how systems process and measure information over time.

Abstract: We consider a response system that updates its internal state in accordance with information input arriving from outside. In this paper, we define as internal time the ``number of kinds'' of codes that have been observed at least once up to a given time, and analyze how the way internal time advances is determined by the statistics of information input (arrival rate and code distribution). When arrivals follow a Poisson process, the average advancing speed of internal time decreases monotonically with time, and if the number of kinds of codes is finite, it eventually approaches an upper limit and saturates. As a result, on long time scales, internal time becomes relatively shorter than physical time. For a uniform code distribution, we provide a closed form for the correspondence between internal time and physical time, and show that the physical time required to ``advance internal time by one step'' increases in later stages. As an ancillary quantity, we quantify by conditional entropy the remaining uncertainty of ``which codes have been observed'' when only internal time is known, and we give unimodality and the maximization time in the uniform case, and upper bounds, equality conditions, and expressions of the difference from the upper bound in the non-uniform case. Finally, we also present a generalization that assigns weights (description lengths) to each code so that internal time is ticked according to the amount of information in the input.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [46] [Hidden free energy released by explicit parity-time-symmetry breaking](https://arxiv.org/abs/2601.06952)
*Hong Qin,William Dorland,Ben Y. Israeli*

Main category: physics.plasm-ph

TL;DR: The paper shows that the two-stream instability results from spontaneous PT-symmetry breaking, and that viscosity can destabilize systems by explicitly breaking PT-symmetry, exposing hidden free energies.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental mechanisms behind the two-stream instability and reveal how viscosity, typically considered dissipative, can paradoxically destabilize systems by breaking PT-symmetry.

Method: Analyzing the two-stream instability through the framework of PT-symmetry breaking in conservative systems, examining how explicit symmetry breaking by viscosity affects system stability.

Result: Viscosity can destabilize systems in parameter regimes that are stable without viscosity, by explicitly breaking PT-symmetry and exposing hidden free energies protected by the symmetry.

Conclusion: Complex systems may contain hidden free energies protected by PT-symmetry, and viscosity, despite being dissipative, can trigger instability by breaking this symmetry and releasing these energies, leading to total variation growth.

Abstract: It is shown that the familiar two-stream instability is the result of spontaneous parity-time (PT)-symmetry breaking in a conservative system, and more importantly, explicit PT-symmetry breaking by viscosity can destabilize the system in certain parameter regimes that are stable when viscosity vanishes. This reveals that complex systems may possess hidden free energies protected by PT-symmetry and viscosity, albeit dissipative, can expose the systems to these freed energies by breaking PT-symmetry explicitly. Such a process is accompanied by instability and total variation growth.

</details>


### [47] [Guiding-center dynamics in a screw-pinch magnetic field](https://arxiv.org/abs/2601.07109)
*Alain J. Brizard*

Main category: physics.plasm-ph

TL;DR: The paper verifies that Kruskal's adiabatic-invariant expansion for radial action matches the magnetic-moment gyroaction expansion up to first order in magnetic-field non-uniformity, providing a non-perturbative expression to test guiding-center approximations.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between exact invariants of full-orbit dynamics and guiding-center approximations in charged particle motion within screw-pinch magnetic fields, enabling better validation of guiding-center models.

Method: Analyze guiding-center dynamics in doubly-symmetric screw-pinch magnetic fields, compare Kruskal's adiabatic-invariant series expansion of radial action integral with perturbation expansion of magnetic-moment gyroaction, and derive non-perturbative integral expressions.

Result: Verification that both expansions match up to first order in magnetic-field non-uniformity, and derivation of magnetic moment as a non-perturbative integral expression that can test guiding-center approximation validity.

Conclusion: The magnetic moment can be expressed non-perturbatively from exact invariants, providing a rigorous framework to validate guiding-center approximations in complex magnetic field configurations.

Abstract: The guiding-center dynamics of charged particles moving in a doubly-symmetric screw-pinch magnetic field is investigated. In particular, we verify that Kruskal's adiabatic-invariant series expansion of the radial action integral associated with the reduced full-orbit radial motion matches the perturbation expansion of the magnetic-moment gyroaction up to first order in magnetic-field non-uniformity. Because the radial action integral is an exact invariant of the full-orbit dynamics, the magnetic moment is therefore represented as non-perturbative integral expression, which can be used to test the validity of the guiding-center approximation.

</details>


### [48] [Discharge characteristics and parameter diagnosis of dielectric barrier discharge patterns in double-gap configuration](https://arxiv.org/abs/2601.07167)
*Tian Shuang,Zhang Han,Zhang Xi,Zhang Xuexue,Li Qing,Li Xuecehng,Run Junxia*

Main category: physics.plasm-ph

TL;DR: Researchers generated novel pattern discharges in dielectric barrier discharge using mixed gas and double-gap boundaries, observing four distinct patterns for the first time and revealing their temporal evolution through optical and electrical analysis.


<details>
  <summary>Details</summary>
Motivation: Pattern discharge in dielectric barrier discharge has broad industrial applications, but understanding the temporal evolution and correlation of different discharge patterns is crucial for optimizing their practical use in fields like material treatment and biomedical applications.

Method: Used mixed gas (75% argon, 25% air) with double-gap boundary (hexagonal and square configurations) at fixed 20 kPa pressure. Applied varying voltage amplitudes to generate patterns. Studied discharge characteristics using optical methods (including time-resolved ICCD imaging) and electrical methods to analyze temporal correlations.

Result: Successfully generated four novel patterns for the first time: single-ring, square-point-line, square lattice, and annular-lattice patterns. Revealed that patterns exhibit multiple discharges per half-voltage cycle with temporal correlation. ICCD imaging showed square lattice pattern ignition occurs radially from outside to inside, and observed morphology results from temporal superposition of luminescence at different positions.

Conclusion: Pattern discharges in DBD are complex temporal phenomena where multiple discharges occur in each half-cycle and are temporally correlated. The observed patterns result from the superposition of luminescence from different spatial positions over time, with ignition propagating radially from outside to inside, providing fundamental insights for pattern discharge applications.

Abstract: Pattern discharge is a common mode in dielectric barrier discharge (DBD) and has broad application prospects in various industrial fields, such as material surface treatment, environmental monitoring, and biomedical applications. In this work, a mixed gas of 75% argon and 25% air is used to generate a pattern discharge. A double-gap boundary composed of hexagonal configuration and square configuration is employed, and the gas pressure is fixed at 20 kPa. By varying the applied voltage amplitude, single-ring pattern, square-point-line pattern, square lattice pattern, and annular-lattice pattern are obtained for the first time. The discharge characteristics and their temporal correlation are studied using both optical method and electrical method. The results show that the discharge patterns exhibit multiple discharges in each half of the voltage cycle, and these discharges are temporally correlated with each other. Time-resolved discharge images of the square lattice pattern are captured using an enhanced charge-coupled device (ICCD). The experimental results reveal that multiple discharges in a half-voltage cycle correspond to the ignition process of the pattern in the radial direction from the outside to the inside. The morphology of the square lattice pattern observed by the naked eye is actually the result of the temporal superposition of luminescence from points at different positions in the evolution process.

</details>


### [49] [Effect of LH and ECR waves on plasma parameters in ADITYA Upgrade tokamak](https://arxiv.org/abs/2601.07205)
*S. Aich,S. Dolui,K. Singh,J. Ghosh,K. A. Jadeja,R. L. Tanna,K. M. Patel,K. Galodiya,A. Patel,L. K. Pradhan,B. K. Shukla,H. Mistry,J. Patel,H. Patel,D. Purohit,K. G. Parmar,P. K. Sharma,J. Kumar,B. Hegde,Abhijeet Kumar,Vismay Raulji,Praveenlal E. V.,T. M. Macwan,R. Kumar,A. Kumar,A. Kumawat,H. Raj,I. Hauque,Komal,S. Banerjee,P. Verma,ADITYA-U team*

Main category: physics.plasm-ph

TL;DR: First observations of ECRH and LHCD effects on plasma parameters in ADITYA-U Tokamak, showing impacts beyond just temperature and current increases.


<details>
  <summary>Details</summary>
Motivation: Ohmic heating becomes inefficient at higher plasma temperatures, requiring additional heating methods like ECRH and LHCD to achieve higher temperatures and drive non-inductive current.

Method: Using 42 GHz-500 kW Electron Cyclotron Resonant Heating (ECRH) system and 3.7 GHz Lower Hybrid Current Drive (LHCD) system installed on ADITYA-U tokamak to study their coupling with plasma.

Result: First-time observations of interesting outcomes in multiple experimentally measured plasma parameters beyond the obvious temperature and current increases, including various coupling effects.

Conclusion: ECRH and LHCD systems produce significant and interesting impacts on plasma behavior in ADITYA-U, with first-time observations reported of their coupling effects on various plasma parameters.

Abstract: The plasma discharges in ADITYA Upgrade Tokamak are produced by means of transformer action, in which Ohmically created plasma is driven by means of a secondary loop voltage. Due to reduction of plasma resistivity after a certain level of plasma temperature, Ohmic heating becomes poor and further achievement of temperature needs other heating techniques. ADITYA-U tokamak is facilitated with a 42 GHz-500 kW Electron Cyclotron Resonant Heating (ECRH) system. Also, there is a Lower Hybrid Current Drive (LHCD) system installed and operated at 3.7 GHz for driving non-inductive plasma current followed by the Ohmic current drive. Though an eventual impact in the rise of plasma temperature and plasma current due to the application of ECRH and LHCD respectively are very obvious, their energy coupling with the plasma results in several interesting outcomes in a number of experimentally measured plasma parameters. The present work addresses such impactful observations that are noticed and reported for the first time in ADITYA-U Tokamak.

</details>


### [50] [Control of Electron Energy Distribution Functions by Current Waveform Tailoring in Inductively Coupled Radio Frequency Plasmas](https://arxiv.org/abs/2601.07386)
*Zhaoyu Chen,Zili Chen,Yu Wang,Jonas Giesekus,Wei Jiang,Yonghua Ding,Donghui Xia,Ya Zhang,Julian Schulze*

Main category: physics.plasm-ph

TL;DR: Novel method using current waveform tailoring in inductively coupled discharges to control electron energy probability functions and plasma chemistry by breaking temporal symmetry of electric field dynamics.


<details>
  <summary>Details</summary>
Motivation: To develop electrical control methods for plasma properties (EEPF and chemistry) in inductively coupled discharges without changing physical parameters like pressure or power.

Method: Using current waveform tailoring (CWT) in the coil of inductively coupled discharges, specifically employing sawtooth waveforms instead of sinusoidal ones to break temporal symmetry of electric field dynamics.

Result: CWT enables control of EEPF, ionization-to-excitation rate ratio, and plasma chemistry by varying the shape of current waveforms in the coil.

Conclusion: Current waveform tailoring provides a novel approach for electrical control of plasma properties in inductively coupled discharges, offering new capabilities for plasma processing applications.

Abstract: Based on two-dimensional particle-in-cell simulations a novel approach towards Electron Energy Probability Function (EEPF) and plasma chemistry control by Current Waveform Tailoring (CWT) in the coil of inductively coupled discharges is proposed. Varying the shape of this current waveform provides electrical control of the dynamics of the electric field in the plasma. Using sawtooth instead of sinusoidal waveforms allows breaking and controlling the temporal symmetry of the electric field dynamics. In this way CWT allows controlling the EEPF, the ionization-to-excitation rate ratio, and the plasma chemistry.

</details>


### [51] [A radiation two-phase flow model for simulating plasma-liquid interactions](https://arxiv.org/abs/2601.07486)
*Ke-Jian Qian,Zhu-Jun Li,Tao Tao,De-Hua Zhang,Rui Yan,Hang Ding*

Main category: physics.plasm-ph

TL;DR: A radiation two-phase flow model using diffuse interface methodology is developed to simulate laser-plasma interactions with tin droplets in EUV sources, capturing plasma expansion and droplet deformation into thin sheets with experimental validation.


<details>
  <summary>Details</summary>
Motivation: Current modeling approaches for laser-produced plasma EUV sources have limitations: empirical pressure-impulse models ignore dynamic plasma feedback, while advanced radiation-hydrodynamic codes fail to resolve late-time droplet hydrodynamics, creating a gap in understanding the complex plasma-liquid interactions.

Method: A radiation two-phase flow model based on diffuse interface methodology that integrates radiation hydrodynamics for plasma with Euler equations for weakly compressible liquid. It extends a five-equation diffuse interface formulation to include radiation transport, thermal conduction, and ionization, enforcing pressure/velocity equilibrium across interfaces with closure models for correct jump conditions.

Result: The model successfully simulates a benchmark pre-pulse scenario where a 50 micron tin droplet is irradiated by a 10 ns laser pulse, capturing rapid plasma expansion and inertial flattening into thin curved sheets over microsecond timescales. It reproduces experimentally observed features (including axial jet) and shows quantitative agreement with experimental data for sheet dimensions and velocity.

Conclusion: The proposed model self-consistently couples laser-plasma physics with compressible droplet dynamics, providing a powerful tool for fundamental studies of plasma-liquid interactions in LPP-EUV source optimization, bridging the gap between existing modeling approaches.

Abstract: In laser-produced plasma (LPP) extreme ultraviolet (EUV) sources, deformation of a tin droplet into an optimal target shape is governed by its interaction with a pre-pulse laser-generated plasma. This interaction is mediated by a transient ablation pressure, whose complex spatio-temporal evolution remains experimentally inaccessible. Existing modeling approaches are limited: Empirical pressure-impulse models neglect dynamic plasma feedback, while advanced radiation-hydrodynamic codes often fail to resolve late-time droplet hydrodynamics. To bridge this gap, we propose a radiation two-phase flow model based on a diffuse interface methodology. The model integrates radiation hydrodynamics for the plasma with the Euler equations for a weakly compressible liquid, extending a five-equation diffuse interface formulation to incorporate radiation transport, thermal conduction, and ionization. This formulation enforces pressure and velocity equilibrium across the diffuse interface region, with closure models constructed to ensure correct jump conditions at interfaces and asymptotically recover the pure-phase equations in bulk regions. Then, we apply the model to simulate a benchmark pre-pulse scenario, where a 50 micron tin droplet is irradiated by a 10 ns laser pulse. The simulations capture the rapid plasma expansion and subsequent inertial flattening of the droplet into a thin, curved sheet over microsecond timescales. Notably, the model reproduces experimentally observed features (such as an axial jet) rarely replicated in prior simulations. Quantitative agreement with experimental data for sheet dimensions and velocity validates the approach. The proposed model self-consistently couples laser-plasma physics with compressible droplet dynamics, providing a powerful tool for fundamental studies of plasma-liquid interactions in LPP-EUV source optimization.

</details>


### [52] [Role of Shafranov shift, zonal structures on the behavior of TAEs, AAEs and microinstabilities in the presence of energetic particles](https://arxiv.org/abs/2601.07652)
*B. Rofman,G. Di Giannatale,A. Mishchenko,E. Lanti,A. Bottino,T. Hayward-Schneider,J. N. Sama,A. Biancalani,B. F. McMillan,S. Brunner,L. Villard*

Main category: physics.plasm-ph

TL;DR: First-principles gyrokinetic simulations show self-consistent finite β equilibrium affects Alfvén Eigenmodes, ITG, and KBM microturbulence in fusion plasmas, with Shafranov shift and axisymmetric perturbations having opposite effects on ITG and TAE turbulent fluxes.


<details>
  <summary>Details</summary>
Motivation: In future fusion reactors, energetic particles (100x hotter than thermal bulk) significantly affect MHD equilibrium via Shafranov shift, requiring understanding of how self-consistent equilibrium impacts various plasma instabilities and resulting transport.

Method: First-principles numerical simulations using gyrokinetic, electromagnetic, global code ORB5 to study effects of self-consistent finite β equilibrium on Alfvén Eigenmodes (destabilized by EPs), ITG, and KBM microturbulence (destabilized by thermal species).

Result: Including axisymmetric (n=0) perturbations reduces turbulent fluxes for ITG cases but enhances fluxes for TAE cases. Axisymmetric Alfvén Eigenmodes (AAEs) play a role in this mechanism, with complex interplay between EP fraction, bulk gradients, and Shafranov shift affecting plasma stability.

Conclusion: Self-consistent equilibrium effects, particularly Shafranov shift and axisymmetric perturbations, significantly impact turbulent transport in fusion plasmas, with opposite effects on ITG and TAE-driven fluxes, revealing important role of AAEs in TAE transport enhancement.

Abstract: In future nuclear fusion reactors, even a small fraction of fusion-born energetic particles (EP) about 100 times hotter than the thermal bulk species, contributes substantially to the kinetic pressure and therefore affect the MHD equilibrium, mainly via the Shafranov shift. In this work, we perform first-principles numerical simulations using the gyrokinetic, electromagnetic, global code ORB5 to study the effect of a self-consistent finite $β$ equilibrium on the arising Alfvén Eigenmodes (destabilized by EPs), Ion Temperature Gradient (ITG), and Kinetic Ballooning Modes (KBM) microturbulence (destabilized by thermal species). Linearly, we explore the complex interplay between EP fraction, bulk gradients and a self-consistent Shafranov shift on the plasma stability. We choose single toroidal mode numbers to represent the system's instabilities and study the characteristic nonlinear evolutions of TAEs, KBMs and ITGs separately and including the axisymmetric field response to each mode separately. This study focuses on the impact of Shafranov shift equilibrium consistency, as well as the self-generated zonal ${E \times B}$ flows, the saturation levels and resulting heat and particle fluxes. In the ITG cases including the $n=0$ perturbations reduces turbulent fluxes, as expected, however, for the TAE cases including the $n=0$ perturbations is shown to enhance the fluxes. We show for the first time that Axisymmetric Alfvén Eigenmodes (AAEs) play a role in this mechanism.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [53] [Primal-Dual algorithms for Abstract convex functions with respect to quadratic functions](https://arxiv.org/abs/2601.07076)
*Ewa Bednarczuk,The Hung Tran*

Main category: math.OC

TL;DR: Proposed primal-dual algorithms for abstract convex saddle point problems using abstract proximal operators, with convergence analysis and numerical validation.


<details>
  <summary>Details</summary>
Motivation: Address saddle point problems where objective functions are abstract convex with respect to quadratic functions, requiring specialized algorithms beyond standard convex optimization.

Method: Developed primal-dual algorithms utilizing abstract proximal operators tailored for abstract convex functions, with convergence analysis under specific conditions.

Result: Algorithms demonstrate convergence under certain restrictions and show effectiveness through numerical experiments on various test problems.

Conclusion: The proposed primal-dual framework with abstract proximal operators provides a viable approach for solving abstract convex saddle point problems, validated by theoretical convergence guarantees and numerical performance.

Abstract: We consider the saddle point problem where the objective functions are abstract convex with respect to the class of quadratic functions. We propose primal-dual algorithms using the corresponding abstract proximal operator and investigate the convergence under certain restrictions. We test our algorithms by several numerical examples.

</details>


### [54] [Robust maximum hands-off optimal control: existence, maximum principle, and $L^{0}$-$L^1$ equivalence](https://arxiv.org/abs/2601.07256)
*Siddhartha Ganguly,Kenji Kashima*

Main category: math.OC

TL;DR: Robust sparse control for uncertain linear systems with L0-L1 equivalence and algorithmic solution


<details>
  <summary>Details</summary>
Motivation: Existing maximum hands-off sparse control lacks robustness to parametric uncertainties in constrained linear systems

Method: Replace L0 objective with convex L1 surrogate, prove equivalence via robust Pontryagin maximum principle, develop algorithmic framework using semi-infinite robust optimization techniques

Result: Established robust hands-off principle (L0 and L1 formulations have identical optimal solutions), proposed viable algorithmic framework, demonstrated effectiveness with illustrative example

Conclusion: Robust sparse control framework successfully handles parametric uncertainties while maintaining sparsity properties through L0-L1 equivalence

Abstract: This work advances the maximum hands-off sparse control framework by developing a robust counterpart for constrained linear systems with parametric uncertainties. The resulting optimal control problem minimizes an $L^{0}$ objective subject to an uncountable, compact family of constraints, and is therefore a nonconvex, nonsmooth robust optimization problem. To address this, we replace the $L^{0}$ objective with its convex $L^{1}$ surrogate and, using a nonsmooth variant of the robust Pontryagin maximum principle, show that the $L^{0}$ and $L^{1}$ formulations have identical sets of optimal solutions -- we call this the robust hands-off principle. Building on this equivalence, we propose an algorithmic framework -- drawing on numerically viable techniques from the semi-infinite robust optimization literature -- to solve the resulting problems. An illustrative example is provided to demonstrate the effectiveness of the approach.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [55] [The past stability of Kasner singularities for the $(3+1)$-dimensional Einstein vacuum spacetime under polarized $U(1)$-symmetry](https://arxiv.org/abs/2601.06957)
*Kai Dong*

Main category: gr-qc

TL;DR: New proof of stability for Kasner solutions under polarized U(1)-symmetry using (2+1) orthonormal-frame decomposition and Fuchsian techniques.


<details>
  <summary>Details</summary>
Motivation: To provide a new proof for a previously established stability result for Kasner solutions of Einstein vacuum equations under polarized U(1)-symmetry, building on recent methodological developments.

Method: Uses (2+1) orthonormal-frame decomposition and careful symmetrization to reduce Einstein vacuum equations to a Fuchsian system coupled with constraint equations, then applies Fuchsian theory with finite speed of constraints propagation.

Result: Perturbed solutions are asymptotically pointwise Kasner, geodesically incomplete, and crushing at the Big Bang singularity; global existence and precise asymptotics up to singularities are obtained.

Conclusion: The new method successfully proves stability of Kasner solutions under polarized U(1)-symmetry, demonstrating the effectiveness of the (2+1) orthonormal-frame decomposition and Fuchsian approach for analyzing singularities in Einstein equations.

Abstract: In this paper, we give a new proof to a past stability result established in Fournodavlos-Rodnianski-Speck (arXiv:2012.05888), for Kasner solutions of the $(3+1)$-dimensional Einstein vacuum equations under polarized $U(1)$-symmetry. Our method, inspired by Beyer-Oliynyk-Olvera-Santamar{\'ı}a-Zheng (arXiv:1907.04071, arXiv:2502.09210), relies on a newly developed $(2+1)$ orthonormal-frame decomposition and a careful symmetrization argument, after which the Fuchsian techniques can be applied.
  We show that the perturbed solutions are asymptotically pointwise Kasner, geodesically incomplete and crushing at the Big Bang singularity. They are achieved by reducing the $(3+1)$ Einstein vacuum equations to a Fuchsian system coupled with several constraint equations, with the symmetry assumption playing an important role in the reduction. Using Fuchsian theory together with finite speed of constraints propagation, we obtain global existence and precise asymptotics of the solutions up to the singularities.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [56] [Surface Dean--Kawasaki equations](https://arxiv.org/abs/2601.06863)
*John Bell,Ana Djurdjevac,Nicolas Perkowski*

Main category: math.PR

TL;DR: Derivation of surface Dean-Kawasaki equation for stochastic particles on hypersurfaces, including geometry-dependent SPDE, weak uniqueness, and numerical discretization preserving fluctuation-dissipation.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical framework for stochastic particle dynamics on curved hypersurfaces that explicitly incorporates geometric effects, extending classical Dean-Kawasaki theory to curved manifolds.

Method: Start from Langevin dynamics in Monge gauge parametrization, derive surface Dean-Kawasaki equation in martingale sense, incorporate induced metric and differential operators, extend to evolving surfaces, prove weak uniqueness for non-interacting case, develop finite-volume discretization preserving fluctuation-dissipation relation.

Result: Derived geometry-dependent SPDE for particle density on hypersurfaces, established weak uniqueness for non-interacting particles, developed numerical scheme preserving fluctuation-dissipation, demonstrated equilibrium properties and dynamical behavior influenced by surface geometry through numerical experiments.

Conclusion: The framework successfully extends Dean-Kawasaki theory to curved hypersurfaces, providing a rigorous mathematical description of stochastic particle dynamics that explicitly accounts for geometric effects, with applications to evolving surfaces and coupled systems.

Abstract: We consider stochastic particle dynamics on hypersurfaces represented in Monge gauge parametrization. Starting from the underlying Langevin system, we derive the surface Dean-Kawasaki (DK) equation and formulate it in the martingale sense. The resulting SPDE explicitly reflects the geometry of the hypersurface through the induced metric and its differential operators. Our framework accommodates both pairwise interactions and environmental potentials, and we extend the analysis to evolving hypersurfaces driven by an SDE that interacts with the particles, yielding the corresponding surface DK equation for the coupled surface-particle system. We establish a weak uniqueness result in the non-interacting case, and we develop a finite-volume discretization preserving the fluctuation-dissipation relation. Numerical experiments illustrate equilibrium properties and dynamical behavior influenced by surface geometry and external potentials.

</details>


### [57] [On a stochastic Cahn-Hilliard-Brinkman model](https://arxiv.org/abs/2601.06698)
*Z. Brzeźniak,A. Ndongmo Ngana,T. Tachim Medjo*

Main category: math.PR

TL;DR: A stochastic Cahn-Hilliard-Brinkman model with multiplicative Wiener noise and dynamical boundary conditions is analyzed, proving existence of weak solutions in both probabilistic and PDE senses.


<details>
  <summary>Details</summary>
Motivation: To extend the deterministic Cahn-Hilliard-Brinkman model from previous work (Colli+Knopf+Schimperna+Signor_2024) to a stochastic setting with random fluctuations, incorporating multiplicative-type Wiener noises in the phase field equations to better model real-world two-phase flows through porous media.

Method: Uses Faedo-Galerkin approximation, Yosida approximation, and compactness methods to construct solutions. The approach combines classical PDE approximation techniques with stochastic analysis to handle the multiplicative Wiener noise in the Cahn-Hilliard equations.

Result: Proves existence of weak solutions in both probabilistic and partial differential equation senses for the stochastic Cahn-Hilliard-Brinkman system with dynamical boundary conditions in smooth 2D or 3D domains.

Conclusion: This represents the first generalization of the deterministic Cahn-Hilliard-Brinkman model to a stochastic setting, establishing mathematical foundations for studying two-phase flows with random fluctuations in porous media.

Abstract: In this paper, we consider a stochastic version of the Cahn-Hilliard-Brinkman model in a smooth two- or three-dimensional domain with dynamical boundary conditions. The system describes creeping two-phase flows and is basically a coupling of the Brinkman equation for the velocity field that governs the flow through the porous media coupled with convective Cahn-Hilliard equations for the phase field, both with two independent sources of randomness given by general multiplicative-type Wiener noises in the Cahn-Hilliard equations. The existence of a weak solution, both in the probabilistic and PDEs sense, is proved. Our construction of a solution is based on the classical Faedo-Galerkin approximation, the Yosida approximation and uses a compactness method. Our paper is the first attempt to generalize the paper \cite{Colli+Knopf+Schimperna+Signor_2024} to a stochastic setting.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [58] [New Calabi-Yau Metrics of Taub-NUT Type on C^{N+1}](https://arxiv.org/abs/2601.06756)
*Tengfei Ma*

Main category: math.DG

TL;DR: Construction of complete non-flat Calabi-Yau metrics on C^{N+1} for N≥3, generalizing Taub-NUT metrics, with tangent cone at infinity R^N, using generalized Gibbons-Hawking ansatz and gluing to resolve severe singularities.


<details>
  <summary>Details</summary>
Motivation: To extend the construction of complete non-flat Calabi-Yau metrics beyond dimensions 2 and 3 (C^2 and C^3 Taub-NUT metrics) to higher dimensions (C^{N+1} for N≥3), addressing the challenge that existing methods encounter more severe singularities in higher dimensions.

Method: Uses the generalized Gibbons-Hawking ansatz, but faces a key obstacle: the volume-form defect fails to decay near certain components of the discriminant locus, producing worse singularities than in dimension three. Resolves this through a gluing procedure.

Result: Successfully constructs a class of complete non-flat Calabi-Yau metrics on C^{N+1} for every N≥3, generalizing Taub-NUT metrics from C^2 and C^3, with tangent cone at infinity being R^N.

Conclusion: The paper demonstrates that despite more severe singularities in higher dimensions when using the generalized Gibbons-Hawking ansatz, these can be resolved through gluing techniques, enabling the construction of complete non-flat Calabi-Yau metrics in dimensions N≥3.

Abstract: We construct a class of complete non-flat Calabi-Yau metrics on C^{N+1} for every N >= 3, which generalize the Taub-NUT metrics from C^2 and C^3 and whose tangent cone at infinity is R^N. The construction relies on the generalized Gibbons-Hawking ansatz. A key obstacle is that the volume-form defect of the ansatz fails to decay near certain components of the discriminant locus, producing singularities more severe than those encountered in dimension three, we resolve this by a gluing procedure.

</details>


### [59] [Coupled continuity equations for constant scalar curvature Kähler metrics](https://arxiv.org/abs/2601.07677)
*Xi Sisi Shen,Kevin Smith*

Main category: math.DG

TL;DR: Study of elliptic system for Kähler metric and closed (1,1)-form, proving higher order estimates and convergence to cscK metrics coupled to harmonic forms, with applications to Kähler-Einstein metrics.


<details>
  <summary>Details</summary>
Motivation: Inspired by parabolic systems from Li-Yuan-Zhang and continuity equations from La Nave-Tian, the paper aims to develop existence and convergence results for constant scalar curvature Kähler (cscK) metrics coupled with harmonic forms, with applications to Kähler-Einstein metrics.

Method: Study a system of elliptic equations for a Kähler metric ω and a closed (1,1)-form α. Assuming uniform estimates for ω, prove higher order estimates using elliptic PDE techniques. Use a simplified version of the system to recover existence results for Kähler-Einstein metrics.

Result: Prove smooth convergence to cscK metrics coupled to harmonic (1,1)-forms. Recover existence results for Kähler-Einstein metrics when c₁(X) < 0. On Riemann surfaces with genus ≥ 2, show smooth convergence to unique Kähler-Einstein metric from large class of initial data.

Conclusion: The elliptic system approach provides a framework for studying convergence to cscK metrics and recovering known existence results for Kähler-Einstein metrics, with particularly strong convergence results on high-genus Riemann surfaces.

Abstract: Inspired by a parabolic system of Li-Yuan-Zhang and the continuity equation of La Nave-Tian, we study a system of elliptic equations for a Kähler metric $ω$ and a closed $(1, 1)$-form $α$. Assuming a uniform estimate for $ω$, we prove higher order estimates and smooth convergence to a cscK metric coupled to a harmonic $(1, 1)$-form. A simplification of the system is used to recover existence results for Kähler-Einstein metrics when $c_1(X) < 0$. On Riemann surfaces with genus at least $2$, we show smooth convergence to the unique Kähler-Einstein metric from a large class of initial data.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [60] [Noise enhances odor source localization](https://arxiv.org/abs/2601.07445)
*Francesco Marcolli,Martin James,Agnese Seminara*

Main category: physics.bio-ph

TL;DR: Proprioceptive noise improves odor source localization accuracy in turbulent plumes by breaking spatiotemporal correlations and leveraging hidden geometric information.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of locating odor sources in turbulent environments using distributed sensors, inspired by biological chemosensation systems. The researchers investigate how proprioceptive noise (errors in sensor position perception) affects inference accuracy.

Method: The study uses Bayesian inference with distributed sensors scattered within odor plumes. It examines the effects of proprioceptive noise and other noise sources on inference accuracy, particularly in the presence of net fluid flow. The approach includes empirical tuning of noise parameters across various distances.

Result: Counterintuitively, proprioceptive noise improves Bayesian inference accuracy rather than degrading it, especially with net fluid flow. An optimal noise level exists that efficiently leverages hidden geometric information in the odor plume. Other noise sources also improve accuracy by breaking spatiotemporal correlations in turbulent plumes.

Conclusion: Noise can paradoxically enhance sensory processing in turbulent environments by breaking correlations and revealing hidden information. These findings could be leveraged to improve both biological sensory systems and robotic odor source localization algorithms.

Abstract: We address the problem of inferring the location of a target that releases odor in the presence of turbulence. Input for the inference is provided by many sensors scattered within the odor plume. Drawing inspiration from distributed chemosensation in biology, we ask whether the accuracy of the inference is affected by proprioceptive noise, i.e., noise on the perceived location of the sensors. Surprisingly, in the presence of a net fluid flow, proprioceptive noise improves Bayesian inference, rather than degrading it. An optimal noise exists that efficiently leverages additional information hidden within the geometry of the odor plume. Empirical tuning of noise functions well across a range of distances and may be implemented in practice. Other sources of noise also improve accuracy, owing to their ability to break the spatiotemporal correlations of the turbulent plume. These counterintuitive benefits of noise may be leveraged to improve sensory processing in biology and robotics.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [61] [Curvature-driven shifts of the Potts transition on spherical Fibonacci graphs: a graph-convolutional transfer-learning study](https://arxiv.org/abs/2601.07346)
*Zheng Zhou,Xu-Yang Hou,Hao Guo*

Main category: cond-mat.stat-mech

TL;DR: Researchers use graph convolutional networks (GCNs) combined with Monte Carlo simulations to study phase transitions in the ferromagnetic q-state Potts model on spherical Fibonacci graphs, finding that curvature-induced connectivity irregularities cause only modest shifts in transition temperatures compared to planar lattices.


<details>
  <summary>Details</summary>
Motivation: To develop a unified phase-classification framework that can handle both regular planar lattices and curved, irregular spherical graphs, and to understand how curvature and connectivity irregularities affect phase transitions in Potts models on curved surfaces.

Method: Combined Swendsen-Wang cluster Monte Carlo simulations with graph convolutional networks (GCNs) that operate directly on adjacency structure and node spins. Used a transfer strategy where Potts spins are binarized into effective Ising variables, allowing a single GCN pretrained on the Ising model to locate transition regions for different q values without retraining.

Result: Curvature- and defect-induced connectivity irregularities produce only modest shifts in inferred transition temperatures relative to planar baselines. The curvature-induced shift of critical temperature is most pronounced at small q and diminishes rapidly as q increases, consistent with the physical picture that Potts models undergo a transition from continuous to weakly first-order phase transitions for q>4 in two dimensions.

Conclusion: The developed GCN-based framework successfully handles both regular and irregular graph structures, revealing that curvature effects on phase transitions in Potts models are relatively modest and strongly q-dependent, with the strongest effects at small q values where correlation lengths are largest.

Abstract: We investigate the ferromagnetic $q$-state Potts model on spherical Fibonacci graphs. These graphs are constructed by embedding quasi-uniform sites on a sphere and defining interactions via a chord-distance cutoff chosen to yield a network approximating four-neighbor connectivity. By combining Swendsen-Wang cluster Monte Carlo simulations with graph convolutional networks (GCNs), which operate directly on the adjacency structure and node spins, we develop a unified phase-classification framework applicable to both regular planar lattices and curved, irregular spherical graphs. Benchmarks on planar lattices demonstrate an efficient transfer strategy: after a fixed binarization of Potts spins into an effective Ising variable, a single GCN pretrained on the Ising model can localize the transition region for different $q$ values without retraining. Applying this strategy to spherical graphs, we find that curvature- and defect-induced connectivity irregularities produce only modest shifts in the inferred transition temperatures relative to planar baselines. Further analysis shows that the curvature-induced shift of the critical temperature is most pronounced at small $q$ and diminishes rapidly as $q$ increases; this trend is consistent with the physical picture that, in two dimensions, the Potts model undergoes a transition from a continuous phase transition to a weakly first-order one for $q>4$, accompanied by a pronounced reduction of the correlation length.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [62] [On the weak coupling limit of the periodic quantum Lorentz gas](https://arxiv.org/abs/2601.07453)
*Massimiliano Gubinelli,Vishnu Sanjay*

Main category: math-ph

TL;DR: Partial progress on weak coupling limit of periodic quantum Lorentz gas observables - some observables have trivial limit described by transport equation, others depend on regularity of Bloch-Wigner transform at resonant momenta.


<details>
  <summary>Details</summary>
Motivation: To understand the weak coupling limit behavior of observables for the periodic quantum Lorentz gas, which is important for connecting quantum dynamics to classical kinetic theory.

Method: Uses the sewing lemma in derivation of kinetic scaling limit for almost every momentum, and analyzes the Bloch-Wigner transform at resonant momenta.

Result: For certain observables, the limit is trivial and described by a transport equation. For other observables, the limit's existence depends on regularity properties of Bloch-Wigner transform at resonant momenta, which remains unproven.

Conclusion: Partial progress made but weak coupling limit for some observables remains open question due to unresolved regularity properties of Bloch-Wigner transform at resonant momenta.

Abstract: We report partial progress on the weak coupling limit behavior of observables for the periodic quantum Lorentz gas. Our results indicate that for certain observables, the limit behavior is trivial and can be described via a transport equation, while for other observables, the existence of the limit hinges on the regularity properties at resonant momenta of a certain Bloch-Wigner transform. We are currently unable to prove or disprove this regularity property, and so the weak coupling limit for these observables remains an open question. A novelty of this work is the use of the sewing lemma in the derivation of the kinetic scaling limit for almost every mometum.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [63] [Automated dimensional analysis for PDEs](https://arxiv.org/abs/2601.06535)
*Michal Habera,Andreas Zilian*

Main category: cs.MS

TL;DR: A framework for integrating physical units into the Unified Form Language (UFL) for finite element analysis, enabling automated dimensional analysis, consistency checks, and nondimensionalization that improves numerical conditioning.


<details>
  <summary>Details</summary>
Motivation: Many finite element frameworks lack built-in support for dimensional analysis, which is fundamental to scientific computing. Physical units are essential for ensuring consistency and correctness in computational physics simulations.

Method: Implemented a symbolic Quantity class in UFL to track units, representing physical dimensions as vectors in ℚⁿ to exploit abelian group structure. Used graph-based visitor pattern to traverse expression trees for automated consistency checks and factorization. The nondimensionalization acts as physics-aware diagonal preconditioning.

Result: Numerical experiments show improved condition numbers for Navier-Stokes saddle-point matrices. The system detects floating-point cancellation errors in Neo-Hooke hyperelasticity small deformation regimes. Successfully handles coupled multiphysics problems like Poisson-Nernst-Planck with derived scaling parameters.

Conclusion: The framework provides systematic unit integration for finite element analysis, with automated nondimensionalization serving as effective preconditioning. Although implemented for FEniCSx, the concepts are generalizable to other UFL-based libraries like Firedrake or DUNE.

Abstract: Physical units are fundamental to scientific computing. However, many finite element frameworks lack built-in support for dimensional analysis. In this work, we present a systematic framework for integrating physical units into the Unified Form Language (UFL). We implement a symbolic Quantity class to track units within variational forms. The implementation exploits the abelian group structure of physical dimensions. We represent them as vectors in $\mathbb{Q}^n$ to simplify operations and improve performance. A graph-based visitor pattern traverses the expression tree to automate consistency checks and factorization. We demonstrate that this automated nondimensionalization functions as the simplest form of Full Operator Preconditioning. It acts as a physics-aware diagonal preconditioner that equilibrates linear systems prior to assembly. Numerical experiments with the Navier--Stokes equations show that this improves the condition number of the saddle-point matrix. Analysis of Neo-Hooke hyperelasticity highlights the detection of floating-point cancellation errors in small deformation regimes. Finally, the Poisson--Nernst--Planck system example illustrates the handling of coupled multiphysics problems with derived scaling parameters. Although the implementation targets the FEniCSx framework, the concepts are general and easily adaptable to other finite element libraries using UFL, such as Firedrake or DUNE.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [64] [Machine learning nonequilibrium phase transitions in charge-density wave insulators](https://arxiv.org/abs/2601.07583)
*Yunhao Fan,Sheng Zhang,Gia-Wei Chern*

Main category: cond-mat.str-el

TL;DR: Machine learning framework predicts nonequilibrium electronic forces for lattice dynamics, enabling efficient simulation of voltage-driven phase transitions.


<details>
  <summary>Details</summary>
Motivation: Nonequilibrium electronic forces are crucial for voltage-driven phase transitions but are computationally expensive to evaluate using exact methods like nonequilibrium Green's function (NEGF) calculations, making long-time dynamical simulations prohibitively expensive.

Method: Developed a machine learning framework that trains a neural network to predict instantaneous local electronic forces directly from lattice configurations, bypassing repeated NEGF calculations. The approach exploits locality of electronic response and combines with Brownian dynamics for simulations.

Result: The machine learning force field quantitatively reproduces domain wall motion and nonequilibrium phase transition dynamics obtained from full NEGF simulations, while achieving orders of magnitude gains in computational efficiency. Demonstrated for gating-induced insulator to metal transition in Holstein model.

Conclusion: Direct force learning establishes an efficient and accurate approach for simulating nonequilibrium lattice dynamics in driven quantum materials, enabling practical simulation of voltage-driven phase transitions.

Abstract: Nonequilibrium electronic forces play a central role in voltage-driven phase transitions but are notoriously expensive to evaluate in dynamical simulations. Here we develop a machine learning framework for adiabatic lattice dynamics coupled to nonequilibrium electrons, and demonstrate it for a gating induced insulator to metal transition out of a charge density wave state in the Holstein model. Although exact electronic forces can be obtained from nonequilibrium Green's function (NEGF) calculations, their high computational cost renders long time dynamical simulations prohibitively expensive. By exploiting the locality of the electronic response, we train a neural network to directly predict instantaneous local electronic forces from the lattice configuration, thereby bypassing repeated NEGF calculations during time evolution. When combined with Brownian dynamics, the resulting machine learning force field quantitatively reproduces domain wall motion and nonequilibrium phase transition dynamics obtained from full NEGF simulations, while achieving orders of magnitude gains in computational efficiency. Our results establish direct force learning as an efficient and accurate approach for simulating nonequilibrium lattice dynamics in driven quantum materials.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [65] [Simulation package for solving dynamic diffraction problems in deformed crystals. Bragg, Laue geometry, asymmetric reflections, bend crystals, dislocations, crystals with arbitrary shapes, strain distributions and time dependent problems](https://arxiv.org/abs/2601.06340)
*Jacek Krzywinski,Aliaksei Halavanau*

Main category: physics.optics

TL;DR: FFT BPM enables fast simulation of dynamic diffraction in deformed crystals with arbitrary shapes across various geometries, with Python implementations available on GitHub.


<details>
  <summary>Details</summary>
Motivation: To provide a fast, easy-to-implement method for simulating dynamic diffraction effects in deformed crystals that can handle arbitrary shapes and various diffraction geometries (Bragg, Laue, asymmetric).

Method: Fast Fourier Transform Beam Propagation Method (FFT BPM) - a straightforward algorithm using FFT for fast computation, implemented in Python with parallel computing capabilities.

Result: Successfully reproduces literature results for bent crystals, dislocations, and finite-shaped crystals simulated using Takagi-Taupin equations. Python code is publicly available on GitHub.

Conclusion: FFT BPM is an effective, computationally efficient method for simulating dynamic diffraction in deformed crystals with arbitrary shapes, offering easy implementation and parallel computing support through open-source Python code.

Abstract: We demonstrate the use of the Fast Fourier Transform Beam Propagation Method (FFT BPM) to simulate dynamic diffraction effects, including scattering from deformed crystals with arbitrary shapes in Bragg, Laue, and asymmetric geometries. The method's straightforward algorithm, combined with FFT, enables fast computation and is easy to implement in Python. It successfully reproduces literature results for bent crystals, dislocations, and finite-shaped crystals simulated using the Takagi-Taupin equations. Python implementations for each case are provided in a public GitHub repository, with the code structured for parallel computing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Data-Driven Reduced-Complexity Modeling of Fluid Flows: A Community Challenge](https://arxiv.org/abs/2601.06183)
*Oliver T. Schmidt,Aaron Towne,Adrian Lozano-Duran,Scott T. M. Dawson,Ricardo Vinuesa*

Main category: cs.LG

TL;DR: Introduces a community challenge with three tracks (compression, forecasting, sensing) for comparing data-driven methods on aerospace flows, with standardized metrics and blind testing.


<details>
  <summary>Details</summary>
Motivation: To facilitate direct comparisons between data-driven methods for complex aerospace flow analysis and build a comprehensive understanding of what works and where current methods fall short.

Method: Organizes a community challenge with three tracks targeting complementary capabilities: compression (compact representations), forecasting (future state prediction), and sensing (inferring unmeasured states). Provides standardized success metrics, evaluation tools, baseline implementations (one classical and one ML per challenge), and uses blind tests on withheld data for final assessment.

Result: The challenge framework is established and open for participation, with outcomes to be disseminated through AIAA Journal Virtual Collection and conference presentations.

Conclusion: The community challenge provides a structured platform for fair comparison of data-driven methods in aerospace flow analysis, encouraging broad participation and transparent assessment of both successes and limitations.

Abstract: We introduce a community challenge designed to facilitate direct comparisons between data-driven methods for compression, forecasting, and sensing of complex aerospace flows. The challenge is organized into three tracks that target these complementary capabilities: compression (compact representations for large datasets), forecasting (predicting future flow states from a finite history), and sensing (inferring unmeasured flow states from limited measurements). Across these tracks, multiple challenges span diverse flow datasets and use cases, each emphasizing different model requirements. The challenge is open to anyone, and we invite broad participation to build a comprehensive and balanced picture of what works and where current methods fall short. To support fair comparisons, we provide standardized success metrics, evaluation tools, and baseline implementations, with one classical and one machine-learning baseline per challenge. Final assessments use blind tests on withheld data. We explicitly encourage negative results and careful analyses of limitations. Outcomes will be disseminated through an AIAA Journal Virtual Collection and invited presentations at AIAA conferences.

</details>


### [67] [Physics-Informed Tree Search for High-Dimensional Computational Design](https://arxiv.org/abs/2601.06444)
*Suvo Banik,Troy D. Loeffler,Henry Chan,Sukriti Manna,Orcun Yildiz,Tom Peterka,Subramanian Sankaranarayanan*

Main category: cs.LG

TL;DR: Physics-informed Monte Carlo Tree Search (MCTS) framework for high-dimensional continuous optimization in science/engineering, combining tree-based RL with surrogate models and physics guidance to efficiently navigate rugged landscapes.


<details>
  <summary>Details</summary>
Motivation: High-dimensional design spaces in physics-based modeling present challenges: expensive function evaluations, unreliable gradients, exponential scaling, multiple local minima, and lack of physical guidance. Conventional optimizers struggle with these rugged, constrained black-box problems.

Method: Physics-informed MCTS framework that extends policy-driven tree-based reinforcement learning to continuous domains. Integrates population-level decision trees with surrogate-guided directional sampling, reward shaping, and hierarchical switching between global exploration and local exploitation.

Result: Superior or comparable performance to standard global optimization baselines on canonical test functions. Demonstrated physics-consistent applicability to: (i) crystal structure optimization, (ii) fitting classical interatomic potentials, and (iii) constrained engineering design problems. Achieves high fidelity and evaluation efficiency while preserving physical constraints.

Conclusion: Physics-informed tree search establishes a scalable, interpretable paradigm for computational design and high-dimensional scientific optimization, bridging discrete decision-making frameworks with continuous search in scientific workflows.

Abstract: High-dimensional design spaces underpin a wide range of physics-based modeling and computational design tasks in science and engineering. These problems are commonly formulated as constrained black-box searches over rugged objective landscapes, where function evaluations are expensive, and gradients are unavailable or unreliable. Conventional global search engines and optimizers struggle in such settings due to the exponential scaling of design spaces, the presence of multiple local basins, and the absence of physical guidance in sampling. We present a physics-informed Monte Carlo Tree Search (MCTS) framework that extends policy-driven tree-based reinforcement concepts to continuous, high-dimensional scientific optimization. Our method integrates population-level decision trees with surrogate-guided directional sampling, reward shaping, and hierarchical switching between global exploration and local exploitation. These ingredients allow efficient traversal of non-convex, multimodal landscapes where physically meaningful optima are sparse. We benchmark our approach against standard global optimization baselines on a suite of canonical test functions, demonstrating superior or comparable performance in terms of convergence, robustness, and generalization. Beyond synthetic tests, we demonstrate physics-consistent applicability to (i) crystal structure optimization from clusters to bulk, (ii) fitting of classical interatomic potentials, and (iii) constrained engineering design problems. Across all cases, the method converges with high fidelity and evaluation efficiency while preserving physical constraints. Overall, our work establishes physics-informed tree search as a scalable and interpretable paradigm for computational design and high-dimensional scientific optimization, bridging discrete decision-making frameworks with continuous search in scientific design workflows.

</details>


### [68] [Free-RBF-KAN: Kolmogorov-Arnold Networks with Adaptive Radial Basis Functions for Efficient Function Learning](https://arxiv.org/abs/2601.07760)
*Shao-Ting Chiu,Siu Wun Cheung,Ulisses Braga-Neto,Chak Shing Lee,Rui Peng Li*

Main category: cs.LG

TL;DR: Free-RBF-KAN improves computational efficiency over original KANs by using adaptive radial basis functions with learnable smoothness and grid alignment, achieving comparable accuracy with faster training/inference.


<details>
  <summary>Details</summary>
Motivation: Original KANs using B-spline basis functions have substantial computational overhead from De Boor's algorithm. While RBF-based KANs improve efficiency, they often sacrifice accuracy compared to original KAN design.

Method: Proposes Free-RBF-KAN with adaptive learning grids and trainable smoothness. Uses freely learnable RBF shapes that dynamically align grid representations with activation patterns. Treats smoothness as a kernel parameter optimized jointly with network weights without increasing computational complexity.

Result: Free-RBF-KAN achieves accuracy comparable to original B-spline-based KAN while delivering faster training and inference. Validated through multiscale function approximation, physics-informed machine learning, and PDE solution operator learning.

Conclusion: Free-RBF-KAN provides a compelling balance between computational efficiency and adaptive resolution, particularly effective for high-dimensional structured modeling tasks.

Abstract: Kolmogorov-Arnold Networks (KANs) have shown strong potential for efficiently approximating complex nonlinear functions. However, the original KAN formulation relies on B-spline basis functions, which incur substantial computational overhead due to De Boor's algorithm. To address this limitation, recent work has explored alternative basis functions such as radial basis functions (RBFs) that can improve computational efficiency and flexibility. Yet, standard RBF-KANs often sacrifice accuracy relative to the original KAN design. In this work, we propose Free-RBF-KAN, a RBF-based KAN architecture that incorporates adaptive learning grids and trainable smoothness to close this performance gap. Our method employs freely learnable RBF shapes that dynamically align grid representations with activation patterns, enabling expressive and adaptive function approximation. Additionally, we treat smoothness as a kernel parameter optimized jointly with network weights, without increasing computational complexity. We provide a general universality proof for RBF-KANs, which encompasses our Free-RBF-KAN formulation. Through a broad set of experiments, including multiscale function approximation, physics-informed machine learning, and PDE solution operator learning, Free-RBF-KAN achieves accuracy comparable to the original B-spline-based KAN while delivering faster training and inference. These results highlight Free-RBF-KAN as a compelling balance between computational efficiency and adaptive resolution, particularly for high-dimensional structured modeling tasks.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [69] [A Linear Combination of Unitaries Decomposition for the Laplace Operator](https://arxiv.org/abs/2601.06370)
*Thomas Hogancamp,Reuben Demirdjian,Daniel Gunlycke*

Main category: quant-ph

TL;DR: Novel quantum circuit decompositions for discrete elliptic operators with various boundary conditions, achieving logarithmic depth scaling and linear dimension scaling.


<details>
  <summary>Details</summary>
Motivation: To develop efficient quantum algorithms for solving discrete elliptic differential equations (Poisson problems) with different boundary conditions, which are fundamental in scientific computing and engineering applications.

Method: Develop linear combination of unitaries (LCU) decompositions for discrete elliptic operators, construct explicit quantum circuits for each unitary term, analyze circuit complexity, and extend to problems with first-order derivative terms with variable coefficients.

Result: Number of unitary terms is independent of grid points and scales linearly with spatial dimension; circuit depth and gate cost scale logarithmically with grid points; favorable scaling when used with Variational Quantum Linear Solver (VQLS).

Conclusion: The proposed LCU decompositions provide efficient quantum implementations for solving elliptic PDEs with various boundary conditions, showing promising scaling properties for practical quantum advantage in scientific computing applications.

Abstract: We provide novel linear combination of unitaries decompositions for a class of discrete elliptic differential operators. Specifically, Poisson problems augmented with periodic, Dirichlet, Neumann, Robin, and mixed boundary conditions are considered on the unit interval and on higher-dimensional rectangular domains. The number of unitary terms required for our decomposition is independent of the number of grid points used in the discretization and scales linearly with the spatial dimension. Explicit circuit constructions for each unitary are given and their complexities analyzed. The worst case depth and elementary gate cost of any such circuit is shown to scale at most logarithmically with respect to number of grid points in the underlying discrete system. We also investigate the cost of using our method within the Variational Quantum Linear Solver algorithm and show favorable scaling. Finally, we extend the proposed decomposition technique to treat problems that include first-order derivative terms with variable coefficients.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [70] [TCLNet: A Hybrid Transformer-CNN Framework Leveraging Language Models as Lossless Compressors for CSI Feedback](https://arxiv.org/abs/2601.06588)
*Zijiu Yang,Qianqian Yang,Shunpu Tang,Tingting Yang,Zhiguo Shi*

Main category: cs.IT

TL;DR: TCLNet is a hybrid Transformer-CNN framework for CSI compression in FDD massive MIMO systems that combines lossy and lossless compression to reduce feedback overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: In FDD massive MIMO systems, downlink CSI feedback overhead becomes a major bottleneck as antenna count increases. Existing deep learning-based CSI compression methods have limitations in capturing both local and global features, restricting compression efficiency.

Method: Proposes TCLNet: a unified CSI compression framework with hybrid Transformer-CNN architecture for lossy compression, and hybrid language model + factorized model design for lossless compression. The lossy module captures both local features and global context, while the lossless module adaptively switches between context-aware coding and parallel coding to optimize rate-distortion-complexity trade-off.

Result: Extensive experiments on real-world and simulated datasets show TCLNet outperforms existing approaches in reconstruction accuracy and transmission efficiency, achieving up to 5 dB performance gain across diverse scenarios. Also demonstrates LLMs can serve as zero-shot CSI lossless compressors with carefully designed prompts.

Conclusion: TCLNet effectively addresses CSI compression challenges in massive MIMO systems by integrating Transformer-CNN architectures for comprehensive feature capture and adaptive lossless compression strategies, achieving superior performance while showing potential for leveraging LLMs in wireless communications.

Abstract: In frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems, downlink channel state information (CSI) plays a crucial role in achieving high spectrum and energy efficiency. However, the CSI feedback overhead becomes a major bottleneck as the number of antennas increases. Although existing deep learning-based CSI compression methods have shown great potential, they still face limitations in capturing both local and global features of CSI, thereby limiting achievable compression efficiency. To address these issues, we propose TCLNet, a unified CSI compression framework that integrates a hybrid Transformer-CNN architecture for lossy compression with a hybrid language model (LM) and factorized model (FM) design for lossless compression. The lossy module jointly exploits local features and global context, while the lossless module adaptively switches between context-aware coding and parallel coding to optimize the rate-distortion-complexity (RDC) trade-off. Extensive experiments on both real-world and simulated datasets demonstrate that the proposed TCLNet outperforms existing approaches in terms of reconstruction accuracy and transmission efficiency, achieving up to a 5 dB performance gain across diverse scenarios. Moreover, we show that large language models (LLMs) can be leveraged as zero-shot CSI lossless compressors via carefully designed prompts.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [71] [A First Course in Sparse Optimization](https://arxiv.org/abs/2601.06173)
*Jun Lu*

Main category: math.HO

TL;DR: A comprehensive tutorial/review paper on sparse optimization covering both sparse signal recovery and sparse regularization techniques, with intuitive explanations and rigorous mathematics.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive resource that serves both as an entry point for newcomers and a rigorous reference for practitioners in sparse optimization, bridging the gap between intuitive understanding and mathematical rigor.

Method: Structured overview approach: begins with foundations and mathematical tools, then covers key algorithms for sparse recovery (basis pursuit, matching pursuit) and sparse regularization (LASSO, elastic net), with real-world applications.

Result: A self-contained tutorial paper that balances intuitive explanations with rigorous mathematical formulations, covering both theoretical foundations and practical applications of sparse optimization.

Conclusion: The paper successfully provides a comprehensive resource that serves dual purposes: as an accessible introduction for students/researchers new to sparse optimization, and as a rigorous reference for practitioners applying these techniques in science and engineering.

Abstract: This article aims to provide a comprehensive overview of sparse optimization, with a focus on both sparse signal recovery and sparse regularization techniques. We will begin by exploring the foundations of sparse optimization, delving into the mathematical tools and models that underpin sparse signal recovery and LASSO. We will then discuss key algorithms for both sparse recovery (e.g., basis pursuit, matching pursuit) and sparse regularization (e.g., LASSO, elastic net), along with their applications in real-world problems. Throughout the text, we balance intuitive explanations with rigorous mathematical formulations to provide a comprehensive resource for both newcomers and experts in the field. Our aim is twofold: to provide a self-contained entry point for students and researchers new to the field, and to offer a rigorous reference for practitioners seeking to apply sparse optimization in science and engineering.

</details>


<div id='math.RA'></div>

# math.RA [[Back]](#toc)

### [72] [Certificate for Orthogonal Equivalence of Real Polynomials by Polynomial-Weighted Principal Component Analysis](https://arxiv.org/abs/2601.06148)
*Martin Helmer,David Hong,Hoon Hong*

Main category: math.RA

TL;DR: PW-PCA provides a scalable method to find orthogonal transformations between polynomials by using polynomial-weighted principal component analysis instead of solving expensive nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: When two real polynomials are equivalent up to orthogonal symmetry, finding the specific orthogonal transformation R that relates them (f(Rx)=g(x)) is computationally expensive using direct nonlinear system solving, especially for high dimensions and degrees.

Method: Introduces Polynomial-Weighted Principal Component Analysis (PW-PCA), showing how it can be effectively computed and used to obtain certificates of orthogonal equivalence between polynomials.

Result: PW-PCA provides a significantly more scalable alternative to direct nonlinear system solving for finding orthogonal transformations between polynomials.

Conclusion: PW-PCA offers an efficient computational framework for determining orthogonal symmetries between polynomials, overcoming the scalability limitations of traditional approaches.

Abstract: Suppose that $f(x) \in \mathbb{R}[x_1,\dots, x_n]$ and $g(x) \in \mathbb{R}[x_1,\dots, x_n]$ are two real polynomials of degree $d$ in $n$ variables. If the polynomials $f$ and $g$ are the same up to orthogonal symmetry a natural question is then what element of the orthogonal group induces the orthogonal symmetry; i.e. to find the element $R\in O(n)$ such that $f(Rx)=g(x)$. One may directly solve this problem by constructing a nonlinear system of equations induced by the relation $f(Rx)=g(x)$ along with the identities of the orthogonal group however this approach becomes quite computationally expensive for larger values of $n$ and $d$. To give an alternative and significantly more scalable solution to this problem, we introduce the concept of Polynomial-Weighted Principal Component Analysis (PW-PCA). We in particular show how PW-PCA can be effectively computed and how these techniques can be used to obtain a certificate of orthogonal equivalence, that is we find the $R\in O(n)$ such that $f(Rx)=g(x)$.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [73] [A density functional theory study of amino acids on Mg and Mg-based alloys](https://arxiv.org/abs/2601.07680)
*John Bolin,Amanda Goold,Olof Hildeberg,Alva Limbäck,Elsebeth Schröder*

Main category: cond-mat.mtrl-sci

TL;DR: DFT study shows amino acids (glycine, proline, hydroxyproline) adsorb on Mg(0001) surface, with alloying (Zn, Li, Al) and water environment affecting binding strength for potential biocompatible implant coatings.


<details>
  <summary>Details</summary>
Motivation: Magnesium has promising properties for biodegradable implants but corrodes too quickly. Using biocompatible amino acid coatings (like those found in collagen) could slow corrosion while maintaining biocompatibility.

Method: Density functional theory (DFT) calculations to study adsorption of glycine, L-proline, and L-hydroxyproline on Mg(0001) surface. Investigated effects of alloying with Zn, Li, or Al, and modeled immersion in water environment.

Result: The study reveals how amino acids bind to Mg surfaces through their functional groups, and how alloying elements and water environment influence adsorption strength and stability of the coatings.

Conclusion: Amino acid coatings on Mg surfaces show promise for creating biocompatible, corrosion-resistant implants, with alloying and environmental conditions playing important roles in optimizing coating performance.

Abstract: Magnesium (Mg) has mechanical properties similar to bone tissue, and Mg ions take part in the metabolism. This makes Mg of interest for biocompatible degradable body implants, provided that its high corrosion rate can be inhibited. Slightly alloying Mg and adding surface coatings can slow down the corrosion processes without significantly changing the mechanical properties. Use of coating molecules that are native to the body increase the likelihood of making the surface biocompatible, for example by use of amino acids. We here present a density functional theory (DFT) study of the adsorption on Mg(0001) of the amino acids glycine, L-proline, and L-hydroxyproline (Hyp), the main amino acid content of collagen. We investigate how binding of the functional groups of Hyp are affected when Mg(0001) is slightly alloyed with zinc, lithium or aluminium, and we also model the immersion of the systems in a water environment to see how this affects the binding.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [74] [Physics-constrained Gaussian Processes for Predicting Shockwave Hugoniot Curves](https://arxiv.org/abs/2601.06655)
*George D. Pasparakis,Himanshu Sharma,Rushik Desai,Chunyu Li,Alejandro Strachan,Lori Graham-Brady,Michael D. Shields*

Main category: cs.CE

TL;DR: Physics-constrained Gaussian Process regression predicts shocked material states along Hugoniot curve using limited shockwave simulation data, incorporating thermodynamic consistency through Rankine-Hugoniot jump conditions.


<details>
  <summary>Details</summary>
Motivation: Need to investigate shock-driven material response for materials discovery and mechanistic insights in regimes where experiments and simulations are costly, using large-scale molecular dynamics as accurate but expensive computational alternative.

Method: Develops physics-constrained Gaussian Process regression with probabilistic Taylor series expansion using Rankine-Hugoniot jump conditions to construct thermodynamically consistent covariance function. Uses optimization over interpretable hyperparameters and identifies regime transitions. Applies to silicon carbide using reverse ballistic molecular dynamics simulations with appropriate interatomic potentials.

Result: Framework reproduces Hugoniot curve with satisfactory accuracy while quantifying prediction uncertainty using Gaussian Process posterior. Establishes Hugoniot curves from limited number of molecular dynamics simulations.

Conclusion: Physics-constrained Gaussian Process regression enables accurate prediction of shocked material states along Hugoniot curve with uncertainty quantification using minimal simulation data, providing valuable tool for materials discovery in shock regimes.

Abstract: A physics-constrained Gaussian Process regression framework is developed for predicting shocked material states along the Hugoniot curve using data from a small number of shockwave simulations. The proposed Gaussian process employs a probabilistic Taylor series expansion in conjunction with the Rankine-Hugoniot jump conditions between the various shocked material states to construct a thermodynamically consistent covariance function. This leads to the formulation of an optimization problem over a small number of interpretable hyperparameters and enables the identification of regime transitions, from a leading elastic wave to trailing plastic and phase transformation waves. This work is motivated by the need to investigate shock-driven material response for materials discovery and for offering mechanistic insights in regimes where experimental characterizations and simulations are costly. The proposed methodology relies on large-scale molecular dynamics which are an accurate but expensive computational alternative to experiments. Under these constraints, the proposed methodology establishes Hugoniot curves from a limited number of molecular dynamics simulations. We consider silicon carbide as a representative material and atomic-level simulations are performed using a reverse ballistic approach together with appropriate interatomic potentials. The framework reproduces the Hugoniot curve with satisfactory accuracy while also quantifying the uncertainty in the predictions using the Gaussian Process posterior.

</details>


### [75] [An adjoint method for training data-driven reduced-order models](https://arxiv.org/abs/2601.07579)
*Donglin Liu,Francisco García Atienza,Mengwu Guo*

Main category: cs.CE

TL;DR: Adjoint-based operator inference framework for robust reduced-order modeling that minimizes trajectory loss between reduced solutions and data, enabling efficient gradient computation through adjoint equations.


<details>
  <summary>Details</summary>
Motivation: Need for robust data-driven reduced-order models that can handle noisy measurements and sparse temporal sampling while maintaining accuracy and stability in large-scale simulations.

Method: Couples continuous-time operator inference with adjoint-state method to minimize trajectory-based loss, uses adjoint equations for efficient gradient computation, and implements gradient-based optimization requiring only one forward and one adjoint solve per iteration.

Result: Validated on three PDEs (viscous Burgers', 2D Fisher-KPP, advection-diffusion). For clean data, similar accuracy to standard operator inference; for sparse/noisy data, better accuracy and enhanced roll-out stability.

Conclusion: Adjoint-based training provides superior robustness for reduced-order modeling under challenging conditions of sparse temporal sampling and measurement noise, making it attractive for large-scale simulations.

Abstract: Reduced-order modeling lies at the interface of numerical analysis and data-driven scientific computing, providing principled ways to compress high-fidelity simulations in science and engineering. We propose a training framework that couples a continuous-time form of operator inference with the adjoint-state method to obtain robust data-driven reduced-order models. This method minimizes a trajectory-based loss between reduced-order solutions and projected snapshot data, which removes the need to estimate time derivatives from noisy measurements and provides intrinsic temporal regularization through time integration. We derive the corresponding continuous adjoint equations to compute gradients efficiently and implement a gradient based optimizer to update the reduced model parameters. Each iteration only requires one forward reduced order solve and one adjoint solve, followed by inexpensive gradient assembly, making the method attractive for large-scale simulations. We validate the proposed method on three partial differential equations: viscous Burgers' equation, the two-dimensional Fisher-KPP equation, and an advection-diffusion equation. We perform systematic comparisons against standard operator inference under two perturbation regimes, namely reduced temporal snapshot density and additive Gaussian noise. For clean data, both approaches deliver similar accuracy, but in situations with sparse sampling and noise, the proposed adjoint-based training provides better accuracy and enhanced roll-out stability.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [76] [Inference-Time Alignment for Diffusion Models via Doob's Matching](https://arxiv.org/abs/2601.06514)
*Jinyuan Chang,Chenguang Duan,Yuling Jiao,Yi Xu,Jerry Zhijian Yang*

Main category: stat.ML

TL;DR: Doob's matching: A novel framework using Doob's h-transform for inference-time alignment of diffusion models without retraining, with theoretical convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Inference-time alignment aims to adapt pre-trained diffusion models to target distributions without retraining, preserving generative capacity while enforcing desired properties. Current guidance methods modify sampling dynamics through additional drift terms, but need improved theoretical foundations and estimation methods.

Method: Introduces Doob's matching framework based on Doob's h-transform. Formulates guidance as gradient of logarithm of Doob's h-function. Uses gradient-penalized regression to simultaneously estimate both the h-function and its gradient, providing consistent guidance estimator.

Result: Theoretical non-asymptotic convergence rates for estimated guidance. Analysis of controllable diffusion processes with non-asymptotic convergence guarantees for generated distributions in 2-Wasserstein distance.

Conclusion: Doob's matching provides a principled framework for inference-time alignment with strong theoretical guarantees, enabling adaptation of diffusion models to target distributions without retraining while maintaining generative capacity.

Abstract: Inference-time alignment for diffusion models aims to adapt a pre-trained diffusion model toward a target distribution without retraining the base score network, thereby preserving the generative capacity of the base model while enforcing desired properties at the inference time. A central mechanism for achieving such alignment is guidance, which modifies the sampling dynamics through an additional drift term. In this work, we introduce Doob's matching, a novel framework for guidance estimation grounded in Doob's $h$-transform. Our approach formulates guidance as the gradient of logarithm of an underlying Doob's $h$-function and employs gradient-penalized regression to simultaneously estimate both the $h$-function and its gradient, resulting in a consistent estimator of the guidance. Theoretically, we establish non-asymptotic convergence rates for the estimated guidance. Moreover, we analyze the resulting controllable diffusion processes and prove non-asymptotic convergence guarantees for the generated distributions in the 2-Wasserstein distance.

</details>


### [77] [Constrained Density Estimation via Optimal Transport](https://arxiv.org/abs/2601.06830)
*Yinan Hu,Estaban Tabak*

Main category: stat.ML

TL;DR: Proposes a framework for density estimation with expectation constraints using Wasserstein distance minimization, with regularization and annealing algorithm for non-smooth constraints.


<details>
  <summary>Details</summary>
Motivation: Need for density estimation methods that can incorporate expectation constraints while maintaining smoothness and avoiding artifacts in the estimated distributions.

Method: Minimizes Wasserstein distance between estimated density and prior, subject to expectation constraints on functions. Includes regularization inequalities to mitigate artifacts. Develops annealing-like algorithm for non-smooth constraints.

Result: Framework effectively handles expectation constraints in density estimation. Algorithm successfully addresses non-smooth constraints. Demonstrated through synthetic and real-world financial examples.

Conclusion: Proposed framework provides effective approach for constrained density estimation with practical applications in finance and other domains requiring expectation constraints.

Abstract: A novel framework for density estimation under expectation constraints is proposed. The framework minimizes the Wasserstein distance between the estimated density and a prior, subject to the constraints that the expected value of a set of functions adopts or exceeds given values. The framework is generalized to include regularization inequalities to mitigate the artifacts in the target measure. An annealing-like algorithm is developed to address non-smooth constraints, with its effectiveness demonstrated through both synthetic and proof-of-concept real world examples in finance.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [78] [Essentially No Energy Barrier Between Independent Fermionic Neural Quantum State Minima](https://arxiv.org/abs/2601.06939)
*David D. Dai,Marin Soljačić*

Main category: cond-mat.dis-nn

TL;DR: The NQS loss landscape is more benign than previously thought, showing mode connectivity between independently trained neural quantum states with minimal energy barriers.


<details>
  <summary>Details</summary>
Motivation: Neural quantum states (NQS) are effective for representing quantum many-body wavefunctions, but their loss landscape remains poorly understood and debated, with concerns about pathological landscapes.

Method: Developed GeoNEB, a path optimizer combining stochastic reconfiguration with nudged elastic band method to construct minimum energy paths between NQS. Tested on a six-electron quantum dot with 1.6M-parameter Psiformer.

Result: Found two independent minima connected by paths with energy barriers ~10^-5 times smaller than system's overall energy scale and ~10^-3 times smaller than linear path barriers. Path respects physical symmetry with angular momentum remaining quantized.

Conclusion: First work to construct optimized paths between independently trained NQS, suggesting NQS loss landscapes may not be as pathological as previously feared, exhibiting mode connectivity similar to conventional deep learning.

Abstract: Neural quantum states (NQS) have proven highly effective in representing quantum many-body wavefunctions, but their loss landscape remains poorly understood and debated. Here, we demonstrate that the NQS loss landscape is more benign and similar to conventional deep learning than previously thought, exhibiting mode connectivity: independently trained NQS are connected by paths in parameter space with essentially no energy barrier. To construct these paths, we develop GeoNEB, a path optimizer integrating efficient stochastic reconfiguration with the nudged elastic band method for constructing minimum energy paths. For the strongly interacting six-electron quantum dot modeled by a $1.6$M-parameter Psiformer, we find two independent minima with expected energy barrier $\sim10^{-5}$ times smaller than the system's overall energy scale and $\sim10^{-3}$ times smaller than the linear path's barrier. The path respects physical symmetry in addition to achieving low energy, with the angular momentum remaining well quantized throughout. Our work is the first to construct optimized paths between independently trained NQS, and it suggests that the NQS loss landscape may not be as pathological as once feared.

</details>


### [79] [Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence](https://arxiv.org/abs/2601.07635)
*Denis D. Caprioti,Matheus Haas,Constantino F. Vasconcelos,Mauricio Girardi-Schappo*

Main category: cond-mat.dis-nn

TL;DR: A pedagogical introduction to the Hopfield model that connects statistical physics to AI applications, with classroom-ready materials for undergraduate physics education.


<details>
  <summary>Details</summary>
Motivation: The Hopfield model is conceptually important at the intersection of statistical mechanics, neural networks, and AI, but is rarely taught in undergraduate physics curricula despite its broad applicability from associative memory to optimization problems.

Method: Provides a concise theoretical introduction grounded in physics concepts, analyzes the model's energy function, dynamics, and pattern stability, discusses practical simulation aspects with available code, and develops classroom-ready example problems.

Result: Creates a pedagogically rich framework that unifies statistical physics, dynamical systems, linear algebra, and computational methods, with practical instructional materials.

Conclusion: By connecting fundamental physics to contemporary AI applications, this work helps prepare physics students to understand and engage with computational tools central to modern research, industry, and society.

Abstract: The Hopfield model, originally inspired by spin-glass physics, occupies a central place at the intersection of statistical mechanics, neural networks, and modern artificial intelligence. Despite its conceptual simplicity and broad applicability -- from associative memory to near-optimal solutions of combinatorial optimization problems -- it is rarely integrated into standard undergraduate physics curricula. In this paper, we present the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. We provide a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyze the model's energy function, dynamics, and pattern stability, and discuss practical aspects of simulation, including a freely available simulation code. To support instruction, we conclude with classroom-ready example problems designed to mirror research practice. By explicitly connecting fundamental physics to contemporary AI applications, this work aims to help prepare physics students to understand, apply, and critically engage with the computational tools increasingly central to research, industry, and society.

</details>
