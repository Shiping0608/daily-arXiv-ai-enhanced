<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 13]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [cs.LG](#cs.LG) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [physics.class-ph](#physics.class-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Learning Ecological and Epidemic Processes using Neural ODEs, Kolmogorov-Arnold Network ODEs and SINDy](https://arxiv.org/abs/2601.09811)
*Maria Vasilyeva,Zheng Wei,Kelum Gajamannage,Hyangim Ji,Aleksei Krasnikov,Alexey Sadovski*

Main category: math.NA

TL;DR: The paper combines epidemic (SIR) and ecological (Lotka-Volterra) models into a coupled LVSIS system, then uses data-driven methods (Neural ODEs, KANODEs, SINDy) to learn dynamics from synthetic data, extending to spatio-temporal analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate the coupled dynamics between epidemic spread and ecological predator-prey interactions, and develop data-driven approaches to learn these complex coupled systems from observational data.

Method: 1. Combine SIR epidemic model with Lotka-Volterra predator-prey model into LVSIS system (4 differential equations). 2. Apply Neural ODEs, KANODEs, and SINDy to learn dynamics from synthetic data. 3. Extend to spatio-temporal models to uncover hidden local couplings.

Result: Numerical experiments with synthetic data demonstrate the learning capabilities of the data-driven frameworks in capturing epidemic and ecological behavior, with extension to spatio-temporal analysis revealing hidden local couplings.

Conclusion: Data-driven methods can effectively learn coupled epidemic-ecological dynamics, providing a framework for analyzing complex biological systems where disease transmission and ecological interactions are intertwined.

Abstract: We consider epidemic and ecological models to investigate their coupled dynamics. Starting with the classical Susceptible-Infected-Recovered (SIR) model for basic epidemic behavior and the predator-prey (Lotka-Volterra, LV) system for ecological interactions, we then combine these frameworks into a coupled Lotka-Volterra-Susceptible-Infected-Susceptible (LVSIS) model. The resulting system consists of four differential equations describing the evolution of susceptible and infected prey and predator populations, incorporating ecological interactions, disease transmission, and spatial dispersal. To learn the underlying dynamics directly from data, we employ several data-driven modeling frameworks: Neural Ordinary Differential Equations (Neural ODEs), Kolmogorov-Arnold Network Ordinary Differential Equations (KANODEs), and Sparse Identification of Nonlinear Dynamics (SINDy). Numerical experiments based on synthetic data are conducted to investigate the learning ability of these models in capturing the epidemic and ecological behavior. We further extend our approach to spatio-temporal models, aiming to uncover hidden local couplings.

</details>


### [2] [An efficient probabilistic scheme for the exit time probability of $α$-stable Lévy process](https://arxiv.org/abs/2601.09882)
*Minglei Yang,Diego del-Castillo-Negrete,Guannan Zhang*

Main category: math.NA

TL;DR: A method for computing exit time probabilities for α-stable Lévy processes using Brownian-Poisson approximations and PIDE framework with Feynman-Kac representation, achieving first-order convergence and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: α-stable Lévy processes model anomalous transport phenomena with discontinuous jumps, and understanding exit probabilities from bounded regions is crucial for analyzing anomalous diffusion behavior.

Method: Approximates α-stable process by combining Brownian motion with compound Poisson process, models exit time probability using partial integro-differential equations (PIDEs), applies Feynman-Kac formula for probabilistic representation, and computes expectations via tailored quadrature rules and interpolation techniques.

Result: The method achieves first-order convergence in time and offers significant computational advantages over standard Monte Carlo and deterministic approaches by avoiding assembly and solution of large dense linear systems, demonstrating accuracy through numerical examples.

Conclusion: The proposed approach provides an efficient and accurate method for computing exit time probabilities for α-stable Lévy processes, with demonstrated applicability to physical transport problems.

Abstract: The α-stable Lévy process, commonly used to describe Lévy flight, is characterized by discontinuous jumps and is widely used to model anomalous transport phenomena. In this study, we investigate the associated exit problem and propose a method to compute the exit time probability, which quantifies the likelihood that a trajectory starting from an initial condition exits a bounded region in phase space within a given time. This estimation plays a key role in understanding anomalous diffusion behavior. The proposed method approximates the α-stable process by combining a Brownian motion with a compound Poisson process. The exit time probability is then modeled using a framework based on partial integro-differential equations (PIDEs). The Feynman-Kac formula provides a probabilistic representation of the solution, involving conditional expectations over stochastic differential equations. These expectations are computed via tailored quadrature rules and interpolation techniques. The proposed method achieves first-order convergence in time and offers significant computational advantages over standard Monte Carlo and deterministic approaches. In particular, it avoids assembling and solving large dense linear systems, resulting in improved efficiency. We demonstrate the method's accuracy and performance through two numerical examples, highlighting its applicability to physical transport problems.

</details>


### [3] [Nonlinear numerical schemes using specular differentiation for initial value problems of first-order ordinary differential equations](https://arxiv.org/abs/2601.09900)
*Kiyuob Jung*

Main category: math.NA

TL;DR: Paper proposes specular differentiation in 1D Euclidean space with fundamental analysis (quasi-Fermat's theorem, quasi-Mean Value Theorem) and develops numerical schemes for solving first-order ODE initial value problems.


<details>
  <summary>Details</summary>
Motivation: To introduce a new concept of specular differentiation in one-dimensional Euclidean space and establish its fundamental theoretical properties, then apply it to develop numerical methods for solving ordinary differential equations.

Method: Proposes specular differentiation in 1D Euclidean space, provides fundamental analysis including quasi-Fermat's theorem and quasi-Mean Value Theorem, develops several numerical schemes for solving first-order ODE initial value problems, selects one scheme based on numerical simulations, and proves its first-order consistency and second-order local convergence.

Result: Establishes theoretical foundation for specular differentiation, develops multiple numerical schemes, selects one optimal scheme through simulations, and proves it has first-order consistency and second-order local convergence for solving first-order ODEs.

Conclusion: Specular differentiation provides a new mathematical framework with practical applications in numerical analysis, particularly for solving ODEs, with proven convergence properties for the selected numerical scheme.

Abstract: This paper proposes specular differentiation in one-dimensional Euclidean space and provides its fundamental analysis, including quasi-Fermat's theorem and the quasi-Mean Value Theorem. As an application, this paper develops several numerical schemes for solving initial value problems for first-order ordinary differential equations. Based on numerical simulations, we select one scheme and prove its first-order consistency and second-order local convergence.

</details>


### [4] [An Efficient Constant-Coefficient MSAV Scheme for Computing Vesicle Growth and Shrinkage](https://arxiv.org/abs/2601.10057)
*Zhiwei Zhang,Shuwang Li,John Lowengrub,Steven M. Wise*

Main category: math.NA

TL;DR: A fast, energy-stable phase-field method for vesicle simulation using constant-coefficient MSAV scheme that eliminates iterative solvers via DCT, achieving 6-15x speedup with maintained accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for vesicle deformation simulation (nonlinear multigrid, MSAV) require iterative solution of variable-coefficient systems at each time step, resulting in high computational cost that limits large-scale simulations.

Method: Constant-coefficient MSAV (CC-MSAV) scheme that incorporates stabilization into the Cahn-Hilliard evolution equation rather than chemical potential, yielding fully decoupled constant-coefficient elliptic problems solvable via fast discrete cosine transform (DCT).

Result: Method achieves O(N^2 log N) complexity per time step with unconditional energy stability and mass conservation. Numerical experiments show second-order accuracy, mass conservation errors below 5×10^-11, and 6-15x overall speedup on grids with N≥2048 compared to classical MSAV.

Conclusion: CC-MSAV provides significant efficiency gains without sacrificing accuracy, making it particularly suitable for large-scale simulations of vesicle dynamics by eliminating iterative solvers entirely.

Abstract: We present a fast, unconditionally energy-stable numerical scheme for simulating vesicle deformation under osmotic pressure using a phase-field approach. The model couples an Allen-Cahn equation for the biomembrane interface with a variable-mobility Cahn-Hilliard equation governing mass exchange across the membrane. Classical approaches, including nonlinear multigrid and Multiple Scalar Auxiliary Variable (MSAV) methods, require iterative solution of variable-coefficient systems at each time step, resulting in substantial computational cost. We introduce a constant-coefficient MSAV (CC-MSAV) scheme that incorporates stabilization directly into the Cahn-Hilliard evolution equation rather than the chemical potential. This reformulation yields fully decoupled constant-coefficient elliptic problems solvable via fast discrete cosine transform (DCT), eliminating iterative solvers entirely. The method achieves O(N^2 log N) complexity per time step while preserving unconditional energy stability and discrete mass conservation. Numerical experiments verify second-order temporal and spatial accuracy, mass conservation to relative errors below 5 x 10^-11, and close agreement with nonlinear multigrid benchmarks. On grids with N >= 2048, CC-MSAV achieves 6-15x overall speedup compared to classical MSAV with optimized preconditioning, while the dominant Cahn-Hilliard subsystem is accelerated by up to two orders of magnitude. These efficiency gains, achieved without sacrificing accuracy, make CC-MSAV particularly well suited for large-scale simulations of vesicle dynamics.

</details>


### [5] [New Second-order Convergent Schemes for Solving decoupled FBSDEs](https://arxiv.org/abs/2601.10149)
*Wenbo Wang,Guangyan Jia*

Main category: math.NA

TL;DR: Proposes new second-order symmetric splitting methods for decoupled FBSDEs, reducing computational cost while maintaining second-order convergence.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient numerical methods for solving decoupled forward-backward stochastic differential equations (FBSDEs) that reduce computational cost while maintaining high-order accuracy, particularly for equations with generators consisting of linear plus nonlinear parts.

Method: Inspired by ADI splitting for PDEs, the generator is split into two functions. For the value process Y, explicit and implicit schemes are alternately applied to these two generators, while using existing algorithms from Zhao & Li (2014) for the control process Z.

Result: Two new schemes are rigorously proven to have second-order convergence rate. The splitting methods show clear advantages for equations with linear+nonlinear generators by reducing iterations needed for implicit schemes, thereby decreasing computational cost while maintaining second-order convergence.

Conclusion: The proposed splitting methods offer computational advantages over existing algorithms, particularly for FBSDEs with linear+nonlinear generators, as demonstrated through numerical examples including the backward stochastic Riccati equation from mean-variance hedging.

Abstract: This paper proposes a new second-order symmetric algorithm for solving decoupled forward-backward stochastic differential equations. Inspired by the alternating direction implicit splitting method for partial differential equations, we split the generator into the sum of two functions. In the computation of the value process Y, explicit and implicit schemes are alternately applied to these two generators, while the algorithms from \citep{ZhaoLi2014} are used for the control process Z. We rigorously prove that the two new schemes have second-order convergence rate. The proposed splitting methods show clear advantages for equations whose generator consists of a linear part plus a nonlinear part, as they reduce the number of iterations required for solving implicit schemes, thereby decreasing computational cost while maintaining second-order convergence. Two numerical examples are provided, including the backward stochastic Riccati equation arising in mean-variance hedging. The numerical results verify the theoretical error analysis and demonstrate the advantage of reduced computational cost compared to the algorithm in \citep{ZhaoLi2014}.

</details>


### [6] [Introduction to optimization methods for training SciML models](https://arxiv.org/abs/2601.10222)
*Alena Kopaničáková,Elisa Riccietti*

Main category: math.NA

TL;DR: This paper provides a unified introduction to optimization methods in ML and SciML, highlighting how different problem structures (stochastic data-driven vs. physics-constrained) shape algorithmic choices and effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the fundamental differences in optimization problem structures between classical machine learning and scientific machine learning, where SciML's physics-informed formulations create unique challenges that limit the effectiveness of standard ML optimization methods.

Method: The paper reviews first- and second-order optimization techniques in both deterministic and stochastic settings, discusses their adaptation to physics-constrained and data-driven SciML models, and provides tutorial examples to illustrate practical strategies.

Result: The analysis reveals that optimization behavior in SciML is governed by spectral properties of physical models rather than data statistics, frequently limiting stochastic methods and motivating deterministic or curvature-aware approaches.

Conclusion: The paper concludes by highlighting open research directions at the interface of scientific computing and scientific machine learning, emphasizing the need for specialized optimization approaches tailored to SciML's unique problem structures.

Abstract: Optimization is central to both modern machine learning (ML) and scientific machine learning (SciML), yet the structure of the underlying optimization problems differs substantially across these domains. Classical ML typically relies on stochastic, sample-separable objectives that favor first-order and adaptive gradient methods. In contrast, SciML often involves physics-informed or operator-constrained formulations in which differential operators induce global coupling, stiffness, and strong anisotropy in the loss landscape. As a result, optimization behavior in SciML is governed by the spectral properties of the underlying physical models rather than by data statistics, frequently limiting the effectiveness of standard stochastic methods and motivating deterministic or curvature-aware approaches. This document provides a unified introduction to optimization methods in ML and SciML, emphasizing how problem structure shapes algorithmic choices. We review first- and second-order optimization techniques in both deterministic and stochastic settings, discuss their adaptation to physics-constrained and data-driven SciML models, and illustrate practical strategies through tutorial examples, while highlighting open research directions at the interface of scientific computing and scientific machine learning.

</details>


### [7] [Restoring similarity in randomized Krylov methods with applications to eigenvalue problems and matrix functions](https://arxiv.org/abs/2601.10248)
*Laura Grigori,Daniel Kressner,Nian Shao,Igor Simunec*

Main category: math.NA

TL;DR: Modified randomized Arnoldi process that maintains similarity with standard Arnoldi's Hessenberg matrix, solving one additional least-squares problem to restore orthogonality.


<details>
  <summary>Details</summary>
Motivation: Randomized Arnoldi is faster than standard Arnoldi for generating well-conditioned Krylov bases, but its Hessenberg matrix differs from standard Arnoldi's, causing convergence irregularities and delays in eigenvalue/matrix function computations.

Method: Modify randomized Arnoldi by enforcing orthogonality between the last Arnoldi vector and previously generated subspace via solving one additional least-squares problem, restoring similarity with standard Arnoldi's Hessenberg matrix.

Result: Modified process produces approximations identical to standard Arnoldi for eigenvalue problems and matrix function evaluations, while maintaining the speed of randomized Arnoldi and robustness of standard Arnoldi.

Conclusion: The modified randomized Arnoldi process combines the computational efficiency of randomized Arnoldi with the mathematical properties and robustness of standard Arnoldi, eliminating convergence irregularities while maintaining speed.

Abstract: The randomized Arnoldi process has been used in large-scale scientific computing because it produces a well-conditioned basis for the Krylov subspace more quickly than the standard Arnoldi process. However, the resulting Hessenberg matrix is generally not similar to the one produced by the standard Arnoldi process, which can lead to delays or spike-like irregularities in convergence. In this paper, we introduce a modification of the randomized Arnoldi process that restores similarity with the Hessenberg matrix generated by the standard Arnoldi process. This is accomplished by enforcing orthogonality between the last Arnoldi vector and the previously generated subspace, which requires solving only one additional least-squares problem. When applied to eigenvalue problems and matrix function evaluations, the modified randomized Arnoldi process produces approximations that are identical to those obtained with the standard Arnoldi process. Numerical experiments demonstrate that our approach is as fast as the randomized Arnoldi process and as robust as the standard Arnoldi process.

</details>


### [8] [Conjugate Gradient Methods are Not Efficient: Experimental Study of the Locality Limitation](https://arxiv.org/abs/2601.10322)
*Ulrich Rüde*

Main category: math.NA

TL;DR: Conjugate Gradient method has locality limitation due to graph sparsity pattern, requiring minimum iterations equal to graph diameter for accurate approximation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explain why Conjugate Gradient method requires a minimum number of iterations before achieving acceptable accuracy, identifying the fundamental limitation as information propagation constraints in the graph structure of sparse matrices.

Method: Analyzes the Conjugate Gradient method through graph theory perspective, modeling the sparsity pattern of the system matrix as a graph where nodes represent variables and edges represent non-zero matrix entries. Examines how information from the right-hand side propagates through this graph structure during iterations.

Result: Identifies that information can only propagate across directly connected graph nodes per iteration, establishing that the graph diameter determines the minimum number of iterations required to achieve acceptable accuracy in the Conjugate Gradient method.

Conclusion: The convergence of Conjugate Gradient is fundamentally limited by the locality of information propagation in the graph induced by matrix sparsity, with graph diameter providing a lower bound on necessary iterations for accurate approximation.

Abstract: The convergence of the Conjugate Gradient method is subject to a locality limitation which imposes a lower bound on the number of iterations required before a qualitatively accurate approximation can be obtained. This limitation originates from the restricted transport of information in the graph induced by the sparsity pattern of the system matrix. In each iteration, information from the right-hand side can propagate only across directly connected graph nodes. The diameter of this graph therefore determines a minimum number of iterations that is necessary to achieve an acceptable level of accuracy.

</details>


### [9] [Regularization of linear inverse problems by rational Krylov methods](https://arxiv.org/abs/2601.10389)
*Stefan Kindermann*

Main category: math.NA

TL;DR: The paper analyzes regularization properties of aggregation and RatCG methods for linear ill-posed problems, showing they form optimal-order regularization schemes with discrepancy principle stopping.


<details>
  <summary>Details</summary>
Motivation: To investigate regularization properties of aggregation and RatCG methods for solving linear ill-posed problems in Hilbert spaces, as these recent algorithms use previously calculated solutions to set up new search spaces.

Method: Analyze aggregation method and RatCG method as rational Krylov space methods (based on rational functions of forward operator), examining their regularization properties when combined with discrepancy principle stopping rule.

Result: These methods form optimal-order regularization schemes when combined with discrepancy principle as stopping rule and when underlying regularization parameters are sufficiently large.

Conclusion: Aggregation and RatCG methods provide effective regularization for linear ill-posed problems when properly combined with discrepancy principle stopping and appropriate parameter selection.

Abstract: For approximately solving linear ill-posed problems in Hilbert spaces, we investigate the regularization properties of the aggregation method and the RatCG method. These recent algorithms use previously calculated solutions of Tikhonov regularization (respectively, Landweber iterations) to set up a new search space on which the least-squares functional is minimized. We outline how these methods can be understood as rational Krylov space methods, i.e., based on the space of rational functions of the forward operator. The main result is that these methods form an optimal-order regularization schemes when combined with the discrepancy principle as stopping rule and when the underlying regularization parameters are sufficiently large.

</details>


### [10] [A Geometric Multigrid Preconditioner for Shifted Boundary Method](https://arxiv.org/abs/2601.10399)
*Michał Wichrowski,Ajay Ajith*

Main category: math.NA

TL;DR: Geometric multigrid preconditioner with Full-Residual Shy Patch smoother enables efficient solution of Shifted Boundary Method systems for high-order discretizations up to p=3 in 3D.


<details>
  <summary>Details</summary>
Motivation: Shifted Boundary Method (SBM) reduces body-fitted meshing burden but creates algebraically complex linear systems that are non-symmetric with non-local boundary coupling, making them resistant to standard AMG and simple smoothers for high-order discretizations.

Method: Developed a geometric multigrid preconditioner with Full-Residual Shy Patch smoother - a subspace correction strategy that filters out some patches while capturing full physics of shifted boundary, overcoming limitations of previous cell-wise approaches.

Result: Method delivers convergence with low mesh dependence, maintaining low and stable iteration counts up to polynomial degree p=3 in 3D for Continuous Galerkin approximations.

Conclusion: SBM can be both geometrically flexible (reducing meshing burden) and algebraically efficient through the proposed geometric multigrid preconditioner with specialized smoother.

Abstract: The Shifted Boundary Method (SBM) trades some part of the burden of body-fitted meshing for increased algebraic complexity. While the resulting linear systems retain the standard $\mathcal{O}(h^{-2})$ conditioning of second-order operators, the non-symmetry and non-local boundary coupling render them resistant to standard Algebraic Multigrid (AMG) and simple smoothers for high-order discretisations. We present a geometric multigrid preconditioner that effectively tames these systems. At its core lies the \emph{Full-Residual Shy Patch} smoother: a subspace correction strategy that filters out some patches while capturing the full physics of the shifted boundary. Unlike previous cell-wise approaches that falter at high polynomial degrees, our method delivers convergence with low mesh dependence. We demonstrate performance for Continuous Galerkin approximations, maintaining low and stable iteration counts up to polynomial degree $p=3$ in 3D, proving that SBM can be both geometrically flexible and algebraically efficient.

</details>


### [11] [Optimal error estimates for a discontinuous Galerkin method on curved boundaries with polygonal meshes](https://arxiv.org/abs/2601.10474)
*Adérito Araújo,Milene Santos*

Main category: math.NA

TL;DR: DG-ROD method restores optimal convergence for discontinuous Galerkin on polygonal approximations of curved domains through polynomial boundary data reconstruction.


<details>
  <summary>Details</summary>
Motivation: Curved boundary approximation by polygonal meshes causes suboptimal convergence in DG methods due to geometric errors in boundary condition transfer.

Method: DG-ROD method extends polynomial reconstruction of boundary data from FEM/FV to DG, with rigorous analysis for 2D linear advection-diffusion-reaction problems with Dirichlet BCs.

Result: Proves existence/uniqueness of discrete solution and optimal error estimates despite polygonal approximations, confirmed by numerical benchmarks.

Conclusion: DG-ROD method theoretically and numerically achieves optimal convergence on polygonal meshes for curved domains, extending prior FEM/FV techniques to DG framework.

Abstract: We consider a discontinuous Galerkin method for the numerical solution of boundary value problems in two-dimensional domains with curved boundaries. A key challenge in this setting is the potential loss of convergence order due to approximating the physical domain by a polygonal mesh. Unless boundary conditions can be accurately transferred from the true boundary to the computational one, such geometric approximation errors generally lead to suboptimal convergence. To overcome this limitation, a higher-order strategy based on polynomial reconstruction of boundary data was introduced for classical finite element methods in [28, 29] and in the finite volume context in [7, 11]. More recently, this approach was extended to discontinuous Galerkin methods in [32], leading to the DG-ROD method, which restores optimal convergence rates on polygonal approximations of domains with curved boundaries. In this work, we provide a rigorous theoretical analysis of the DG-ROD method, establishing existence and uniqueness of the discrete solution and deriving error estimates for a two-dimensional linear advection-diffusion-reaction problem with homogeneous Dirichlet boundary conditions on both convex and non-convex domains. Following and extending techniques from classical finite element methods [29], we prove that, under suitable regularity assumptions on the exact solution, the DG-ROD method achieves optimal convergence despite polygonal approximations. Finally, we illustrate and confirm the theoretical results with a numerical benchmark.

</details>


### [12] [Chebyshev Accelerated Subspsace Eigensolver for Pseudo-hermitian Hamiltonians](https://arxiv.org/abs/2601.10557)
*Edoardo Di Napoli,Clément Richefort,Xinzhe Wu*

Main category: math.NA

TL;DR: ChASE eigensolver extended to pseudo-hermitian Hamiltonians for excitonic materials, achieving similar performance to hermitian version with new oblique Rayleigh-Ritz projection and optimized parallel implementation.


<details>
  <summary>Details</summary>
Motivation: Studying optoelectronic materials requires computing thousands of eigenpairs of pseudo-hermitian Hamiltonians. Iterative eigensolvers are preferred for scalability on exascale systems, but existing solvers like ChASE only handle hermitian matrices.

Method: Extended ChASE to pseudo-hermitian Hamiltonians with: 1) oblique Rayleigh-Ritz projection for quadratic convergence without explicit dual basis construction, 2) parallel implementation of recursive matrix-product in Chebyshev filter with minimal global communications.

Result: The new pseudo-hermitian solver achieves similar convergence and performance as the original hermitian ChASE. Numerical analysis and experimental tests validate the approach.

Conclusion: Successfully extended ChASE to handle pseudo-hermitian Hamiltonians for excitonic materials, maintaining performance while adding specialized numerical methods for non-hermitian eigenvalue problems.

Abstract: Studying the optoelectronic structure of materials can require the computation of up to several thousands of the smallest eigenpairs of a pseudo-hermitian Hamiltonian. Iterative eigensolvers may be preferred over direct methods for this task since their complexity is a function of the desired fraction of the spectrum. In addition, they generally rely on highly optimized and scalable kernels such as matrix-vector multiplications that leverage the massive parallelism and the computational power of modern exascale systems. \textit{Chebyshev Accelerated Subspace iteration Eigensolver} (ChASE) is able to compute several thousands of the most extreme eigenpairs of dense hermitian matrices with proven scalability over massive parallel accelerated clusters. This work presents an extension of ChASE to solve for a portion of the spectrum of pseudo-hermitian Hamiltonians as they appear in the treatment of excitonic materials. The new pseudo-hermitian solver achieves similar convergence and performance as the hermitian one. By exploiting the numerical structure and spectral properties of the Hamiltonian matrix, we propose an oblique variant of Rayleigh-Ritz projection featuring quadratic convergence of the Ritz-values with no explicit construction of the dual basis set. Additionally, we introduce a parallel implementation of the recursive matrix-product operation appearing in the Chebyshev filter with limited amount of global communications. Our development is supported by a full numerical analysis and experimental tests.

</details>


### [13] [Stable evaluation of derivatives for barycentric and continued fraction representations of rational functions](https://arxiv.org/abs/2601.10667)
*Tobin A. Driscoll,Yuxing Zhou*

Main category: math.NA

TL;DR: First numerically stable methods for derivative evaluation in barycentric rational approximation, including O(n) algorithm for all derivatives, plus extension of O(n) TCF first derivative algorithm to higher orders.


<details>
  <summary>Details</summary>
Motivation: Existing fast algorithms exist for rational function approximation in both barycentric and Thiele continued fraction representations, but there's a need for numerically stable methods for derivative evaluation, particularly for higher-order derivatives.

Method: Develop numerically stable methods for derivative evaluation in barycentric representation, including an O(n) algorithm for computing all derivatives. Also extend an existing O(n) algorithm for Thiele continued fraction first derivative evaluation to handle higher-order derivatives.

Result: Numerical experiments confirm that the proposed methods are both robust (numerically stable) and efficient, with linear time complexity for derivative evaluation.

Conclusion: The paper presents the first numerically stable derivative evaluation methods for barycentric rational approximation and extends efficient derivative computation to higher orders for Thiele continued fractions, providing practical tools for scientific computing applications.

Abstract: Fast algorithms for approximation by rational functions exist for both barycentric and Thiele continued fraction (TCF) representations. We present the first numerically stable methods for derivative evaluation in the barycentric representation, including an $O(n)$ algorithm for all derivatives. We also extend an earlier $O(n)$ algorithm for evaluation of the TCF first derivative to higher orders. Numerical experiments confirm the robustness and efficiency of the proposed methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [14] [Lossless Strichartz estimates on the square torus over short time intervals](https://arxiv.org/abs/2601.09895)
*Connor Quinn*

Main category: math.AP

TL;DR: Lossless Strichartz estimates at critical exponent for Schrödinger equation on square torus with frequency-localized data on small time windows.


<details>
  <summary>Details</summary>
Motivation: To establish precise dispersive estimates for the Schrödinger equation on compact domains (square torus) at the critical scaling exponent, which is important for understanding wave propagation and nonlinear PDE analysis.

Method: Frequency localization of initial data and analysis on small time windows whose length depends on the frequency parameter λ ≫ 1, focusing on the critical exponent q_c = 2(n+1)/(n-1).

Result: Proves lossless Strichartz estimates at the critical exponent q_c for the Schrödinger equation on the square torus with frequency-localized initial data.

Conclusion: Achieves optimal dispersive estimates on compact domains by carefully controlling time windows relative to frequency scales, providing important tools for nonlinear Schrödinger equation analysis on tori.

Abstract: We prove lossless Strichartz estimates at the critical exponent $q_c = \frac{2(n+1)}{n-1}$ on the square torus for the Schrödinger equation with frequency localized initial data on small time windows with length depending on the frequency parameter $λ\gg 1$.

</details>


### [15] [On the Dirichlet boundary value problem on Cartan-Hadamard manifolds](https://arxiv.org/abs/2601.09930)
*Marcos P. Cavalcante,José M. Espinar,Diego A. Marín*

Main category: math.AP

TL;DR: Extends non-existence results for bounded solutions of semi-linear elliptic equations from hyperbolic spaces to Cartan-Hadamard manifolds using convex hypersurface comparison techniques.


<details>
  <summary>Details</summary>
Motivation: To generalize previous results by Bonorino and Klaser on non-existence of bounded solutions to semi-linear elliptic equations from hyperbolic spaces to more general Cartan-Hadamard manifolds, overcoming the lack of totally geodesic foliations that were crucial in the hyperbolic setting.

Method: Develops a novel comparison technique based on convex hypersurfaces inspired by Choi, Gálvez, and Lozano, which allows working without the totally geodesic foliations available in hyperbolic spaces.

Result: Establishes non-existence of bounded (viscosity) solutions to Δu + f(u) = 0 on Cartan-Hadamard manifolds with prescribed asymptotic boundary conditions, extending previous hyperbolic space results.

Conclusion: Demonstrates the interplay between curvature, Laplacian spectrum, and asymptotic boundary geometry in determining existence/non-existence of solutions, with the convex hypersurface technique providing a powerful tool for Cartan-Hadamard manifolds lacking special foliations.

Abstract: In this paper, we investigate the Dirichlet boundary value problem on Cartan-Hadamard manifolds, focusing on the non-existence of bounded (viscosity) solutions to semi-linear elliptic equations of the form $Δu + f(u) = 0$ in domains with prescribed asymptotic boundary, extending previous results by Bonorino and Klaser originally established for hyperbolic spaces. Using a novel comparison technique based on convex hypersurfaces inspired by Choi, Gálvez, and Lozano, we overcome the absence of totally geodesic foliations, which are instrumental in the hyperbolic space. Our results highlight the interplay between curvature, the spectrum of the Laplacian, and the geometry of the asymptotic boundary.

</details>


### [16] [Stability and instability of small BGK waves](https://arxiv.org/abs/2601.10030)
*Dongfen Bian,Emmanuel Grenier,Wenrui Huang,Benoit Pausader*

Main category: math.AP

TL;DR: Linear stability of small BGK waves depends on sign of energy distribution derivative at zero energy


<details>
  <summary>Details</summary>
Motivation: To establish a criterion for determining linear stability/instability of small Bernstein-Green-Kruskal (BGK) waves in plasma physics

Method: Mathematical analysis proving that stability is determined by the sign of the derivative of energy distributions at zero energy

Result: Proved that linear stability of small BGK waves is determined by the sign of dE/dε at ε=0

Conclusion: The derivative of energy distribution at zero energy serves as a simple criterion for predicting linear stability of small BGK waves

Abstract: The aim of this article is to prove that the linear stability or instability of small Bernstein-Green-Kruskal (BGK) waves is determined by the sign of the derivative of their energy distributions at $0$ energy.

</details>


### [17] [Transport equation theory in the Triebel-Lizorkin spaces and its applications to the ideal fluid flows](https://arxiv.org/abs/2601.10071)
*Qianyuan Zhang,Kai Yan*

Main category: math.AP

TL;DR: Develops transport equation theory in Triebel-Lizorkin spaces, obtains commutator estimates without divergence-free condition, proves local well-posedness, and applies to ideal MHD system with complete Hadamard well-posedness.


<details>
  <summary>Details</summary>
Motivation: To establish a general theory for transport equations in Triebel-Lizorkin spaces that can be applied to various evolution equations including fluid dynamics models, overcoming limitations of conventional divergence-free conditions.

Method: Uses Bony paraproduct decomposition and vector-valued maximal function inequalities to derive commutator estimates, then combines method of characteristics with compactness arguments to obtain a priori estimates and prove local well-posedness.

Result: Develops new transport equation theory in Triebel-Lizorkin spaces, obtains complete local well-posedness for ideal MHD system covering both sub-critical and critical regimes, and provides corresponding blow-up criteria.

Conclusion: The paper establishes a comprehensive transport theory in Triebel-Lizorkin spaces that refines and substantially extends previous work, with broad applicability to evolution equations in fluid dynamics and related fields.

Abstract: In this paper, we develop a general theory for the transport equation within the framework of Triebel-Lizorkin spaces. We first derive commutator estimates in these spaces, dispensing with the conventional divergence-free condition, via the Bony paraproduct decomposition and vector-valued maximal function inequalities. Building on these estimates and combining the method of characteristics with a compactness argument, we then obtain the new a priori estimates and prove local well-posedness for the transport equation in Triebel-Lizorkin spaces. The resulting theory is applicable to a wide range of evolution equations, including models for incompressible and compressible ideal fluid flows, shallow water waves, among others. As an illustration, we consider the incompressible ideal magnetohydrodynamics (MHD) system. Employing the general transport theory developed here yields a complete local well-posedness result in the sense of Hadamard, covering both sub-critical and critical regularity regimes, and provides corresponding blow-up criteria for the ideal MHD equations in Triebel-Lizorkin spaces. Our results refine and substantially extend earlier work in this direction.

</details>


### [18] [Characteristics of drift effects in the quasi-geostrophic equation arising from nonlinear symmetry](https://arxiv.org/abs/2601.10185)
*Masakazu Yamamoto*

Main category: math.AP

TL;DR: Comparison of two similar 2D diffusion equations in meteorology (quasi-geostrophic vs convection-diffusion) showing how nonlinear effects act differently despite same scale and order, quantified through large-time behavior and asymptotic profile distortions.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental differences between two similar diffusion equations used in meteorology that have the same scale and differentiation order but different nonlinear effect directions, and to quantify these differences through asymptotic analysis.

Method: Comparative analysis of quasi-geostrophic and convection-diffusion equations, examining their large-time behavior and asymptotic profiles, focusing on how nonlinear distortions differ despite structural similarities.

Result: The main difference lies in the direction of nonlinear effects: one acts along isothermal surfaces while the other acts along temperature gradients. This difference is quantified through the large-time behavior of solutions and comparison of nonlinear distortions in asymptotic profiles.

Conclusion: Despite structural similarities, the two meteorological diffusion equations exhibit fundamentally different nonlinear behaviors due to the direction of nonlinear effects, with spatial symmetry of first approximations playing a crucial role in understanding these differences.

Abstract: This paper compares two similar diffusion equations that appear in meteorology. One is the quasi-geostrophic equation, and the other is the convection-diffusion equation. Both are two-dimensional bilinear equations, and the order of differentiation is the same. Naturally, their scales also coincide. However, the direction in which the nonlinear effects act differs: one acts along the isothermal surface, while the other acts along the temperature gradient in a specified direction. The main assertion quantifies this difference through the large-time behavior of their solutions. In particular, the nonlinear distortions in the asymptotic profiles of both equations are compared. In this context, the spatial symmetry of the first approximation plays a crucial role, but the solutions require no symmetry.

</details>


### [19] [Optimisation of the lowest Robin eigenvalue in exterior domains of the hyperbolic plane](https://arxiv.org/abs/2601.10280)
*Antonio Celentano,David Krejcirik,Vladimir Lotoreichik*

Main category: math.AP

TL;DR: The paper studies the Robin Laplacian in the exterior of convex domains in hyperbolic plane, showing the essential spectrum is [1/4,∞) and establishing isoperimetric inequalities for the lowest Robin eigenvalue.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties of the Robin Laplacian in hyperbolic geometry, particularly in exterior domains, and establish geometric optimization results analogous to Euclidean isoperimetric inequalities.

Method: Mathematical analysis of the Robin Laplacian operator in hyperbolic plane, using spectral theory and geometric comparison techniques. The approach involves studying essential spectrum, discrete eigenvalues, and establishing comparison results between convex domains and geodesic disks.

Result: 1) Essential spectrum is [1/4,∞). 2) Under convexity, discrete eigenvalues below 1/4 exist iff Robin parameter is below a non-positive critical constant. 3) Main result: The lowest Robin eigenvalue for exterior of convex domain does not exceed that for exterior of geodesic disk with boundary curvature not smaller than averaged curvature. 4) Geodesic disk maximizes lowest Robin eigenvalue under fixed area/perimeter constraints.

Conclusion: The paper establishes fundamental spectral properties of the Robin Laplacian in hyperbolic exterior domains and proves isoperimetric-type inequalities showing geodesic disks are extremal for maximizing the lowest Robin eigenvalue under geometric constraints.

Abstract: We consider the Robin Laplacian in the exterior of a bounded simply-connected Lipschitz domain in the hyperbolic plane. We show that the essential spectrum of this operator is $[\frac14,\infty)$ and that, under convexity assumption on the domain, there exist discrete eigenvalues below $\frac14$ if, and only if, the Robin parameter is below a non-positive critical constant, which depends on the shape of the domain. As the main result, we prove that the lowest Robin eigenvalue for the exterior of a bounded geodesically convex domain $Ω$ in the hyperbolic plane does not exceed such an eigenvalue for the exterior of the geodesic disk, whose geodesic curvature of the boundary is not smaller than the averaged geodesic curvature of the boundary of $Ω$. This result implies as a consequence that under fixed area or fixed perimeter constraints the exterior of the geodesic disk maximises the lowest Robin eigenvalue among exteriors of bounded geodesically convex domains. Moreover, we obtain under the same geometric constraints a reverse inequality between the critical constants.

</details>


### [20] [High-Contrast Transmission Resonances for the Lamé System](https://arxiv.org/abs/2601.10290)
*Long Li,Mourad Sini*

Main category: math.AP

TL;DR: Sharp asymptotic description of elastic scattering resonances in high-contrast 3D Lamé transmission problems, revealing wavelength-scale clusters near Neumann eigenvalues and subwavelength resonances near zero with lifetime dichotomy.


<details>
  <summary>Details</summary>
Motivation: To understand scattering resonances in high-contrast elastic materials where interior-to-exterior Lamé moduli and densities scale as 1/τ as τ→0, which is relevant for wave phenomena in composite materials and metamaterials.

Method: Analyzes the self-adjoint Hamiltonian's resolvent poles using asymptotic methods as τ→0. Studies meromorphic continuation of resolvent and obtains sharp asymptotics for resonances near real axis. Derives explicit expansions with finite-rank leading terms and quantitative remainder bounds.

Result: 1) Near each nonzero Neumann eigenvalue: cluster of resonances just below it with imaginary parts of order τ. 2) Near zero (subwavelength): resonances with real parts of order √τ and lifetime dichotomy - imaginary parts of order τ generically, but order τ² for explicit admissible set E. 3) For microresonators: wavelength-scale dominant contribution is anisotropic elastic point scatterer; near zero eigenvalue, leading-order behavior is monopole or dipole type with rigorous criterion distinguishing cases.

Conclusion: Provides complete classification of long-lived elastic resonances in high-contrast limit, establishing sharp asymptotic descriptions and resolvent asymptotics for both wavelength-scale and subwavelength regimes, with applications to elastic metamaterials and wave scattering.

Abstract: We consider the Lamé transmission problem in $\mathbb{R}^3$ with a bounded isotropic elastic inclusion in a high-contrast setting, where the interior-to-exterior Lamé moduli and densities scale like $1/τ$ as $τ\to0$. We study the scattering resonances of the associated self-adjoint Hamiltonian, defined as the poles of the meromorphic continuation of its resolvent.
  We obtain a sharp asymptotic description of resonances near the real axis as $τ\to0$. Near each nonzero Neumann eigenvalue of the interior Lamé operator there is a cluster of resonances lying just below it in the complex plane; in this wavelength-scale regime the imaginary parts are of order $τ$ with non-vanishing leading coefficients. In addition, near zero (a subwavelength regime), we identify resonances with real parts of order $\sqrtτ$ and prove a lifetime dichotomy: their imaginary parts are of order $τ$ generically, but of order $τ^2$ for an explicit admissible set $\mathcal E$. This yields a classification of long-lived elastic resonances in the high-contrast limit.
  We also establish resolvent asymptotics for both fixed-size resonators and microresonators. We derive explicit expansions with a finite-rank leading term and quantitative remainder bounds, valid near both wavelength-scale and subwavelength resonances. For microresonators, at the wavelength scale the dominant contribution is an anisotropic elastic point scatterer. Near the zero eigenvalue, the leading-order behaviour is of monopole or dipole type, and we give a rigorous criterion distinguishing the two cases.

</details>


### [21] [Global minimizers for a two-sided biharmonic Alt-Caffarelli problem](https://arxiv.org/abs/2601.10297)
*Hans-Christoph Grunau,Marius Müller*

Main category: math.AP

TL;DR: The paper studies global minimizers of biharmonic Alt-Caffarelli functionals, finding half-space solutions are global minimizers for two-sided but not one-sided problems, identifying new classes with constant Laplacian, and showing two-sided minimizers don't satisfy PDEs.


<details>
  <summary>Details</summary>
Motivation: To understand the properties and classification of global minimizers for biharmonic analogues of the Alt-Caffarelli functional, extending previous two-dimensional results to arbitrary dimensions and examining differences between one-sided and two-sided problems.

Method: Mathematical analysis of biharmonic Alt-Caffarelli functionals, building on recent work by Lamboley and Nahon that reduced potential minimizers in dimension two to four categories, and extending this analysis to arbitrary dimensions.

Result: Half-space solutions are global minimizers for two-sided problems but not one-sided; identified new classes of global minimizers with constant Laplacian; three of the four categories from 2D persist in any dimension; two-sided minimizers don't satisfy PDEs (unlike one-sided).

Conclusion: The study reveals fundamental differences between one-sided and two-sided biharmonic Alt-Caffarelli problems, provides classification of global minimizers across dimensions, and shows surprising regularity properties (or lack thereof) for these minimizers.

Abstract: We study global minimizers of biharmonic analogues of the Alt-Caffarelli functional. It turns out that half-space solutions are global minimizers for the two-sided Alt-Caffarelli functional, but not in the one-sided case. In addition, we identify a further class of global minimizers, all of which have constant Laplacian. Recent work by J. Lamboley and M. Nahon reduces potential global minimizers in dimension two to four possible categories. Our work shows that three of these categories persist in any dimension and are in fact global minimizers.
  Moreover, we show that minimizers of the two-sided biharmonic Alt-Caffarelli problem do in general not satisfy a partial differential equation, not even with a signed measure as right-hand-side. This is in sharp contrast to the corresponding one-sided problem.

</details>


### [22] [Optimality in nonlocal time-dependent obstacle problems](https://arxiv.org/abs/2601.10417)
*Ioannis Athanasopoulos,Luis Caffarelli,Emmanouil Milakis*

Main category: math.AP

TL;DR: The paper demonstrates how quasiconvexity helps determine optimal regularity for time derivatives and establishes continuity conditions in nonlocal time-dependent obstacle problems.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize the regularity properties of solutions to nonlocal time-dependent obstacle problems, particularly focusing on the temporal derivative's behavior.

Method: Utilizes quasiconvexity properties to analyze optimal regularity of time derivatives and establishes mathematical conditions for continuity in nonlocal obstacle problems.

Result: Shows effectiveness of quasiconvexity in addressing optimal regularity of temporal derivatives and provides conditions ensuring continuity in nonlocal time-dependent obstacle settings.

Conclusion: Quasiconvexity is a powerful tool for analyzing regularity properties in nonlocal time-dependent obstacle problems, enabling characterization of temporal derivative behavior and continuity conditions.

Abstract: This paper showcases the effectiveness of the quasiconvexity property in addressing the optimal regularity of the temporal derivative and establishes conditions for its continuity in nonlocal time-dependent obstacle problems.

</details>


### [23] [A Riemannian Autocorrelation Function and its Application to Non-Local Isoperimetric Energies](https://arxiv.org/abs/2601.10481)
*Michael Bleher,Denis Brazke,Sebastian Nill*

Main category: math.AP

TL;DR: The paper studies non-local isoperimetric energies on spheres using a Riemannian autocorrelation function, linking it to BV functions and establishing Γ-convergence results.


<details>
  <summary>Details</summary>
Motivation: To analyze non-local isoperimetric energies on the round sphere by introducing a Riemannian generalization of Matheron's set covariogram from convex geometry, connecting it to functions of bounded variation and perimeter theory.

Method: Introduce a Riemannian autocorrelation function c_Ω for measurable sets on compact Riemannian manifolds. Characterize BV functions via geodesic difference quotients, show Ω has finite perimeter iff c_Ω is Lipschitz. Reformulate non-local energies E_{γ,ε} in terms of c_Ω and analyze Γ-convergence as ε→0.

Result: Established equivalence between finite perimeter sets and Lipschitz autocorrelation functions with Lipschitz constant related to perimeter. Successfully reformulated non-local energies using autocorrelation function and computed Γ-limit as ε→0.

Conclusion: The Riemannian autocorrelation function provides a powerful tool for analyzing non-local isoperimetric problems on manifolds, connecting geometric measure theory with variational analysis through Γ-convergence techniques.

Abstract: We study a family of non-local isoperimetric energies $E_{γ,\varepsilon}$ on the round sphere $M = S^n$, where the non-local interaction kernel $K_\varepsilon$ is the fundamental solution of the Helmholtz operator $1 - \varepsilon^2 Δ$. To analyse these energies, we introduce a Riemannian autocorrelation function $c_Ω$ associated to a measurable set $Ω\subset M$, defined on any compact, connected, oriented Riemannian manifold without boundary $(M^n,g)$ of dimension $n\ge2$. This function is intimately linked to Matheron's set covariogram from convex geometry. By establishing a characterisation of functions of bounded variation $BV(M)$ in terms of geodesic difference quotients, we show that $Ω$ has finite perimeter if and only if $c_Ω$ is Lipschitz, and we relate the Lipschitz constant to the perimeter of $Ω$. We show that on the round sphere $E_{γ,\varepsilon}$ admits a reformulation in terms of $c_Ω$, which allows us to compute the limit as $\varepsilon \to 0$ in a variational sense, that is, in the framework of $Γ$-convergence.

</details>


### [24] [A proof of the soliton resolution conjecture for the Benjamin--Ono equation](https://arxiv.org/abs/2601.10488)
*Louise Gassot,Patrick Gérard,Peter D. Miller*

Main category: math.AP

TL;DR: Proof of soliton resolution conjecture for Benjamin-Ono equation: any sufficiently regular decaying solution decomposes into finite sum of solitons plus radiation in long-time asymptotics.


<details>
  <summary>Details</summary>
Motivation: To prove the soliton resolution conjecture for the Benjamin-Ono equation, which describes how solutions decompose into fundamental components (solitons and radiation) over long time scales.

Method: Uses representation formula for solutions and detailed analysis of distorted Fourier transform associated with the Lax operator; establishes correspondence between spectral theory of Lax operator and soliton resolution terms.

Result: Successfully proves soliton resolution conjecture: every sufficiently regular decaying solution decomposes into finite sum of solitons with different velocities plus radiative remainder in long-time asymptotics.

Conclusion: The paper provides rigorous proof of soliton resolution for Benjamin-Ono equation, connecting spectral theory of Lax operator to asymptotic decomposition of solutions into solitons and radiation.

Abstract: We give a proof of the soliton resolution conjecture for the Benjamin--Ono equation, namely every solution with sufficiently regular and decaying initial data can be written as a finite sum of soliton solutions with different velocities up to a radiative remainder term in the long--time asymptotics. We provide a detailed correspondence between the spectral theory of the Lax operator associated to the initial data and the different terms of the soliton resolution expansion. The proof is based on a new use of a representation formula of the solution due to the second author, and on a detailed analysis of the distorted Fourier transform associated to the Lax operator.

</details>


### [25] [Michael-Simon inequality for anisotropic energies close to the area via multilinear Kakeya-type bounds](https://arxiv.org/abs/2601.10647)
*Guido De Philippis,Alessandro Pigati*

Main category: math.AP

TL;DR: The paper proves Michael-Simon inequalities for anisotropic surface energies in 2D surfaces in 3D space, requiring convex integrands close to isotropic area or satisfying atomic conditions.


<details>
  <summary>Details</summary>
Motivation: Generalize classical isotropic area to anisotropic functionals and establish Michael-Simon inequalities (analogous to Sobolev inequalities for surfaces) for these anisotropic energies, which lack monotonicity formulas available in isotropic cases.

Method: For k=2, n=3: 1) Show Michael-Simon inequality holds for convex integrands F close to 1 in C^1 norm. 2) Introduce new functional inequality for vector fields on plane as quantitative version of Alberti's rank-one theorem. 3) Simplify Almgren's unpublished proof using this inequality. 4) Extend to integrands including ℓ^p norms for p∈(1,∞). 5) For general F satisfying atomic condition, prove equivalence between Michael-Simon validity and compactness of rectifiable varifolds.

Result: 1) Michael-Simon inequality established for convex F close to isotropic area. 2) New functional inequality for planar vector fields proved. 3) Michael-Simon inequality extended to ℓ^p norm integrands. 4) Equivalence shown between Michael-Simon validity and varifold compactness for atomic condition integrands.

Conclusion: Anisotropic Michael-Simon inequalities hold for important classes of integrands in 2D surfaces in 3D, with connections to varifold compactness, providing tools for anisotropic geometric analysis despite lack of monotonicity formulas.

Abstract: Given an anisotropic integrand $F:\text{Gr}_k(\mathbb R^n)\to(0,\infty)$, we can generalize the classical isotropic area by looking at the functional $$\mathcal{F}(Σ^k):=\int_ΣF(T_xΣ)\,d\mathcal{H}^k.$$ While a monotonicity formula is not available for critical points, when $k=2$ and $n=3$ we show that the Michael-Simon inequality holds if $F$ is convex and close to $1$ (in $C^1$), meaning that $\mathcal{F}$ is close to the usual area.
  Our argument is partly based on some key ideas of Almgren, who proved this result in an unpublished manuscript, but we largely simplify his original proof by showing a new functional inequality for vector fields on the plane, which can be seen as a quantitative version of Alberti's rank-one theorem.
  As another byproduct, we also show Michael-Simon for another class of integrands which includes the $\ell^p$ norms for $p\in(1,\infty)$. For a general $F$ satisfying the atomic condition, we also show that the validity of Michael-Simon is equivalent to compactness of rectifiable varifolds.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [RLC Parameters of a Two-Wire Line with the Finite Element Method](https://arxiv.org/abs/2601.09829)
*Marc Boulé*

Main category: physics.comp-ph

TL;DR: Tutorial on computing DC resistance, inductance, and capacitance of parallel wires using finite element method with open-source ONELAB software.


<details>
  <summary>Details</summary>
Motivation: To provide a practical tutorial for computing key electrical parameters (R, L, C) of parallel wire configurations using finite element analysis, including modeling of insulation and defects.

Method: Uses 3D finite element method with infinite domain modeling for electrostatic and magnetostatic fields, electrokinetic formulation for current flow, and considers insulation and physical defects. Implemented with open-source ONELAB software.

Result: Provides complete simulation framework with code listing, validated against analytical models (when available) and commercial Altair Flux software.

Conclusion: Demonstrates a practical, open-source approach for accurate computation of wire parameters using finite element analysis, validated against established methods.

Abstract: This tutorial paper shows how to compute the DC (or low-frequency) resistance, inductance and capacitance of a pair of parallel wires using the finite element method. A three-dimensional infinite domain (open boundary) modeling of electrostatic and magnetostatic fields is presented, along with the electrokinetic formulation for the current flow inside the wires. The effects of the insulation and of a proposed physical defect in the wires are also considered. The open-source ONELAB software is used to perform the simulations and the code listing is provided. Comparisons using analytical models (when applicable) and the Altair Flux software are performed to help validate the simulations.

</details>


### [27] [A Level Set Method on Particle Flow Maps](https://arxiv.org/abs/2601.09939)
*Jinjin He,Taiyuan Zhang,Zhiqi Li,Junwei Zhou,Duowen Chen,Bo Zhu*

Main category: physics.comp-ph

TL;DR: PFM-LS method combines particle-based level set storage in interface regions with grid-based representation elsewhere, using bidirectional flow maps and differential form interpretation for superior interface tracking with sub-grid feature preservation.


<details>
  <summary>Details</summary>
Motivation: Traditional level-set methods struggle with preserving fine geometric features and sub-grid details during complex deformations, especially in interface tracking applications requiring high fidelity.

Method: Hybrid particle-grid approach storing level-set values, gradients, and Hessians on particles in narrow interface band, using bidirectional flow maps for advection, interpreting level set as 3-form and gradient as 1-form, dual-timescale approach with long-range maps for values/gradients and frequent short-range map reinitialization for Hessian, adaptive particle control, and hybrid particle-grid quasi-Newton redistancing.

Result: PFM-LS achieves state-of-the-art volume preservation and shape fidelity in 2D and 3D benchmarks, outperforming existing level-set methods while preserving sub-grid features that traditional methods cannot capture.

Conclusion: The PFM-LS method provides a robust framework for high-fidelity interface tracking with exceptional geometric accuracy, combining the strengths of particle and grid representations to overcome limitations of conventional level-set approaches.

Abstract: This paper introduces a Particle Flow Map Level Set (PFM-LS) method for high-fidelity interface tracking. We store level-set values, gradients, and Hessians on particles concentrated in a narrow band around the interface, advecting them via bidirectional flow maps while using a conventional grid-based representation elsewhere. By interpreting the level set value as a 3-form and its gradient as a 1-form, PFM-LS achieves exceptional geometric fidelity during complex deformations and preserves sub-grid features that traditional methods cannot capture. Our dual-timescale approach utilizes long-range maps for values and gradients, with frequent reinitialization of short-range maps for the distortion-sensitive Hessian, alongside adaptive particle control that maintains sufficient density within the narrow band. We also develop a hybrid particle-grid quasi-Newton redistancing scheme that preserves fine-scale features while enforcing the signed-distance property. Benchmark comparisons in 2D and 3D demonstrate that PFM-LS achieves state-of-the-art volume preservation and shape fidelity against a broad range of existing level-set methods.

</details>


### [28] [A volume penalization method for solving conjugate scalar transport with interfacial jump conditions](https://arxiv.org/abs/2601.10134)
*Ming Liu,Yosuke Hasegawa*

Main category: physics.comp-ph

TL;DR: A novel immersed boundary method for conjugate scalar transport with interfacial jumps on complex geometries, achieving <3% error vs body-fitted meshes.


<details>
  <summary>Details</summary>
Motivation: Accurate simulation of conjugate scalar transport with interfacial jump conditions on complex geometries is challenging but important for thermal/chemical processes.

Method: Developed a novel interfacial treatment in volume penalization method (immersed boundary method) for general conjugate scalar transport with flux and scalar jumps. First solves Neumann boundary problems, then extends to general cases with additional source terms representing jump conditions.

Result: Verified with 1D diffusion problem showing improved accuracy and unified governing equations. Applied to fluid-solid coupled diffusion and advection-diffusion problems with scalar/flux jumps. Achieved <3% average relative deviation vs body-fitted mesh simulations.

Conclusion: The proposed scheme successfully handles conjugate scalar transport with interfacial jumps on complex geometries using immersed boundary methods with good accuracy comparable to body-fitted approaches.

Abstract: Conjugate scalar transport with interfacial jump conditions on complex interfacial geometries is common in thermal and chemical processes, while its accurate and efficient simulations are still quite challenging. In the present study, a novel treatment of a two-phase interface in the volume penalization method, a kind of immersed boundary method, for solving conjugate scalar transport with general interfacial boundary conditions is developed. We first propose an interfacial treatment for solving an advection-diffusion equation with a Neumann boundary condition, and then extend it to general conjugate scalar transport with both interfacial flux and scalar jumps. A one-dimensional diffusion problem is solved to verify the present scheme and demonstrate the advantage of the present scheme in improving accuracy and unifying the governing equations in the two phases with an additional source term representing the local jump condition of the interfacial scalar flux. Then, the present scheme is further applied to fluid-solid coupled scalar diffusion and advection-diffusion problems with the scalar and its flux jumps across the interface. The simulation results of the present scheme generally show good agreement with reference results obtained by body-fitted mesh simulations with average relative deviations less than 3.0%.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [29] [A feasibility study for a Doppler Reflectometer System in the JT-60SA tokamak](https://arxiv.org/abs/2601.09906)
*D. Carralero,T. Happel,T. Estrada,T. Tokuzawa,J. Martínez,E. de la Luna,A. Cappa,J. García*

Main category: physics.plasm-ph

TL;DR: Feasibility study for installing a Doppler reflectometer diagnostic system in JT-60SA tokamak, showing viable design using minimal port space.


<details>
  <summary>Details</summary>
Motivation: To support JT-60SA research plan by enabling measurement of plasma turbulence and flows through Doppler reflectometry, which is relevant for multiple research fields in fusion plasma physics.

Method: Three-step approach: 1) Identify scientific scope and program for DR in JT-60SA context, 2) Use ray tracing code for feasibility study to determine geometric solution for core and edge probing, 3) Preliminary conceptual design discussion including minimum viable and baseline systems.

Result: Successfully identified viable geometric solution for DR installation that can probe both core and edge plasma in required wave number range. Conceptual design could use only a small fraction of a horizontal port, leaving space for other diagnostics.

Conclusion: Installation of a Doppler reflectometer system in JT-60SA is both viable and practical, with potential to significantly contribute to the tokamak's research objectives while using minimal port space.

Abstract: In this work we present a study on the viability and practicality of installing a Doppler reflectometer (DR) system in the JT-60SA advanced tokamak. First, we discuss its scientific scope in the context of the JT-60SA research plan. We identify a number of fields in which a DR would be very relevant for the accomplishment of said plan and outline a scientific program for the diagnostic. Then, starting from a number of design hypothesis, we use a ray tracing code to carry out a feasibility study for a number of relevant scenarios and identify a geometric solution for the installation of a DR such that both core and edge can be probed in the prescribed wave number range, thus achieving the proposed scientific objectives. Finally, we perform a preliminary discussion on the different possibilities for a conceptual design (including a minimum viable system and a baseline system) and their requirements in terms of components and space. We conclude that a viable conceptual design could be carried out using a small fraction of a horizontal port, leaving room for additional diagnostic systems.

</details>


### [30] [Effects of parallel magnetic fields on sheaths near biased electrodes in a highly collisional Z-pinch plasma](https://arxiv.org/abs/2601.10039)
*C. R. Skolar,B. Srinivasan*

Main category: physics.plasm-ph

TL;DR: Sheath formation near biased electrodes with parallel magnetic fields in Z-pinch plasmas shows non-monotonic potential profiles, with classical sheath forming within electron gyroradius due to magnetized electrons gyrating into walls. Perpendicular current is suppressed by magnetic field resistivity, while significant parallel flows emerge.


<details>
  <summary>Details</summary>
Motivation: Sheath formation near biased electrodes in magnetic fields parallel to the wall is understudied, especially in Z-pinch fusion experiments where understanding plasma-electrode interactions is crucial for device performance and stability.

Method: 1X-2V Boltzmann-Poisson simulations of an axial cut at the pinch radius between two biased electrodes with parallel magnetic field. Collision frequencies artificially increased to enhance thermalization in smaller simulation domain versus actual experiment size.

Result: Non-monotonic sheath profiles with potential increasing away from wall to a peak before decaying. Classical sheath forms within electron gyroradius due to magnetized electrons gyrating into wall. Perpendicular current density three orders lower than unmagnetized predictions due to magnetic field acting as high resistivity. Significant parallel flows from pressure tensor-Lorentz force balance induce parallel current density three orders larger than perpendicular.

Conclusion: Magnetic field parallel to electrodes fundamentally alters sheath physics in Z-pinch plasmas: sheath structure insensitive to bias potential, perpendicular current suppressed by magnetic resistivity, and significant parallel flows dominate current transport, differing substantially from unmagnetized cases.

Abstract: Sheath formation near biased electrodes in magnetic fields parallel to the wall is an understudied topic, especially within the context of Z-pinch fusion experiments. We perform 1X-2V Boltzmann-Poisson simulations of an axial cut at the pinch radius of a Z-pinch plasma between two biased electrodes with a magnetic field parallel to the wall. The collision frequencies are artificially increased to enhance thermalization of the plasma in the smaller simulation domain versus the actual experiment size; this increases the perpendicular mobility and partially de-magnetizes the ions resulting in non-monotonic sheath profiles with the potential increasing away from the wall to a peak before decaying. A classical sheath forms within an electron gyroradius from the wall not due to the natural thermal motion of the electrons, but due to the magnetized electrons gyrating into the wall; therefore, the sheath structure does not significantly change with bias potential or between electrodes. With increasing bias potential, a current is induced perpendicular to the wall due to changes in ion flow, differing from unmagnetized cases where current is induced by changes in electron flow. The magnetic field acts as a high resistivity with the perpendicular current density being three orders of magnitude lower than unmagnetized theoretical predictions. There is, however, significant flow parallel to the wall from the force balance between the pressure tensor and Lorentz force. These parallel flows induce a parallel current density three orders of magnitude larger than the perpendicular current density.

</details>


### [31] [Updated electrical design of the Diagnostic Neutral Beam Injector in RFX-mod2](https://arxiv.org/abs/2601.10293)
*Marco Barbisan,Bruno Laterza,Luca Cinnirella,Lionello Marrelli,Federico Molon,Simone Peruzzo,Enrico Zampiva*

Main category: physics.plasm-ph

TL;DR: The paper describes upgrades to the Diagnostic Neutral Beam Injector (DNBI) for RFX-mod2 experiment to enable core plasma measurements via CXRS and MSE diagnostics.


<details>
  <summary>Details</summary>
Motivation: The DNBI is crucial for providing novel information about Reversed Field Pinch plasma confinement by enabling core measurements of ion speed, temperature, impurity content, and magnetic field properties through Charge Exchange Recombination Spectroscopy and Motional Stark Effect diagnostics.

Method: The DNBI uses an arc discharge H+ source with 4-grid 50 keV acceleration system, producing a 50 ms, 5 A ion beam that is neutralized via gas target. Residual ions are deflected by magnetic field before injection. The system undergoes extraordinary maintenance and structural upgrades to improve reliability and safety, including electrical plant and control system improvements.

Result: The paper presents the latest upgrades to the DNBI's electrical plants and control system, though specific performance metrics from these upgrades are not detailed in the abstract.

Conclusion: The upgraded DNBI will significantly enhance the RFX-mod2 experiment's diagnostic capabilities for studying Reversed Field Pinch plasma confinement by enabling core plasma measurements previously unavailable.

Abstract: The Diagnostic Neutral Beam Injector of the RFX-mod2 experiment (Consorzio RFX, Padova) is expected to provide novel and significant information about the Reversed Field Pinch confinement of fusion plasmas. The injection of the hydrogen beam in the plasma will allow Charge Exchange Recombination Spectroscopy (CXRS) and Motional Stark Effect diagnostics (MSE) to measure several quantities: ion speed, ion temperature, impurity content, intensity and pitch of the magnetic field. The DNBI is of particular importance for allowing the determination of these quantities at the core of the plasma. The present DNBI, built by the Budker Institute of Plasma Physics, features an arc discharge H+ source, coupled to a 4-grid 50 keV acceleration system. The 50 ms, 5 A ion beam is neutralized by charge exchange by means of a gas target; residual ions are then deflected by a magnetic field before injection in the torus chamber. The beam can be modulated at maximum 250 Hz. The DNBI will undergo extraordinary maintenance and a structural upgrade to improve its reliability and safety. This contribution presents the latest upgrades of the electrical plants and of the control system of the DNBI.

</details>


### [32] [From Weibel seeds to collisionless dynamos beyond pair-plasmas](https://arxiv.org/abs/2601.10472)
*Lise Hanebring,James Juno,Ammar Hakim,Jason M. TenBarge,Istvan Pusztai*

Main category: physics.plasm-ph

TL;DR: Simulations of collisionless turbulence in intracluster medium capture both magnetic seed field generation via electron Weibel instability and subsequent dynamo amplification using 10-moment fluid solver with realistic mass ratio.


<details>
  <summary>Details</summary>
Motivation: Bridging spatiotemporal scales of magnetic seed field generation and dynamo amplification in weakly collisional intracluster medium presents extreme numerical challenges that need to be addressed.

Method: Use 10-moment collisionless fluid solver (Gkeyll) evolving full pressure tensor for all species with ion-to-electron mass ratio of 100; electron heat-flux closure regulates pressure isotropization and sets magnetic Reynolds number.

Result: Simulations capture both electron Weibel instability seed generation and dynamo amplification; investigate transition between kinetic pair-plasma-like regime and MHD-like dynamo regime based on closure strength.

Conclusion: The approach enables study of magnetic field evolution from seed generation to dynamo amplification in collisionless intracluster medium with realistic mass ratios, revealing different regimes based on closure parameters.

Abstract: Bridging the spatiotemporal scales of magnetic seed field generation and subsequent dynamo amplification in the weakly collisional intracluster medium presents an extreme numerical challenge. We perform collisionless turbulence simulations with initially unmagnetized electrons that capture both magnetic seed generation via the electron Weibel instability and the ensuing dynamo amplification. Going beyond existing pair-plasma studies, we use an ion-to-electron mass ratio of 100 for which we find electron and ion dynamics are sufficiently decoupled. These simulations are enabled by the 10-moment collisionless fluid solver of Gkeyll, which evolves the full pressure tensor for all species. The electron heat-flux closure regulates pressure isotropization and effectively sets the magnetic Reynolds number. We investigate how the strength of of the closure influences the transition between a regime reminiscent of previous kinetic pair-plasma simulations and a more MHD-like dynamo regime.

</details>


### [33] [Reply to "Comment on Nuclear Fusion 66, 016012 (2026) by Richard Fitzpatrick, A Simple Model of Current Ramp-Up and Ramp-Down in Tokamaks" by A.H. Boozer](https://arxiv.org/abs/2601.10509)
*Richard Fitzpatrick*

Main category: physics.plasm-ph

TL;DR: This is a follow-up report addressing comments by Dr. A.H. Boozer on the author's previous paper about current ramp-up/down models in tokamaks.


<details>
  <summary>Details</summary>
Motivation: To respond to and address the specific comments and critiques made by Dr. A.H. Boozer regarding the author's previous work on tokamak current ramp-up and ramp-down modeling.

Method: The report appears to be a response paper that analyzes and addresses the comments made in arXiv:2601.05977, likely involving theoretical analysis, clarification of assumptions, and discussion of the model's validity.

Result: The abstract doesn't specify results, but the report presumably provides responses to Boozer's comments, clarifications of the original model, and potentially refinements or defenses of the approach.

Conclusion: This follow-up report engages with peer feedback to improve understanding and address potential issues with the original tokamak current ramp model, contributing to ongoing scientific discourse in fusion research.

Abstract: This report is a follow up to my paper "A simple model of current ramp-up and ramp-down in tokamaks" [Nucl. Fusion 66, 016012 (2026)] in the light of comments on the paper recently made by Dr. A.H. Boozer (arXiv:2601.05977).

</details>


### [34] [Canonical Vorticity Perspective on Magnetogenesis: Unifying Weibel, Biermann, and Beyond](https://arxiv.org/abs/2601.10570)
*Modhuchandra Laishram,Young Dae Yoon*

Main category: physics.plasm-ph

TL;DR: The paper presents a canonical vorticity framework that unifies various magnetogenesis processes in collisionless plasmas, extending to relativistic regimes and validating predictions with particle-in-cell simulations.


<details>
  <summary>Details</summary>
Motivation: To bridge cosmology and plasma physics by studying the origin of magnetic fields in the universe, and to develop a unified framework for understanding magnetogenesis processes in collisionless plasmas.

Method: Formulation of a canonical vorticity framework using canonical vorticity (weighted sum of fluid vorticity and magnetic field) as the canonical variable, extending to relativistic regimes and validating with particle-in-cell simulations.

Result: The framework unifies several magnetogenesis processes (Biermann battery, Weibel instability) and predicts new pressure tensorial configurations as fundamental sources of self-generated magnetic fields and vorticity. In relativistic regimes, identifies an additional source called "kineclinicity effect."

Conclusion: The canonical vorticity framework provides a unified approach to magnetogenesis with implications for both laboratory and astrophysical plasma environments, validated through systematic particle-in-cell simulations.

Abstract: We briefly review the current status of magnetogenesis, a cross-disciplinary field that bridges cosmology and plasma physics, studying the origin of magnetic fields in the universe. We formulate a canonical vorticity framework to investigate kinetic plasma physics-based magnetogenesis processes in a collisionless plasma. By considering canonical vorticity, a weighted sum of the fluid vorticity and the magnetic field as the canonical variable, this framework unifies several magnetogenesis processes, including the Biermann battery, the Weibel instability, and predicts several new pressure tensorial configurations as the fundamental source of self-generated magnetic field and vorticity in plasma. The framework is further extended to relativistic regime where an additional source of canonical vorticity, termed as kineclinicity effect, is identified. The theoretical predictions are systematically validated using particle-in-cell simulations, highlighting their implications for laboratory and astrophysical plasma environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: The paper introduces in-context operator learning for optimal transport, where a single solution operator learns to map distribution pairs to OT maps using few-shot samples as prompts without gradient updates at inference.


<details>
  <summary>Details</summary>
Motivation: To develop a framework that can learn optimal transport maps between probability distributions using only few-shot samples as context, enabling efficient inference without gradient updates and providing theoretical guarantees for different settings.

Method: Parameterize a solution operator for optimal transport and develop scaling-law theory in two regimes: nonparametric setting (low-dimensional manifold of source-target pairs) and parametric setting (e.g., Gaussian families). Provide explicit architecture for parametric case and generalization bounds for both.

Result: Established generalization bounds quantifying how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity in nonparametric setting. For parametric setting, provided explicit architecture that recovers exact OT map and finite-sample excess-risk bounds. Numerical experiments validate framework on synthetic transports and generative-modeling benchmarks.

Conclusion: The paper successfully introduces in-context operator learning for optimal transport with theoretical guarantees in both nonparametric and parametric settings, demonstrating practical applicability through numerical experiments and providing a framework for few-shot OT learning without gradient updates at inference.

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [36] [Unbounded symbols, heat flow, and Toeplitz operators](https://arxiv.org/abs/2601.10711)
*Sam Looi*

Main category: math.FA

TL;DR: The paper disproves the Berger-Coburn heat-flow conjecture for Toeplitz operators on Bargmann space, showing that the natural domain operator U_g can be unbounded even when the form-defined operator T_g is bounded, due to a gap between linear and quadratic averaging of symbols.


<details>
  <summary>Details</summary>
Motivation: To investigate the Berger-Coburn heat-flow conjecture for Toeplitz operators on Bargmann space, which concerns the relationship between boundedness of operators and heat transform properties of their symbols.

Method: The authors construct a smooth, nonnegative radial symbol g satisfying the coherent-state admissibility hypothesis with bounded heat transforms for all t>0, then analyze the operators T_g (form-defined) and U_g (natural-domain) to demonstrate their decoupling in the unbounded symbols regime.

Result: The main result disproves the natural domain extension of the Berger-Coburn conjecture: T_g can be bounded while U_g is unbounded. Boundedness of U_g is equivalent to |g|^2 dμ being a Fock-Carleson measure, strictly stronger than the linear average condition governing T_g.

Conclusion: The failure mechanism is a gap between pointwise and uniform control of Gaussian averaging of |g|^2. This is a global phenomenon where geometry at infinity, not local singularities, causes unboundedness. Heat-flow regularity is irreversible, and bootstrapping cannot resolve the gap between sufficiency and critical time conditions.

Abstract: We disprove the natural domain extension of the Berger--Coburn heat-flow conjecture for Toeplitz operators on the Bargmann space and identify the failure mechanism as a gap between pointwise and uniform control of a Gaussian averaging of the squared modulus of the symbol, a gap that is invisible to the linear form $T_g$. We establish that the form-defined operator $T_g$ and the natural-domain operator $U_g$ decouple in the unbounded symbols regime: while $T_g$ is governed by linear averaging, $U_g$ is controlled by the quadratic intensity of $|g|^2$. We construct a smooth, nonnegative radial symbol $g$ satisfying the coherent-state admissibility hypothesis with bounded heat transforms for all time $t>0$; for this symbol, $T_g$ is bounded, yet $U_g$ is unbounded. This is a strictly global phenomenon: under the coherent-state hypothesis, local singularities are insufficient to cause unboundedness, leaving the ``geometry at infinity'' as the sole obstruction. Boundedness of $U_g$ is equivalent to the condition that $|g|^2 dμ$ is a Fock--Carleson measure, a condition strictly stronger than the linear average $g dμ$ governing $T_g$. Finally, regarding the gap between the known sub-critical sufficiency condition and the critical heat time, we prove that heat-flow regularity is irreversible in this context and show that bootstrapping strategies cannot resolve the gap between sufficiency and critical time.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [37] [A universal Bochner formula for scalar curvature](https://arxiv.org/abs/2601.10618)
*Sven Hirsch*

Main category: math.DG

TL;DR: A universal Bochner formula for scalar curvature that generalizes several known results in geometric analysis.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework that captures various important formulas in geometric analysis related to scalar curvature, providing a common foundation for different special cases.

Method: Introduces a universal Bochner formula for scalar curvature that serves as a general framework, from which specific cases can be derived as special instances.

Result: The formula successfully unifies several important results: stability inequality for minimal slicings, Schrödinger-Lichnerowicz-type formula, and higher-dimensional Stern's level-set identity.

Conclusion: The universal Bochner formula provides a powerful unifying framework that connects various important results in geometric analysis related to scalar curvature, potentially leading to new insights and applications.

Abstract: We introduce a universal Bochner formula for scalar curvature that contains, as special cases, the stability inequality for minimal slicings, a Schrödinger-Lichnerowicz-type formula, and a higher-dimensional version of Stern's level-set identity.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [38] [Variable coherence model for free-electron laser pulses](https://arxiv.org/abs/2601.09885)
*Austin Bartunek,Nils H. Sommerfeld,Francois Mauger*

Main category: physics.optics

TL;DR: The Variable Coherence Model (VCM) enables continuous control over FEL pulse noise characteristics while maintaining fixed average pulse parameters like bandwidth, allowing simulation of pulses ranging from maximally random to fully coherent.


<details>
  <summary>Details</summary>
Motivation: To develop a more flexible model for simulating free-electron laser pulses from self-amplified spontaneous emission that allows continuous control over pulse noise characteristics while keeping average parameters constant, building on existing partial coherence models.

Method: Extends the established partial coherence model by implementing a variable coherence width parameter, enabling systematic statistical analysis of intensity and sub-pulse distributions in both time and frequency domains across different FEL parameter regimes.

Result: Demonstrates continuous control over pulse noise characteristics, shows how sub-pulse statistics vary with coherence width, and illustrates the model's ability to generate pulses across the full spectrum from maximally random to fully coherent, with practical impact shown in absorption simulations.

Conclusion: The VCM provides a powerful tool for simulating FEL pulses with tunable coherence properties, offering researchers the ability to systematically study how coherence affects pulse characteristics and experimental outcomes like absorption measurements.

Abstract: We introduce the variable coherence model (VCM) for simulating free-electron laser (FEL) pulses generated through self-amplified spontaneous emission. Building on the established partial coherence model of [T. Pfeifer et. al, Opt. Lett. 35, 3441 (2010)], we demonstrate that the implementation of a variable coherence width allows for continuous control over the pulses' characteristic noise, while keeping the average pulse parameters such as the bandwidth fixed. We demonstrate this through systematic statistical analyses of the intensity and number of sub-pulses in VCM pulses, in both time and frequency. In particular, we analyze how the sub-pulse statistics are affected by the coherence width parameter. We perform our analyses across three distinct regimes of FEL parameters and demonstrate how the VCM can generate pulses that range from maximally random to fully coherent. Finally, we illustrate the effect of the VCM variable coherence width on an absorption simulation.

</details>


### [39] [Near-Unity-Efficiency Gas Gratings for Ultraviolet, Visible, and Infrared High-Power Lasers](https://arxiv.org/abs/2601.09963)
*Ke Ou,Harsha Rajesh,Sida Cao,Debolina Chakraborty,Victor M. Perez-Ramirez,Devdigvijay Singh,Caleb Redshaw,Pelin Dedeler,Albertine Oudin,Eugene Kur,Michelle M. Wang,Julia M. Mikhailova,Livia Lancia,Caterina Riconda,Pierre Michel,Matthew R. Edwards*

Main category: physics.optics

TL;DR: Gas gratings created via DUV laser interference in ozone-doped gas flows enable efficient diffraction (up to 99%) of various laser wavelengths and pulse durations, with high damage thresholds and debris immunity for high-energy laser applications.


<details>
  <summary>Details</summary>
Motivation: To develop transient optics that overcome limitations of conventional solid optics - specifically debris/shard immunity and low damage thresholds - for efficient manipulation of high-energy lasers.

Method: Using interfering DUV lasers to induce density modulations in ozone-doped gas flows via photochemical reactions, creating volume diffraction gratings. Comprehensive characterization of performance under varying conditions including imprint fluence, gas composition, and grating geometries.

Result: Achieved up to 99% full beam diffraction efficiency across DUV to near-IR wavelengths and nanosecond to femtosecond pulse durations while preserving focusability and wavefront quality. Carbon dioxide addition significantly enhances performance, with stable operation over hours.

Conclusion: Gas gratings provide a promising method for high-energy laser manipulation with superior damage resistance and debris immunity. Results validate theoretical models and suggest optimal parameters for scaling to high-energy applications.

Abstract: Interfering deep ultraviolet (DUV) lasers can induce substantial density modulations in an ozone-doped gas flow via photochemical reactions, creating volume diffraction gratings. These transient optics are immune to target debris and shrapnel and feature orders-of-magnitude higher damage thresholds than conventional solid optics, providing a promising method for efficiently manipulating high-energy lasers. In this work, we describe gas gratings that can efficiently diffract probe beams across a variety of wavelengths and pulse durations, ranging from deep ultraviolet to near-infrared and from nanosecond to femtosecond, achieving a full beam diffraction efficiency up to 99% while preserving the focusability and wavefront quality. In addition, we present a comprehensive characterization of the performance of the gas gratings under various experimental conditions, including imprint fluence, gas composition, and grating geometries, showing significant enhancement of this process with the addition of carbon dioxide. We also demonstrate stable performance over hours of operation. Our results validate a previously developed theoretical model and suggest optimal parameters to efficiently scale gas gratings to high-energy applications.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [40] [Shallow-KAN Based Solution of Moving Boundary PDEs](https://arxiv.org/abs/2601.09818)
*Tarus Pande,V M S K Minnikanti,Shyamprasad Karagadde*

Main category: math-ph

TL;DR: Shallow KAN framework solves moving boundary PDEs (Stefan problems) without measurement data, using physics-informed residuals and interface-focused resampling for accurate temperature and interface reconstruction.


<details>
  <summary>Details</summary>
Motivation: To develop a compact and efficient alternative to MLP-based approaches for solving moving boundary PDEs (Stefan problems) that can directly approximate both temperature distribution and moving interface without requiring measurement data.

Method: Proposes a shallow Kolmogorov-Arnold Network (KAN) framework that directly approximates temperature distribution T(x,t) and moving interface Γ(t). Enforces governing PDEs, phase equilibrium, and Stefan condition through physics-informed residuals. Uses interface-focused collocation resampling to enhance accuracy. Extends to 2D using level-set based formulation for interface propagation solved within KAN framework.

Result: Numerical experiments in 1D and 2D show accurate reconstructions of both temperature fields and interface dynamics. Validated with semi-infinite analytical solutions. Demonstrates KANs can solve complex moving boundary problems without measurement data.

Conclusion: KANs are capable of solving complex moving boundary problems without measurement data, offering a compact and efficient alternative to MLP-based approaches for moving boundary PDEs.

Abstract: Kolmogorov-Arnold Networks (KANs) require significantly smaller architectures compared to multilayer perceptron (MLP)-based approaches, while retaining expressive power through spline-based activations. We propose a shallow KAN framework that directly approximates the temperature distribution T(x,t) and the moving interface $Γ(t)$, enforcing the governing PDEs, phase equilibrium, and Stefan condition through physics-informed residuals. To enhance accuracy, we employ interface-focused collocation resampling. Numerical experiments in one and two dimensions show that the framework achieves accurate reconstructions of both temperature fields and interface dynamics, highlighting the potential of KANs as a compact and efficient alternative for moving boundary PDEs. First, we validate the model with semi-infinite analytical solutions. Subsequently, the model is extended to 2D using a level-set based formulation for interface propagation, which is solved within the KAN framework. This work demonstrates that KANs are capable of solving complex moving boundary problems without the need for measurement data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [41] [Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation](https://arxiv.org/abs/2601.10577)
*Serena Grazia De Benedictis,Amedeo Altavilla,Nicoletta Del Buono*

Main category: cs.CV

TL;DR: The paper introduces a topology-aware segmentation evaluation framework based on the Jordan Curve Theorem to assess structural coherence of segmentation masks, addressing limitations of conventional metrics.


<details>
  <summary>Details</summary>
Motivation: Conventional segmentation metrics (pixel-wise, region-based, boundary-focused) often fail to capture structural and topological coherence. Small inaccuracies can yield high scores while masks fail to preserve object shape or connectivity, especially problematic in medical imaging and object delineation where topological correctness is crucial.

Method: Defines "Jordan-segmentatable masks" based on digital Jordan Curve Theorem. Uses digital topology and homology theory to extract a 4-curve candidate from masks, verifies topological validity using Betti numbers (β₀ = β₁ = 1). A mask is Jordan-segmentatable when its complement splits into exactly two 8-connected components.

Result: Provides a mathematically rigorous, unsupervised criterion for assessing structural coherence of segmentation masks. The framework combines digital Jordan theory and homological invariants to evaluate topological correctness.

Conclusion: The proposed topology-aware framework offers a valuable alternative to standard evaluation metrics, particularly in applications where preserving topological correctness is essential, addressing a fundamental limitation of conventional segmentation assessment methods.

Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $β_0 = β_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [42] [On gradient stability in nonlinear PDE models and inference in interacting particle systems](https://arxiv.org/abs/2601.10326)
*Aurélien Castre,Richard Nickl*

Main category: math.ST

TL;DR: The paper develops a Banach space implicit function theorem approach to verify gradient stability for non-linear PDE inverse problems, with applications to reaction-diffusion systems and McKean-Vlasov models, leading to statistical identifiability and polynomial-time convergence of Langevin sampling for posterior inference.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous statistical identifiability and computational tractability for non-linear PDE inverse problems by verifying gradient stability conditions, which are crucial for statistical inference and posterior sampling algorithms.

Method: Uses a Banach space version of the implicit function theorem to analyze parameter-to-solution maps θ↦G(θ) of non-linear PDEs, verifying gradient stability conditions (Nickl & Wang 2024) and providing injectivity estimates for statistical identifiability.

Result: Develops general framework for gradient stability verification, applies to non-linear reaction-diffusion systems and McKean-Vlasov interacting particle models, proves polynomial-time convergence of Langevin-type algorithms for posterior sampling of interaction potentials.

Conclusion: The Banach space implicit function approach provides a powerful tool for establishing gradient stability in non-linear PDE inverse problems, enabling rigorous statistical identifiability results and efficient posterior sampling algorithms with provable convergence guarantees.

Abstract: We consider general parameter to solution maps $θ\mapsto \mathcal G(θ)$ of non-linear partial differential equations and describe an approach based on a Banach space version of the implicit function theorem to verify the gradient stability condition of Nickl&Wang (JEMS 2024) for the underlying non-linear inverse problem, providing also injectivity estimates and corresponding statistical identifiability results. We illustrate our methods in two examples involving a non-linear reaction diffusion system as well as a McKean--Vlasov interacting particle model, both with periodic boundary conditions. We apply our results to prove the polynomial time convergence of a Langevin-type algorithm sampling the posterior measure of the interaction potential arising from a discrete aggregate measurement of the interacting particle system.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [43] [Electronic structure theory of H$_{3}$S: Plane-wave-like valence states, density-of-states peak and its guaranteed proximity to the Fermi level](https://arxiv.org/abs/2601.10016)
*Ryosuke Akashi*

Main category: cond-mat.supr-con

TL;DR: The paper explains the mechanism behind the DOS peak formation in H3S superconductors under pressure, showing it results from specific plane-wave hybridization near the Fermi surface.


<details>
  <summary>Details</summary>
Motivation: While superconductivity in H3S under extreme pressure has been explained theoretically, the mechanism behind the crucial peaked electronic density of states (DOS) that enables high transition temperatures has remained unclear and requires investigation.

Method: The researchers performed detailed analysis of first-principles electronic wave functions, showing they are significantly plane-wave-like. They extracted nearly uniform models from Fourier-mode analysis of self-consistent potentials and atomic pseudopotentials to reproduce band structure with few parameters.

Result: The DOS peak is shown to be a consequence of hybridization of specific plane waves. The adjacency of Jones' large zone to the plane-wave spherical Fermi surface is identified as the root cause of multiple plane-wave hybridization, DOS peak formation, and its proximity to the Fermi level.

Conclusion: The theory resolves the minimal modeling problem of electronic states in H3S and establishes a mechanism that may help boost transition temperatures in pressure-induced superconductors.

Abstract: Superconductivity in sulfur superhydride H$_{3}$S under extreme pressures has been explained theoretically, but it requires a peaked concentration of the electronic density of states (DOS), which has been found in first-principles calculations. The mechanism of this peak formation, though vital for its high transition temperature, has however remained obscure. We address this problem through detailed analysis of the first-principles electronic wave functions. The valence wave functions are shown to be significantly plane-wave-like. From the Fourier-mode analysis of the self-consistent potential and atomic pseudopotentials, we extract the nearly uniform models that accurately reproduce the first-principles band structure with very few parameters. The DOS peak is shown to be the consequence of the hybridization of specific plane waves. Adjacency of Jones' large zone to the plane-wave spherical Fermi surface is posited to be the root cause of the multiple plane-wave hybridization, the DOS peak formation and its proximity to the Fermi level. The present theory resolves the minimal modeling problem of electronic states in H$_{3}$S, as well as establishes a mechanism that may help to boost the transition temperatures in pressure induced superconductors.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [44] [Remarks on the convex integration technique applied to singular stochastic partial differential equations](https://arxiv.org/abs/2601.09990)
*Hongjie Dong,Kazuo Yamazaki*

Main category: math.PR

TL;DR: Review of convex integration as an alternative approach to solving singular stochastic PDEs, with analysis showing its limitations for proving non-uniqueness in the Φ⁴ model.


<details>
  <summary>Details</summary>
Motivation: To explore convex integration as a potential alternative to existing theories (regularity structures and paracontrolled distributions) for constructing solutions to singular stochastic PDEs, and to assess its applicability to specific models like the Φ⁴ quantum field theory model.

Method: Review and analysis of recent developments in convex integration techniques for singular stochastic PDEs, with specific examination of the Φ⁴ model to demonstrate limitations in proving non-uniqueness.

Result: Convex integration emerges as a possible approach for constructing solutions to singular stochastic PDEs, but appears unlikely to prove non-uniqueness for the specific Φ⁴ model from quantum field theory.

Conclusion: While convex integration offers a promising alternative approach to existing theories for singular stochastic PDEs, it has limitations for certain models like Φ⁴, suggesting the need for continued development of multiple techniques in this challenging field.

Abstract: Singular stochastic partial differential equations informally refer to the partial differential equations with rough random force that leads to the products in the nonlinear terms becoming ill-defined. Besides the theories of regularity structures and paracontrolled distributions, the technique of convex integration has emerged as a possible approach to construct a solution to such singular stochastic partial differential equations. We review recent developments in this area, and also demonstrate that an application of the convex integration technique to prove non-uniqueness seems unlikely for a particular singular stochastic partial differential equation, specifically the $Φ^{4}$ model from quantum field theory.

</details>


### [45] [Malliavin Calculus for the stochastic Cahn-Hilliard equation driven by fractional noise](https://arxiv.org/abs/2601.10490)
*Dimitrios Dimitriou,Dimitris Farazakis,Georgia Karali*

Main category: math.PR

TL;DR: The paper analyzes the Cahn-Hilliard equation with additive fractional white noise in 1D, using Malliavin calculus to prove existence of density for the stochastic solution.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence of density for stochastic solutions to the Cahn-Hilliard equation perturbed by fractional white noise, which combines temporal fractional noise with spatial white noise.

Method: Apply Malliavin calculus in one spatial dimension, construct localizing sequences to prove local existence of Malliavin derivative, and derive sharp estimates for stochastic integrals in the mild formulation.

Result: The stochastic solution admits continuous paths almost surely, has locally existing Malliavin derivative, and its law is absolutely continuous with respect to Lebesgue measure, establishing existence of density.

Conclusion: The paper successfully establishes density existence for the Cahn-Hilliard equation with fractional white noise through rigorous Malliavin calculus techniques and sharp stochastic integral estimates.

Abstract: The stochastic partial differential equation analyzed in this work is the Cahn-Hilliard equation perturbed by an additive fractional white noise (fractional in time and white in space). We work in the case of one spatial dimension and apply Malliavin calculus to investigate the existence of a density for the stochastic solution $u$. In particular, we show that $u$ admits continuous paths almost surely and construct a localizing sequence through which we prove that its Malliavin derivative exists locally, and that its law is absolutely continuous with respect to the Lebesgue measure on $\bf R$, establishing thus that a density exists. A key contribution of this work is the analysis of the stochastic integral appearing in the mild formulation: we derive sharp estimates for the expectation of the $p$-th power ($p \geq 2$) of the $L^{\infty}(D)$-norm of this stochastic integral as well as for the integral involving the $L^{\infty}(D)$-norm of the operator associated with the kernel appearing in the integral representation of the fractional noise, all of which are essential for this study.

</details>


### [46] [Smoothness of martingale observables and generalized Feynman-Kac formulas](https://arxiv.org/abs/2601.10539)
*Alex Karrila,Lauri Viitasaari*

Main category: math.PR

TL;DR: The paper proves that under Hörmander conditions, all martingale observables of Itô processes are smooth, leading to a generalized Feynman-Kac formula for degenerate diffusions and applications to SLE.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish smoothness properties for martingale observables in stochastic processes, particularly for degenerate diffusions where standard elliptic regularity theory doesn't apply. This enables solving PDE boundary-value problems with degenerate coefficients and has applications to Schramm-Loewner evolutions (SLE).

Method: The method uses the Hörmander criterion (hypoellipticity condition) on Itô processes to prove smoothness of martingale observables. The approach combines stochastic analysis with PDE theory, extending the Feynman-Kac formula to degenerate diffusions and boundary stopping problems.

Result: Main result: Under Hörmander conditions, all martingale observables of Itô processes are smooth. This yields a generalized Feynman-Kac formula providing smooth solutions to certain PDE boundary-value problems, even for degenerate diffusions and with boundary stopping under mild regularity assumptions.

Conclusion: The paper establishes fundamental smoothness properties for martingale observables under Hörmander conditions, with significant applications to PDE theory (generalized Feynman-Kac formula) and stochastic geometry (SLE problems via Girsanov transforms).

Abstract: We prove that, under the Hörmander criterion on an Itô process, all its martingale observables are smooth. As a consequence, we also obtain a generalized Feynman-Kac formula providing smooth solutions to certain PDE boundary-value problems, while allowing for degenerate diffusions as well as boundary stopping (under very mild boundary regularity assumptions). We also highlight an application to a question posed on Schramm-Loewner evolutions, by making certain Girsanov transform martingales accessible via Itô calculus.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [47] [Discrete versus continuous -- lattice models and their exact continuous counterparts](https://arxiv.org/abs/2601.10184)
*Lorenzo Fusi,Oliver Křenek,Vít Průša,Casey Rodriguez,Rebecca Tozzi,Martin Vejvoda*

Main category: physics.class-ph

TL;DR: The paper establishes a systematic correspondence between discrete lattice models of interacting particles and continuous PDE models using Fourier analysis, focusing on dispersion relations across different boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between discrete lattice models (used in statistical mechanics and condensed matter physics) and continuous PDE models (used in continuum mechanics), providing a rigorous mathematical framework for understanding their correspondence.

Method: Systematic application of Fourier analysis tools to map between discrete and continuous settings, analyzing nearest-neighbor and multiple-neighbor interaction models, progressing from infinite to periodic to finite lattices with fixed-end boundary conditions.

Result: Establishes a comprehensive correspondence framework showing how discrete lattice models can be systematically related to continuous PDE counterparts through Fourier analysis, with dispersion relations serving as the primary connecting element.

Conclusion: The paper provides a unified mathematical framework for understanding the relationship between discrete particle models and continuum descriptions, demonstrating that Fourier analysis serves as a powerful tool for systematically connecting these different modeling approaches across various boundary conditions.

Abstract: We review and study the correspondence between discrete lattice/chain models of interacting particles and their continuous counterparts represented by partial differential equations. We study the correspondence problem for nearest neighbour interaction lattice models as well as for multiple-neighbour interaction lattice models, and we gradually proceed from infinite lattices to periodic lattices and finally to finite lattices with fixed ends/zero Dirichlet boundary conditions. The whole study is framed as systematic specialisation of Fourier analysis tools from the continuous to the discrete setting and vice versa, and the correspondence between the discrete and continuous models is examined primarily with regard to the dispersion relation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [48] [Stable Differentiable Modal Synthesis for Learning Nonlinear Dynamics](https://arxiv.org/abs/2601.10453)
*Victor Zheleznov,Stefan Bilbao,Alec Wright,Simon King*

Main category: cs.SD

TL;DR: Combines scalar auxiliary variable techniques with neural ODEs to create stable differentiable models for learning nonlinear dynamics, applied to high-amplitude string vibration.


<details>
  <summary>Details</summary>
Motivation: Modal methods for physical modelling synthesis need extensions to nonlinear problems like high-amplitude string vibration, but existing approaches either use analytical SAV techniques or data-driven neural ODEs separately. The paper aims to combine both for stable differentiable learning of nonlinear dynamics.

Method: Integrates scalar auxiliary variable (SAV) techniques with neural ordinary differential equations (neural ODEs) to create stable differentiable models. Leverages analytical solutions for linear vibration of system modes to keep physical parameters accessible without needing parameter encoders in the architecture.

Result: Successfully trained model reproduces nonlinear dynamics of high-amplitude string vibration using synthetic data. Sound examples demonstrate practical application.

Conclusion: The proposed hybrid approach combines the stability of SAV techniques with the learning capabilities of neural ODEs, enabling stable differentiable models that can learn nonlinear dynamics while maintaining physical parameter accessibility.

Abstract: Modal methods are a long-standing approach to physical modelling synthesis. Extensions to nonlinear problems are possible, including the case of a high-amplitude vibration of a string. A modal decomposition leads to a densely coupled nonlinear system of ordinary differential equations. Recent work in scalar auxiliary variable techniques has enabled construction of explicit and stable numerical solvers for such classes of nonlinear systems. On the other hand, machine learning approaches (in particular neural ordinary differential equations) have been successful in modelling nonlinear systems automatically from data. In this work, we examine how scalar auxiliary variable techniques can be combined with neural ordinary differential equations to yield a stable differentiable model capable of learning nonlinear dynamics. The proposed approach leverages the analytical solution for linear vibration of system's modes so that physical parameters of a system remain easily accessible after the training without the need for a parameter encoder in the model architecture. As a proof of concept, we generate synthetic data for the nonlinear transverse vibration of a string and show that the model can be trained to reproduce the nonlinear dynamics of the system. Sound examples are presented.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [49] [The transformation mechanisms among cuboctahedra, Ino's decahedra and icosahedra structures of magic-size gold nanoclusters](https://arxiv.org/abs/2601.10434)
*Ehsan Rahmatizad Khajehpasha,Mohammad Ismaeil Safa,Nasrin Eyvazi,Marco Krummenacher,Stefan Goedecker*

Main category: cond-mat.mtrl-sci

TL;DR: Machine learning reveals new transformation pathways and lower-energy structures for gold nanoclusters, challenging previous understanding of their structural dynamics.


<details>
  <summary>Details</summary>
Motivation: Gold nanoclusters have competing structural motifs with small energy differences, leading to structural coexistence and interconversion. Previous studies may have missed physically relevant transformation pathways and lower-energy structures.

Method: Used high-accuracy machine learned potential trained on ~20,000 DFT reference data points to investigate transformation pathways for Au55, Au147, Au309 and Au561 nanoclusters. Performed saddle point searches and Minima Hopping sampling to identify transformation barriers and low-energy structures.

Result: Found high-symmetry transformations proceed through single barriers via soft-mode-driven jitterbug-type and slip-dislocation motions. Identified lower-barrier asymmetric pathways leading to disordered, Jahn-Teller-stabilized amorphous icosahedra. Discovered new global minima for Au309 and Au561 with energies up to 2.8 eV lower than previously reported. Transformation pathways give reasonable transformation times that roughly agree with experiments.

Conclusion: Both the shapes and transformation pathways studied in previous investigations are not physically relevant. The newly identified pathways and structures provide more accurate understanding of gold nanocluster dynamics that better matches experimental observations.

Abstract: Gold nanoclusters possess multiple competing structural motifs with small energy differences, enabling structural coexistence and interconversion. Using a high-accuracy machine learned potential trained on some 20'000 density functional theory reference data points, we investigate transformation pathways connecting both high-symmetry and amorphous cuboctahedra, Ino's decahedra and icosahedra for Au55, Au147, Au309 and Au561 nanoclusters. Our saddle point searches reveal that high-symmetry transformations from cuboctahedra and Ino's decahedra to icosahedra proceed through a single barrier and represent soft-mode-driven jitterbug-type and slip-dislocation motions. In addition, we identify lower-barrier asymmetric transformation pathways that drive the system into disordered, Jahn-Teller-stabilized amorphous icosahedra. Minima Hopping sampling further uncovers, in this context, many such low-symmetry minima. Some of the newly identified global minima for Au309 and Au561 have energies that are up to 2.8 eV lower than the previously reported global minima. Hence, both the shapes and the transformation pathways studied in previous investigations are not the physically relevant ones. In contrast to the previously studied pathways, our transformation pathways give reasonable transformation times that are in rough agreement with experiments.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [50] [Volume penalization method for simulating flows around a rotating solid with multiple reference frame and sliding mesh](https://arxiv.org/abs/2601.10230)
*Ming Liu,Yosuke Hasegawa*

Main category: physics.flu-dyn

TL;DR: Combines volume penalization method with MRF and sliding mesh techniques to simulate rotating solids in fluid flows, achieving ~5% accuracy compared to body-fitted methods.


<details>
  <summary>Details</summary>
Motivation: Turbomachinery simulation is challenging for rotating solids with complex geometry. Need accurate methods to simulate flows around rotating objects without complex body-fitted meshes.

Method: Combines volume penalization method (VPM) with multiple reference frame (MRF) and sliding mesh (SLM) approaches. Uses level-set function for geometry representation on Cartesian grids. Develops unified governing equations with VPM body-forcing terms for both fluid and solid regions.

Result: Simulated flows around rotating cuboid at various speeds using VPM+MRF and VPM+SLM. Compared with body-fitted method (BFM). Relative deviations in pressure drop and torque predictions were around 5%, validating the VPM approach.

Conclusion: The proposed VPM combined with MRF or SLM provides accurate simulation of flows around rotating solids with complex geometry, offering ~5% accuracy compared to traditional body-fitted methods while using simpler Cartesian grids.

Abstract: Despite the significant role of turbomachinery in fluid-based energy transfer, precise simulation of rotating solid objects with complex geometry is a challenging task. In the present study, the volume penalization method (VPM) is combined with multiple reference frame (MRF) and sliding mesh (SLM), respectively, so as to develop immersed-boundary approaches for simulating flows around a rotating solid. The level-set function is adopted to represent arbitrary geometries embedded in Cartesian grids. The VPM body-forcing terms in the momentum equation are proposed for MRF and SLM, respectively, so as to build unified governing equations for both fluid and solid regions. The flows around a rotating cuboid under various rotating speeds are simulated by the present schemes, namely, VPM with MRF, and VPM with SLM, and compared to corresponding simulations by the body-fitted method (BFM). The results suggest the relative deviations of predicted pressure drop and torque between the present VPM and BFM are around 5%, demonstrating the validity of the present VPM.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [51] [Design, Fabrication and Testing of a D-Shaped High Temperature Superconducting Magnet](https://arxiv.org/abs/2601.10295)
*Upendra Prasad,Mahesh Ghate,Piyush Raj,Deven Kanabar,Pankaj Varmora,Swati Roy,Arun Panchal,Dhaval Bhavsar,Anees Bano,Nitish Kumar,Bhadresh Parghi,Akhilesh Yadav,Mohd. Umer,Vijay Vasava,Raton Mandal,Rajkumar Ahirwar,Megha Thaker*

Main category: physics.acc-ph

TL;DR: Development of a compact D-shaped superconducting magnet using REBCO HTS tapes for tokamak applications, including design, fabrication, and testing of a winding pack integrated with cryogenic systems.


<details>
  <summary>Details</summary>
Motivation: High-temperature superconductors enable compact, high-field tokamak magnets for increased fusion power through on-axis high magnetic fields, addressing the demand for higher fusion power output.

Method: Developed a toroidal configuration with 8 D-shaped, 4 poloidal field, and central solenoid HTS magnets using REBCO tapes; fabricated long-length HTS cable, winding pack, integrated with cryogenic casing and vacuum enclosure; designed terminations, joints, and power supply interfacing.

Result: Conceptualized a toroidal configuration with 0.42 m major radius producing 0.23 T on-axis toroidal magnetic field; demonstrated fabrication feasibility of D-shaped coil using stacked HTS cable; completed design, fabrication, and integration of winding pack with cryogenic systems.

Conclusion: Successfully designed and fabricated a compact D-shaped HTS magnet system for tokamak applications, demonstrating feasibility of using REBCO HTS tapes for compact high-field fusion magnets with integrated cryogenic and power systems.

Abstract: High-temperature technical superconductors are potential candidates for compact and high-field tokamak magnets. The demand for higher fusion power can be met with an on-axis high magnetic field due to toroidal magnets. An R&D activity has been initiated at the Institute for Plasma Research, India, to develop a compact D-shaped superconducting magnet utilizing REBCO high-temperature superconducting tapes. Under this initiative, a toroidal configuration with a major radius of 0.42 m, consisting of eight D-shaped, four poloidal field, and a central solenoid high-temperature superconducting magnets producing an on-axis toroidal magnetic field of 0.23 T has been conceptualized. The fabrication feasibility of a D-shaped coil for this toroidal configuration also envisaged using stacked high-temperature superconducting cable. In this paper, we report the design of a compact D-shaped coil, the fabrication of a long length HTS cable, a winding pack, and its integration with a cryogenic casing and vacuum enclosure. The winding pack terminations, joints, its interfacing with the power supply, and performance testing are also reported in this paper.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [52] [Computing Statistical Properties of Velocity Fields on Current Quantum Hardware](https://arxiv.org/abs/2601.10166)
*Miriam Goldack,Yosi Atia,Ori Alberton,Karl Jansen*

Main category: quant-ph

TL;DR: Quantum algorithms for CFD can efficiently represent spatial fields using qubits, but extracting statistical properties from simulations is challenging. This work presents methods to compute velocity field statistics directly from quantum circuits without full tomography, demonstrating accurate results on current quantum hardware.


<details>
  <summary>Details</summary>
Motivation: Quantum CFD algorithms offer favorable scaling by encoding physical fields into quantum states, but efficient readout of simulation results remains a significant challenge that has received limited attention in literature.

Method: Developed methods to extract statistical properties (central moments and structure functions) of spatial velocity fields directly from parameterized ansatz circuits, avoiding computationally expensive full quantum state tomography. Implemented approach for 1D velocity fields encoding 16 spatial points with 4 qubits.

Result: Demonstrated high accuracy on current quantum devices (IBMQ's Heron2 system ibm_fez) using Qedma's error mitigation software QESEM. Successfully analyzed both a sine wave signal and four snapshots from Burgers' equation evolution.

Conclusion: The presented methods enable efficient extraction of statistical properties from quantum CFD simulations, making such computations practical on current quantum hardware and advancing the field of quantum computational fluid dynamics.

Abstract: Quantum algorithms are gaining attention in Computational Fluid Dynamics (CFD) for their favorable scaling, as encoding physical fields into quantum probability amplitudes enables representation of two to the power of n spatial points with only n qubits. A key challenge in Quantum CFD is the efficient readout of simulation results, a topic that has received limited attention in literature. This work presents methods to extract statistical properties of spatial velocity fields, such as central moments and structure functions, directly from parameterized ansatz circuits, avoiding full quantum state tomography. As a proof of concept, we implement our approach for 1D velocity fields, encoding 16 spatial points with 4 qubits, and analyze both a sine wave signal and four snapshots from Burgers' equation evolution. Using Qedma's error mitigation software QESEM, we demonstrate that such computations achieve high accuracy on current quantum devices, specifically IBMQ's Heron2 system ibm_fez.

</details>
