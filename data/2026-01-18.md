<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 13]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math.PR](#math.PR) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.class-ph](#physics.class-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math-ph](#math-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Learning Ecological and Epidemic Processes using Neural ODEs, Kolmogorov-Arnold Network ODEs and SINDy](https://arxiv.org/abs/2601.09811)
*Maria Vasilyeva,Zheng Wei,Kelum Gajamannage,Hyangim Ji,Aleksei Krasnikov,Alexey Sadovski*

Main category: math.NA

TL;DR: Combines epidemic (SIR) and ecological (Lotka-Volterra) models into coupled LVSIS system, then applies data-driven methods (Neural ODEs, KANODEs, SINDy) to learn dynamics from synthetic data, extending to spatio-temporal analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate coupled dynamics between epidemic spread and ecological interactions, and develop data-driven approaches to learn these complex systems directly from observational data.

Method: 1. Develops coupled LVSIS model combining SIR epidemic framework with Lotka-Volterra predator-prey system; 2. Applies three data-driven methods: Neural ODEs, KANODEs, and SINDy; 3. Uses synthetic data for numerical experiments; 4. Extends to spatio-temporal models to uncover hidden local couplings.

Result: Numerical experiments demonstrate the learning capabilities of data-driven models in capturing epidemic and ecological behavior, with extension to spatio-temporal analysis revealing hidden local coupling patterns.

Conclusion: Data-driven approaches can effectively learn coupled epidemic-ecological dynamics, with potential applications in understanding complex biological systems and uncovering hidden spatial interactions.

Abstract: We consider epidemic and ecological models to investigate their coupled dynamics. Starting with the classical Susceptible-Infected-Recovered (SIR) model for basic epidemic behavior and the predator-prey (Lotka-Volterra, LV) system for ecological interactions, we then combine these frameworks into a coupled Lotka-Volterra-Susceptible-Infected-Susceptible (LVSIS) model. The resulting system consists of four differential equations describing the evolution of susceptible and infected prey and predator populations, incorporating ecological interactions, disease transmission, and spatial dispersal. To learn the underlying dynamics directly from data, we employ several data-driven modeling frameworks: Neural Ordinary Differential Equations (Neural ODEs), Kolmogorov-Arnold Network Ordinary Differential Equations (KANODEs), and Sparse Identification of Nonlinear Dynamics (SINDy). Numerical experiments based on synthetic data are conducted to investigate the learning ability of these models in capturing the epidemic and ecological behavior. We further extend our approach to spatio-temporal models, aiming to uncover hidden local couplings.

</details>


### [2] [An efficient probabilistic scheme for the exit time probability of $α$-stable Lévy process](https://arxiv.org/abs/2601.09882)
*Minglei Yang,Diego del-Castillo-Negrete,Guannan Zhang*

Main category: math.NA

TL;DR: A method for computing exit time probabilities of α-stable Lévy processes using PIDE framework and Feynman-Kac formula with first-order convergence and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: α-stable Lévy processes model anomalous transport phenomena with discontinuous jumps, but computing exit time probabilities (likelihood of exiting bounded regions within given time) is challenging and important for understanding anomalous diffusion behavior.

Method: Approximates α-stable process by combining Brownian motion with compound Poisson process. Models exit time probability using partial integro-differential equations (PIDEs) framework. Uses Feynman-Kac formula for probabilistic representation involving conditional expectations over SDEs. Computes expectations via tailored quadrature rules and interpolation techniques.

Result: Method achieves first-order convergence in time and offers significant computational advantages over standard Monte Carlo and deterministic approaches. Avoids assembling and solving large dense linear systems, resulting in improved efficiency. Demonstrated accuracy and performance through two numerical examples showing applicability to physical transport problems.

Conclusion: Proposed method provides an efficient and accurate approach for computing exit time probabilities of α-stable Lévy processes, enabling better understanding of anomalous diffusion behavior in physical transport problems with computational advantages over existing methods.

Abstract: The α-stable Lévy process, commonly used to describe Lévy flight, is characterized by discontinuous jumps and is widely used to model anomalous transport phenomena. In this study, we investigate the associated exit problem and propose a method to compute the exit time probability, which quantifies the likelihood that a trajectory starting from an initial condition exits a bounded region in phase space within a given time. This estimation plays a key role in understanding anomalous diffusion behavior. The proposed method approximates the α-stable process by combining a Brownian motion with a compound Poisson process. The exit time probability is then modeled using a framework based on partial integro-differential equations (PIDEs). The Feynman-Kac formula provides a probabilistic representation of the solution, involving conditional expectations over stochastic differential equations. These expectations are computed via tailored quadrature rules and interpolation techniques. The proposed method achieves first-order convergence in time and offers significant computational advantages over standard Monte Carlo and deterministic approaches. In particular, it avoids assembling and solving large dense linear systems, resulting in improved efficiency. We demonstrate the method's accuracy and performance through two numerical examples, highlighting its applicability to physical transport problems.

</details>


### [3] [Nonlinear numerical schemes using specular differentiation for initial value problems of first-order ordinary differential equations](https://arxiv.org/abs/2601.09900)
*Kiyuob Jung*

Main category: math.NA

TL;DR: Proposes specular differentiation in 1D Euclidean space with fundamental analysis and develops numerical schemes for solving first-order ODE initial value problems.


<details>
  <summary>Details</summary>
Motivation: To introduce a new differentiation concept (specular differentiation) in one-dimensional space and establish its fundamental mathematical properties, then apply it to develop numerical methods for solving ordinary differential equations.

Method: Proposes specular differentiation in 1D Euclidean space, provides fundamental analysis including quasi-Fermat's theorem and quasi-Mean Value Theorem. Develops several numerical schemes for solving first-order ODE initial value problems, selects one scheme based on numerical simulations.

Result: Establishes fundamental properties of specular differentiation. For the selected numerical scheme, proves first-order consistency and second-order local convergence through analysis and numerical simulations.

Conclusion: Specular differentiation provides a new mathematical framework with practical applications in numerical analysis for solving differential equations, with proven convergence properties for the developed numerical scheme.

Abstract: This paper proposes specular differentiation in one-dimensional Euclidean space and provides its fundamental analysis, including quasi-Fermat's theorem and the quasi-Mean Value Theorem. As an application, this paper develops several numerical schemes for solving initial value problems for first-order ordinary differential equations. Based on numerical simulations, we select one scheme and prove its first-order consistency and second-order local convergence.

</details>


### [4] [An Efficient Constant-Coefficient MSAV Scheme for Computing Vesicle Growth and Shrinkage](https://arxiv.org/abs/2601.10057)
*Zhiwei Zhang,Shuwang Li,John Lowengrub,Steven M. Wise*

Main category: math.NA

TL;DR: A fast, unconditionally energy-stable numerical scheme (CC-MSAV) for vesicle deformation simulation using phase-field approach, achieving 6-15x speedup over classical methods while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Classical approaches for simulating vesicle deformation under osmotic pressure (nonlinear multigrid and MSAV methods) require iterative solution of variable-coefficient systems at each time step, resulting in substantial computational cost that limits large-scale simulations.

Method: Introduces a constant-coefficient MSAV (CC-MSAV) scheme that incorporates stabilization directly into the Cahn-Hilliard evolution equation rather than the chemical potential. This reformulation yields fully decoupled constant-coefficient elliptic problems solvable via fast discrete cosine transform (DCT), eliminating iterative solvers entirely.

Result: Method achieves O(N^2 log N) complexity per time step while preserving unconditional energy stability and discrete mass conservation. Numerical experiments show second-order temporal/spatial accuracy, mass conservation to relative errors below 5×10^-11, and close agreement with benchmarks. On grids with N≥2048, CC-MSAV achieves 6-15x overall speedup compared to classical MSAV, with Cahn-Hilliard subsystem accelerated by up to two orders of magnitude.

Conclusion: The CC-MSAV scheme provides significant efficiency gains without sacrificing accuracy, making it particularly well suited for large-scale simulations of vesicle dynamics by eliminating iterative solvers and enabling fast DCT-based solutions.

Abstract: We present a fast, unconditionally energy-stable numerical scheme for simulating vesicle deformation under osmotic pressure using a phase-field approach. The model couples an Allen-Cahn equation for the biomembrane interface with a variable-mobility Cahn-Hilliard equation governing mass exchange across the membrane. Classical approaches, including nonlinear multigrid and Multiple Scalar Auxiliary Variable (MSAV) methods, require iterative solution of variable-coefficient systems at each time step, resulting in substantial computational cost. We introduce a constant-coefficient MSAV (CC-MSAV) scheme that incorporates stabilization directly into the Cahn-Hilliard evolution equation rather than the chemical potential. This reformulation yields fully decoupled constant-coefficient elliptic problems solvable via fast discrete cosine transform (DCT), eliminating iterative solvers entirely. The method achieves O(N^2 log N) complexity per time step while preserving unconditional energy stability and discrete mass conservation. Numerical experiments verify second-order temporal and spatial accuracy, mass conservation to relative errors below 5 x 10^-11, and close agreement with nonlinear multigrid benchmarks. On grids with N >= 2048, CC-MSAV achieves 6-15x overall speedup compared to classical MSAV with optimized preconditioning, while the dominant Cahn-Hilliard subsystem is accelerated by up to two orders of magnitude. These efficiency gains, achieved without sacrificing accuracy, make CC-MSAV particularly well suited for large-scale simulations of vesicle dynamics.

</details>


### [5] [New Second-order Convergent Schemes for Solving decoupled FBSDEs](https://arxiv.org/abs/2601.10149)
*Wenbo Wang,Guangyan Jia*

Main category: math.NA

TL;DR: Proposes new second-order symmetric algorithms for decoupled FBSDEs using ADI splitting, reducing computational cost while maintaining second-order convergence.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient numerical methods for solving decoupled forward-backward stochastic differential equations, particularly for equations with generators consisting of linear plus nonlinear parts, aiming to reduce computational cost while preserving high-order accuracy.

Method: Inspired by ADI splitting for PDEs, the generator is split into two functions. For the value process Y, explicit and implicit schemes are alternately applied to these two generators. For the control process Z, algorithms from Zhao & Li (2014) are used. The approach reduces iterations needed for implicit schemes.

Result: Two new schemes are rigorously proven to have second-order convergence rate. The splitting methods show clear advantages for equations with linear+nonlinear generators, reducing computational cost while maintaining second-order convergence. Numerical examples (including backward stochastic Riccati equation) verify theoretical error analysis and demonstrate reduced computational cost compared to Zhao & Li (2014).

Conclusion: The proposed splitting methods provide efficient second-order algorithms for decoupled FBSDEs, particularly beneficial for equations with mixed linear-nonlinear structure, offering computational savings without sacrificing accuracy.

Abstract: This paper proposes a new second-order symmetric algorithm for solving decoupled forward-backward stochastic differential equations. Inspired by the alternating direction implicit splitting method for partial differential equations, we split the generator into the sum of two functions. In the computation of the value process Y, explicit and implicit schemes are alternately applied to these two generators, while the algorithms from \citep{ZhaoLi2014} are used for the control process Z. We rigorously prove that the two new schemes have second-order convergence rate. The proposed splitting methods show clear advantages for equations whose generator consists of a linear part plus a nonlinear part, as they reduce the number of iterations required for solving implicit schemes, thereby decreasing computational cost while maintaining second-order convergence. Two numerical examples are provided, including the backward stochastic Riccati equation arising in mean-variance hedging. The numerical results verify the theoretical error analysis and demonstrate the advantage of reduced computational cost compared to the algorithm in \citep{ZhaoLi2014}.

</details>


### [6] [Introduction to optimization methods for training SciML models](https://arxiv.org/abs/2601.10222)
*Alena Kopaničáková,Elisa Riccietti*

Main category: math.NA

TL;DR: The paper provides a unified introduction to optimization methods in ML and SciML, explaining how different problem structures (stochastic data-driven vs. physics-constrained) require different algorithmic approaches.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between optimization approaches in classical machine learning and scientific machine learning by understanding how their fundamentally different problem structures (data-driven vs. physics-constrained) require different algorithmic strategies.

Method: The paper reviews and compares first- and second-order optimization techniques in both deterministic and stochastic settings, discusses their adaptation to SciML models, and provides tutorial examples to illustrate practical strategies.

Result: The analysis reveals that SciML optimization is governed by spectral properties of physical models rather than data statistics, making standard stochastic methods less effective and motivating deterministic or curvature-aware approaches.

Conclusion: Optimization in SciML requires specialized approaches due to physics-induced coupling, stiffness, and anisotropy in loss landscapes, highlighting the need for continued research at the interface of scientific computing and machine learning.

Abstract: Optimization is central to both modern machine learning (ML) and scientific machine learning (SciML), yet the structure of the underlying optimization problems differs substantially across these domains. Classical ML typically relies on stochastic, sample-separable objectives that favor first-order and adaptive gradient methods. In contrast, SciML often involves physics-informed or operator-constrained formulations in which differential operators induce global coupling, stiffness, and strong anisotropy in the loss landscape. As a result, optimization behavior in SciML is governed by the spectral properties of the underlying physical models rather than by data statistics, frequently limiting the effectiveness of standard stochastic methods and motivating deterministic or curvature-aware approaches. This document provides a unified introduction to optimization methods in ML and SciML, emphasizing how problem structure shapes algorithmic choices. We review first- and second-order optimization techniques in both deterministic and stochastic settings, discuss their adaptation to physics-constrained and data-driven SciML models, and illustrate practical strategies through tutorial examples, while highlighting open research directions at the interface of scientific computing and scientific machine learning.

</details>


### [7] [Restoring similarity in randomized Krylov methods with applications to eigenvalue problems and matrix functions](https://arxiv.org/abs/2601.10248)
*Laura Grigori,Daniel Kressner,Nian Shao,Igor Simunec*

Main category: math.NA

TL;DR: Modified randomized Arnoldi process that restores similarity with standard Arnoldi's Hessenberg matrix while maintaining computational speed.


<details>
  <summary>Details</summary>
Motivation: Randomized Arnoldi is faster than standard Arnoldi for large-scale computing but produces different Hessenberg matrices that cause convergence issues like delays and spike-like irregularities.

Method: Modify randomized Arnoldi by enforcing orthogonality between the last Arnoldi vector and previously generated subspace, requiring only one additional least-squares problem.

Result: Modified process produces approximations identical to standard Arnoldi for eigenvalue problems and matrix function evaluations, combining speed of randomized Arnoldi with robustness of standard Arnoldi.

Conclusion: The modified randomized Arnoldi process achieves both computational efficiency and numerical robustness, making it superior to both original methods for large-scale scientific computing.

Abstract: The randomized Arnoldi process has been used in large-scale scientific computing because it produces a well-conditioned basis for the Krylov subspace more quickly than the standard Arnoldi process. However, the resulting Hessenberg matrix is generally not similar to the one produced by the standard Arnoldi process, which can lead to delays or spike-like irregularities in convergence. In this paper, we introduce a modification of the randomized Arnoldi process that restores similarity with the Hessenberg matrix generated by the standard Arnoldi process. This is accomplished by enforcing orthogonality between the last Arnoldi vector and the previously generated subspace, which requires solving only one additional least-squares problem. When applied to eigenvalue problems and matrix function evaluations, the modified randomized Arnoldi process produces approximations that are identical to those obtained with the standard Arnoldi process. Numerical experiments demonstrate that our approach is as fast as the randomized Arnoldi process and as robust as the standard Arnoldi process.

</details>


### [8] [Conjugate Gradient Methods are Not Efficient: Experimental Study of the Locality Limitation](https://arxiv.org/abs/2601.10322)
*Ulrich Rüde*

Main category: math.NA

TL;DR: The Conjugate Gradient method's convergence is limited by graph locality - information propagates only one graph edge per iteration, so graph diameter sets a minimum iteration count for accurate solutions.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize the fundamental locality limitation of the Conjugate Gradient method, which stems from how information propagates through the graph structure of sparse matrices.

Method: Analyzes the Conjugate Gradient method through the lens of graph theory, viewing the sparse matrix as a graph where non-zero entries represent edges, and studying how information from the right-hand side propagates through this graph structure iteration by iteration.

Result: Identifies that information can only travel one graph edge per iteration, establishing that the graph diameter determines a lower bound on the number of iterations needed to achieve acceptable accuracy.

Conclusion: The Conjugate Gradient method has an inherent locality limitation: convergence speed is fundamentally constrained by the diameter of the graph induced by the matrix sparsity pattern, setting a minimum iteration requirement independent of algorithmic improvements.

Abstract: The convergence of the Conjugate Gradient method is subject to a locality limitation which imposes a lower bound on the number of iterations required before a qualitatively accurate approximation can be obtained. This limitation originates from the restricted transport of information in the graph induced by the sparsity pattern of the system matrix. In each iteration, information from the right-hand side can propagate only across directly connected graph nodes. The diameter of this graph therefore determines a minimum number of iterations that is necessary to achieve an acceptable level of accuracy.

</details>


### [9] [Regularization of linear inverse problems by rational Krylov methods](https://arxiv.org/abs/2601.10389)
*Stefan Kindermann*

Main category: math.NA

TL;DR: The paper analyzes regularization properties of aggregation and RatCG methods for solving linear ill-posed problems, showing they form optimal-order regularization schemes when combined with the discrepancy principle.


<details>
  <summary>Details</summary>
Motivation: To investigate regularization properties of recent algorithms (aggregation method and RatCG method) for solving linear ill-posed problems in Hilbert spaces, and to understand their connection to rational Krylov space methods.

Method: The methods use previously calculated solutions of Tikhonov regularization (or Landweber iterations) to create new search spaces where the least-squares functional is minimized. The paper analyzes these methods as rational Krylov space methods based on rational functions of the forward operator.

Result: The main result shows that these methods form optimal-order regularization schemes when combined with the discrepancy principle as stopping rule, provided the underlying regularization parameters are sufficiently large.

Conclusion: Aggregation and RatCG methods provide effective regularization approaches for linear ill-posed problems, with theoretical guarantees of optimal-order convergence when properly parameterized and stopped using the discrepancy principle.

Abstract: For approximately solving linear ill-posed problems in Hilbert spaces, we investigate the regularization properties of the aggregation method and the RatCG method. These recent algorithms use previously calculated solutions of Tikhonov regularization (respectively, Landweber iterations) to set up a new search space on which the least-squares functional is minimized. We outline how these methods can be understood as rational Krylov space methods, i.e., based on the space of rational functions of the forward operator. The main result is that these methods form an optimal-order regularization schemes when combined with the discrepancy principle as stopping rule and when the underlying regularization parameters are sufficiently large.

</details>


### [10] [A Geometric Multigrid Preconditioner for Shifted Boundary Method](https://arxiv.org/abs/2601.10399)
*Michał Wichrowski,Ajay Ajith*

Main category: math.NA

TL;DR: The paper presents a geometric multigrid preconditioner with a Full-Residual Shy Patch smoother that effectively solves linear systems from the Shifted Boundary Method, overcoming AMG limitations and achieving stable convergence up to p=3 in 3D.


<details>
  <summary>Details</summary>
Motivation: The Shifted Boundary Method (SBM) reduces body-fitted meshing burden but creates non-symmetric, non-local boundary coupled linear systems that resist standard Algebraic Multigrid (AMG) and simple smoothers, especially for high-order discretizations.

Method: Develops a geometric multigrid preconditioner with a Full-Residual Shy Patch smoother - a subspace correction strategy that filters some patches while capturing full physics of shifted boundaries, unlike previous cell-wise approaches.

Result: The method delivers convergence with low mesh dependence and maintains low, stable iteration counts up to polynomial degree p=3 in 3D for Continuous Galerkin approximations.

Conclusion: The proposed geometric multigrid approach proves that SBM can be both geometrically flexible (reducing meshing burden) and algebraically efficient (solving resulting linear systems effectively).

Abstract: The Shifted Boundary Method (SBM) trades some part of the burden of body-fitted meshing for increased algebraic complexity. While the resulting linear systems retain the standard $\mathcal{O}(h^{-2})$ conditioning of second-order operators, the non-symmetry and non-local boundary coupling render them resistant to standard Algebraic Multigrid (AMG) and simple smoothers for high-order discretisations. We present a geometric multigrid preconditioner that effectively tames these systems. At its core lies the \emph{Full-Residual Shy Patch} smoother: a subspace correction strategy that filters out some patches while capturing the full physics of the shifted boundary. Unlike previous cell-wise approaches that falter at high polynomial degrees, our method delivers convergence with low mesh dependence. We demonstrate performance for Continuous Galerkin approximations, maintaining low and stable iteration counts up to polynomial degree $p=3$ in 3D, proving that SBM can be both geometrically flexible and algebraically efficient.

</details>


### [11] [Optimal error estimates for a discontinuous Galerkin method on curved boundaries with polygonal meshes](https://arxiv.org/abs/2601.10474)
*Adérito Araújo,Milene Santos*

Main category: math.NA

TL;DR: The paper provides a rigorous theoretical analysis of the DG-ROD method, a discontinuous Galerkin approach that restores optimal convergence rates on polygonal approximations of domains with curved boundaries for advection-diffusion-reaction problems.


<details>
  <summary>Details</summary>
Motivation: When solving boundary value problems on curved domains using polygonal meshes, geometric approximation errors typically cause suboptimal convergence. The DG-ROD method was developed to overcome this limitation by using polynomial reconstruction of boundary data to maintain optimal convergence rates despite polygonal domain approximations.

Method: The paper analyzes the DG-ROD (Reconstruction of Domain) method for discontinuous Galerkin discretizations. The method uses polynomial reconstruction of boundary data to accurately transfer boundary conditions from the true curved boundary to the computational polygonal boundary. The analysis extends techniques from classical finite element methods to establish theoretical foundations for this approach.

Result: The authors prove existence and uniqueness of the discrete solution and derive error estimates for a 2D linear advection-diffusion-reaction problem with homogeneous Dirichlet boundary conditions. They demonstrate that under suitable regularity assumptions, the DG-ROD method achieves optimal convergence rates on both convex and non-convex domains despite using polygonal approximations of curved boundaries.

Conclusion: The DG-ROD method successfully restores optimal convergence for discontinuous Galerkin methods on polygonal approximations of curved domains. The theoretical analysis provides rigorous justification for the method's effectiveness, and numerical benchmarks confirm the theoretical results, making the approach reliable for practical applications involving curved boundaries.

Abstract: We consider a discontinuous Galerkin method for the numerical solution of boundary value problems in two-dimensional domains with curved boundaries. A key challenge in this setting is the potential loss of convergence order due to approximating the physical domain by a polygonal mesh. Unless boundary conditions can be accurately transferred from the true boundary to the computational one, such geometric approximation errors generally lead to suboptimal convergence. To overcome this limitation, a higher-order strategy based on polynomial reconstruction of boundary data was introduced for classical finite element methods in [28, 29] and in the finite volume context in [7, 11]. More recently, this approach was extended to discontinuous Galerkin methods in [32], leading to the DG-ROD method, which restores optimal convergence rates on polygonal approximations of domains with curved boundaries. In this work, we provide a rigorous theoretical analysis of the DG-ROD method, establishing existence and uniqueness of the discrete solution and deriving error estimates for a two-dimensional linear advection-diffusion-reaction problem with homogeneous Dirichlet boundary conditions on both convex and non-convex domains. Following and extending techniques from classical finite element methods [29], we prove that, under suitable regularity assumptions on the exact solution, the DG-ROD method achieves optimal convergence despite polygonal approximations. Finally, we illustrate and confirm the theoretical results with a numerical benchmark.

</details>


### [12] [Chebyshev Accelerated Subspsace Eigensolver for Pseudo-hermitian Hamiltonians](https://arxiv.org/abs/2601.10557)
*Edoardo Di Napoli,Clément Richefort,Xinzhe Wu*

Main category: math.NA

TL;DR: ChASE eigensolver extended to pseudo-hermitian Hamiltonians for excitonic materials, achieving similar performance to hermitian version with oblique Rayleigh-Ritz projection and optimized parallel implementation.


<details>
  <summary>Details</summary>
Motivation: Computing thousands of eigenpairs for excitonic materials requires efficient solvers for pseudo-hermitian Hamiltonians, where iterative methods are preferred over direct methods for scalability on exascale systems.

Method: Extended ChASE solver to handle pseudo-hermitian Hamiltonians with oblique Rayleigh-Ritz projection for quadratic convergence, plus optimized parallel implementation of Chebyshev filter with reduced global communications.

Result: The pseudo-hermitian solver achieves similar convergence and performance as the original hermitian ChASE solver, with numerical analysis and experimental validation supporting the developments.

Conclusion: Successfully extended ChASE to pseudo-hermitian problems relevant to excitonic materials, maintaining performance while introducing novel oblique projection techniques and communication-efficient parallel implementation.

Abstract: Studying the optoelectronic structure of materials can require the computation of up to several thousands of the smallest eigenpairs of a pseudo-hermitian Hamiltonian. Iterative eigensolvers may be preferred over direct methods for this task since their complexity is a function of the desired fraction of the spectrum. In addition, they generally rely on highly optimized and scalable kernels such as matrix-vector multiplications that leverage the massive parallelism and the computational power of modern exascale systems. \textit{Chebyshev Accelerated Subspace iteration Eigensolver} (ChASE) is able to compute several thousands of the most extreme eigenpairs of dense hermitian matrices with proven scalability over massive parallel accelerated clusters. This work presents an extension of ChASE to solve for a portion of the spectrum of pseudo-hermitian Hamiltonians as they appear in the treatment of excitonic materials. The new pseudo-hermitian solver achieves similar convergence and performance as the hermitian one. By exploiting the numerical structure and spectral properties of the Hamiltonian matrix, we propose an oblique variant of Rayleigh-Ritz projection featuring quadratic convergence of the Ritz-values with no explicit construction of the dual basis set. Additionally, we introduce a parallel implementation of the recursive matrix-product operation appearing in the Chebyshev filter with limited amount of global communications. Our development is supported by a full numerical analysis and experimental tests.

</details>


### [13] [Stable evaluation of derivatives for barycentric and continued fraction representations of rational functions](https://arxiv.org/abs/2601.10667)
*Tobin A. Driscoll,Yuxing Zhou*

Main category: math.NA

TL;DR: Fast, stable algorithms for computing derivatives of rational approximations in barycentric and Thiele continued fraction forms.


<details>
  <summary>Details</summary>
Motivation: Existing fast algorithms for rational approximation exist, but there's a need for numerically stable methods for derivative evaluation in these representations, particularly for barycentric forms where stable derivative algorithms were lacking.

Method: Developed first numerically stable methods for derivative evaluation in barycentric representation, including O(n) algorithm for all derivatives. Extended earlier O(n) algorithm for Thiele continued fraction first derivative to higher order derivatives.

Result: Numerical experiments confirm the proposed methods are both robust (numerically stable) and efficient (O(n) complexity).

Conclusion: The paper provides practical, stable algorithms for derivative evaluation in two important rational approximation representations, filling a gap in the computational mathematics toolbox.

Abstract: Fast algorithms for approximation by rational functions exist for both barycentric and Thiele continued fraction (TCF) representations. We present the first numerically stable methods for derivative evaluation in the barycentric representation, including an $O(n)$ algorithm for all derivatives. We also extend an earlier $O(n)$ algorithm for evaluation of the TCF first derivative to higher orders. Numerical experiments confirm the robustness and efficiency of the proposed methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [14] [Lossless Strichartz estimates on the square torus over short time intervals](https://arxiv.org/abs/2601.09895)
*Connor Quinn*

Main category: math.AP

TL;DR: Lossless Strichartz estimates at critical exponent for Schrödinger equation on square torus with frequency-localized data on small time windows.


<details>
  <summary>Details</summary>
Motivation: To establish sharp Strichartz estimates for the Schrödinger equation on compact domains (square torus) at the critical scaling exponent, which is important for understanding dispersive properties and well-posedness of nonlinear Schrödinger equations.

Method: Frequency localization of initial data, analysis on small time windows whose length depends on frequency parameter λ, working on square torus geometry, proving estimates at critical exponent q_c = 2(n+1)/(n-1).

Result: Proves lossless Strichartz estimates at the critical exponent q_c for the Schrödinger equation on square torus with frequency-localized initial data on appropriately scaled time windows.

Conclusion: Achieves sharp dispersive estimates on compact domains by combining frequency localization with appropriately scaled time windows, providing optimal control at the critical scaling exponent.

Abstract: We prove lossless Strichartz estimates at the critical exponent $q_c = \frac{2(n+1)}{n-1}$ on the square torus for the Schrödinger equation with frequency localized initial data on small time windows with length depending on the frequency parameter $λ\gg 1$.

</details>


### [15] [On the Dirichlet boundary value problem on Cartan-Hadamard manifolds](https://arxiv.org/abs/2601.09930)
*Marcos P. Cavalcante,José M. Espinar,Diego A. Marín*

Main category: math.AP

TL;DR: Extends non-existence results for bounded solutions of semi-linear elliptic equations from hyperbolic spaces to Cartan-Hadamard manifolds using novel comparison techniques.


<details>
  <summary>Details</summary>
Motivation: To extend previous results by Bonorino and Klaser on non-existence of bounded solutions to semi-linear elliptic equations from hyperbolic spaces to more general Cartan-Hadamard manifolds, overcoming the lack of totally geodesic foliations.

Method: Uses a novel comparison technique based on convex hypersurfaces inspired by Choi, Gálvez, and Lozano, which works without relying on totally geodesic foliations that were essential in hyperbolic space approaches.

Result: Establishes non-existence of bounded (viscosity) solutions to Δu + f(u) = 0 in domains with prescribed asymptotic boundary on Cartan-Hadamard manifolds, extending previous hyperbolic space results.

Conclusion: Demonstrates the interplay between curvature, Laplacian spectrum, and asymptotic boundary geometry, showing how convex hypersurface techniques can replace totally geodesic foliations in proving non-existence results.

Abstract: In this paper, we investigate the Dirichlet boundary value problem on Cartan-Hadamard manifolds, focusing on the non-existence of bounded (viscosity) solutions to semi-linear elliptic equations of the form $Δu + f(u) = 0$ in domains with prescribed asymptotic boundary, extending previous results by Bonorino and Klaser originally established for hyperbolic spaces. Using a novel comparison technique based on convex hypersurfaces inspired by Choi, Gálvez, and Lozano, we overcome the absence of totally geodesic foliations, which are instrumental in the hyperbolic space. Our results highlight the interplay between curvature, the spectrum of the Laplacian, and the geometry of the asymptotic boundary.

</details>


### [16] [Stability and instability of small BGK waves](https://arxiv.org/abs/2601.10030)
*Dongfen Bian,Emmanuel Grenier,Wenrui Huang,Benoit Pausader*

Main category: math.AP

TL;DR: Linear stability of small BGK waves depends on sign of energy distribution derivative at zero energy.


<details>
  <summary>Details</summary>
Motivation: To establish a simple criterion for determining linear stability of small BGK waves in plasma physics.

Method: Mathematical analysis of Bernstein-Green-Kruskal waves, focusing on energy distribution properties near zero energy.

Result: Proves that linear stability/instability is determined by sign of energy distribution derivative at 0 energy.

Conclusion: Provides clear mathematical criterion for stability analysis of small BGK waves based on energy distribution properties.

Abstract: The aim of this article is to prove that the linear stability or instability of small Bernstein-Green-Kruskal (BGK) waves is determined by the sign of the derivative of their energy distributions at $0$ energy.

</details>


### [17] [Transport equation theory in the Triebel-Lizorkin spaces and its applications to the ideal fluid flows](https://arxiv.org/abs/2601.10071)
*Qianyuan Zhang,Kai Yan*

Main category: math.AP

TL;DR: General transport equation theory in Triebel-Lizorkin spaces with commutator estimates, local well-posedness, and applications to fluid models including ideal MHD.


<details>
  <summary>Details</summary>
Motivation: Develop a comprehensive theory for transport equations in Triebel-Lizorkin spaces that applies to various evolution equations in fluid dynamics, overcoming limitations of conventional divergence-free conditions.

Method: Uses Bony paraproduct decomposition and vector-valued maximal function inequalities for commutator estimates, combines method of characteristics with compactness arguments for a priori estimates and local well-posedness.

Result: Establishes new a priori estimates, proves local well-posedness for transport equations in Triebel-Lizorkin spaces, and provides complete local well-posedness for ideal MHD system covering sub-critical and critical regimes with blow-up criteria.

Conclusion: The developed transport theory significantly extends previous work and provides a powerful framework applicable to various fluid models, with demonstrated effectiveness for ideal MHD equations.

Abstract: In this paper, we develop a general theory for the transport equation within the framework of Triebel-Lizorkin spaces. We first derive commutator estimates in these spaces, dispensing with the conventional divergence-free condition, via the Bony paraproduct decomposition and vector-valued maximal function inequalities. Building on these estimates and combining the method of characteristics with a compactness argument, we then obtain the new a priori estimates and prove local well-posedness for the transport equation in Triebel-Lizorkin spaces. The resulting theory is applicable to a wide range of evolution equations, including models for incompressible and compressible ideal fluid flows, shallow water waves, among others. As an illustration, we consider the incompressible ideal magnetohydrodynamics (MHD) system. Employing the general transport theory developed here yields a complete local well-posedness result in the sense of Hadamard, covering both sub-critical and critical regularity regimes, and provides corresponding blow-up criteria for the ideal MHD equations in Triebel-Lizorkin spaces. Our results refine and substantially extend earlier work in this direction.

</details>


### [18] [Characteristics of drift effects in the quasi-geostrophic equation arising from nonlinear symmetry](https://arxiv.org/abs/2601.10185)
*Masakazu Yamamoto*

Main category: math.AP

TL;DR: Comparison of two similar diffusion equations in meteorology (quasi-geostrophic vs convection-diffusion) showing how nonlinear effects act differently despite having same scale and differentiation order, quantified through large-time behavior analysis.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the differences between two similar meteorological diffusion equations that appear structurally identical in terms of scale and differentiation order, but differ in how nonlinear effects operate spatially.

Method: Analyze and compare the large-time asymptotic behavior of solutions to both equations, focusing on nonlinear distortions in their asymptotic profiles, with particular attention to spatial symmetry in first approximations.

Result: The paper demonstrates that while both equations share similar mathematical structure, their nonlinear effects act differently: one along isothermal surfaces and the other along temperature gradients, leading to quantifiable differences in asymptotic solution behavior.

Conclusion: The key difference between quasi-geostrophic and convection-diffusion equations lies in the spatial direction of nonlinear effects, which significantly influences their large-time behavior despite mathematical similarities, with spatial symmetry playing a crucial role in first approximations.

Abstract: This paper compares two similar diffusion equations that appear in meteorology. One is the quasi-geostrophic equation, and the other is the convection-diffusion equation. Both are two-dimensional bilinear equations, and the order of differentiation is the same. Naturally, their scales also coincide. However, the direction in which the nonlinear effects act differs: one acts along the isothermal surface, while the other acts along the temperature gradient in a specified direction. The main assertion quantifies this difference through the large-time behavior of their solutions. In particular, the nonlinear distortions in the asymptotic profiles of both equations are compared. In this context, the spatial symmetry of the first approximation plays a crucial role, but the solutions require no symmetry.

</details>


### [19] [Optimisation of the lowest Robin eigenvalue in exterior domains of the hyperbolic plane](https://arxiv.org/abs/2601.10280)
*Antonio Celentano,David Krejcirik,Vladimir Lotoreichik*

Main category: math.AP

TL;DR: The paper studies the Robin Laplacian in the exterior of convex domains in hyperbolic plane, showing the essential spectrum is [1/4,∞), and proving isoperimetric inequalities: geodesic disk maximizes the lowest Robin eigenvalue among convex exteriors under fixed area/perimeter constraints.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties of the Robin Laplacian in hyperbolic geometry, particularly for exterior domains, and establish isoperimetric-type inequalities analogous to known results in Euclidean settings.

Method: Analyzes the Robin Laplacian operator in hyperbolic plane exterior domains using spectral theory, convex geometry, and comparison techniques with geodesic disks. Employs geometric constraints and curvature comparisons.

Result: Essential spectrum is [1/4,∞); discrete eigenvalues exist below 1/4 only if Robin parameter is below a domain-dependent critical constant. Geodesic disk maximizes lowest Robin eigenvalue among convex exteriors under fixed area/perimeter constraints.

Conclusion: The paper establishes fundamental spectral properties and optimal geometric shapes for Robin eigenvalues in hyperbolic exterior domains, providing hyperbolic analogs of classical Euclidean isoperimetric inequalities for spectral optimization.

Abstract: We consider the Robin Laplacian in the exterior of a bounded simply-connected Lipschitz domain in the hyperbolic plane. We show that the essential spectrum of this operator is $[\frac14,\infty)$ and that, under convexity assumption on the domain, there exist discrete eigenvalues below $\frac14$ if, and only if, the Robin parameter is below a non-positive critical constant, which depends on the shape of the domain. As the main result, we prove that the lowest Robin eigenvalue for the exterior of a bounded geodesically convex domain $Ω$ in the hyperbolic plane does not exceed such an eigenvalue for the exterior of the geodesic disk, whose geodesic curvature of the boundary is not smaller than the averaged geodesic curvature of the boundary of $Ω$. This result implies as a consequence that under fixed area or fixed perimeter constraints the exterior of the geodesic disk maximises the lowest Robin eigenvalue among exteriors of bounded geodesically convex domains. Moreover, we obtain under the same geometric constraints a reverse inequality between the critical constants.

</details>


### [20] [High-Contrast Transmission Resonances for the Lamé System](https://arxiv.org/abs/2601.10290)
*Long Li,Mourad Sini*

Main category: math.AP

TL;DR: The paper analyzes scattering resonances in 3D elastic high-contrast inclusions, obtaining sharp asymptotics for resonance clusters near Neumann eigenvalues and zero, with lifetime dichotomies and explicit expansions.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral properties and resonance behavior of elastic wave scattering in high-contrast materials, which is important for applications in metamaterials, wave manipulation, and sensing technologies.

Method: Mathematical analysis of the Lamé transmission problem using asymptotic methods, meromorphic continuation of resolvents, and spectral theory to study resonance clusters in the high-contrast limit (τ→0).

Result: Sharp asymptotic description of resonances: clusters near Neumann eigenvalues with O(τ) imaginary parts; subwavelength resonances near zero with O(√τ) real parts and lifetime dichotomy (O(τ) vs O(τ²) imaginary parts). Explicit expansions with finite-rank leading terms and rigorous criteria for monopole/dipole behavior.

Conclusion: The work provides a complete classification of long-lived elastic resonances in high-contrast materials, with explicit asymptotic formulas and quantitative bounds that distinguish between wavelength-scale and subwavelength regimes, offering fundamental insights for elastic metamaterial design.

Abstract: We consider the Lamé transmission problem in $\mathbb{R}^3$ with a bounded isotropic elastic inclusion in a high-contrast setting, where the interior-to-exterior Lamé moduli and densities scale like $1/τ$ as $τ\to0$. We study the scattering resonances of the associated self-adjoint Hamiltonian, defined as the poles of the meromorphic continuation of its resolvent.
  We obtain a sharp asymptotic description of resonances near the real axis as $τ\to0$. Near each nonzero Neumann eigenvalue of the interior Lamé operator there is a cluster of resonances lying just below it in the complex plane; in this wavelength-scale regime the imaginary parts are of order $τ$ with non-vanishing leading coefficients. In addition, near zero (a subwavelength regime), we identify resonances with real parts of order $\sqrtτ$ and prove a lifetime dichotomy: their imaginary parts are of order $τ$ generically, but of order $τ^2$ for an explicit admissible set $\mathcal E$. This yields a classification of long-lived elastic resonances in the high-contrast limit.
  We also establish resolvent asymptotics for both fixed-size resonators and microresonators. We derive explicit expansions with a finite-rank leading term and quantitative remainder bounds, valid near both wavelength-scale and subwavelength resonances. For microresonators, at the wavelength scale the dominant contribution is an anisotropic elastic point scatterer. Near the zero eigenvalue, the leading-order behaviour is of monopole or dipole type, and we give a rigorous criterion distinguishing the two cases.

</details>


### [21] [Global minimizers for a two-sided biharmonic Alt-Caffarelli problem](https://arxiv.org/abs/2601.10297)
*Hans-Christoph Grunau,Marius Müller*

Main category: math.AP

TL;DR: The paper studies global minimizers of biharmonic Alt-Caffarelli functionals, finding half-space solutions are global minimizers for two-sided but not one-sided problems, identifying new constant-Laplacian minimizers, and showing two-sided minimizers don't satisfy PDEs.


<details>
  <summary>Details</summary>
Motivation: To understand the structure and properties of global minimizers for biharmonic analogues of the Alt-Caffarelli functional, extending previous two-dimensional classification work to arbitrary dimensions.

Method: Mathematical analysis of biharmonic Alt-Caffarelli functionals, building on Lamboley and Nahon's classification in dimension two, extending to arbitrary dimensions, and studying PDE properties of minimizers.

Result: Half-space solutions are global minimizers for two-sided but not one-sided problems; identified new constant-Laplacian minimizers; three of four two-dimensional categories persist as global minimizers in any dimension; two-sided minimizers don't satisfy PDEs.

Conclusion: The study reveals fundamental differences between one-sided and two-sided biharmonic Alt-Caffarelli problems, with two-sided minimizers lacking PDE structure despite being global minimizers, and provides complete classification of minimizer categories across dimensions.

Abstract: We study global minimizers of biharmonic analogues of the Alt-Caffarelli functional. It turns out that half-space solutions are global minimizers for the two-sided Alt-Caffarelli functional, but not in the one-sided case. In addition, we identify a further class of global minimizers, all of which have constant Laplacian. Recent work by J. Lamboley and M. Nahon reduces potential global minimizers in dimension two to four possible categories. Our work shows that three of these categories persist in any dimension and are in fact global minimizers.
  Moreover, we show that minimizers of the two-sided biharmonic Alt-Caffarelli problem do in general not satisfy a partial differential equation, not even with a signed measure as right-hand-side. This is in sharp contrast to the corresponding one-sided problem.

</details>


### [22] [Optimality in nonlocal time-dependent obstacle problems](https://arxiv.org/abs/2601.10417)
*Ioannis Athanasopoulos,Luis Caffarelli,Emmanouil Milakis*

Main category: math.AP

TL;DR: The paper demonstrates how quasiconvexity property helps determine optimal regularity of temporal derivative and continuity conditions in nonlocal time-dependent obstacle problems.


<details>
  <summary>Details</summary>
Motivation: To understand and establish regularity properties for temporal derivatives in nonlocal time-dependent obstacle problems, which are important for analyzing solutions to such problems.

Method: Utilizes quasiconvexity property as a key analytical tool to study the optimal regularity of temporal derivatives and establish continuity conditions.

Result: Shows effectiveness of quasiconvexity in addressing optimal regularity and establishes specific conditions under which temporal derivative continuity holds.

Conclusion: Quasiconvexity is an effective property for analyzing regularity of temporal derivatives in nonlocal time-dependent obstacle problems, providing conditions for continuity.

Abstract: This paper showcases the effectiveness of the quasiconvexity property in addressing the optimal regularity of the temporal derivative and establishes conditions for its continuity in nonlocal time-dependent obstacle problems.

</details>


### [23] [A Riemannian Autocorrelation Function and its Application to Non-Local Isoperimetric Energies](https://arxiv.org/abs/2601.10481)
*Michael Bleher,Denis Brazke,Sebastian Nill*

Main category: math.AP

TL;DR: The paper studies non-local isoperimetric energies on spheres using a Riemannian autocorrelation function, establishes connections between perimeter and Lipschitz properties, and analyzes Γ-convergence as the non-local parameter vanishes.


<details>
  <summary>Details</summary>
Motivation: To analyze non-local isoperimetric energies on Riemannian manifolds, particularly spheres, by developing tools that connect geometric measure theory with non-local interactions, extending classical convex geometry concepts to curved spaces.

Method: Introduces a Riemannian autocorrelation function for measurable sets on compact manifolds, establishes characterization of BV functions via geodesic difference quotients, links perimeter to Lipschitz properties, and reformulates non-local energies to study Γ-convergence as ε→0.

Result: Shows that a set has finite perimeter if and only if its autocorrelation function is Lipschitz, relates Lipschitz constant to perimeter, and computes the Γ-limit of non-local isoperimetric energies on spheres as the interaction range vanishes.

Conclusion: The Riemannian autocorrelation function provides a powerful tool for analyzing non-local geometric energies on manifolds, connecting perimeter theory with Lipschitz properties and enabling variational analysis of Γ-convergence for non-local isoperimetric problems.

Abstract: We study a family of non-local isoperimetric energies $E_{γ,\varepsilon}$ on the round sphere $M = S^n$, where the non-local interaction kernel $K_\varepsilon$ is the fundamental solution of the Helmholtz operator $1 - \varepsilon^2 Δ$. To analyse these energies, we introduce a Riemannian autocorrelation function $c_Ω$ associated to a measurable set $Ω\subset M$, defined on any compact, connected, oriented Riemannian manifold without boundary $(M^n,g)$ of dimension $n\ge2$. This function is intimately linked to Matheron's set covariogram from convex geometry. By establishing a characterisation of functions of bounded variation $BV(M)$ in terms of geodesic difference quotients, we show that $Ω$ has finite perimeter if and only if $c_Ω$ is Lipschitz, and we relate the Lipschitz constant to the perimeter of $Ω$. We show that on the round sphere $E_{γ,\varepsilon}$ admits a reformulation in terms of $c_Ω$, which allows us to compute the limit as $\varepsilon \to 0$ in a variational sense, that is, in the framework of $Γ$-convergence.

</details>


### [24] [A proof of the soliton resolution conjecture for the Benjamin--Ono equation](https://arxiv.org/abs/2601.10488)
*Louise Gassot,Patrick Gérard,Peter D. Miller*

Main category: math.AP

TL;DR: Proof of soliton resolution conjecture for Benjamin-Ono equation: solutions decompose into finite sum of solitons plus radiative remainder in long-time asymptotics.


<details>
  <summary>Details</summary>
Motivation: To prove the soliton resolution conjecture for the Benjamin-Ono equation, which describes how solutions with regular decaying initial data decompose into solitons and radiation in the long-time limit.

Method: Uses representation formula for solutions and detailed analysis of distorted Fourier transform associated with the Lax operator. Establishes correspondence between spectral theory of Lax operator and terms in soliton resolution expansion.

Result: Successfully proves the soliton resolution conjecture: every sufficiently regular decaying solution can be written as finite sum of soliton solutions with different velocities plus radiative remainder term in long-time asymptotics.

Conclusion: The paper provides complete proof of soliton resolution for Benjamin-Ono equation, connecting spectral theory of Lax operator to soliton decomposition and establishing rigorous long-time asymptotic behavior.

Abstract: We give a proof of the soliton resolution conjecture for the Benjamin--Ono equation, namely every solution with sufficiently regular and decaying initial data can be written as a finite sum of soliton solutions with different velocities up to a radiative remainder term in the long--time asymptotics. We provide a detailed correspondence between the spectral theory of the Lax operator associated to the initial data and the different terms of the soliton resolution expansion. The proof is based on a new use of a representation formula of the solution due to the second author, and on a detailed analysis of the distorted Fourier transform associated to the Lax operator.

</details>


### [25] [Michael-Simon inequality for anisotropic energies close to the area via multilinear Kakeya-type bounds](https://arxiv.org/abs/2601.10647)
*Guido De Philippis,Alessandro Pigati*

Main category: math.AP

TL;DR: The paper proves Michael-Simon inequalities for anisotropic area functionals in 2D surfaces in 3D space, extending classical results to convex integrands close to isotropic area and to ℓ^p norms.


<details>
  <summary>Details</summary>
Motivation: Classical Michael-Simon inequalities for isotropic area functionals are well-known, but extending them to anisotropic functionals (where area depends on surface orientation) is challenging. The motivation is to establish these important geometric inequalities for broader classes of anisotropic integrands, which have applications in geometric analysis and calculus of variations.

Method: The authors build on unpublished work by Almgren but simplify his proof significantly. A key innovation is proving a new functional inequality for vector fields on the plane, which serves as a quantitative version of Alberti's rank-one theorem. This inequality helps establish the Michael-Simon inequality for convex integrands close to 1 in C¹ norm. The approach also handles ℓ^p norms through different techniques.

Result: Main results: (1) For k=2, n=3, if F is convex and close to 1 in C¹ norm, then the Michael-Simon inequality holds for the anisotropic functional ℱ. (2) The inequality also holds for integrands including ℓ^p norms for p∈(1,∞). (3) For general F satisfying the atomic condition, validity of Michael-Simon inequality is equivalent to compactness of rectifiable varifolds.

Conclusion: The paper successfully extends Michael-Simon inequalities to important classes of anisotropic integrands, providing both specific results for convex integrands near isotropic area and ℓ^p norms, as well as a general characterization connecting the inequality to varifold compactness. The simplified proof and new functional inequality represent significant contributions to anisotropic geometric analysis.

Abstract: Given an anisotropic integrand $F:\text{Gr}_k(\mathbb R^n)\to(0,\infty)$, we can generalize the classical isotropic area by looking at the functional $$\mathcal{F}(Σ^k):=\int_ΣF(T_xΣ)\,d\mathcal{H}^k.$$ While a monotonicity formula is not available for critical points, when $k=2$ and $n=3$ we show that the Michael-Simon inequality holds if $F$ is convex and close to $1$ (in $C^1$), meaning that $\mathcal{F}$ is close to the usual area.
  Our argument is partly based on some key ideas of Almgren, who proved this result in an unpublished manuscript, but we largely simplify his original proof by showing a new functional inequality for vector fields on the plane, which can be seen as a quantitative version of Alberti's rank-one theorem.
  As another byproduct, we also show Michael-Simon for another class of integrands which includes the $\ell^p$ norms for $p\in(1,\infty)$. For a general $F$ satisfying the atomic condition, we also show that the validity of Michael-Simon is equivalent to compactness of rectifiable varifolds.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [RLC Parameters of a Two-Wire Line with the Finite Element Method](https://arxiv.org/abs/2601.09829)
*Marc Boulé*

Main category: physics.comp-ph

TL;DR: Tutorial on computing DC resistance, inductance, and capacitance of parallel wires using finite element method with open-source ONELAB software.


<details>
  <summary>Details</summary>
Motivation: To provide a practical tutorial for computing key electrical parameters (R, L, C) of parallel wire configurations using finite element analysis, including consideration of insulation effects and physical defects.

Method: Uses 3D infinite domain modeling of electrostatic and magnetostatic fields with electrokinetic formulation for current flow. Open-source ONELAB software for simulations, with code provided. Includes insulation and defect modeling.

Result: Simulations performed and validated through comparisons with analytical models (when applicable) and commercial Altair Flux software to ensure accuracy.

Conclusion: Provides a complete tutorial approach for computing wire parameters using finite element method with open-source tools, validated against established methods.

Abstract: This tutorial paper shows how to compute the DC (or low-frequency) resistance, inductance and capacitance of a pair of parallel wires using the finite element method. A three-dimensional infinite domain (open boundary) modeling of electrostatic and magnetostatic fields is presented, along with the electrokinetic formulation for the current flow inside the wires. The effects of the insulation and of a proposed physical defect in the wires are also considered. The open-source ONELAB software is used to perform the simulations and the code listing is provided. Comparisons using analytical models (when applicable) and the Altair Flux software are performed to help validate the simulations.

</details>


### [27] [A Level Set Method on Particle Flow Maps](https://arxiv.org/abs/2601.09939)
*Jinjin He,Taiyuan Zhang,Zhiqi Li,Junwei Zhou,Duowen Chen,Bo Zhu*

Main category: physics.comp-ph

TL;DR: PFM-LS method combines particle-based level set storage in interface regions with grid-based representation elsewhere, using bidirectional flow maps and differential form interpretation to achieve superior interface tracking with exceptional geometric fidelity and sub-grid feature preservation.


<details>
  <summary>Details</summary>
Motivation: Traditional level-set methods struggle with preserving geometric fidelity during complex deformations and capturing sub-grid features, especially in interface regions where high accuracy is needed.

Method: Hybrid particle-grid approach storing level-set values, gradients, and Hessians on particles in narrow band around interface; uses bidirectional flow maps for advection; interprets level set as 3-form and gradient as 1-form; dual-timescale approach with long-range maps for values/gradients and frequent short-range maps for Hessian; adaptive particle control; hybrid particle-grid quasi-Newton redistancing.

Result: PFM-LS achieves state-of-the-art volume preservation and shape fidelity in 2D and 3D benchmarks, outperforming existing level-set methods while preserving fine-scale features that traditional methods cannot capture.

Conclusion: The PFM-LS method successfully addresses limitations of traditional level-set methods by combining particle-based precision at interfaces with grid-based efficiency elsewhere, achieving exceptional geometric fidelity and feature preservation through innovative differential form interpretation and dual-timescale flow map approach.

Abstract: This paper introduces a Particle Flow Map Level Set (PFM-LS) method for high-fidelity interface tracking. We store level-set values, gradients, and Hessians on particles concentrated in a narrow band around the interface, advecting them via bidirectional flow maps while using a conventional grid-based representation elsewhere. By interpreting the level set value as a 3-form and its gradient as a 1-form, PFM-LS achieves exceptional geometric fidelity during complex deformations and preserves sub-grid features that traditional methods cannot capture. Our dual-timescale approach utilizes long-range maps for values and gradients, with frequent reinitialization of short-range maps for the distortion-sensitive Hessian, alongside adaptive particle control that maintains sufficient density within the narrow band. We also develop a hybrid particle-grid quasi-Newton redistancing scheme that preserves fine-scale features while enforcing the signed-distance property. Benchmark comparisons in 2D and 3D demonstrate that PFM-LS achieves state-of-the-art volume preservation and shape fidelity against a broad range of existing level-set methods.

</details>


### [28] [A volume penalization method for solving conjugate scalar transport with interfacial jump conditions](https://arxiv.org/abs/2601.10134)
*Ming Liu,Yosuke Hasegawa*

Main category: physics.comp-ph

TL;DR: A novel immersed boundary method for conjugate scalar transport with interfacial jumps on complex geometries, achieving <3% error compared to body-fitted mesh simulations.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient simulation of conjugate scalar transport with interfacial jump conditions on complex geometries is challenging but important for thermal and chemical processes.

Method: Developed a novel interfacial treatment in volume penalization method (immersed boundary method) for general conjugate scalar transport with both flux and scalar jumps. First proposed treatment for Neumann boundary conditions, then extended to general cases with additional source term representing local jump conditions.

Result: Verified with 1D diffusion problem showing improved accuracy and unified governing equations. Applied to fluid-solid coupled diffusion and advection-diffusion problems with scalar and flux jumps. Results show <3.0% average relative deviation from body-fitted mesh simulations.

Conclusion: The proposed scheme successfully handles conjugate scalar transport with general interfacial jump conditions on complex geometries using immersed boundary methods, achieving good accuracy comparable to body-fitted mesh approaches.

Abstract: Conjugate scalar transport with interfacial jump conditions on complex interfacial geometries is common in thermal and chemical processes, while its accurate and efficient simulations are still quite challenging. In the present study, a novel treatment of a two-phase interface in the volume penalization method, a kind of immersed boundary method, for solving conjugate scalar transport with general interfacial boundary conditions is developed. We first propose an interfacial treatment for solving an advection-diffusion equation with a Neumann boundary condition, and then extend it to general conjugate scalar transport with both interfacial flux and scalar jumps. A one-dimensional diffusion problem is solved to verify the present scheme and demonstrate the advantage of the present scheme in improving accuracy and unifying the governing equations in the two phases with an additional source term representing the local jump condition of the interfacial scalar flux. Then, the present scheme is further applied to fluid-solid coupled scalar diffusion and advection-diffusion problems with the scalar and its flux jumps across the interface. The simulation results of the present scheme generally show good agreement with reference results obtained by body-fitted mesh simulations with average relative deviations less than 3.0%.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [29] [A feasibility study for a Doppler Reflectometer System in the JT-60SA tokamak](https://arxiv.org/abs/2601.09906)
*D. Carralero,T. Happel,T. Estrada,T. Tokuzawa,J. Martínez,E. de la Luna,A. Cappa,J. García*

Main category: physics.plasm-ph

TL;DR: Feasibility study for installing a Doppler reflectometer system in JT-60SA tokamak, showing viable design using minimal port space while achieving scientific objectives.


<details>
  <summary>Details</summary>
Motivation: To support JT-60SA research plan by enabling turbulence and flow measurements in both core and edge plasma regions, which are critical for understanding plasma confinement and stability.

Method: Three-step approach: 1) Identify scientific scope and program for DR in JT-60SA context, 2) Use ray tracing code for feasibility study to determine geometric solution for core and edge probing, 3) Develop conceptual design options (minimum viable and baseline systems) with component and space requirements.

Result: Successfully identified viable geometric solution for DR installation that can probe both core and edge plasma in required wave number range. Conceptual design shows system can be implemented using only a small fraction of a horizontal port, leaving space for other diagnostics.

Conclusion: A Doppler reflectometer system is both viable and practical for JT-60SA, requiring minimal port space while achieving the proposed scientific objectives for turbulence and flow measurements in core and edge plasma regions.

Abstract: In this work we present a study on the viability and practicality of installing a Doppler reflectometer (DR) system in the JT-60SA advanced tokamak. First, we discuss its scientific scope in the context of the JT-60SA research plan. We identify a number of fields in which a DR would be very relevant for the accomplishment of said plan and outline a scientific program for the diagnostic. Then, starting from a number of design hypothesis, we use a ray tracing code to carry out a feasibility study for a number of relevant scenarios and identify a geometric solution for the installation of a DR such that both core and edge can be probed in the prescribed wave number range, thus achieving the proposed scientific objectives. Finally, we perform a preliminary discussion on the different possibilities for a conceptual design (including a minimum viable system and a baseline system) and their requirements in terms of components and space. We conclude that a viable conceptual design could be carried out using a small fraction of a horizontal port, leaving room for additional diagnostic systems.

</details>


### [30] [Effects of parallel magnetic fields on sheaths near biased electrodes in a highly collisional Z-pinch plasma](https://arxiv.org/abs/2601.10039)
*C. R. Skolar,B. Srinivasan*

Main category: physics.plasm-ph

TL;DR: Sheath formation near biased electrodes with parallel magnetic fields in Z-pinch fusion shows non-monotonic potential profiles, classical sheath formation within electron gyroradius, and significant parallel currents despite low perpendicular currents.


<details>
  <summary>Details</summary>
Motivation: Sheath formation near biased electrodes in magnetic fields parallel to the wall is understudied, especially in Z-pinch fusion experiments, where understanding plasma-electrode interactions is crucial.

Method: 1X-2V Boltzmann-Poisson simulations of axial cut at pinch radius between two biased electrodes with parallel magnetic field; artificially increased collision frequencies to enhance thermalization in smaller simulation domain.

Result: Non-monotonic sheath profiles with potential peaks away from wall; classical sheath forms within electron gyroradius due to magnetized electrons gyrating into wall; perpendicular current density three orders lower than unmagnetized predictions; parallel current density three orders larger than perpendicular current due to parallel flows.

Conclusion: Magnetic field acts as high resistivity for perpendicular currents, but significant parallel flows and currents emerge from force balance between pressure tensor and Lorentz force, with sheath structure insensitive to bias potential.

Abstract: Sheath formation near biased electrodes in magnetic fields parallel to the wall is an understudied topic, especially within the context of Z-pinch fusion experiments. We perform 1X-2V Boltzmann-Poisson simulations of an axial cut at the pinch radius of a Z-pinch plasma between two biased electrodes with a magnetic field parallel to the wall. The collision frequencies are artificially increased to enhance thermalization of the plasma in the smaller simulation domain versus the actual experiment size; this increases the perpendicular mobility and partially de-magnetizes the ions resulting in non-monotonic sheath profiles with the potential increasing away from the wall to a peak before decaying. A classical sheath forms within an electron gyroradius from the wall not due to the natural thermal motion of the electrons, but due to the magnetized electrons gyrating into the wall; therefore, the sheath structure does not significantly change with bias potential or between electrodes. With increasing bias potential, a current is induced perpendicular to the wall due to changes in ion flow, differing from unmagnetized cases where current is induced by changes in electron flow. The magnetic field acts as a high resistivity with the perpendicular current density being three orders of magnitude lower than unmagnetized theoretical predictions. There is, however, significant flow parallel to the wall from the force balance between the pressure tensor and Lorentz force. These parallel flows induce a parallel current density three orders of magnitude larger than the perpendicular current density.

</details>


### [31] [Updated electrical design of the Diagnostic Neutral Beam Injector in RFX-mod2](https://arxiv.org/abs/2601.10293)
*Marco Barbisan,Bruno Laterza,Luca Cinnirella,Lionello Marrelli,Federico Molon,Simone Peruzzo,Enrico Zampiva*

Main category: physics.plasm-ph

TL;DR: The paper describes upgrades to the Diagnostic Neutral Beam Injector (DNBI) for the RFX-mod2 fusion experiment, which will enable core plasma measurements via Charge Exchange Recombination Spectroscopy and Motional Stark Effect diagnostics.


<details>
  <summary>Details</summary>
Motivation: The DNBI is crucial for obtaining novel measurements of plasma confinement properties in the Reversed Field Pinch configuration, particularly at the core region where such data has been difficult to obtain.

Method: The DNBI uses a hydrogen ion source with 50 keV acceleration, gas target neutralization, and magnetic deflection of residual ions. The paper focuses on electrical plant and control system upgrades to improve reliability and safety.

Result: The upgraded DNBI system will provide 50 ms, 5 A neutral hydrogen beams with 250 Hz modulation capability, enabling measurements of ion speed, temperature, impurity content, and magnetic field properties in plasma cores.

Conclusion: The DNBI upgrades will significantly enhance diagnostic capabilities for the RFX-mod2 experiment, providing essential data for understanding Reversed Field Pinch plasma confinement and advancing fusion research.

Abstract: The Diagnostic Neutral Beam Injector of the RFX-mod2 experiment (Consorzio RFX, Padova) is expected to provide novel and significant information about the Reversed Field Pinch confinement of fusion plasmas. The injection of the hydrogen beam in the plasma will allow Charge Exchange Recombination Spectroscopy (CXRS) and Motional Stark Effect diagnostics (MSE) to measure several quantities: ion speed, ion temperature, impurity content, intensity and pitch of the magnetic field. The DNBI is of particular importance for allowing the determination of these quantities at the core of the plasma. The present DNBI, built by the Budker Institute of Plasma Physics, features an arc discharge H+ source, coupled to a 4-grid 50 keV acceleration system. The 50 ms, 5 A ion beam is neutralized by charge exchange by means of a gas target; residual ions are then deflected by a magnetic field before injection in the torus chamber. The beam can be modulated at maximum 250 Hz. The DNBI will undergo extraordinary maintenance and a structural upgrade to improve its reliability and safety. This contribution presents the latest upgrades of the electrical plants and of the control system of the DNBI.

</details>


### [32] [From Weibel seeds to collisionless dynamos beyond pair-plasmas](https://arxiv.org/abs/2601.10472)
*Lise Hanebring,James Juno,Ammar Hakim,Jason M. TenBarge,Istvan Pusztai*

Main category: physics.plasm-ph

TL;DR: Simulations of collisionless turbulence capture both magnetic seed field generation via electron Weibel instability and subsequent dynamo amplification in intracluster medium using realistic ion-to-electron mass ratio.


<details>
  <summary>Details</summary>
Motivation: Bridging the spatiotemporal scales between magnetic seed field generation and dynamo amplification in the weakly collisional intracluster medium presents extreme numerical challenges that need to be addressed.

Method: Use collisionless turbulence simulations with initially unmagnetized electrons, employing 10-moment collisionless fluid solver (Gkeyll) that evolves full pressure tensor for all species. Use realistic ion-to-electron mass ratio of 100 to decouple electron and ion dynamics.

Result: Electron heat-flux closure regulates pressure isotropization and effectively sets the magnetic Reynolds number. The study investigates how closure strength influences transition between kinetic pair-plasma regime and more MHD-like dynamo regime.

Conclusion: The approach enables capturing both seed field generation via electron Weibel instability and subsequent dynamo amplification, bridging previously separate scales in intracluster medium studies.

Abstract: Bridging the spatiotemporal scales of magnetic seed field generation and subsequent dynamo amplification in the weakly collisional intracluster medium presents an extreme numerical challenge. We perform collisionless turbulence simulations with initially unmagnetized electrons that capture both magnetic seed generation via the electron Weibel instability and the ensuing dynamo amplification. Going beyond existing pair-plasma studies, we use an ion-to-electron mass ratio of 100 for which we find electron and ion dynamics are sufficiently decoupled. These simulations are enabled by the 10-moment collisionless fluid solver of Gkeyll, which evolves the full pressure tensor for all species. The electron heat-flux closure regulates pressure isotropization and effectively sets the magnetic Reynolds number. We investigate how the strength of of the closure influences the transition between a regime reminiscent of previous kinetic pair-plasma simulations and a more MHD-like dynamo regime.

</details>


### [33] [Reply to "Comment on Nuclear Fusion 66, 016012 (2026) by Richard Fitzpatrick, A Simple Model of Current Ramp-Up and Ramp-Down in Tokamaks" by A.H. Boozer](https://arxiv.org/abs/2601.10509)
*Richard Fitzpatrick*

Main category: physics.plasm-ph

TL;DR: This report addresses comments by Dr. A.H. Boozer on the author's previous paper about tokamak current ramp-up/ramp-down modeling, clarifying and responding to critiques.


<details>
  <summary>Details</summary>
Motivation: To respond to and address specific comments and critiques made by Dr. A.H. Boozer regarding the author's previous work on tokamak current ramp-up and ramp-down modeling, ensuring the validity and clarity of the original research.

Method: The author analyzes and responds to the comments point-by-point, providing clarifications, corrections, and additional explanations where necessary to defend or refine the original model.

Result: The report successfully addresses the critiques, either correcting minor issues, clarifying misunderstandings, or defending the original approach, thereby strengthening the validity of the initial model.

Conclusion: The original model remains valid with some clarifications, and the dialogue with Dr. Boozer contributes to refining the understanding of tokamak current ramp-up/ramp-down dynamics.

Abstract: This report is a follow up to my paper "A simple model of current ramp-up and ramp-down in tokamaks" [Nucl. Fusion 66, 016012 (2026)] in the light of comments on the paper recently made by Dr. A.H. Boozer (arXiv:2601.05977).

</details>


### [34] [Canonical Vorticity Perspective on Magnetogenesis: Unifying Weibel, Biermann, and Beyond](https://arxiv.org/abs/2601.10570)
*Modhuchandra Laishram,Young Dae Yoon*

Main category: physics.plasm-ph

TL;DR: The paper presents a canonical vorticity framework that unifies various magnetogenesis processes in collisionless plasmas, predicting new pressure tensorial configurations as sources of self-generated magnetic fields and vorticity.


<details>
  <summary>Details</summary>
Motivation: To develop a unified theoretical framework for understanding the origin of magnetic fields in the universe (magnetogenesis) that bridges cosmology and plasma physics, addressing how magnetic fields are generated in collisionless plasmas.

Method: Formulates a canonical vorticity framework using canonical vorticity (weighted sum of fluid vorticity and magnetic field) as the canonical variable. Extends the framework to relativistic regime, identifying the "kineclinicity effect" as an additional source. Validates theoretical predictions using particle-in-cell simulations.

Result: The framework unifies several magnetogenesis processes including Biermann battery and Weibel instability, predicts new pressure tensorial configurations as fundamental sources of self-generated magnetic fields and vorticity, and identifies relativistic kineclinicity effects.

Conclusion: The canonical vorticity framework provides a comprehensive approach to study magnetogenesis in collisionless plasmas, with validated predictions that have implications for both laboratory and astrophysical plasma environments.

Abstract: We briefly review the current status of magnetogenesis, a cross-disciplinary field that bridges cosmology and plasma physics, studying the origin of magnetic fields in the universe. We formulate a canonical vorticity framework to investigate kinetic plasma physics-based magnetogenesis processes in a collisionless plasma. By considering canonical vorticity, a weighted sum of the fluid vorticity and the magnetic field as the canonical variable, this framework unifies several magnetogenesis processes, including the Biermann battery, the Weibel instability, and predicts several new pressure tensorial configurations as the fundamental source of self-generated magnetic field and vorticity in plasma. The framework is further extended to relativistic regime where an additional source of canonical vorticity, termed as kineclinicity effect, is identified. The theoretical predictions are systematically validated using particle-in-cell simulations, highlighting their implications for laboratory and astrophysical plasma environments.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [35] [Remarks on the convex integration technique applied to singular stochastic partial differential equations](https://arxiv.org/abs/2601.09990)
*Hongjie Dong,Kazuo Yamazaki*

Main category: math.PR

TL;DR: Review of convex integration as an alternative approach to solving singular SPDEs, with analysis showing its limitations for proving non-uniqueness in the Φ⁴ model.


<details>
  <summary>Details</summary>
Motivation: To explore convex integration as a potential method for constructing solutions to singular stochastic PDEs (SPDEs) with ill-defined nonlinear terms, and to assess its applicability to specific models like the Φ⁴ quantum field theory model.

Method: Review of existing convex integration techniques for singular SPDEs, with specific analysis applied to the Φ⁴ model to evaluate whether convex integration can demonstrate non-uniqueness of solutions.

Result: Convex integration emerges as a viable approach for constructing solutions to some singular SPDEs, but analysis suggests it is unlikely to prove non-uniqueness for the specific Φ⁴ model from quantum field theory.

Conclusion: While convex integration offers a promising alternative to existing theories (regularity structures and paracontrolled distributions) for singular SPDEs, its applicability for proving non-uniqueness appears limited for certain important models like the Φ⁴ equation.

Abstract: Singular stochastic partial differential equations informally refer to the partial differential equations with rough random force that leads to the products in the nonlinear terms becoming ill-defined. Besides the theories of regularity structures and paracontrolled distributions, the technique of convex integration has emerged as a possible approach to construct a solution to such singular stochastic partial differential equations. We review recent developments in this area, and also demonstrate that an application of the convex integration technique to prove non-uniqueness seems unlikely for a particular singular stochastic partial differential equation, specifically the $Φ^{4}$ model from quantum field theory.

</details>


### [36] [Malliavin Calculus for the stochastic Cahn-Hilliard equation driven by fractional noise](https://arxiv.org/abs/2601.10490)
*Dimitrios Dimitriou,Dimitris Farazakis,Georgia Karali*

Main category: math.PR

TL;DR: The paper analyzes the Cahn-Hilliard equation with additive fractional white noise in 1D, proving existence of density for the stochastic solution using Malliavin calculus.


<details>
  <summary>Details</summary>
Motivation: To study the stochastic Cahn-Hilliard equation perturbed by fractional white noise (fractional in time, white in space) and establish properties of its solution, particularly the existence of a density for the stochastic solution.

Method: Uses Malliavin calculus in one spatial dimension. Constructs a localizing sequence to prove local existence of Malliavin derivative, and shows absolute continuity of the law with respect to Lebesgue measure. Derives sharp estimates for expectations of p-th powers of L∞-norms of stochastic integrals and associated operators.

Result: Proves that the stochastic solution u admits continuous paths almost surely, has locally existing Malliavin derivative, and its law is absolutely continuous with respect to Lebesgue measure on ℝ (establishing existence of density).

Conclusion: The stochastic Cahn-Hilliard equation with additive fractional white noise has a solution with continuous paths and admits a density, with key technical contributions in estimating stochastic integrals and associated operators.

Abstract: The stochastic partial differential equation analyzed in this work is the Cahn-Hilliard equation perturbed by an additive fractional white noise (fractional in time and white in space). We work in the case of one spatial dimension and apply Malliavin calculus to investigate the existence of a density for the stochastic solution $u$. In particular, we show that $u$ admits continuous paths almost surely and construct a localizing sequence through which we prove that its Malliavin derivative exists locally, and that its law is absolutely continuous with respect to the Lebesgue measure on $\bf R$, establishing thus that a density exists. A key contribution of this work is the analysis of the stochastic integral appearing in the mild formulation: we derive sharp estimates for the expectation of the $p$-th power ($p \geq 2$) of the $L^{\infty}(D)$-norm of this stochastic integral as well as for the integral involving the $L^{\infty}(D)$-norm of the operator associated with the kernel appearing in the integral representation of the fractional noise, all of which are essential for this study.

</details>


### [37] [Smoothness of martingale observables and generalized Feynman-Kac formulas](https://arxiv.org/abs/2601.10539)
*Alex Karrila,Lauri Viitasaari*

Main category: math.PR

TL;DR: The paper proves smoothness of martingale observables for Itô processes satisfying Hörmander's criterion, leading to a generalized Feynman-Kac formula for degenerate diffusions and applications to SLE.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties of martingale observables in stochastic processes and extend classical results to more general settings including degenerate diffusions and boundary stopping problems.

Method: Uses Hörmander's criterion on Itô processes to prove smoothness of martingale observables, develops a generalized Feynman-Kac formula, and applies Itô calculus to Girsanov transforms.

Result: All martingale observables are smooth under Hörmander's criterion; obtains generalized Feynman-Kac formula for degenerate diffusions with boundary stopping; applies to Schramm-Loewner evolutions.

Conclusion: The Hörmander criterion ensures smoothness of martingale observables, enabling new PDE solution methods and applications to SLE theory through Itô calculus techniques.

Abstract: We prove that, under the Hörmander criterion on an Itô process, all its martingale observables are smooth. As a consequence, we also obtain a generalized Feynman-Kac formula providing smooth solutions to certain PDE boundary-value problems, while allowing for degenerate diffusions as well as boundary stopping (under very mild boundary regularity assumptions). We also highlight an application to a question posed on Schramm-Loewner evolutions, by making certain Girsanov transform martingales accessible via Itô calculus.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [38] [Stable Differentiable Modal Synthesis for Learning Nonlinear Dynamics](https://arxiv.org/abs/2601.10453)
*Victor Zheleznov,Stefan Bilbao,Alec Wright,Simon King*

Main category: cs.SD

TL;DR: Combines scalar auxiliary variable techniques with neural ODEs to create stable differentiable models for learning nonlinear dynamics from data, applied to string vibration synthesis.


<details>
  <summary>Details</summary>
Motivation: Modal methods for physical modelling synthesis need extensions to nonlinear problems like high-amplitude string vibration. While recent SAV techniques enable stable solvers for nonlinear systems, and neural ODEs can learn nonlinear dynamics from data, there's a need to combine these approaches for stable differentiable learning of physical systems.

Method: Proposes combining scalar auxiliary variable (SAV) techniques with neural ordinary differential equations (neural ODEs) to create stable differentiable models. Leverages analytical solution for linear vibration of system's modes so physical parameters remain accessible after training without needing parameter encoders in architecture.

Result: The model can be trained to reproduce nonlinear dynamics of a string system using synthetic data for nonlinear transverse vibration. Sound examples are presented as proof of concept.

Conclusion: Successfully demonstrates that SAV techniques can be combined with neural ODEs to yield stable differentiable models capable of learning nonlinear dynamics while maintaining accessibility to physical parameters after training.

Abstract: Modal methods are a long-standing approach to physical modelling synthesis. Extensions to nonlinear problems are possible, including the case of a high-amplitude vibration of a string. A modal decomposition leads to a densely coupled nonlinear system of ordinary differential equations. Recent work in scalar auxiliary variable techniques has enabled construction of explicit and stable numerical solvers for such classes of nonlinear systems. On the other hand, machine learning approaches (in particular neural ordinary differential equations) have been successful in modelling nonlinear systems automatically from data. In this work, we examine how scalar auxiliary variable techniques can be combined with neural ordinary differential equations to yield a stable differentiable model capable of learning nonlinear dynamics. The proposed approach leverages the analytical solution for linear vibration of system's modes so that physical parameters of a system remain easily accessible after the training without the need for a parameter encoder in the model architecture. As a proof of concept, we generate synthetic data for the nonlinear transverse vibration of a string and show that the model can be trained to reproduce the nonlinear dynamics of the system. Sound examples are presented.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [39] [Unbounded symbols, heat flow, and Toeplitz operators](https://arxiv.org/abs/2601.10711)
*Sam Looi*

Main category: math.FA

TL;DR: The paper disproves the Berger-Coburn heat-flow conjecture extension for Toeplitz operators on Bargmann space, showing that the form-defined operator T_g and natural-domain operator U_g decouple for unbounded symbols, with U_g requiring stronger quadratic control than T_g's linear averaging.


<details>
  <summary>Details</summary>
Motivation: To investigate the natural domain extension of the Berger-Coburn heat-flow conjecture for Toeplitz operators on Bargmann space, specifically understanding when the form-defined operator T_g and natural-domain operator U_g coincide or diverge in their boundedness properties.

Method: The authors analyze the gap between pointwise and uniform control of Gaussian averaging of |g|^2, construct counterexamples with smooth radial symbols satisfying coherent-state admissibility, and prove irreversibility of heat-flow regularity while showing bootstrapping cannot resolve critical time gaps.

Result: Disproved the conjecture by showing T_g and U_g decouple for unbounded symbols: T_g boundedness depends on linear averaging of g, while U_g requires quadratic intensity of |g|^2. Constructed a smooth radial symbol where T_g is bounded but U_g is unbounded, revealing this as a global phenomenon tied to geometry at infinity.

Conclusion: The failure mechanism is a gap between pointwise and uniform Gaussian averaging invisible to T_g. Boundedness of U_g is equivalent to |g|^2 dμ being a Fock-Carleson measure, strictly stronger than the linear condition for T_g. Heat-flow regularity is irreversible, and bootstrapping cannot close the sufficiency-critical time gap.

Abstract: We disprove the natural domain extension of the Berger--Coburn heat-flow conjecture for Toeplitz operators on the Bargmann space and identify the failure mechanism as a gap between pointwise and uniform control of a Gaussian averaging of the squared modulus of the symbol, a gap that is invisible to the linear form $T_g$. We establish that the form-defined operator $T_g$ and the natural-domain operator $U_g$ decouple in the unbounded symbols regime: while $T_g$ is governed by linear averaging, $U_g$ is controlled by the quadratic intensity of $|g|^2$. We construct a smooth, nonnegative radial symbol $g$ satisfying the coherent-state admissibility hypothesis with bounded heat transforms for all time $t>0$; for this symbol, $T_g$ is bounded, yet $U_g$ is unbounded. This is a strictly global phenomenon: under the coherent-state hypothesis, local singularities are insufficient to cause unboundedness, leaving the ``geometry at infinity'' as the sole obstruction. Boundedness of $U_g$ is equivalent to the condition that $|g|^2 dμ$ is a Fock--Carleson measure, a condition strictly stronger than the linear average $g dμ$ governing $T_g$. Finally, regarding the gap between the known sub-critical sufficiency condition and the critical heat time, we prove that heat-flow regularity is irreversible in this context and show that bootstrapping strategies cannot resolve the gap between sufficiency and critical time.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [40] [Electronic structure theory of H$_{3}$S: Plane-wave-like valence states, density-of-states peak and its guaranteed proximity to the Fermi level](https://arxiv.org/abs/2601.10016)
*Ryosuke Akashi*

Main category: cond-mat.supr-con

TL;DR: The paper explains the mechanism behind the peaked electronic density of states in H₃S superconductors under extreme pressure, revealing it's caused by hybridization of specific plane waves due to Jones' large zone adjacency to the Fermi surface.


<details>
  <summary>Details</summary>
Motivation: While superconductivity in H₃S under extreme pressure has been theoretically explained, the mechanism behind the crucial peaked electronic density of states (DOS) that enables high transition temperatures has remained unclear and needed investigation.

Method: The researchers performed detailed analysis of first-principles electronic wave functions, showing they are significantly plane-wave-like. They used Fourier-mode analysis of self-consistent potentials and atomic pseudopotentials to extract nearly uniform models that accurately reproduce the band structure with few parameters.

Result: The DOS peak is shown to result from hybridization of specific plane waves. The adjacency of Jones' large zone to the plane-wave spherical Fermi surface is identified as the root cause of multiple plane-wave hybridization, DOS peak formation, and its proximity to the Fermi level.

Conclusion: The theory resolves the minimal modeling problem of electronic states in H₃S and establishes a mechanism that may help boost transition temperatures in pressure-induced superconductors by explaining the DOS peak formation.

Abstract: Superconductivity in sulfur superhydride H$_{3}$S under extreme pressures has been explained theoretically, but it requires a peaked concentration of the electronic density of states (DOS), which has been found in first-principles calculations. The mechanism of this peak formation, though vital for its high transition temperature, has however remained obscure. We address this problem through detailed analysis of the first-principles electronic wave functions. The valence wave functions are shown to be significantly plane-wave-like. From the Fourier-mode analysis of the self-consistent potential and atomic pseudopotentials, we extract the nearly uniform models that accurately reproduce the first-principles band structure with very few parameters. The DOS peak is shown to be the consequence of the hybridization of specific plane waves. Adjacency of Jones' large zone to the plane-wave spherical Fermi surface is posited to be the root cause of the multiple plane-wave hybridization, the DOS peak formation and its proximity to the Fermi level. The present theory resolves the minimal modeling problem of electronic states in H$_{3}$S, as well as establishes a mechanism that may help to boost the transition temperatures in pressure induced superconductors.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [41] [Variable coherence model for free-electron laser pulses](https://arxiv.org/abs/2601.09885)
*Austin Bartunek,Nils H. Sommerfeld,Francois Mauger*

Main category: physics.optics

TL;DR: The Variable Coherence Model (VCM) simulates FEL pulses with adjustable coherence width, enabling continuous control over pulse noise while maintaining fixed average parameters like bandwidth.


<details>
  <summary>Details</summary>
Motivation: To develop a more flexible model for simulating free-electron laser pulses that allows continuous control over coherence properties, building on existing partial coherence models to better represent the range of possible FEL pulse characteristics.

Method: Extends the partial coherence model by implementing a variable coherence width parameter, enabling systematic statistical analysis of intensity and sub-pulse distributions in both time and frequency domains across different FEL parameter regimes.

Result: The VCM successfully generates pulses ranging from maximally random to fully coherent by adjusting the coherence width parameter, with demonstrated effects on sub-pulse statistics and absorption simulations.

Conclusion: The Variable Coherence Model provides a versatile framework for simulating FEL pulses with tunable coherence properties, offering researchers a tool to explore how coherence affects pulse characteristics and experimental outcomes.

Abstract: We introduce the variable coherence model (VCM) for simulating free-electron laser (FEL) pulses generated through self-amplified spontaneous emission. Building on the established partial coherence model of [T. Pfeifer et. al, Opt. Lett. 35, 3441 (2010)], we demonstrate that the implementation of a variable coherence width allows for continuous control over the pulses' characteristic noise, while keeping the average pulse parameters such as the bandwidth fixed. We demonstrate this through systematic statistical analyses of the intensity and number of sub-pulses in VCM pulses, in both time and frequency. In particular, we analyze how the sub-pulse statistics are affected by the coherence width parameter. We perform our analyses across three distinct regimes of FEL parameters and demonstrate how the VCM can generate pulses that range from maximally random to fully coherent. Finally, we illustrate the effect of the VCM variable coherence width on an absorption simulation.

</details>


### [42] [Near-Unity-Efficiency Gas Gratings for Ultraviolet, Visible, and Infrared High-Power Lasers](https://arxiv.org/abs/2601.09963)
*Ke Ou,Harsha Rajesh,Sida Cao,Debolina Chakraborty,Victor M. Perez-Ramirez,Devdigvijay Singh,Caleb Redshaw,Pelin Dedeler,Albertine Oudin,Eugene Kur,Michelle M. Wang,Julia M. Mikhailova,Livia Lancia,Caterina Riconda,Pierre Michel,Matthew R. Edwards*

Main category: physics.optics

TL;DR: Gas gratings created by DUV laser interference in ozone-doped gas flows achieve up to 99% diffraction efficiency across broad wavelength and pulse duration ranges, with high damage thresholds and stable operation.


<details>
  <summary>Details</summary>
Motivation: Need for optics that can withstand high-energy laser manipulation without damage from debris, as conventional solid optics have limited damage thresholds.

Method: Using interfering DUV lasers to induce density modulations in ozone-doped gas flows via photochemical reactions, creating transient volume diffraction gratings. Characterized performance under various conditions including imprint fluence, gas composition, and grating geometries.

Result: Achieved up to 99% full beam diffraction efficiency across DUV to near-IR wavelengths and nanosecond to femtosecond pulse durations while preserving focusability and wavefront quality. Performance enhanced with carbon dioxide addition and demonstrated stable operation over hours.

Conclusion: Gas gratings provide promising method for efficiently manipulating high-energy lasers with superior damage thresholds, validated theoretical model, and identified optimal parameters for scaling to high-energy applications.

Abstract: Interfering deep ultraviolet (DUV) lasers can induce substantial density modulations in an ozone-doped gas flow via photochemical reactions, creating volume diffraction gratings. These transient optics are immune to target debris and shrapnel and feature orders-of-magnitude higher damage thresholds than conventional solid optics, providing a promising method for efficiently manipulating high-energy lasers. In this work, we describe gas gratings that can efficiently diffract probe beams across a variety of wavelengths and pulse durations, ranging from deep ultraviolet to near-infrared and from nanosecond to femtosecond, achieving a full beam diffraction efficiency up to 99% while preserving the focusability and wavefront quality. In addition, we present a comprehensive characterization of the performance of the gas gratings under various experimental conditions, including imprint fluence, gas composition, and grating geometries, showing significant enhancement of this process with the addition of carbon dioxide. We also demonstrate stable performance over hours of operation. Our results validate a previously developed theoretical model and suggest optimal parameters to efficiently scale gas gratings to high-energy applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation](https://arxiv.org/abs/2601.10577)
*Serena Grazia De Benedictis,Amedeo Altavilla,Nicoletta Del Buono*

Main category: cs.CV

TL;DR: The paper introduces a topology-aware segmentation evaluation framework based on the Jordan Curve Theorem for digital images, defining "Jordan-segmentatable masks" that ensure proper topological separation of interior and exterior regions.


<details>
  <summary>Details</summary>
Motivation: Current segmentation evaluation metrics (pixel-wise, region-based, boundary-focused) fail to capture structural and topological coherence. They can give high scores to masks with small boundary inaccuracies, holes, or fragmentation that don't preserve object shape or connectivity, which is critical in applications like medical imaging.

Method: The authors define Jordan-segmentatable masks using digital topology and homology theory. They extract a 4-curve candidate from masks and verify topological validity using Betti numbers (β₀ = β₁ = 1). A mask is Jordan-segmentatable when its complement splits into exactly two 8-connected components, ensuring proper topological separation.

Result: The framework provides a mathematically rigorous, unsupervised criterion for assessing segmentation structural coherence. It offers an alternative to conventional metrics by focusing on topological correctness rather than just pixel-level accuracy.

Conclusion: This topology-aware approach based on digital Jordan theory and homological invariants provides valuable evaluation for applications where preserving topological correctness is essential, addressing limitations of traditional segmentation metrics.

Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $β_0 = β_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [44] [On gradient stability in nonlinear PDE models and inference in interacting particle systems](https://arxiv.org/abs/2601.10326)
*Aurélien Castre,Richard Nickl*

Main category: math.ST

TL;DR: The paper presents a Banach space implicit function theorem approach to verify gradient stability for non-linear PDE inverse problems, with applications to reaction-diffusion systems and McKean-Vlasov models, leading to statistical identifiability and polynomial-time convergence of Langevin algorithms for posterior sampling.


<details>
  <summary>Details</summary>
Motivation: To develop rigorous mathematical tools for analyzing non-linear PDE inverse problems, specifically to verify gradient stability conditions needed for statistical inference and to establish identifiability results for parameter estimation in complex systems.

Method: Uses a Banach space version of the implicit function theorem to analyze parameter-to-solution maps θ ↦ G(θ) of non-linear PDEs, verifying the gradient stability condition of Nickl & Wang (2024) and providing injectivity estimates.

Result: Establishes statistical identifiability results for non-linear inverse problems, demonstrates the approach on reaction-diffusion systems and McKean-Vlasov models, and proves polynomial-time convergence of Langevin-type algorithms for posterior sampling of interaction potentials.

Conclusion: The Banach space implicit function theorem provides a powerful framework for analyzing gradient stability in non-linear PDE inverse problems, enabling rigorous statistical inference and efficient computational algorithms for parameter estimation in complex systems.

Abstract: We consider general parameter to solution maps $θ\mapsto \mathcal G(θ)$ of non-linear partial differential equations and describe an approach based on a Banach space version of the implicit function theorem to verify the gradient stability condition of Nickl&Wang (JEMS 2024) for the underlying non-linear inverse problem, providing also injectivity estimates and corresponding statistical identifiability results. We illustrate our methods in two examples involving a non-linear reaction diffusion system as well as a McKean--Vlasov interacting particle model, both with periodic boundary conditions. We apply our results to prove the polynomial time convergence of a Langevin-type algorithm sampling the posterior measure of the interaction potential arising from a discrete aggregate measurement of the interacting particle system.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [45] [A universal Bochner formula for scalar curvature](https://arxiv.org/abs/2601.10618)
*Sven Hirsch*

Main category: math.DG

TL;DR: The paper introduces a universal Bochner formula for scalar curvature that unifies several known geometric formulas and inequalities.


<details>
  <summary>Details</summary>
Motivation: To develop a unifying framework that connects various geometric formulas involving scalar curvature, including stability inequalities, Schrödinger-Lichnerowicz formulas, and Stern's level-set identities, into a single universal formula.

Method: The authors develop a universal Bochner formula for scalar curvature that serves as a master equation from which multiple known results can be derived as special cases.

Result: The paper establishes a universal formula that contains: 1) stability inequality for minimal slicings, 2) Schrödinger-Lichnerowicz-type formula, and 3) higher-dimensional version of Stern's level-set identity as special cases.

Conclusion: The universal Bochner formula provides a unifying framework that connects various important results in geometric analysis and offers new insights into the relationships between different scalar curvature formulas.

Abstract: We introduce a universal Bochner formula for scalar curvature that contains, as special cases, the stability inequality for minimal slicings, a Schrödinger-Lichnerowicz-type formula, and a higher-dimensional version of Stern's level-set identity.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [46] [Design, Fabrication and Testing of a D-Shaped High Temperature Superconducting Magnet](https://arxiv.org/abs/2601.10295)
*Upendra Prasad,Mahesh Ghate,Piyush Raj,Deven Kanabar,Pankaj Varmora,Swati Roy,Arun Panchal,Dhaval Bhavsar,Anees Bano,Nitish Kumar,Bhadresh Parghi,Akhilesh Yadav,Mohd. Umer,Vijay Vasava,Raton Mandal,Rajkumar Ahirwar,Megha Thaker*

Main category: physics.acc-ph

TL;DR: Design and fabrication of a compact D-shaped superconducting magnet using REBCO HTS tapes for tokamak applications, achieving 0.23 T on-axis toroidal field.


<details>
  <summary>Details</summary>
Motivation: High-temperature superconductors enable compact, high-field tokamak magnets for higher fusion power, requiring development of practical superconducting magnet systems.

Method: Developed toroidal configuration with 0.42 m major radius using eight D-shaped, four poloidal field, and central solenoid HTS magnets; fabricated long HTS cable, winding pack, and integrated with cryogenic casing and vacuum enclosure.

Result: Successfully designed and fabricated compact D-shaped coil system with winding pack terminations, joints, power supply interfacing, and conducted performance testing.

Conclusion: Demonstrated feasibility of fabricating D-shaped coils using stacked HTS cable for compact tokamak magnet applications, advancing HTS magnet technology for fusion research.

Abstract: High-temperature technical superconductors are potential candidates for compact and high-field tokamak magnets. The demand for higher fusion power can be met with an on-axis high magnetic field due to toroidal magnets. An R&D activity has been initiated at the Institute for Plasma Research, India, to develop a compact D-shaped superconducting magnet utilizing REBCO high-temperature superconducting tapes. Under this initiative, a toroidal configuration with a major radius of 0.42 m, consisting of eight D-shaped, four poloidal field, and a central solenoid high-temperature superconducting magnets producing an on-axis toroidal magnetic field of 0.23 T has been conceptualized. The fabrication feasibility of a D-shaped coil for this toroidal configuration also envisaged using stacked high-temperature superconducting cable. In this paper, we report the design of a compact D-shaped coil, the fabrication of a long length HTS cable, a winding pack, and its integration with a cryogenic casing and vacuum enclosure. The winding pack terminations, joints, its interfacing with the power supply, and performance testing are also reported in this paper.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [47] [Volume penalization method for simulating flows around a rotating solid with multiple reference frame and sliding mesh](https://arxiv.org/abs/2601.10230)
*Ming Liu,Yosuke Hasegawa*

Main category: physics.flu-dyn

TL;DR: Combines volume penalization method with MRF and sliding mesh techniques to simulate flows around rotating solids with complex geometry on Cartesian grids.


<details>
  <summary>Details</summary>
Motivation: Turbomachinery simulation is challenging for rotating solids with complex geometry; existing methods need improvement for accurate flow simulation around rotating objects.

Method: Develops immersed-boundary approaches by combining volume penalization method (VPM) with multiple reference frame (MRF) and sliding mesh (SLM) techniques. Uses level-set function for geometry representation on Cartesian grids and proposes unified governing equations with body-forcing terms.

Result: Simulations of rotating cuboid show relative deviations of pressure drop and torque predictions around 5% compared to body-fitted method, validating the VPM approach.

Conclusion: The proposed VPM combined with MRF/SLM provides valid immersed-boundary approaches for simulating flows around rotating solids with complex geometry, achieving good agreement with traditional body-fitted methods.

Abstract: Despite the significant role of turbomachinery in fluid-based energy transfer, precise simulation of rotating solid objects with complex geometry is a challenging task. In the present study, the volume penalization method (VPM) is combined with multiple reference frame (MRF) and sliding mesh (SLM), respectively, so as to develop immersed-boundary approaches for simulating flows around a rotating solid. The level-set function is adopted to represent arbitrary geometries embedded in Cartesian grids. The VPM body-forcing terms in the momentum equation are proposed for MRF and SLM, respectively, so as to build unified governing equations for both fluid and solid regions. The flows around a rotating cuboid under various rotating speeds are simulated by the present schemes, namely, VPM with MRF, and VPM with SLM, and compared to corresponding simulations by the body-fitted method (BFM). The results suggest the relative deviations of predicted pressure drop and torque between the present VPM and BFM are around 5%, demonstrating the validity of the present VPM.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: The paper introduces in-context operator learning for optimal transport, enabling learning of a single solution operator that maps distribution pairs to OT maps using few-shot samples as prompts without gradient updates at inference.


<details>
  <summary>Details</summary>
Motivation: To develop a framework that can solve optimal transport problems in-context using only few-shot samples from distributions, eliminating the need for gradient updates during inference and enabling efficient OT map computation.

Method: Parameterize solution operator and develop scaling-law theory in two regimes: nonparametric setting (tasks on low-dimensional manifolds) with generalization bounds, and parametric setting (Gaussian families) with explicit architecture that recovers exact OT maps.

Result: Established generalization bounds quantifying in-context accuracy scaling with prompt size, intrinsic task dimension, and model capacity; provided explicit architecture for parametric cases; validated framework on synthetic transports and generative-modeling benchmarks.

Conclusion: The framework successfully enables in-context learning of optimal transport operators using few-shot samples, with theoretical guarantees in both nonparametric and parametric settings, validated by experimental results.

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [49] [Discrete versus continuous -- lattice models and their exact continuous counterparts](https://arxiv.org/abs/2601.10184)
*Lorenzo Fusi,Oliver Křenek,Vít Průša,Casey Rodriguez,Rebecca Tozzi,Martin Vejvoda*

Main category: physics.class-ph

TL;DR: The paper establishes a systematic correspondence between discrete lattice models of interacting particles and continuous PDE models using Fourier analysis, focusing on dispersion relations across different boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between discrete lattice/chain models of interacting particles and their continuous PDE counterparts, providing a rigorous mathematical framework for understanding their correspondence and relationships.

Method: Systematic specialization of Fourier analysis tools from continuous to discrete settings and vice versa, examining nearest-neighbor and multiple-neighbor interaction lattice models across infinite, periodic, and finite lattices with various boundary conditions.

Result: Establishes correspondence between discrete and continuous models primarily through dispersion relations, demonstrating how Fourier analysis tools can be systematically adapted across different lattice configurations and boundary conditions.

Conclusion: Provides a comprehensive framework for understanding the mathematical correspondence between discrete lattice models and continuous PDE models, with Fourier analysis serving as the unifying tool for analyzing dispersion relations across different system configurations.

Abstract: We review and study the correspondence between discrete lattice/chain models of interacting particles and their continuous counterparts represented by partial differential equations. We study the correspondence problem for nearest neighbour interaction lattice models as well as for multiple-neighbour interaction lattice models, and we gradually proceed from infinite lattices to periodic lattices and finally to finite lattices with fixed ends/zero Dirichlet boundary conditions. The whole study is framed as systematic specialisation of Fourier analysis tools from the continuous to the discrete setting and vice versa, and the correspondence between the discrete and continuous models is examined primarily with regard to the dispersion relation.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [50] [The transformation mechanisms among cuboctahedra, Ino's decahedra and icosahedra structures of magic-size gold nanoclusters](https://arxiv.org/abs/2601.10434)
*Ehsan Rahmatizad Khajehpasha,Mohammad Ismaeil Safa,Nasrin Eyvazi,Marco Krummenacher,Stefan Goedecker*

Main category: cond-mat.mtrl-sci

TL;DR: Machine learning potential reveals new transformation pathways and lower-energy structures for gold nanoclusters, challenging previous assumptions about their shapes and transformation mechanisms.


<details>
  <summary>Details</summary>
Motivation: Gold nanoclusters have competing structural motifs with small energy differences, leading to structural coexistence and interconversion. Previous studies may have missed physically relevant transformation pathways and lower-energy structures.

Method: Used high-accuracy machine learned potential trained on ~20,000 DFT reference data points to investigate transformation pathways for Au55, Au147, Au309, and Au561 nanoclusters. Conducted saddle point searches and Minima Hopping sampling to identify transformation barriers and low-energy structures.

Result: Identified new transformation pathways: high-symmetry transformations proceed through single barriers with soft-mode-driven motions, while lower-barrier asymmetric pathways lead to disordered amorphous icosahedra. Found new global minima for Au309 and Au561 that are up to 2.8 eV lower than previously reported. Transformation times from these pathways align better with experimental observations.

Conclusion: Previous studies investigated incorrect shapes and transformation pathways. The newly identified pathways and lower-energy structures are physically relevant and provide transformation times consistent with experiments, fundamentally changing understanding of gold nanocluster transformations.

Abstract: Gold nanoclusters possess multiple competing structural motifs with small energy differences, enabling structural coexistence and interconversion. Using a high-accuracy machine learned potential trained on some 20'000 density functional theory reference data points, we investigate transformation pathways connecting both high-symmetry and amorphous cuboctahedra, Ino's decahedra and icosahedra for Au55, Au147, Au309 and Au561 nanoclusters. Our saddle point searches reveal that high-symmetry transformations from cuboctahedra and Ino's decahedra to icosahedra proceed through a single barrier and represent soft-mode-driven jitterbug-type and slip-dislocation motions. In addition, we identify lower-barrier asymmetric transformation pathways that drive the system into disordered, Jahn-Teller-stabilized amorphous icosahedra. Minima Hopping sampling further uncovers, in this context, many such low-symmetry minima. Some of the newly identified global minima for Au309 and Au561 have energies that are up to 2.8 eV lower than the previously reported global minima. Hence, both the shapes and the transformation pathways studied in previous investigations are not the physically relevant ones. In contrast to the previously studied pathways, our transformation pathways give reasonable transformation times that are in rough agreement with experiments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [51] [Computing Statistical Properties of Velocity Fields on Current Quantum Hardware](https://arxiv.org/abs/2601.10166)
*Miriam Goldack,Yosi Atia,Ori Alberton,Karl Jansen*

Main category: quant-ph

TL;DR: Quantum algorithms for CFD can efficiently extract statistical properties of velocity fields directly from quantum circuits without full state tomography, demonstrated on IBM quantum hardware.


<details>
  <summary>Details</summary>
Motivation: Quantum CFD offers exponential scaling advantages for spatial representation, but efficient readout of simulation results remains a key challenge that has received limited attention in literature.

Method: Develop methods to extract statistical properties (central moments and structure functions) directly from parameterized ansatz circuits, avoiding full quantum state tomography. Implement for 1D velocity fields with 16 spatial points encoded in 4 qubits.

Result: Demonstrated high accuracy computations on current quantum devices (IBMQ's Heron2 system ibm_fez) using Qedma's error mitigation software QESEM, analyzing both sine wave signals and Burgers' equation snapshots.

Conclusion: The approach enables practical extraction of CFD statistical properties from quantum simulations, making quantum CFD more feasible on current quantum hardware with error mitigation.

Abstract: Quantum algorithms are gaining attention in Computational Fluid Dynamics (CFD) for their favorable scaling, as encoding physical fields into quantum probability amplitudes enables representation of two to the power of n spatial points with only n qubits. A key challenge in Quantum CFD is the efficient readout of simulation results, a topic that has received limited attention in literature. This work presents methods to extract statistical properties of spatial velocity fields, such as central moments and structure functions, directly from parameterized ansatz circuits, avoiding full quantum state tomography. As a proof of concept, we implement our approach for 1D velocity fields, encoding 16 spatial points with 4 qubits, and analyze both a sine wave signal and four snapshots from Burgers' equation evolution. Using Qedma's error mitigation software QESEM, we demonstrate that such computations achieve high accuracy on current quantum devices, specifically IBMQ's Heron2 system ibm_fez.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [52] [Shallow-KAN Based Solution of Moving Boundary PDEs](https://arxiv.org/abs/2601.09818)
*Tarus Pande,V M S K Minnikanti,Shyamprasad Karagadde*

Main category: math-ph

TL;DR: Shallow KAN framework solves moving boundary PDEs (Stefan problems) using physics-informed residuals and interface-focused collocation resampling, achieving accurate temperature and interface reconstructions without measurement data.


<details>
  <summary>Details</summary>
Motivation: To develop a compact and efficient alternative to MLP-based approaches for solving moving boundary PDEs, leveraging KANs' smaller architecture requirements while maintaining expressive power through spline-based activations.

Method: Proposes a shallow KAN framework that directly approximates temperature distribution and moving interface, enforcing governing PDEs, phase equilibrium, and Stefan condition through physics-informed residuals. Uses interface-focused collocation resampling for enhanced accuracy. Validated with semi-infinite analytical solutions and extended to 2D using level-set based formulation solved within KAN framework.

Result: Numerical experiments in 1D and 2D show accurate reconstructions of both temperature fields and interface dynamics. The framework successfully solves complex moving boundary problems without needing measurement data.

Conclusion: KANs demonstrate potential as a compact and efficient alternative for moving boundary PDEs, capable of solving complex problems without measurement data through physics-informed learning and interface-focused techniques.

Abstract: Kolmogorov-Arnold Networks (KANs) require significantly smaller architectures compared to multilayer perceptron (MLP)-based approaches, while retaining expressive power through spline-based activations. We propose a shallow KAN framework that directly approximates the temperature distribution T(x,t) and the moving interface $Γ(t)$, enforcing the governing PDEs, phase equilibrium, and Stefan condition through physics-informed residuals. To enhance accuracy, we employ interface-focused collocation resampling. Numerical experiments in one and two dimensions show that the framework achieves accurate reconstructions of both temperature fields and interface dynamics, highlighting the potential of KANs as a compact and efficient alternative for moving boundary PDEs. First, we validate the model with semi-infinite analytical solutions. Subsequently, the model is extended to 2D using a level-set based formulation for interface propagation, which is solved within the KAN framework. This work demonstrates that KANs are capable of solving complex moving boundary problems without the need for measurement data.

</details>
