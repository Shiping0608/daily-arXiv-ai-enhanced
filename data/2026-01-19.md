<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 19]
- [math.AP](#math.AP) [Total: 10]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [nlin.AO](#nlin.AO) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [High-Order Lie Derivatives from Taylor Series in the ADTAYL Package](https://arxiv.org/abs/2601.10828)
*Nedialko S. Nedialkov,John D. Pryce*

Main category: math.NA

TL;DR: Numerical method for computing high-order Lie derivatives using MATLAB ADTAYL package, achieving orders of magnitude speedup over symbolic evaluation.


<details>
  <summary>Details</summary>
Motivation: High-order Lie derivatives are computationally expensive when evaluated symbolically, especially as order increases. There's a need for more efficient numerical approaches to compute these derivatives for nonlinear systems analysis.

Method: Uses MATLAB ADTAYL package to compute Lie derivatives numerically by exploiting Röbenack's observation that these derivatives coincide (up to factorial scaling) with Taylor coefficients of expressions built from Taylor expansions about trajectory points and associated variational matrices.

Result: Demonstrates orders of magnitude speedups over symbolic evaluation using MATLAB Symbolic Math Toolbox, tested on a gantry crane model.

Conclusion: The presented numerical approach provides a compact and efficient alternative to symbolic computation of high-order Lie derivatives, enabling faster nonlinear systems analysis.

Abstract: High-order Lie derivatives are essential in nonlinear systems analysis. If done symbolically, their evaluation becomes increasingly expensive as the order increases. We present a compact and efficient numerical approach for computing Lie derivatives of scalar, vector, and covector fields using the MATLAB ADTAYL package. The method exploits a fact noted by Röbenack: that these derivatives coincide, up to factorial scaling, with the Taylor coefficients of expressions built from a Taylor expansion about a trajectory point and, when required, the associated variational matrix. Computational results for a gantry crane model demonstrate orders of magnitude speedups over symbolic evaluation using the MATLAB Symbolic Math Toolbox.

</details>


### [2] [Qualitative reconstruction methods for imaging interior Robin interfaces in EIT from Robin-to-Dirichlet data](https://arxiv.org/abs/2601.10839)
*Rafael Ceja Ayala,Malena I. Español,Govanni Granados*

Main category: math.NA

TL;DR: This paper develops qualitative reconstruction methods for EIT inverse shape problems using Robin transmission conditions and the RtD map as data, enabling identification of interior defects.


<details>
  <summary>Details</summary>
Motivation: The paper addresses an inverse shape problem in electrical impedance tomography for nondestructive testing, where interior defects are modeled using Robin transmission conditions. The motivation is to develop effective reconstruction methods for identifying interior regions when using Robin boundary conditions on both exterior and interior surfaces with the Robin-to-Dirichlet map as measurement data.

Method: The authors develop qualitative (non-iterative) reconstruction methods based on the Linear Sampling Method (LSM) and the Regularized Factorization Method (RFM). They derive new analytical characterizations that enable these methods to identify interior regions. The paper also proposes a numerical implementation incorporating regularization strategies.

Result: Through experiments, the methods reliably reconstruct interior regions of interest. The proposed approach successfully identifies interior defects using the Robin-to-Dirichlet map data within the Robin transmission condition framework.

Conclusion: The paper presents effective qualitative reconstruction methods for EIT inverse shape problems with Robin transmission conditions, demonstrating that both LSM and RFM approaches can successfully identify interior regions using RtD map data with proper regularization.

Abstract: We consider an inverse shape problem arising in electrical impedance tomography (EIT) for nondestructive testing, in which interior defects are modeled through Robin transmission conditions. Unlike classical formulations, we impose Robin boundary conditions on both the exterior measurement surface and the interior interface, and use the Robin-to-Dirichlet (RtD) map as the available data. Within this setting, we develop qualitative (non-iterative) reconstruction methods based on the Linear Sampling Method (LSM) and the Regularized Factorization Method (RFM), and derive new analytical characterizations that enable these methods to identify interior regions. We further propose a numerical implementation that incorporates regularization strategies and demonstrate, through experiments, that the methods reliably reconstruct interior regions of interest.

</details>


### [3] [A Structure-Preserving Scheme for the Time-Dependent Ginzburg-Landau Model with BCS Gap Coupling](https://arxiv.org/abs/2601.10887)
*Boyi Wang,Saurav Shenoy,Daniel Fortino,Long-Qing Chen,Wenrui Hao*

Main category: math.NA

TL;DR: A structure-preserving numerical scheme for a hybrid model coupling TDGL vortex dynamics with BCS gap equation, enabling stable simulations of superconducting vortex formation across temperature regimes.


<details>
  <summary>Details</summary>
Motivation: The classical TDGL equation is limited to near-critical temperatures, while the BCS theory describes superconductivity more broadly. There's a need for a model that extends TDGL applicability beyond critical temperature while maintaining physical consistency, and computational challenges arise from the nonlinear coupled structure.

Method: Developed a hybrid model coupling TDGL equation for vortex dynamics with nonlinear BCS gap equation. Created a maximum bound preserving, energy-stable implicit-explicit (IMEX) scheme with rigorous structure-preserving properties for stable simulations.

Result: The scheme achieves long-time stability and physical consistency. 2D and 3D simulations successfully capture temporal/spatial vortex formation, alignment, and superconductivity suppression under increasing magnetic fields, demonstrating accuracy and robustness.

Conclusion: The proposed hybrid model and structure-preserving IMEX scheme effectively extend TDGL applicability beyond critical temperature while maintaining physical consistency, providing a reliable computational approach for studying superconducting vortex dynamics across temperature regimes.

Abstract: We propose a structure-preserving scheme for a hybrid model that couples the time-dependent Ginzburg-Landau (TDGL) equation of superconducting vortex dynamics and the nonlinear Bardeen-Cooper-Schrieffer (BCS) gap equation. This formulation is consistent with the classical TDGL equation in the near-critical temperature, while extending the applicability of the existing TDGL model to regimes beyond the critical temperature. The resulting system poses significant computational challenges due to its nonlinear and coupled structure. To achieve stable and reliable simulations of the vortex dynamics and accompanying morphological transitions, we develop a maximum bound preserving, energy-stable implicit-explicit (IMEX) scheme. The structure-preserving properties of the scheme are rigorously established, ensuring long-time stability and physical consistency. Through two- and three-dimensional simulations, the hybrid model successfully captures the temporal and spatial formation and alignment of vortices and the suppression of superconductivity under increasing magnetic fields, demonstrating both the accuracy and robustness of the proposed computational approach.

</details>


### [4] [A Non-compact Positivity-Preserving Scheme for Parabolic PDE via Conditional Expectation](https://arxiv.org/abs/2601.10977)
*Haoran Xu,Jie Ren,Xingye Yue*

Main category: math.NA

TL;DR: A novel non-compact, positivity-preserving scheme for linear non-divergence form parabolic equations using Feynman-Kac formula and wide stencil approximations, with robust boundary condition treatments achieving various convergence rates.


<details>
  <summary>Details</summary>
Motivation: Classical schemes often fail for anisotropic diffusion with mixed derivatives unless the covariance matrix is diagonally dominated. Existing methods like BZ and semi-Lagrangian schemes suffer from accuracy loss at boundaries. There's a need for positivity-preserving schemes that handle complex boundary conditions effectively.

Method: Based on Feynman-Kac formula, expresses solution as conditional expectation of associated diffusion process. Uses wide stencil scheme instead of compact Markov chain approximations to approximate conditional expectation. Introduces specialized boundary treatments: quad-tree schemes for Dirichlet boundaries (non-uniform and uniform stopping time), discrete specular reflection for Neumann boundaries, and modular wrapping for periodic boundaries.

Result: Schemes are unconditionally stable and positivity preserving due to probabilistic structure. Achieves O(Δt^{1/2}) accuracy for Dirichlet (non-uniform) and Neumann boundaries, O(Δt) for Dirichlet (uniform) and periodic boundaries under practical scaling Δt ∼ h. Numerical experiments confirm predicted L^∞ convergence rates for all boundary types.

Conclusion: The proposed non-compact, positivity-preserving scheme effectively handles anisotropic diffusion with mixed derivatives and provides robust boundary condition treatments with proven convergence rates, addressing limitations of classical methods.

Abstract: We propose a novel non-compact, positivity-preserving scheme for linear non-divergence form parabolic equations. Based on the Feynman-Kac formula, the solution is expressed as a conditional expectation of an associated diffusion process. Instead of using compact Markov chain approximations, we employ a wide stencil scheme to approximate the conditional expectation, ensuring consistency and positivity preservation. This method is effective for anisotropic diffusion with mixed derivatives, where classical schemes often fail unless the covariance matrix is diagonally dominated.
  A key feature of our framework is its robust treatment of boundary conditions, which avoids the accuracy loss commonly encountered in BZ and semi-Lagrangian schemes. For Dirichlet boundaries, we introduce (i) a quad-tree non-uniform stopping time scheme with O($Δt^{1/2}$) accuracy and (ii) a quad-tree uniform stopping time scheme with O($Δt$) accuracy. For Neumann boundaries, we use discrete specular reflection with O($Δt^{1/2}$) convergence, while periodic boundaries are treated using modular wrapping, achieving O($Δt$) accuracy. All analyses are conducted under the practical scaling $Δt \sim h$.
  Except for the uniform stopping time scheme, all schemes are explicit. The schemes are unconditionally stable and positive preserving, thanks to the probabilistic structure. To ensure consistency, a non-compact stencil is involved, which leads to the large time step constraint $Δt \sim h$. Numerical experiments confirm the predicted $L^\infty$ convergence rates for all types of boundary conditions.

</details>


### [5] [A model order reduction based adaptive parareal method for time-dependent partial differential equations](https://arxiv.org/abs/2601.10981)
*Xiaoying Dai,Miao Hu,Shuwei Shen*

Main category: math.NA

TL;DR: Adaptive parareal method using model order reduction to construct coarse propagator dynamically for time-dependent PDEs.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency of parallel-in-time methods for long-term evolution problems by adaptively constructing coarse propagators using data from fine propagators.

Method: Model order reduction based adaptive parareal method that uses fine propagator data with MOR techniques to construct coarse propagator adaptively in each parareal iteration.

Result: Method successfully applied to 3D time-dependent advection-diffusion equations with Kolmogorov and ABC flows, showing good performance for long-term evolution problems.

Conclusion: The adaptive parareal method with model order reduction is effective for simulating long-term evolution of time-dependent PDEs, particularly for complex 3D flows.

Abstract: In this paper, we propose a model order reduction based adaptive parareal method for time-dependent partial differential equations. By using the data obtained by the fine propagator in each iteration of the plain parareal method together with some model order reduction technique, we construct the coarse propagator adaptively in each parareal iteration, and then obtain our adaptive parareal method. We apply this new method to solve some 3D time-dependent advection-diffusion equations with the Kolmogorov flow and the ABC flow. Numerical results show the good performance of our method in simulating long-term evolution problems.

</details>


### [6] [Exact Constraint Enforcement in Physics-Informed Extreme Learning Machines using Null-Space Projection Framework](https://arxiv.org/abs/2601.10999)
*Rishi Mishra,Smriti,Balaji Srinivasan,Sundararajan Natarajan,Ganapathy Krishnamurthi*

Main category: math.NA

TL;DR: NP-PIELM enforces boundary conditions exactly via null-space projection, eliminating penalty weights and preserving single-shot training efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional PIELMs use penalty terms for boundary conditions, resulting in approximate satisfaction that is sensitive to user-specified weights and can propagate errors into the interior solution.

Method: Null-Space Projected PIELM (NP-PIELM) achieves exact constraint enforcement through algebraic projection in coefficient space, exploiting the geometric structure of the admissible coefficient manifold via null space decomposition of the boundary operator.

Result: The method eliminates penalty coefficients, dual variables, and problem-specific constructions while preserving single-shot training efficiency. Numerical experiments on elliptic and parabolic problems with complex geometries and mixed boundary conditions validate the framework.

Conclusion: NP-PIELM provides an exact boundary condition enforcement method for physics-informed extreme learning machines that is more robust and eliminates sensitivity to penalty weights while maintaining computational efficiency.

Abstract: Physics-informed extreme learning machines (PIELMs) typically impose boundary and initial conditions through penalty terms, yielding only approximate satisfaction that is sensitive to user-specified weights and can propagate errors into the interior solution. This work introduces Null-Space Projected PIELM (NP-PIELM), achieving exact constraint enforcement through algebraic projection in coefficient space. The method exploits the geometric structure of the admissible coefficient manifold, recognizing that it admits a decomposition through the null space of the boundary operator. By characterizing this manifold via a translation-invariant representation and projecting onto the kernel component, optimization is restricted to constraint-preserving directions, transforming the constrained problem into unconstrained least-squares where boundary conditions are satisfied exactly at discrete collocation points. This eliminates penalty coefficients, dual variables, and problem-specific constructions while preserving single-shot training efficiency. Numerical experiments on elliptic and parabolic problems including complex geometries and mixed boundary conditions validate the framework.

</details>


### [7] [B-spline-Based ALE-MFS Framework for Evolving Domains](https://arxiv.org/abs/2601.11041)
*Muhammad Ammad,Leevan Ling,Shu Ma*

Main category: math.NA

TL;DR: A B-spline based arbitrary Lagrangian-Eulerian method of fundamental solutions (ALE-MFS) for curvature-driven motion of 2D evolving domains, using meshless MFS for interior velocity computation and adaptive B-splines for robust curvature reconstruction.


<details>
  <summary>Details</summary>
Motivation: To develop a robust, meshless method for simulating curvature-driven motion of evolving domains that avoids volumetric meshing challenges and handles strongly nonconvex shapes and large deformations effectively.

Method: Combines B-spline reconstruction for boundary normals/curvature with ALE-MFS: boundary points track material motion, interior velocities computed via harmonic extension using MFS with sources on fixed auxiliary circle. Uses square collocation or zero-padded least-squares systems with a posteriori error estimates from LOOCV hatmatrix formulation.

Result: Square collocation works for moderately complex geometries; zero-padded least-squares improves interior velocity regularity and transport accuracy for strongly nonconvex shapes. ALE-MFS generates high-quality moving meshes with better minimum angles and slower mesh ratio growth than classical FEM mesh-motion strategies.

Conclusion: ALE-MFS provides a practical, meshless alternative for challenging moving-interface simulations that is easily integrable with ALE-finite element methods and robust for complex geometries and large deformations.

Abstract: We develop and analyze a B-spline based arbitrary Lagrangian-Eulerian method of fundamental solutions (ALE-MFS) for curvature-driven motion of two-dimensional evolving domains. Boundary points move with the material to track the geometric flow, while interior points move within an ALE framework via a harmonic extension of the boundary velocity, computed by a meshless MFS with sources on a fixed auxiliary circle, thus avoiding volumetric meshing. Boundary normals and curvature are reconstructed by an adaptive local B-spline scheme that remains robust for strongly nonconvex shapes and large deformations. A posteriori error estimates are obtained from a hatmatrix formulation of leave-one-out cross-validation (LOOCV) for both square collocation and zero-padded least-squares systems, and are complemented by maximum principle indicators for harmonic problems. Numerical experiments on circular, star-shaped, and amoeba-like domains show that square collocation suffices for moderately complex geometries, while zero-padded least-squares significantly improves interior velocity regularity and pointwise transport accuracy for strongly nonconvex shapes, without altering the source or collocation sets. The ALE-MFS algorithm also generates high-quality moving meshes for ALE-finite element methods, with larger minimum angles and slower mesh ratio growth than classical FEM mesh-motion strategies, suggesting a practical and easily integrable alternative for challenging moving-interface simulations.

</details>


### [8] [An Adaptive Lagrangian B-Spline Framework for Point Cloud Manifold Evolution](https://arxiv.org/abs/2601.11051)
*Muhammad Ammad,Leevan Ling*

Main category: math.NA

TL;DR: Adaptive Lagrangian framework for evolving point-cloud surfaces using localized B-spline patches, enabling meshless evolution with high-order geometric estimates and adaptive refinement.


<details>
  <summary>Details</summary>
Motivation: To develop a robust method for geometric evolution of point-cloud data representing smooth surfaces in 3D, overcoming limitations of mesh-based approaches and enabling direct evolution from discrete samples.

Method: Constructs overlapping localized tensor-product B-spline patches from point clouds, uses analytic B-spline representations to compute geometric invariants, employs conditioning-aware interpolation with Gauss-Seidel refinement, and implements adaptive knot insertion and point redistribution based on geometric error indicators.

Result: The method efficiently and accurately reproduces various surface evolution phenomena including mean-curvature flow, anisotropic deformations, and coupled surface-field dynamics, demonstrating the framework's precision and versatility.

Conclusion: Localized B-spline methods provide precise and versatile tools for dynamic manifold approximation, offering a robust meshless approach to surface evolution from point-cloud data with adaptive refinement capabilities.

Abstract: We extend our recent curve-evolution framework based on localized B-spline interpolation to present an adaptive Lagrangian framework for the geometric evolution of point-cloud data representing smooth, codimension-one surfaces in $\mathbb{R}^3$. The method constructs overlapping, localized tensor-product B-spline patches, enabling direct, meshless surface evolution from discrete samples. Within each patch, the differentiable B-spline representation yields analytic, high-order estimates of intrinsic geometric invariants, supporting curvature-driven and geometry-coupled flows. The organization of control points facilitates coherent updates of both surface samples and spline coefficients under intrinsic velocity fields. A conditioning-aware formulation of the local interpolation system, combined with a Gauss-Seidel refinement of control points, maintains interpolation quality throughout the evolution. Adaptive knot insertion and point redistribution, guided by geometric error indicators and local sampling density, preserve surface resolution and regularity during deformation. Numerical experiments demonstrate efficient and accurate reproduction of surface evolution phenomena, including mean-curvature flow, anisotropic deformations, and coupled surface-field dynamics, establishing localized B-spline methods as precise and versatile tools for dynamic manifold approximation.

</details>


### [9] [Numerical Treatment of Non-local Integral Operators in the Framework of Evolutionary Equations](https://arxiv.org/abs/2601.11132)
*Sebastian Franz,Sascha Trostorff*

Main category: math.NA

TL;DR: Theoretical analysis and numerical approximation of abstract differential equations with non-local integral operators, including well-posedness conditions, convergence proofs, and simulations.


<details>
  <summary>Details</summary>
Motivation: To study abstract differential equations that incorporate non-local integral operators, which arise in various applications involving memory effects, distributed parameters, or non-local interactions in physical systems.

Method: Using evolutionary equations theory to analyze well-posedness, then developing a numerical approximation method with convergence proofs based on conditions on the integral kernel and solution properties.

Result: Established well-posedness conditions for the abstract differential equations, developed a convergent numerical approximation method, and provided simulation results demonstrating the approach.

Conclusion: The paper successfully combines theoretical analysis with numerical implementation for abstract differential equations with non-local operators, providing both mathematical foundations and practical computational methods.

Abstract: Using the theory of evolutionary equations, we consider abstract differential equations including non-local integral operators. After providing a condition for the well-posedness of the addressed equation we consider a numerical method of approximating its solution. We provide convergence proofs under conditions on the kernel of the integral operator and the solution and finish the paper with some simulation results.

</details>


### [10] [Eigenvector-based acceleration strategies for gradient-type methods](https://arxiv.org/abs/2601.11145)
*Jean-Paul Chehab,Gaspard Kemlin,Marcos Raydan,Yousef Saad*

Main category: math.NA

TL;DR: Proposes strategies to speed up gradient-type methods by relaxing optimal step lengths to avoid zigzagging, using eigenvector approximations from Lanczos method for acceleration.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient methods like steepest descent and minimal residual suffer from the negative zigzag effect when using optimal step lengths, which slows convergence. The paper aims to overcome this limitation by relaxing step length requirements to enable more efficient exploration of the search space.

Method: Relaxes optimal step length requirements in gradient methods to avoid zigzagging, allowing iterates to explore entire space. When search directions approach eigenvectors of the Hessian matrix, leverages properties of the Lanczos method to accelerate convergence toward the global minimizer.

Result: The proposed strategies show improved convergence rates for gradient-type methods applied to strictly convex quadratics and functions by avoiding zigzag patterns and exploiting eigenvector approximations.

Conclusion: Relaxing optimal step lengths in gradient methods combined with Lanczos-based acceleration when approaching eigenvectors provides effective speedup strategies for minimizing strictly convex functions, overcoming traditional limitations of gradient descent methods.

Abstract: Several strategies are described and analyzed to speed-up gradient-type methods when applied to the minimization of strictly convex quadratics and strictly convex functions. The proposed techniques focus on relaxing the traditional optimal step length associated with gradient methods, including the steepest descent (SD) and the minimal residual (MR) methods. Such a relaxation avoids the well-known negative zigzag effect and allows the iterates to move in the entire space which in turn implies that every so often the search direction approaches some eigenvector of the underlying Hessian matrix. The proposed speedups then rely on taking advantage of the properties of the Lanczos method once a search direction that approaches an eigenvector has been identified in order to accelerate the convergence towards the global minimizer. After analyzing the proposed strategies, we illustrate them on the global minimization of strictly convex functions.

</details>


### [11] [An efficient solver based on low-rank approximation and Neumann matrix series for unsteady diffusion-type partial differential equations with random coefficients](https://arxiv.org/abs/2601.11152)
*Yujun Zhu,Min Li,Yulan Ning,Ju Ming*

Main category: math.NA

TL;DR: Proposed efficient solver for unsteady diffusion PDEs with random coefficients using generalized low-rank matrix approximation and Neumann series expansion to reduce computational cost.


<details>
  <summary>Details</summary>
Motivation: Address computational challenge of repeatedly solving large-scale linear systems from spatial/temporal discretizations under uncertainty in diffusion-type PDEs with random coefficients.

Method: Generalized low-rank matrix approximation for stochastic stiffness matrices + Neumann matrix series expansion for matrix inverses, transforming high-dimensional inversion to low-dimensional multiplications.

Result: Significantly reduces computational cost and storage requirements while maintaining high numerical accuracy; error analysis provided; applied to unsteady stochastic diffusion equations and distributed optimal control problems.

Conclusion: Numerical results demonstrate feasibility and effectiveness of proposed solver for uncertainty quantification problems.

Abstract: In this paper, we develop an efficient numerical solver for unsteady diffusion-type partial differential equations with random coefficients. A major computational challenge in such problems lies in repeatedly handling large-scale linear systems arising from spatial and temporal discretizations under uncertainty. To address this issue, we propose a novel generalized low-rank matrix approximation to represent the stochastic stiffness matrices, and approximate their inverses using the Neumann matrix series expansion. This approach transforms high-dimensional matrix inversion into a sequence of low-dimensional matrix multiplications. Therefore, the solver significantly reduces the computational cost and storage requirements while maintaining high numerical accuracy. The error analysis of the proposed solver is also provided. Finally, we apply the method to two classic uncertainty quantification problems: unsteady stochastic diffusion equations and the associated distributed optimal control problems. Numerical results demonstrate the feasibility and effectiveness of the proposed solver.

</details>


### [12] [Adaptive Randomized Extended Bregman-Kaczmarz Method for Combined Optimization Problems](https://arxiv.org/abs/2601.11157)
*Zeyu Dong,Aqin Xiao,Guojian Yin,Junfeng Yin*

Main category: math.NA

TL;DR: Proposes an adaptive randomized averaging block extended Bregman-Kaczmarz (aRABEBK) method for solving inverse problems with data-fidelity and regularization terms, featuring automatic step-size adjustment for faster convergence.


<details>
  <summary>Details</summary>
Motivation: Combined optimization problems with data-fidelity and regularization terms are common in inverse problems, but existing methods require manual tuning of relaxation parameters and may converge slowly.

Method: Develops an adaptive randomized averaging block extended Bregman-Kaczmarz method with iteration-wise relaxation parameters that are automatically adjusted using residual information, enabling more aggressive step sizes without manual tuning.

Result: Establishes convergence theory with expected linear convergence rate guarantees. Numerical experiments on synthetic and real datasets for sparse and minimum-norm least-squares problems show faster convergence and improved robustness compared to state-of-the-art extended Kaczmarz and Bregman-Kaczmarz algorithms.

Conclusion: The proposed aRABEBK method provides an effective adaptive approach for solving inverse problems with combined optimization terms, achieving superior performance through automatic parameter adjustment and faster convergence rates.

Abstract: Combined optimization problems that couple data-fidelity and regularization terms arise naturally in a wide range of inverse problems. In this paper, we study an adaptive randomized averaging block extended Bregman-Kaczmarz (aRABEBK) method for solving such problems. The proposed method incorporates iteration-wise relaxation parameters that are automatically adjusted using residual information, allowing for more aggressive step sizes without additional manual tuning. We establish a convergence theory for the proposed framework and derive expected linear convergence rate guarantees. Numerical experiments on both synthetic and real data sets for sparse and minimum-norm least-squares problems demonstrate that our aRABEBK method achieves faster convergence and improved robustness compared with state-of-the-art extended Kaczmarz and Bregman-Kaczmarz-type algorithms, including its nonadaptive counterpart.

</details>


### [13] [Discontinuous Galerkin schemes for multi-dimensional coupled hyperbolic systems](https://arxiv.org/abs/2601.11172)
*Niklas Kolbe,Siegfried Müller,Aleksey Sikstel*

Main category: math.NA

TL;DR: New Runge-Kutta discontinuous Galerkin schemes for coupled conservation law systems with sharp interfaces, using relaxation approach and local projection to avoid expensive nonlinear half-Riemann problems.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical schemes for coupled systems of conservation laws separated by sharp interfaces, particularly for fluid-structure interaction problems, without requiring computationally expensive solutions of nonlinear half-Riemann problems.

Method: Derived from Jin-Xin relaxation approach with problem-specific modification of interface coupling conditions, using local projection techniques. Higher-order time discretization achieved through strong stability preserving Runge-Kutta methods with asymptotic preserving implicit-explicit treatment of coupled relaxation system.

Result: The paper introduces novel Runge-Kutta discontinuous Galerkin schemes that efficiently handle coupled conservation law systems with fixed sharp interfaces, demonstrated through application to multi-dimensional fluid-structure coupling problems.

Conclusion: The proposed relaxation-based approach provides an efficient alternative to traditional methods for interface coupling problems, with demonstrated applicability to complex fluid-structure interaction scenarios using compressible Euler equations and linear elastic models.

Abstract: A novel class of Runge-Kutta discontinuous Galerkin schemes for coupled systems of conservation laws in multiple space dimensions that are separated by a fixed sharp interface is introduced. The schemes are derived from a relaxation approach and a local projection and do not require expensive solutions of nonlinear half-Riemann problems. The underlying Jin-Xin relaxation involves a problem specific modification of the coupling condition at the interface, for which a simple construction algorithm is presented. The schemes are endowed with higher order time discretization by means of strong stability preserving Runge-Kutta methods. These are derived from an asymptotic preserving implicit-explicit treatment of the coupled relaxation system taken to the discrete relaxation limit. In a case study the application to a multi-dimensional fluid-structure coupling problem employing the compressible Euler equations and a linear elastic model is discussed.

</details>


### [14] [A Machine-Learned Near-Well Model in OPM Flow](https://arxiv.org/abs/2601.11193)
*Peter von Schultzendorff,Tor Harald Sandve,Birane Kane,David Landa-Marbán,Jakub Wiktor Both,Jan Martin Nordbotten*

Main category: math.NA

TL;DR: Integration of neural networks into OPM Flow reservoir simulator for hybrid modeling, demonstrated with a data-driven near-well model that outperforms traditional Peaceman models and local grid refinement.


<details>
  <summary>Details</summary>
Motivation: Hybrid reservoir simulation combining physics-based models with ML components offers high fidelity and fast inference, but requires tight integration with automatic differentiation frameworks for efficient nonlinear solvers, inverse problems, and optimization.

Method: First integration of neural networks into OPM Flow simulator. Networks trained in TensorFlow and imported as native AD functions. Applied to create a data-driven near-well model by training neural network to infer Peaceman-like well index from fine-scale ensemble simulations of near-well region.

Result: Successfully integrated neural networks into OPM Flow as native AD functions. The data-driven near-well model tested on CO2 storage examples shows high fidelity to fine-scale results at low computational cost, outperforming traditional Peaceman models and local grid refinement.

Conclusion: The OPM Flow-Neural Network framework enables efficient hybrid modeling with seamless integration into existing workflows, demonstrating significant potential for accurate reservoir simulation with reduced computational expense.

Abstract: Recent advances in reservoir simulation increasingly utilize hybrid approaches that couple physics-based simulators with machine-learning (ML) components. ML components offer high fidelity to training data and fast inference, enabling efficient and accurate modeling of complex multi-scale or multi-physics phenomena. Modern reservoir simulators rely on automatic differentiation (AD) to support efficient and flexible strategies for nonlinear solvers, inverse problems, and optimization problems. Efficient hybrid modeling therefore requires tight integration of the ML components with the simulator's AD framework.
  We present the first integration of neural networks into the high-performance reservoir simulator OPM Flow. Networks are trained in TensorFlow and imported into OPM, where they are accessed as native AD functions. This presents an efficient framework for hybrid modeling and enables seamless integration in existing simulator workflows.
  As an application, we introduce a novel, data-driven near-well model. Near-well models are essential in reservoir simulation for accurately representing singular pressure gradients around wells. Commonly used are the Peaceman near-well model and its extensions, or local grid refinement around the wells. Peaceman-type models are limited to simplified flow regimes, whereas local grid refinement is computationally expensive. We address these limitations by training a neural network to infer a Peaceman-like well index from fine-scale ensemble simulations of the near-well region. It is then integrated into OPM Flow with the new framework. Tested on relevant examples for CO$_2$ storage, the method offers high fidelity to fine-scale results at low computational cost, demonstrating the potential of the OPM Flow-Neural Network framework for hybrid modeling.

</details>


### [15] [A Recovery-Based Error Indicator for Finite Difference Methods](https://arxiv.org/abs/2601.11308)
*Ferhat Sindy,Annalisa Buffa,Marco Picasso*

Main category: math.NA

TL;DR: A novel recovery-based error indicator for high-order Finite Difference Methods using polynomial interpolation into Finite Element space for gradient error estimation.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate error estimation method for high-order Finite Difference Methods that can handle various problem types including discontinuous coefficients and wave equations in homogeneous/heterogeneous media.

Method: Interpolate Finite Difference grid values into polynomial Finite Element space, then apply a recovery-based error indicator with polynomial-preserving property to estimate gradient errors.

Result: The error indicator demonstrates good performance and accuracy across multiple numerical experiments including 2D Poisson problems, elliptic problems with discontinuous coefficients, and 2D/3D wave equations with various finite difference schemes.

Conclusion: The proposed recovery-based error indicator is effective for error estimation in high-order Finite Difference Methods across diverse problem types and spatial dimensions.

Abstract: A novel recovery-based error indicator for high-order Finite Difference Methods, based on post-processing of the Finite Difference values is presented. The values obtained on the Finite Difference grid are interpolated into a suitable polynomial Finite Element space. A recovery-based error indicator, with the polynomial-preserving property, is then applied to estimate the gradient error. The performance and accuracy of the proposed error indicator are demonstrated through several numerical experiments, including the two-dimensional Poisson problem solved using second- and fourth-order finite difference schemes. Additional experiments are conducted on elliptic problems with discontinuous coefficients, as well as on the two and three-dimensional wave equation in homogeneous media with second- and fourth-order finite differences, and in heterogeneous media with second-order finite differences.

</details>


### [16] [Constructing Orthonormal Rational Function Vectors with an application in Rational Approximation](https://arxiv.org/abs/2601.11317)
*Robbe Vermeiren*

Main category: math.NA

TL;DR: Two algorithms for constructing orthonormal rational function bases using k-Hessenberg matrices, applied to rational approximation problems with exponential pole clustering near singularities.


<details>
  <summary>Details</summary>
Motivation: Need efficient methods for constructing orthonormal bases of rational function vectors for rational approximation problems, particularly when dealing with exponentially clustered poles near singularities.

Method: Extend pencil-based inverse generalized eigenvalue problem to rational vectors of arbitrary length k using k-Hessenberg matrices. Develop two algorithms: 1) updating algorithm based on similarity transformations using rotations, 2) Krylov-type algorithm related to rational Arnoldi method.

Result: Successfully recover optimal lightning + polynomial convergence rate for rational approximation of √z on [0,1], demonstrating robustness for handling exponentially clustered poles near singularities.

Conclusion: The proposed methods provide robust algorithms for constructing orthonormal rational function bases and solving rational approximation problems, particularly effective for handling exponentially clustered poles near singularities.

Abstract: We present two algorithms for constructing orthonormal bases of rational function vectors with respect to a discrete inner product, and discuss how to use them for a rational approximation problem. Building on the pencil-based formulation of the inverse generalized eigenvalue problem by Van Buggenhout et al.\ (2022), we extend it to rational vectors of arbitrary length $k$, where the recurrence relations are represented by a pair of $k$-Hessenberg matrices, i.e., matrices with possibly $k$ nonzero subdiagonals. An updating algorithm based on similarity transformations using rotations and a Krylov-type algorithm related to the rational Arnoldi method are derived. The performance is demonstrated on the rational approximation of $\sqrt{z}$ on $[0,1]$, where the optimal lightning + polynomial convergence rate of Herremans, Huybrechs, and Trefethen (2023) is successfully recovered. This illustrates the robustness of the proposed methods for handling exponentially clustered poles near singularities.

</details>


### [17] [Solving the Fisher nonlinear differential equations via Physics-Informed Neural Networks: A Comprehensive Retraining Study and Comparative Analysis with the Finite Difference Method](https://arxiv.org/abs/2601.11406)
*Ahmed Aberqi,Ahmed Miloudi*

Main category: math.NA

TL;DR: This paper applies Physics-Informed Neural Networks (PINNs) to solve the 1D nonlinear Fisher-KPP equation, comparing results with analytical and finite difference solutions, and analyzes retraining strategies and error performance.


<details>
  <summary>Details</summary>
Motivation: To demonstrate PINNs as a viable alternative to traditional numerical methods for solving complex nonlinear PDEs like the Fisher-KPP equation, which models important reaction-diffusion phenomena in population dynamics and flame propagation.

Method: Standard PINN framework with neural network architecture, physics-informed loss function, and investigation of retraining strategies to optimize performance. Validation through comparison with analytical solution and Finite Difference Method (FDM) numerical solution.

Result: PINNs accurately approximate the Fisher-KPP equation solution, with thorough quantitative error analysis showing competitive performance against traditional methods. The study reveals critical insights about model retraining challenges, particularly regarding optimizer state management.

Conclusion: PINNs are an effective alternative to traditional numerical methods for solving nonlinear differential equations, demonstrating good accuracy while highlighting important considerations for training efficiency and model complexity balance.

Abstract: Physics-Informed Neural Networks (PINNs) represent a groundbreaking paradigm in scientific computing, seamlessly integrating the robust framework of deep learning with fundamental physical laws. This paper meticulously applies the standard PINN framework to solve the challenging one-dimensional nonlinear Fisher-KPP equation, a critical model in reaction-diffusion dynamics describing phenomena such as population spread and flame propagation. We detail a comprehensive methodology, encompassing the neural network architecture, the physics-informed loss function, and an in-depth investigation into retraining strategies aimed at optimizing model performance. Our approach is rigorously validated through a direct comparison of the PINN solution against both the known analytical solution and a numerical solution derived from the Finite Difference Method (FDM). Through this work, we elucidate the intricate balance between model complexity, training efficiency, and accuracy. Results highlight the PINN's remarkable capability in accurately approximating the solution to this complex PDE, while also shedding light on the critical aspects and challenges of model retraining, particularly concerning the optimizer's state. This study provides a thorough quantitative error analysis, demonstrating the efficacy of PINNs as a viable and competitive alternative to traditional numerical methods for solving nonlinear differential equations, and discusses their broader applications across various scientific domains.

</details>


### [18] [Tensor field tomography with attenuation and refraction: adjoint operators for the dynamic case and numerical experiments](https://arxiv.org/abs/2601.11483)
*Lukas Vierus,Thomas Schuster,Bernadette Hahn*

Main category: math.NA

TL;DR: Tensor field tomography with refraction, attenuation, and time-dependence using geodesic ray transforms; adjoint representations derived; numerical implementation shows integral methods outperform PDE-based approaches in efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive tensor field tomography framework that accounts for realistic physical phenomena including refraction, attenuation, and time-dependence, which are often neglected in simplified models but crucial for accurate reconstructions in practical applications.

Method: Uses attenuated ray transforms along geodesic curves defined by a Riemannian metric (refraction index). Formulates tomography as an inverse source problem via transport equations. Derives two adjoint representations (dynamic/static). Implements damped Landweber method with Nesterov acceleration for static fields, comparing integral vs PDE-based formulations with viscosity approximation for transport equations.

Result: Integral representation significantly outperforms PDE-based methods in computational efficiency while achieving comparable reconstruction accuracy. Noise analysis and refraction effects show improved accuracy when refraction is properly modeled, justifying the increased computational cost.

Conclusion: Including refraction in the forward model is beneficial despite increased numerical cost, as it improves reconstruction accuracy. Integral-based methods are computationally superior to PDE-based approaches for tensor field tomography with refraction.

Abstract: This article is concerned with tensor field tomography in a fairly general setting, that takes refraction, attenuation and time-dependence of tensor fields into account. The mathematical model is given by attenuated ray transforms of the fields along geodesic curves corresponding to a Riemannian metric that is defined by the index of refraction. The data are given at the boundary tangent bundle of the domain and it is well-known that they can be characterized as boundary data of a transport equation turning tensor field tomography into an inverse source problem. This way the adjoint of the forward mapping can be computed using the integral representation or, equivalently, associated to a dual transport equation. The article offers and proves two different representations for the adjoint mappings both in the dynamic and static case. The numerical implementation is demonstrated and evaluated for static fields using the damped Landweber method with Nesterov acceleration applied to both, the integral and PDE-based formulations. The transport equations are solved using a viscosity approximation. The error analysis reveals that the integral representation significantly outperforms PDE-based methods in terms of computational efficiency while achieving comparable reconstruction accuracy. The impact of noise and deviations from straight-line trajectories are investigated confirming improved accuracy if refraction is taken into account. We conclude that the inclusion of refraction to the forward model pays in spite of increased numerical cost.

</details>


### [19] [Efficient error estimators for Generalized Nyström](https://arxiv.org/abs/2601.11493)
*Lorenzo Lazzarino,Katherine J. Pearce,Nathaniel Pritchard*

Main category: math.NA

TL;DR: Extension of leave-one-out error estimation framework to generalized Nyström decomposition for rectangular matrices, with three new estimators validated through experiments.


<details>
  <summary>Details</summary>
Motivation: Randomized algorithms in numerical linear algebra need efficient ways to assess approximation accuracy without additional expensive matrix accesses. While recent work developed leave-one-out error estimators for randomized SVD and standard Nyström, the generalized Nyström decomposition for rectangular matrices lacked such estimators.

Method: Extends the leave-one-out framework to generalized Nyström decomposition by deriving three new leave-one-out error estimators specifically designed for this approach that works with general rectangular matrices.

Result: Three new leave-one-out error estimators were developed for generalized Nyström decomposition, and their effectiveness was validated through numerical experiments.

Conclusion: The work successfully extends the leave-one-out error estimation framework to generalized Nyström decomposition, providing efficient accuracy assessment for randomized methods applied to rectangular matrices without additional matrix accesses.

Abstract: Randomized algorithms in numerical linear algebra have proven to be effective in ameliorating issues of scalability when working with large matrices, efficiently producing accurate low-rank approximations. A key remaining challenge, however, is to efficiently assess the approximation accuracy of randomized methods without additional expensive matrix accesses. Recent work has addressed this issue by deriving fast leave-one-out error estimators for the randomized SVD and Nyström decomposition, enabling accurate error estimation with no additional matrix accesses. In this work, we extend the leave-one-out framework to the generalized Nyström decomposition, an approach that can be applied to general rectangular matrices. We do this by deriving three new leave-one-out error estimators and validating their effectiveness through numerical experiments.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [20] [Critical points of the two-dimensional Ambrosio-Tortorelli functional with convergence of the phase-field energy](https://arxiv.org/abs/2601.10875)
*Jean-François Babadjian,Martin Rakovsky,Rémy Rodiac*

Main category: math.AP

TL;DR: The paper extends previous results on Ambrosio-Tortorelli functional convergence, showing that in 2D, convergence of just the phase-field energy to the length energy term is sufficient for critical point properties to transfer to the Mumford-Shah functional.


<details>
  <summary>Details</summary>
Motivation: Previous work required convergence of the full Ambrosio-Tortorelli energy to the Mumford-Shah energy to guarantee that critical points of the former yield critical points of the latter. The authors aim to weaken this requirement in the 2D setting.

Method: The authors analyze sequences of critical points of the Ambrosio-Tortorelli functional with uniform energy bounds. They focus on the two-dimensional case and study convergence properties when only the phase-field energy converges to the length energy term of the Mumford-Shah functional.

Result: In 2D, convergence of just the phase-field energy to the length energy term is sufficient to ensure that the first inner variation converges, making the limit function u a critical point of the Mumford-Shah functional in the sense of inner variations.

Conclusion: The paper establishes a weaker convergence condition for transferring critical point properties from Ambrosio-Tortorelli to Mumford-Shah functionals in two dimensions, requiring only phase-field energy convergence rather than full energy convergence.

Abstract: We consider a family $\{(u_\varepsilon, v_\varepsilon)\}_{\varepsilon>0}$ of critical points of the Ambrosio-Tortorelli functional. Assuming a uniform energy bound, the sequence $\{(u_\varepsilon, v_\varepsilon)\}_{\varepsilon>0}$ converges in $L^2(Ω)$ to a limit $(u, 1)$ as $\varepsilon \to 0$, where $u$ is in $SBV^2(Ω)$. It was previously shown that if the full Ambrosio-Tortorelli energy associated to $(u_\varepsilon,v_\varepsilon)$ converges to the Mumford-Shah energy of $u$, then the first inner variation converges as well. In particular, $u$ is a critical point of the Mumford-Shah functional in the sense of inner variations. In this work, focusing on the two-dimensional setting, we extend this result under the sole convergence of the phase-field energy to the length energy term in the Mumford-Shah functional.

</details>


### [21] [Normalized solutions of Nehari-Pankov type to indefinite variational problems](https://arxiv.org/abs/2601.10941)
*Damien Galant,Tobias Weth*

Main category: math.AP

TL;DR: The paper develops an abstract framework for existence and multiplicity of solutions with prescribed norm to nonlinear equations, applying it to Schrödinger equations on graphs, biharmonic equations on tori, and various PDEs with Dirichlet conditions.


<details>
  <summary>Details</summary>
Motivation: To establish a unified abstract approach for finding solutions with prescribed norm (mass) to nonlinear equations, addressing the challenge of existence and multiplicity in various settings including mass-supercritical cases where traditional methods may fail.

Method: Develops an abstract framework for equations Au = λu + I'(u) where A is self-adjoint with compact resolvent. Solutions are detected as ground states of Nehari-Pankov type for λ-dependent action functionals, with λ varying in spectral gaps. Key insight: H-norms of λ-dependent solution families form connected sets. Uses Weyl estimates, variational characterizations of eigenvalues, eigenfunction bounds, and analytic number theory to estimate connected set sizes.

Result: 1) Infinitely many solutions with prescribed arbitrarily large mass for Schrödinger equations on compact graphs (including mass-supercritical case). 2) Similar existence and multiplicity for biharmonic semilinear equation on 2-torus. 3) Multiple solutions with prescribed small mass for second/higher order equations in bounded domains with Dirichlet conditions.

Conclusion: The abstract framework successfully provides existence and multiplicity results for solutions with prescribed norm across various nonlinear equations, demonstrating the power of connecting λ-dependent solution families and spectral gap analysis.

Abstract: We consider abstract nonlinear equations of the form $A u = λu + I'(u)$, where $A$ is a self-adjoint operator with compact resolvent on a Hilbert space $H$, $λ\in \mathbf{R}$ is a parameter, and $u \mapsto I'(u)$ is a superlinear term of variational nature. In this abstract setting, we develop a new approach to existence and multiplicity of solutions with prescribed norm in $H$. We then consider various applications of this approach. First, we obtain, under fairly general assumptions including the mass-supercritical case, the existence of infinitely many solutions to a class of nonlinear (time-independent) Schrödinger equations on a compact graph $\mathcal{G}$ with prescribed (arbitrarily large) mass. Moreover, we derive a similar existence and multiplicity result for a biharmonic semilinear equation in the $2$-torus. For a larger class of second order and higher order equations in a bounded domain with Dirichlet boundary conditions, we also show the existence of multiple solutions with prescribed small mass.
  The solutions we obtain are detected as ground states of Nehari-Pankov type for the associated $λ$-dependent action functional, where $λ$ varies in a spectral gap between sufficiently large eigenvalues of $A$. The key observation in this abstract framework is the fact that the $H$-norms of these $λ$-dependent solution families form connected sets. To estimate the size of these connected sets in specific settings, we use Weyl type estimates for the length of spectral gaps, variational characterizations of eigenvalues, bounds for associated eigenfunctions and a classical bound from analytic number theory.

</details>


### [22] [Optimal Trudinger-Moser inequalities on complete noncompact Riemannian manifolds: Revisit of the argument from the local inequalities to global ones](https://arxiv.org/abs/2601.10996)
*Jungang Li,Guozhen Lu*

Main category: math.AP

TL;DR: This short note clarifies a proof in a previous paper and provides an alternative approach from local to global inequalities.


<details>
  <summary>Details</summary>
Motivation: To simplify and clarify part of the proof of Theorem 1.3 from a referenced paper [8], and to demonstrate an alternative method for deriving global inequalities from local ones.

Method: The paper presents a simplified clarification of the original proof and introduces an alternative argument that transitions from local inequalities to global ones.

Result: Provides a clearer understanding of Theorem 1.3's proof and establishes an alternative pathway from local to global inequalities.

Conclusion: The note successfully clarifies the referenced theorem's proof and offers a valuable alternative approach for connecting local and global inequalities.

Abstract: The main purpose of this short note, on the one hand, to is clarify some part of the proof of Theorem 1.3 in [8] in a simple way, and on the other hand, to give an alternative argument from local inequalities to global ones.

</details>


### [23] [Inverse Spectral Problem With Low Regularity Refractive Index](https://arxiv.org/abs/2601.11146)
*Kewen Bu,Youjun Deng,Yan Jiang,Kai Zhang*

Main category: math.AP

TL;DR: Radial refractive index n is not uniquely determined by special transmission eigenvalues alone, but becomes uniquely determined when supplemented with partial a priori information.


<details>
  <summary>Details</summary>
Motivation: To investigate whether a radial refractive index n can be uniquely determined from spectral data (transmission eigenvalues), which is important for inverse scattering problems and material characterization.

Method: Mathematical analysis of piecewise twice continuously differentiable functions and twice continuously differentiable functions (or continuously differentiable functions with Lipschitz continuous derivative). First shows non-uniqueness for piecewise functions, then proves uniqueness with additional information.

Result: For piecewise twice continuously differentiable functions, n is NOT uniquely determined by special transmission eigenvalues alone. However, for twice continuously differentiable functions (or continuously differentiable with Lipschitz derivative), n IS uniquely determined on [0,1] by all special transmission eigenvalues when supplemented with partial a priori information.

Conclusion: The uniqueness of determining radial refractive index from spectral data depends on function regularity and requires additional a priori information. While not uniquely determined by eigenvalues alone for piecewise functions, uniqueness can be achieved for smoother functions with supplementary information.

Abstract: This article investigates the unique determination of a radial refractive index n from spectral data. First, we demonstrate that for piecewise twice continuously differentiable functions, n is not uniquely determined by the special transmission eigenvalues associated with radially symmetric eigenfunctions. Subsequently we prove that if n \in M is twice continuously differentiable functions(or continuously differentiable functions with Lipschitz continuous derivative), then n is uniquely determined on [0,1] by all special transmission eigenvalues when supplemented by partial a priori information on the refractive index.

</details>


### [24] [On a Mullins-Sekerka model for the growth of active droplets modelling protocells: Stability analysis and numerical computations](https://arxiv.org/abs/2601.11155)
*Harald Garcke,Kei Fong Lam,Robert Nürnberg,Andrea Signori*

Main category: math.AP

TL;DR: Analysis of chemically active Mullins-Sekerka models showing droplet growth, instability, splitting, and complex dynamics relevant to protocell modeling.


<details>
  <summary>Details</summary>
Motivation: Mullins-Sekerka models with chemical reactions can simulate grow-divide cycles observed in protocells and living systems, providing insights into chemical compartment organization fundamental to life.

Method: Theoretical analysis of radially symmetric solutions, stability analysis in radial and planar geometries, analysis of multilayered shell-type solutions, and development of parametric finite element method handling topological changes (splitting/merging).

Result: Existence of radially symmetric solutions proven, stability analysis completed, numerical simulations verify theoretical findings and show complex dynamics including multiple instabilities, droplet splitting, and shell-type solution emergence.

Conclusion: Chemically active Mullins-Sekerka models successfully capture complex droplet dynamics relevant to protocell behavior, with theoretical analysis supported by numerical simulations showing rich pattern formation including splitting and shell structures.

Abstract: Mullins-Sekerka models with chemical reactions can lead to scenarios where droplets grow, become unstable, split, grow and undergo further division. These grow and division cycles have been proposed as a model for protocells and are believed to play a fundamental role in living systems by providing chemical compartments which are important in the organization of living systems. This paper analyses chemically active Mullins-Sekerka models. Existence of radially symmetric solutions is shown and a detailed stability analysis in radial as well as planar situations is given. In particular, we also analyze multilayered solutions leading to shell-type situations. Finally, we introduce a numerical method based on a parametric finite element approach that explicitly accounts for topological changes, thereby allowing for droplet splitting and merging. Several numerical simulations verify the findings of the theoretical stability analysis and show complex dynamical behavior, including multiple instabilities, splittings of droplets and appearance of shell-type solutions.

</details>


### [25] [Ergodic pairs for fractional Hamilton-Jacobi equations on bounded domains: large solutions](https://arxiv.org/abs/2601.11241)
*Alexander Quaas,Erwin Topp*

Main category: math.AP

TL;DR: Study of ergodic problem for viscous Hamilton-Jacobi equations with censored fractional Laplacian diffusion, focusing on cases where nonlinear gradient term scaling ≤ fractional diffusion order.


<details>
  <summary>Details</summary>
Motivation: Extend understanding of ergodic problems from classical second-order elliptic operators to nonlocal fractional operators with censored boundary conditions, addressing challenges from state-dependent operators.

Method: Use vanishing discount method to analyze approximated solutions, providing qualitative properties for ergodic problem including blow-up rates and characterization of ergodic constant.

Result: Existence of ergodic pairs involving solutions that blow-up on boundary ∂Ω, with precise blow-up rates and characterization of ergodic constant.

Conclusion: Successfully extends results from second-order counterpart to censored fractional Laplacian case, overcoming difficulties from state-dependency of operator where classical invariance properties don't apply.

Abstract: In this article, we study the ergodic problem associated to viscous Hamilton-Jacobi equation where the diffusion is governed by the censored fractional Laplacian, a nonlocal elliptic operator restricted to a bounded domain $Ω\subset \mathbb{R}^N$. We restrict ourselves to the case in which the nonlinear gradient term has a scaling less or equal than the fractional order of the diffusion. In similarity to its second-order counterpart, we provide existence of ergodic pairs involving solutions that blow-up on $\partial Ω$. We use the celebrated vanishing discount method, where the analysis of the approximated solutions have its own interest, leading to qualitative properties for the ergodic problem such as precise blow-up rates for the solution and characterization of the ergodic constant. The main difficulties arise from the state-dependency of the operator, from which the arguments of the local case based on well-known invariance properties of the Laplacian are not longer at disposal.

</details>


### [26] [Structured Deformations in Linearized Elasticity](https://arxiv.org/abs/2601.11333)
*Manuel Friedrich,José Matias,Elvira Zappale*

Main category: math.AP

TL;DR: Extends structured deformation theory to linearized elasticity with integral energy representation featuring bulk and surface contributions.


<details>
  <summary>Details</summary>
Motivation: To bridge structured deformation theory with linearized elasticity, providing a comprehensive energy framework that accounts for both volumetric and interfacial effects in deformation processes.

Method: Two approaches: 1) Direct method using global relaxation techniques in BD (bounded deformation) space, 2) Approximation from nonlinear elastic energies of nonsimple materials.

Result: Successfully derived integral representation for energy in linearized elasticity with structured deformations, incorporating both bulk and surface energy contributions.

Conclusion: Established a rigorous mathematical framework for structured deformations in linearized elasticity, enabling analysis of materials with both smooth and singular deformation features through combined bulk-surface energy representation.

Abstract: We extend the theory of structured deformations to the setting of linearized elasticity by providing an integral representation for the underlying energy that features bulk and surface contributions. Our derivation is obtained both via a direct approach by means of a global method for relaxation in BD and via an approximation from nonlinear elastic energies associated to {nonsimple} materials.

</details>


### [27] [Elastic Calderón Problem via Resonant Hard Inclusions: Linearisation of the N-D Map and Density Reconstruction](https://arxiv.org/abs/2601.11356)
*Huaian Diao,Mourad Sini,Ruixiang Tang*

Main category: math.AP

TL;DR: Constructive inverse problem approach using resonant metamaterials to recover density in elastic media via effective negative density shift and linearization.


<details>
  <summary>Details</summary>
Motivation: To develop a metamaterial-inspired analytic framework for solving inverse coefficient problems in linear elasticity by leveraging nanoscale resonators in reconstruction algorithms.

Method: Embed subwavelength periodic array of resonant high-density inclusions to create effective medium with uniform negative density shift, then linearize the Neumann-to-Dirichlet map around this background and use complex geometric optics solutions for reconstruction.

Result: Derived explicit reconstruction formula for Fourier transform of density ρ, achieving global density recovery with operator norm estimate ‖Λ_D-Λ_𝒫‖ ≤ Ca^α𝒫^6 as a→0.

Conclusion: Provides concrete paradigm for using nanoscale resonators in reconstruction algorithms and establishes constructive strategy for elastic Calderón-type inverse problems via metamaterial-inspired approach.

Abstract: We study an elastic Calderon-type inverse problem: recover the mass density $ρ(x)$ in a bounded domain $Ω\subset\mathbb{R}^3$ from the Neumann-to-Dirichlet map associated with the isotropic Lamé system $\mathcal{L}_{λ,μ}u+ω^2ρ(x)u=0$. We introduce a constructive strategy that embeds a subwavelength periodic array of resonant high-density (hard) inclusions to create an effective medium with a uniform negative density shift. Specifically, we place a periodic cluster of inclusions of size $a$ and density $ρ_1\asymp a^{-2}$ strictly inside $Ω$. For frequencies $ω$ tuned to an eigenvalue of the elastic Newton (Kelvin) operator of a single inclusion, we show that as $a\to0$ and the number of inclusions $M\to\infty$, the Neumann-to-Dirichlet map $Λ_D$ converges to an effective map $Λ_{\mathcal{P}}$ corresponding to a background density shift $-\mathcal{P}^2$, with the operator norm estimate $\|Λ_D-Λ_{\mathcal{P}}\|\le Ca^α\mathcal{P}^6$ for some $α>0$ determined by the geometric scaling. Around this negative background we derive a first-order linearization of $Λ_{\mathcal{P}}$ in terms of $ρ$ and the Newton volume potential for the shifted Lamé operator. Testing the linearized relation with complex geometric optics solutions yields an explicit reconstruction formula for the Fourier transform of $ρ$, and hence a global density recovery scheme. The results provide a metamaterial-inspired analytic framework for inverse coefficient problems in linear elasticity and a concrete paradigm for leveraging nanoscale resonators in reconstruction algorithms.

</details>


### [28] [Homogenized moderately wrinkled shell theory from 3D Koiter's linear elasticity](https://arxiv.org/abs/2601.11384)
*Pedro Hernández-Llanos,Rajesh Mahadevan,Ravi Prakash*

Main category: math.AP

TL;DR: Derivation of periodically wrinkled shell models from 3D linear elasticity using two-scale convergence, with specific scaling parameter p=2 for the mid-surface wrinkling.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical models for shells with periodic wrinkling patterns by deriving them rigorously from three-dimensional linear elasticity theory, accounting for the interplay between the small parameter ε and scaling exponent p.

Method: Uses two-scale convergence technique starting from 3D linear elasticity. Assumes mid-surface has periodic wrinkling pattern: ψ(x₁,x₂) + ε^pθ(x₁/ε, x₂/ε)a₃(x₁,x₂) with p=2, where θ is [0,1)²-periodic. Employs Koiter's model for shell strain energy.

Result: Different shell theories emerge depending on the behavior of small parameter ε>0 and p>1. For the specific case p=2, a particular periodically wrinkled shell model is derived through the two-scale convergence approach.

Conclusion: The two-scale convergence method successfully derives periodically wrinkled shell models from 3D elasticity, with different theories appearing based on parameter scaling relationships, providing rigorous mathematical foundations for analyzing shells with periodic surface wrinkling.

Abstract: In this paper we derive, by two$-$scale convergence, periodically wrinked shell models starting from three dimensional linear elasticity, depending of the behaviour of the small parameter $\varepsilon>0$ and $p>1$, differents theories appear. We assume that the mid-surface of the shell is given by $\displaystyle ψ(x_1,x_2)+\varepsilon^pθ\left(\frac{x_1}{\varepsilon},\frac{x_2}{\varepsilon}\right)\vect{a}_{3}(x_1,x_2)$, where $θ$ is $[0,1)^2$-periodic function and $p=2$. We also assume that the strain energy of the shell has the Koiter's model.

</details>


### [29] [Global $C^{1,α}$-Regularity for Musielak-Orlicz Equations in Divergence Form](https://arxiv.org/abs/2601.11495)
*Hlel Missaoui*

Main category: math.AP

TL;DR: Global C^{1,α} regularity established for bounded generalized solutions of elliptic equations with Musielak-Orlicz growth under Dirichlet/Neumann boundary conditions, extending previous results.


<details>
  <summary>Details</summary>
Motivation: To establish regularity results for elliptic equations with non-standard growth conditions (Musielak-Orlicz growth) that generalize and extend existing results for variable exponent spaces, Orlicz spaces, and (p,q) growth situations.

Method: Analysis of elliptic equations in divergence form with Musielak-Orlicz growth conditions, focusing on the interplay between non-standard growth conditions and boundary behavior for Dirichlet and Neumann boundary conditions.

Result: Proved global C^{1,α} regularity for bounded generalized solutions, establishing new conditions that connect non-standard growth with boundary behavior in generalized settings.

Conclusion: The paper successfully extends regularity theory to Musielak-Orlicz growth settings, providing a unified framework that encompasses several important special cases and reveals new conditions governing the interaction between growth conditions and boundary behavior.

Abstract: In this paper, we establish global $C^{1,α}$-regularity for bounded generalized solutions of elliptic equations in divergence form with Musielak-Orlicz growth and subject to Dirichlet or Neumann boundary conditions. In fact, our findings extend and generalize several important regularity results in cases of special attention such as variable exponent spaces, Orlicz spaces, and some $(p,q)$ situations. We also point out new conditions in the analysis that focus on the interplay between non-standard growth conditions and the boundary behavior in such generalized examples.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [30] [Learning collision operators from plasma phase space data using differentiable simulators](https://arxiv.org/abs/2601.10885)
*Diogo D. Carvalho,Pablo J. Bilbao,Warren B. Mori,Luis O. Silva,E. Paulo Alves*

Main category: physics.plasm-ph

TL;DR: A differentiable kinetic simulator with gradient-based optimization learns collision operators from plasma phase space data, outperforming particle-track methods and matching theoretical predictions.


<details>
  <summary>Details</summary>
Motivation: To develop a computational approach for inferring collision operators from plasma dynamics data without prior assumptions about time-scales, reducing memory requirements compared to traditional methods.

Method: Combines a differentiable kinetic simulator (differentiable Fokker-Planck solver) with gradient-based optimization to learn collision operators that best describe phase space dynamics from 2D Particle-in-Cell simulation data.

Result: Learned operators are more accurate than particle-track estimates, require no prior time-scale assumptions, significantly reduce memory requirements, and show excellent agreement with theoretical predictions in non-relativistic electrostatic scenarios.

Conclusion: Differentiable simulators provide a powerful and computationally efficient approach for inferring novel operators for various problems including electromagnetically dominated collisional dynamics and stochastic wave-particle interactions.

Abstract: We propose a methodology to infer collision operators from phase space data of plasma dynamics. Our approach combines a differentiable kinetic simulator, whose core component in this work is a differentiable Fokker-Planck solver, with a gradient-based optimisation method to learn the collisional operators that best describe the phase space dynamics. We test our method using data from two-dimensional Particle-in-Cell simulations of spatially uniform thermal plasmas, and learn the collision operator that captures the self-consistent electromagnetic interaction between finite-size charged particles over a wide variety of simulation parameters. We demonstrate that the learned operators are more accurate than alternative estimates based on particle tracks, while making no prior assumptions about the relevant time-scales of the processes and significantly reducing memory requirements. We find that the retrieved operators, obtained in the non-relativistic regime, are in excellent agreement with theoretical predictions derived for electrostatic scenarios. Our results show that differentiable simulators offer a powerful and computational efficient approach to infer novel operators for a wide rage of problems, such as electromagnetically dominated collisional dynamics and stochastic wave-particle interactions.

</details>


### [31] [Vortex Solitons and Filamentation of Electromagnetic Beams in Relativistically Degenerate Plasmas](https://arxiv.org/abs/2601.10855)
*Nikolai Maltsev,Vazha I. Berezhiani*

Main category: physics.plasm-ph

TL;DR: Electromagnetic vortex beams can form stable localized solitons in relativistically degenerate plasmas, carrying orbital angular momentum with topologically protected vortex cores that maintain zero intensity at the center.


<details>
  <summary>Details</summary>
Motivation: To understand how electromagnetic vortex beams propagate and maintain stability in dense astrophysical plasmas, particularly for hard X-ray radiation applications, and to investigate the formation and stability of vortex solitons in relativistically degenerate plasma environments.

Method: Studied propagation and stability of electromagnetic vortex beams in relativistically degenerate plasmas through analysis of localized vortex solitons carrying orbital angular momentum, examining both linear and nonlinear stability properties.

Result: Vortex solitons exist but undergo azimuthal symmetry-breaking instabilities with growth rates dependent on beam power, propagation constant, and topological charge. The dominant instability mode determines filament formation during breakup. Vortex solitons act as nonlinear attractors with finite basins of attraction, while vortex cores remain topologically protected with strictly zero field intensity at beam centers.

Conclusion: Electromagnetic vortex beams can form stable localized solitons in relativistically degenerate plasmas across a broad range of degeneracy parameters, with topologically protected vortex cores. These findings are relevant for hard X-ray radiation propagation in dense astrophysical plasmas.

Abstract: We study the propagation and stability of electromagnetic vortex beams in relativistically de generate plasmas. We show that such plasmas support localized vortex solitons carrying orbital angular momentum and analyze their linear and nonlinear stability. Vortex solitons undergo az imuthal symmetry-breaking instabilities whose growth rates depend on beam power, propagation constant, and topological charge, with the dominant mode determining the number of filaments formed during breakup. We further demonstrate that vortex solitons act as nonlinear attractors with a finite basin of attraction, while the vortex core remains topologically protected, maintaining a strictly zero field intensity at the beam center throughout the evolution. The results persist across a broad range of degeneracy parameters and are relevant to hard X-ray radiation propagating in dense astrophysical plasmas.

</details>


### [32] [Study of circular cross-section plasmas in HL-2A tokamak: MHD equilibrium, stability and operational \b{eta} limit](https://arxiv.org/abs/2601.11014)
*SHEN Yong,DONG Jiaqi,SHI Zhongbing,HE Hongda,ZHAO Kaijun,PENG Xiaodong,QU Hongpeng,LI Jia,SUN Aiping*

Main category: physics.plasm-ph

TL;DR: Study of MHD equilibrium and instability in circular cross-section tokamak plasmas on HL-2A, showing how safety factors and beta affect kink mode stability and establishing beta limits.


<details>
  <summary>Details</summary>
Motivation: Circular cross-section plasma is fundamental for magnetic confinement fusion. Understanding MHD equilibrium and instability in this basic configuration is crucial for tokamak operation and stability.

Method: Based on HL-2A limiter discharge experiments, investigating MHD equilibrium and instability through analysis of safety factors (q₀, qₐ), beta parameters, and their effects on kink modes.

Result: Internal kink mode (m/n=1/1) unstable at q₀=0.95; beta increase triggers external kink modes; qₐ>2 with q₀ slightly >1 stabilizes kink modes; maximum beta limit β(max)~2.01I_N; HL-2A operational beta limit β_N^c~2.0; higher q₀ reduces beta limit (β_N~1.8 at q₀=1.3).

Conclusion: Safety factors and beta parameters critically determine MHD stability in circular tokamak plasmas. Optimal q₀ near 1 provides best stability, while higher q₀ reduces achievable beta limits. The established beta scaling provides practical guidance for HL-2A operation.

Abstract: Circular cross-section plasma is the most basic form of tokamak plasma and the fundamental configuration for magnetic confinement fusion experiments. Based on the HL-2A limiter discharge experiments, the magnetohydrodynamic (MHD) equilibrium and MHD instability of circular cross-section tokamak plasmas are investigated in this work. The results show that when q_0=0.95, the internal kink mode of m/n=1/1 is always unstable. The increase in plasma \b{eta} (the ratio of thermal pressure to magnetic pressure) can lead to the appearance of external kink modes. The combination of axial safety factor q_0 and edge safety factor q_a determines the equilibrium configuration of the plasma and also affects the MHD stability of the equilibrium, but its growth rate is also related to the size of \b{eta}. Under the condition of q_a>2 and q_0 slightly greater than 1, the internal kink mode and surface kink mode can be easily stabilized. However the plasma becomes unstable again and the instability intensity increases as q_0 continues to increase when q_0 exceeds 1. As the poloidal beta (\b{eta}_p) increases, the MHD instability develops, the equilibrium configuration of MHD elongates laterally, and the Shafranov displacement increases, which in turn has the effect on suppressing instability. Calculations have shown that the maximum \b{eta} value imposed by the ideal MHD mode in a plasma with free boundary in tokamak experiments is proportional to the normalized current I_N (I_N=I_p (MA)/a(m)B_0 (T)), and the achievable maximum beta \b{eta}(max) is calibrated to be 2.01I_N,i.e. \b{eta}(max)~2.01I_N. The operational \b{eta} limit of HL-2A circular cross-section plasma is approximately \b{eta}_N^c~2.0. Too high a value of q_0 is not conducive to MHD stability and leads the \b{eta} limit value to decrease. When q_0=1.3, we obtain a maximum value of \b{eta}_N of approximately 1.8.

</details>


### [33] [A new class of special functions arising in plasma linear susceptibility tensor calculations](https://arxiv.org/abs/2601.11276)
*Roberto Ricci*

Main category: physics.plasm-ph

TL;DR: The paper introduces and analyzes a new class of special functions related to Bessel, Anger and Weber functions, originally motivated by plasma physics applications for linear susceptibility tensor calculations.


<details>
  <summary>Details</summary>
Motivation: The functions were originally motivated by linear susceptibility tensor calculations in hot, magnetized plasmas. There was a need for functions that could handle the mathematical complexities of plasma physics applications more efficiently.

Method: The authors show these functions are solutions of an inhomogeneous Bessel ODE with specific initial conditions. They derive recurrence relations, alternative representations using incomplete Anger-Weber functions, and a series expansion in terms of integer-order Bessel functions using the Jacobi-Anger formula. They also exploit recurrence properties to derive the linear susceptibility tensor more simply.

Result: The new functions admit a simple series expansion in terms of Bessel functions of integer order. In plasma applications, this leads to expressions involving infinite sums of Bessel function products that converge slowly for large gyro-radii. The recurrence properties enable a simpler derivation of the linear susceptibility tensor that avoids this convergence problem.

Conclusion: The paper establishes fundamental properties of these special functions and demonstrates their practical utility in plasma physics by providing a more efficient method for calculating linear susceptibility tensors, avoiding slow-convergence issues in numerical evaluations.

Abstract: We investigate some fundamental properties of a peculiar class of special functions strictly related to Bessel, Anger and Weber functions, whose introduction was originally motivated by linear susceptibility tensor calculations in a hot, magnetised plasma. We show that these functions are solutions of an inhomogeneous Bessel ODE, with specified initial conditions and a distinct right-hand-side term fulfilling the Nielsen's requirement. Beside deriving recurrence relations and an alternative representation involving incomplete Anger-Weber functions, we show that these functions admit a simple series expansion in terms of Bessel functions of integer order, obtained by resorting to the Jacobi-Anger formula. In plasma applications this eventually leads to expressions involving infinite sums of products of Bessel functions, not particularly apt to numerical evaluation ought to their slow convergence rate when the particle's gyro-radius is larger than the wavelength. By exploiting the previously determined recurrence properties of the new class of functions we present a particularly simple derivation of the linear susceptibility tensor that enables to avoid this inconvenience.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [34] [A numerical study on the effect of rolling friction on clogging of pores in particle-laden flows](https://arxiv.org/abs/2601.11121)
*Sagar G. Nayak,Zhenjiang You,Yuchen Dai,Geoff Wang,Prapanch Nair*

Main category: physics.flu-dyn

TL;DR: The paper studies how rolling resistance affects particle clogging in porous media using direct numerical simulations with DEM-IBM coupling.


<details>
  <summary>Details</summary>
Motivation: Rolling resistance in particle-fluid systems affects pore clogging but its direct influence is not well studied, despite being important for understanding permeability impairment in porous reservoirs.

Method: Developed a DEM library coupled with an open-source immersed boundary method (IBM) solver for pore and particle resolved direct numerical simulations (DNS) in 3D.

Result: Presented several 3D validations for the DEM library and DEM-IBM coupling, and studied the effect of rolling resistance on clogging at pore entry.

Conclusion: Rolling resistance significantly influences particle clogging behavior in porous media, which has important implications for understanding permeability reduction in reservoir engineering applications.

Abstract: Particulate matter in a fluid injected into a porous reservoir impairs its permeability spatio-temporally due to pore clogging. As particle volume fraction increases near the pore throats, inter-particle contact mechanics determine their jamming and subsequent pore clogging behavior. During contact of particles submerged in a fluid, in addition to sliding friction, a rolling resistance develops due to a several micromechanical and hydrodynamic factors. A coefficient of rolling friction is often used as a lumped parameter to characterize particle rigidity, particle shape, lubrication and fluid mediated resistance, however its direct influence on the clogging behavior is not well studied in literature. We study the effect of rolling resistance on the clogging behavior of a dense suspension at pore scale using direct numerical simulations (DNS). A discrete element method (DEM) library is developed and coupled with an open-source immersed boundary method (IBM) based solver to perform pore and particle resolved simulations. Several 3D validations are presented for the DEM library and the DEM-IBM coupling and the effect of rolling resistance on clogging at a pore entry is studied.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [35] [The rise and fall of stretched bond errors: Extending the analysis of Perdew-Zunger self-interaction corrections of reaction barrier heights beyond the LSDA](https://arxiv.org/abs/2601.11454)
*Yashpal Singh,Juan E Peralta,Koblar Alan Jackson*

Main category: physics.chem-ph

TL;DR: Self-interaction corrections improve DFT barrier height predictions; orbital analysis shows stretched bond orbitals near transition states are major contributors; XC/H ratio identifies self-interaction errors; SCAN functional may have reached best possible accuracy for semi-local functionals with Perdew-Zunger SIC.


<details>
  <summary>Details</summary>
Motivation: To understand how self-interaction corrections (SIC) improve chemical reaction barrier height predictions in density functional theory, and to identify which orbitals contribute most to these corrections through detailed orbital-by-orbital analysis.

Method: Used Fermi-Löwdin Orbital Self-Interaction Correction calculations on three semi-local density functional approximations from different rungs of Jacob's Ladder. Analyzed four representative reactions from BH76 benchmark set along reaction pathways (reactants → transition state → products). Introduced XC/H ratio (self-exchange-correlation energy to self-Hartree energy) as indicator of one-electron self-interaction error.

Result: Major contribution to SIC of barrier heights comes from stretched bond orbitals near transition state configurations. XC/H ratio varies significantly for different orbitals in practical DFAs, with largest values for stretched or strongly lobed orbitals. Significant differences in XC/H for corresponding orbitals in reactant, transition state, and product configurations identify major contributors to SIC corrections.

Conclusion: SCAN meta-generalized gradient approximation may have attained the best accuracy possible for a semi-local functional using Perdew-Zunger SIC approach for barrier height predictions. XC/H ratio serves as useful diagnostic tool for identifying orbitals with significant self-interaction errors.

Abstract: Incorporating self-interaction corrections (SIC) significantly improves chemical reaction barrier height predictions made using density functional theory methods. We present a detailed, orbital-by-orbital analysis of these corrections for three semi-local density functional approximations (DFAs) situated on the three lowest rungs of the Jacob's Ladder of approximations. The analysis is based on Fermi-Löwdin Orbital Self-Interaction Correction calculations performed at several steps along the reaction pathway from the reactants (R) to the transition state (TS) to the products (P) for four representative reactions selected from the BH76 benchmark set. For all three functionals, the major contribution to self-interaction corrections of the barrier heights can be traced to stretched bond orbitals that develop near the TS configuration. The magnitude of the ratio of the self-exchange-correlation energy to the self-Hartree energy (XC/H) for a given orbital is introduced as an indicator of one-electron self-interaction error. For the exact, but unknown density functional, XC/H = 1.0 for all orbitals, while for the practical DFAs studied here, XC/H spans a range of values. The largest values are obtained for stretched or strongly lobed orbitals. We show that significant differences in XC/H for corresponding orbitals in the R, TS, and P configurations can be used to identify the major contributors to the SIC of barrier heights and reaction energies. Based on such comparisons, we suggest that barrier height predictions made using the SCAN meta-generalized gradient approximation may have attained the best accuracy possible for a semi-local functional using the Perdew-Zunger SIC approach.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [36] [Specular differentiation in normed vector spaces and its application to nonsmooth convex optimization](https://arxiv.org/abs/2601.10950)
*Kiyuob Jung*

Main category: math.OC

TL;DR: Specular differentiation generalizes classical differentiation concepts and enables optimization of nonsmooth convex functions where traditional methods fail.


<details>
  <summary>Details</summary>
Motivation: To develop a more general differentiation framework that can handle nonsmooth convex functions in normed vector spaces, addressing limitations of classical Gâteaux and Fréchet differentiation.

Method: Introduces specular differentiation as a generalization of Gâteaux and Fréchet differentiation, establishes theoretical properties (Quasi-Mean Value Theorem, Quasi-Fermat's Theorem), and develops three numerical optimization methods using this framework.

Result: Numerical experiments show the proposed methods successfully minimize non-differentiable functions that classical optimization methods cannot handle.

Conclusion: Specular differentiation provides a powerful theoretical framework and practical numerical methods for optimizing nonsmooth convex functions in higher-dimensional spaces.

Abstract: This paper introduces specular differentiation, which generalizes Gâteaux and Fréchet differentiation in normed vector spaces. Fundamental theoretical properties of specular differentiation are investigated, including the Quasi-Mean Value Theorem and Quasi-Fermat's Theorem. As an application, three numerical methods using specular differentiation are devised to optimize nonsmooth convex functions in higher-dimensional Euclidean spaces. Numerical experiments demonstrate that the proposed methods are capable of minimizing non-differentiable functions that classical methods fail to minimize.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [37] [Towards precision quantitative measurement of radiation reaction within the classical radiation-dominated regime](https://arxiv.org/abs/2601.10728)
*Minghao Ma,Ke Liu,Ge Zhou,Zhida Yang,Yulin Xin,Jiadong Yang,Pengfei Zhu,Yipeng Wu,Min Chen,Tongpu Yu,Wenchao Yan,Jie Zhang*

Main category: physics.optics

TL;DR: Proposes experimental setup using petawatt laser colliding with MeV electron beam to study radiation reaction in classical radiation-dominated regime, with three key observables for validation.


<details>
  <summary>Details</summary>
Motivation: Radiation reaction is a fundamental but incompletely validated process in laser-particle interactions, lacking definitive experimental verification, especially for the transition from classical to quantum regimes.

Method: Collision of high-intensity petawatt-class laser with tens-of-MeV electron beam from a LINAC to access classical radiation-dominated regime where radiation reaction dominates but quantum effects remain modest.

Result: Numerical simulations show three key observables: (1) energy spectra measurement for quantum correction factor validation, (2) collision time delay control with charge-counting for intensity dependence mapping, (3) verification of 90° photon emission under recoil condition 2γ ≳ a₀.

Conclusion: These experimental measurements will establish benchmarks for radiation reaction models spanning classical-to-quantum regime, providing critical insights into fundamental strong-field quantum electrodynamics.

Abstract: Radiation reaction (RR) is a fundamental yet incompletely validated process in laser-particle interactions, since it lacks quantitatively definitive experimental verifications, especially the transition from classical to quantum regime. Herein, we propose a novel experimental scenario for investigating radiation RR within the classical radiation-dominated regime (CRDR), via the collision of a high-intensity petawatt-class laser with a tens-of-MeV electron beam from a LINAC. This approach enables access to a distinct parameter regime wherein RR dominates electron dynamics while quantum effects remain modest. Numerical simulations demonstrate that three key observables exist for identifying the RR within this CRDR regime: (i) quantitative measurement of energy spectra to validate the quantum correction factor; (ii) control of the collision time delay with charge-counting to map intensity dependence of RR; and (iii) verification of large angle ($90^\circ$) photon emission under the recoil condition $2γ\gtrsim a_0$. These experimental measurements will establish the benchmarks for RR models spanning the classical-to-quantum regime, thereby providing critical insights into fundamental strong-field quantum electrodynamics.

</details>


### [38] [Generation and Enhancement of Persistent Nanoscale Magnetization in All-Dielectric Metasurfaces by Optically Injected and Localized Free Carriers](https://arxiv.org/abs/2601.11003)
*Shivaksh Rawat,Samyobrata Mukherjee,Gennady Shvets*

Main category: physics.optics

TL;DR: Time-varying dielectric metasurfaces enable frequency conversion and temporal scattering of metasurface-guided waves, generating quasistatic magnetic fields and residual magnetization.


<details>
  <summary>Details</summary>
Motivation: To explore how time-varying dielectric metasurfaces with sharp optical resonances can serve as temporal interfaces for manipulating metasurface-guided waves and generating novel electromagnetic phenomena.

Method: Using analytical methods and electromagnetic simulations to study localized free-carrier generation in metasurfaces, enabling frequency conversion and temporal scattering of infrared metasurface-guided waves.

Result: Demonstrated generation of frequency-shifted, time-refracted, and reflected infrared metasurface-guided waves, along with creation of large, highly localized quasistatic magnetic fields and residual nanoscale magnetization that persists after wave departure.

Conclusion: Time interfaces in metasurfaces enable partitioning of electromagnetic energy between scattered waves, free carrier motion, and quasistatic magnetic fields, offering new possibilities for dynamic wave manipulation and magnetic field generation at nanoscale.

Abstract: Time-varying dielectric metasurfaces supporting sharp optical resonances with a non-trivial electromagnetic field distribution represent a unique platform for realizing temporal interfaces for metasurface-guided waves (MGWs). Rapidly changing metasurface resonance enables frequency conversion and temporal scattering of a concurrently propagating MGW. Using analytical methods and electromagnetic simulations, we demonstrate that localized free-carrier generation can be engineered to produce frequency-shifted, time-refracted, and reflected infrared MGWs. Furthermore, we demonstrate that such time interfaces can be utilized to generate large, highly localized quasistatic magnetic fields within the metasurfaces. The resulting nanoscale magnetization, supported by the residual circulating currents, persists after the departure of the time-scattered MGWs. We further demonstrate that the initial electromagnetic energy of the injected MGWs is partitioned between the time-reflected/refracted MGWs, residual motion of the free carriers, and a quasistatic magnetic field.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [39] [Temporal Complexity and Self-Organization in an Exponential Dense Associative Memory Model](https://arxiv.org/abs/2601.11478)
*Marco Cafiso,Paolo Paradisi*

Main category: nlin.AO

TL;DR: Dense Associative Memory models with exponential interactions show self-organizing temporal complexity characterized by intermittent neural avalanches and scale-free statistics, emerging in noise intensity ranges rather than at single critical points.


<details>
  <summary>Details</summary>
Motivation: While Dense Associative Memory models have been studied for storage capacity, their temporal self-organizing behavior during learning has received little attention. The authors aim to investigate this temporal complexity using a stochastic exponential DAM model.

Method: Analyzed a stochastic exponential DAM (SEDAM) model using Temporal Complexity framework, examining transition events in neural avalanche structures and coincidence structures. Systematically explored dependence on noise intensity and memory load parameters.

Result: SEDAM exhibits complex intermittency with nontrivial temporal correlations and scale-free behavior, indicating spontaneous self-organizing dynamics. These regimes emerge in small noise intensity intervals (not single critical points), with critical region noise range decreasing slightly as memory load increases.

Conclusion: Temporal Complexity provides a complementary framework for understanding learning in neural systems, revealing links between memory load and self-organizing capacity. The study supports extended criticality concept where complex dynamics emerge in parameter ranges rather than at isolated critical points.

Abstract: Dense Associative Memory (DAM) models generalize the classical Hopfield model by incorporating n-body or exponential interactions that greatly enhance storage capacity. While the criticality of DAM models has been largely investigated, mainly within a statistical equilibrium picture, little attention has been devoted to the temporal self-organizing behavior induced by learning. In this work, we investigate the behavior of a stochastic exponential DAM (SEDAM) model through the lens of Temporal Complexity (TC), a framework that characterizes complex systems by intermittent transition events between order and disorder and by scale-free temporal statistics. Transition events associated with birth-death of neural avalanche structures are exploited for the TC analyses and compared with analogous transition events based on coincidence structures. We systematically explore how TC indicators depend on control parameters, i.e., noise intensity and memory load. Our results reveal that the SEDAM model exhibits regimes of complex intermittency characterized by nontrivial temporal correlations and scale-free behavior, indicating the spontaneous emergence of self-organizing dynamics. These regimes emerge in small intervals of noise intensity values, which, in agreement with the extended criticality concept, never shrink to a single critical point. Further, the noise intensity range needed to reach the critical region, where self-organizing behavior emerges, slightly decreases as the memory load increases. This study highlights the relevance of TC as a complementary framework for understanding learning and information processing in artificial and biological neural systems, revealing the link between the memory load and the self-organizing capacity of the network.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [40] [On spectral interference of the short-time Fourier transform and its nonlinear variations](https://arxiv.org/abs/2601.10910)
*Shrikant Chand,James Nolen,Hau-Tieng Wu*

Main category: math.CA

TL;DR: The paper analyzes spectral interference in time-frequency representations, focusing on STFT with Gaussian windows and reassignment methods like synchrosqueezing. It quantifies frequency resolution limits, describes interference patterns, and develops new frameworks to mitigate distortion.


<details>
  <summary>Details</summary>
Motivation: Spectral interference severely distorts time-frequency representations in physical applications, creating misleading frequency estimates. Understanding when and why this occurs is crucial for accurate signal analysis.

Method: Theoretical analysis of STFT with Gaussian windows and nonlinear refinements (reassignment, synchrosqueezing) using a two-component harmonic model. Methods include asymptotic analysis, Bargmann transform connections, Möbius geometry, and measure mapping perspectives.

Result: Identifies critical frequency gap threshold for STFT resolution, describes interference bubble patterns, characterizes phase winding behavior, connects reassignment to holomorphic structures, and develops generalized synchrosqueezing framework to mitigate interference.

Conclusion: The paper provides fundamental understanding of spectral interference phenomena, quantifies resolution limits, and offers new theoretical frameworks that explain when reassignment methods work effectively versus when they produce misleading representations.

Abstract: Spectral interference, the frequency counterpart of the beating phenomenon in the time domain, can severely distort time-frequency representations (TFRs) in physical applications. We study this phenomenon for the short-time Fourier transform (STFT) with a Gaussian window and for nonlinear refinements based on the reassignment method, with an emphasis on the synchrosqueezing transform (SST). Working with a two-component harmonic model, we quantify when STFT can (and cannot) resolve two nearby frequencies: a sharp transition occurs at a critical gap that scales inversely to kernel bandwidth and depends explicitly on the amplitude ratio. Below this threshold, the spectrogram ridges undergo bifurcation and form repeating time-frequency bubbles, which we describe asymptotically and, in the balanced-amplitude case, approximate closely by ellipses. We then analyze the STFT phase, showing a canonical winding behavior, and relate the complex-valued SST reassignment map to a holomorphic structure via the Bargmann transform. In the two-component setting the reassignment rule admits an explicit Mobius-geometry description, sending frequency lines to circular arcs in the complex plane. Finally, viewing SST and reassignment through a measure mapping perspective, we derive small-kernel asymptotics that explain when reassignment sharpens energy and when it produces distorted or misleading TFRs; we also introduce a generalized synchrosqueezing framework that isolates the role of STFT weighting and clarifies how alternative choices can mitigate interference in certain regimes.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [41] [Walk based Laplacians for Modeling Diffusion on Complex Networks](https://arxiv.org/abs/2601.11338)
*Francesca Arrigo,Fabio Durastante*

Main category: cs.SI

TL;DR: The paper introduces a novel framework for modeling diffusion on complex networks using walk-based Laplacian operators that incorporate memory effects by excluding or downweighting backtracking trajectories.


<details>
  <summary>Details</summary>
Motivation: Standard Laplacian operators for network diffusion modeling don't account for memory effects like backtracking, where walkers immediately revisit nodes. There's a need for more flexible operators that can incorporate such memory effects while maintaining computational efficiency for large networks.

Method: Develop a parametric family of walk-based Laplacians: (1) standard walk-based Laplacians counting all traversals, (2) nonbacktracking variants eliminating immediate reversals, and (3) backtrack-downweighted variants providing continuous interpolation between regimes. Use Krylov subspace methods for efficient computation and GPU acceleration.

Result: The operators extend standard Laplacian definitions while preserving some properties. Efficient algorithms enable application to large networks. Numerical experiments on real-world networks validate modeling flexibility and demonstrate computational efficiency with GPU acceleration.

Conclusion: The proposed framework provides a flexible approach to modeling diffusion with memory effects on complex networks, with efficient computational methods that scale to large networks through GPU acceleration.

Abstract: We develop a novel framework for modeling diffusion on complex networks by constructing Laplacian-like operators based on walks around a graph. Our approach introduces a parametric family of walk-based Laplacians that naturally incorporate memory effects by excluding or downweighting backtracking trajectories, where walkers immediately revisit nodes. The framework includes: (i) walk-based Laplacians that count all traversals in the network; (ii) nonbacktracking variants that eliminate immediate reversals; and (iii) backtrack-downweighted variants that provide a continuous interpolation between these two regimes. We establish that these operators extend the definition of the standard Laplacian and also preserve some of its properties. We present efficient algorithms using Krylov subspace methods for computing them, ensuring applicability of our proposed framework to large networks. Extensive numerical experiments on real-world networks validate the modeling flexibility of our approach and demonstrate the computational efficiency of the proposed algorithms, including GPU acceleration.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [42] [NAVIS: A LAMMPS-Python framework for efficient computation of nanochannel velocity and thermal interfacial slip](https://arxiv.org/abs/2601.11391)
*Sleeba Varghese,Sobin Alosious,Jesper Schmidt Hansen,Billy Dean Todd*

Main category: cond-mat.soft

TL;DR: NAVIS is a LAMMPS-Python toolkit for computing hydrodynamic friction and thermal resistance at solid-fluid interfaces using equilibrium molecular dynamics.


<details>
  <summary>Details</summary>
Motivation: To provide computational researchers with a practical toolkit for studying interfacial friction and thermal transport in nanofluidic systems, which are key factors for efficient applications.

Method: Equilibrium molecular dynamics (EMD) methods using LAMMPS package with Python post-processing to compute Navier friction coefficient and Kapitza thermal resistance at solid-fluid interfaces.

Result: A pedagogical framework and toolkit implementation demonstrated on two systems: water-graphene for hydrodynamic slip and water-CNT for thermal slip, with detailed instructions for simulation and analysis.

Conclusion: NAVIS provides a useful computational toolkit for researchers studying interfacial phenomena in nanofluidic systems, enabling systematic investigation of friction and thermal transport at solid-fluid interfaces.

Abstract: We present NAVIS (NAnochannel Velocity and thermal Interfacial Slip), a LAMMPS-Python scripted toolkit for computing the Navier (hydrodynamic) friction coefficient and Kapitza (thermal) resistance at arbitrary solid-fluid interfaces. NAVIS is based on equilibrium molecular dynamics (EMD) methods for calculating the linear response friction and thermal resistance at the interface, as well as the corresponding velocity and temperature slips. The methodology is based on our previous studies (Hansen, et al., Phys. Rev. E 84, 016313 (2011); Varghese et al., J. Chem. Phys. 154, 184707 (2021); Alosious, et al., J. Chem. Phys. 151, 194502 (2019); Alosious, et al., Langmuir 37, 2355-2361 (2021)), and in this work we provide a pedagogical framework for the implementation of this toolkit on two systems: (i) a water-graphene system (for hydrodynamic slip) and (ii) a water-CNT system (for thermal slip). We provide detailed instructions for performing the EMD simulations using the LAMMPS package and processing the simulation outputs using Python modules to obtain the desired quantities of interest. We expect the toolkit to be useful for computational researchers studying interfacial friction and thermal transport, key factors for efficient and practical applications of nanofluidic systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs](https://arxiv.org/abs/2601.11259)
*Lorenzo Tomada,Federico Pichi,Gianluigi Rozza*

Main category: cs.LG

TL;DR: LD-GCN is a novel encoder-free graph neural network architecture for model order reduction of parameterized PDEs that learns interpretable latent dynamics while preserving geometric information.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based MOR methods fail to combine geometric inductive biases with interpretable latent behavior, either overlooking dynamics-driven features or disregarding spatial information.

Method: Proposes Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns global low-dimensional representations conditioned on inputs/parameters, models temporal evolution in latent space via time-stepping, and decodes trajectories onto geometrically parameterized domains using GNNs.

Result: Mathematically validated via universal approximation theorem for encoder-free architectures; numerically tested on complex computational mechanics problems with physical/geometric parameters, including detection of bifurcating phenomena for Navier-Stokes equations.

Conclusion: LD-GCN successfully addresses the gap by combining geometric inductive biases with interpretable latent dynamics, enabling time-extrapolation, zero-shot prediction through latent interpolation, and enhanced interpretability of reduced dynamics.

Abstract: Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking dynamics-driven features or disregarding spatial information. In this work, we address this gap by introducing Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns a global, low-dimensional representation of dynamical systems conditioned on external inputs and parameters. The temporal evolution is modeled in the latent space and advanced through time-stepping, allowing for time-extrapolation, and the trajectories are consistently decoded onto geometrically parameterized domains using a GNN. Our framework enhances interpretability by enabling the analysis of the reduced dynamics and supporting zero-shot prediction through latent interpolation. The methodology is mathematically validated via a universal approximation theorem for encoder-free architectures, and numerically tested on complex computational mechanics problems involving physical and geometric parameters, including the detection of bifurcating phenomena for Navier-Stokes equations. Code availability: https://github.com/lorenzotomada/ld-gcn-rom

</details>


### [44] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: A paired autoencoder framework for solving inverse problems with observational inconsistencies, using learned latent space mappings to enable regularized inversion and handle partial/noisy/OOD data while maintaining physical consistency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving inverse problems with observational inconsistencies (partial, noisy, or out-of-distribution data) while maintaining consistency with underlying physical models, which traditional methods struggle with.

Method: Two autoencoders (one for parameter space, one for observation space) connected by learned mappings between their latent spaces. This enables surrogate regularized inversion and optimization in low-dimensional latent spaces, with ability to reconstruct corrupted data before parameter estimation.

Result: Produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of same architecture, especially in scenarios with data inconsistencies. Demonstrated on medical tomography and geophysical seismic-waveform inversion.

Conclusion: The paired autoencoder framework provides a flexible, data-driven approach for solving inverse problems with observational inconsistencies while maintaining physical consistency, broadly applicable to various scientific and engineering applications.

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>
