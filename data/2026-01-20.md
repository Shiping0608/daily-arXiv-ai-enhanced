<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 19]
- [math.AP](#math.AP) [Total: 10]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [math.OC](#math.OC) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.CA](#math.CA) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [High-Order Lie Derivatives from Taylor Series in the ADTAYL Package](https://arxiv.org/abs/2601.10828)
*Nedialko S. Nedialkov,John D. Pryce*

Main category: math.NA

TL;DR: Efficient numerical method for computing high-order Lie derivatives using MATLAB ADTAYL package, achieving orders of magnitude speedup over symbolic evaluation.


<details>
  <summary>Details</summary>
Motivation: High-order Lie derivatives are crucial for nonlinear systems analysis, but symbolic evaluation becomes increasingly expensive as order increases, creating computational bottlenecks.

Method: Numerical approach using MATLAB ADTAYL package that exploits Röbenack's observation: Lie derivatives coincide (up to factorial scaling) with Taylor coefficients of expressions built from Taylor expansion about trajectory points and associated variational matrices.

Result: Demonstrated orders of magnitude speedups over symbolic evaluation using MATLAB Symbolic Math Toolbox, tested on a gantry crane model.

Conclusion: The presented numerical method provides a compact and efficient alternative to symbolic evaluation for computing high-order Lie derivatives, significantly reducing computational cost while maintaining accuracy.

Abstract: High-order Lie derivatives are essential in nonlinear systems analysis. If done symbolically, their evaluation becomes increasingly expensive as the order increases. We present a compact and efficient numerical approach for computing Lie derivatives of scalar, vector, and covector fields using the MATLAB ADTAYL package. The method exploits a fact noted by Röbenack: that these derivatives coincide, up to factorial scaling, with the Taylor coefficients of expressions built from a Taylor expansion about a trajectory point and, when required, the associated variational matrix. Computational results for a gantry crane model demonstrate orders of magnitude speedups over symbolic evaluation using the MATLAB Symbolic Math Toolbox.

</details>


### [2] [Qualitative reconstruction methods for imaging interior Robin interfaces in EIT from Robin-to-Dirichlet data](https://arxiv.org/abs/2601.10839)
*Rafael Ceja Ayala,Malena I. Español,Govanni Granados*

Main category: math.NA

TL;DR: Developed qualitative reconstruction methods for EIT inverse shape problems using Robin transmission conditions and RtD map data, enabling identification of interior defects through LSM and RFM with numerical implementation.


<details>
  <summary>Details</summary>
Motivation: Address inverse shape problems in electrical impedance tomography for nondestructive testing, where interior defects are modeled with Robin transmission conditions, requiring new reconstruction methods for this non-classical formulation.

Method: Developed qualitative (non-iterative) reconstruction methods based on Linear Sampling Method (LSM) and Regularized Factorization Method (RFM) using Robin-to-Dirichlet (RtD) map data with Robin boundary conditions on both exterior and interior surfaces.

Result: Derived new analytical characterizations enabling identification of interior regions, proposed numerical implementation with regularization strategies, and demonstrated through experiments that methods reliably reconstruct interior regions of interest.

Conclusion: Successfully developed and validated qualitative reconstruction methods for EIT inverse shape problems with Robin transmission conditions, providing effective tools for nondestructive testing applications.

Abstract: We consider an inverse shape problem arising in electrical impedance tomography (EIT) for nondestructive testing, in which interior defects are modeled through Robin transmission conditions. Unlike classical formulations, we impose Robin boundary conditions on both the exterior measurement surface and the interior interface, and use the Robin-to-Dirichlet (RtD) map as the available data. Within this setting, we develop qualitative (non-iterative) reconstruction methods based on the Linear Sampling Method (LSM) and the Regularized Factorization Method (RFM), and derive new analytical characterizations that enable these methods to identify interior regions. We further propose a numerical implementation that incorporates regularization strategies and demonstrate, through experiments, that the methods reliably reconstruct interior regions of interest.

</details>


### [3] [A Structure-Preserving Scheme for the Time-Dependent Ginzburg-Landau Model with BCS Gap Coupling](https://arxiv.org/abs/2601.10887)
*Boyi Wang,Saurav Shenoy,Daniel Fortino,Long-Qing Chen,Wenrui Hao*

Main category: math.NA

TL;DR: A structure-preserving numerical scheme for a hybrid superconducting model coupling TDGL vortex dynamics with BCS gap equation, enabling stable simulations of vortex formation and superconductivity suppression under magnetic fields.


<details>
  <summary>Details</summary>
Motivation: To extend the applicability of existing TDGL models beyond near-critical temperatures by coupling with BCS theory, while addressing computational challenges of the nonlinear coupled system for reliable vortex dynamics simulations.

Method: Developed a maximum bound preserving, energy-stable implicit-explicit (IMEX) scheme for the hybrid TDGL-BCS model, with rigorous establishment of structure-preserving properties ensuring long-time stability and physical consistency.

Result: The hybrid model successfully captures temporal and spatial vortex formation/alignment and superconductivity suppression under increasing magnetic fields in 2D/3D simulations, demonstrating accuracy and robustness of the computational approach.

Conclusion: The proposed structure-preserving scheme enables stable and reliable simulations of superconducting vortex dynamics across temperature regimes, extending TDGL applicability while maintaining physical consistency through rigorous numerical properties.

Abstract: We propose a structure-preserving scheme for a hybrid model that couples the time-dependent Ginzburg-Landau (TDGL) equation of superconducting vortex dynamics and the nonlinear Bardeen-Cooper-Schrieffer (BCS) gap equation. This formulation is consistent with the classical TDGL equation in the near-critical temperature, while extending the applicability of the existing TDGL model to regimes beyond the critical temperature. The resulting system poses significant computational challenges due to its nonlinear and coupled structure. To achieve stable and reliable simulations of the vortex dynamics and accompanying morphological transitions, we develop a maximum bound preserving, energy-stable implicit-explicit (IMEX) scheme. The structure-preserving properties of the scheme are rigorously established, ensuring long-time stability and physical consistency. Through two- and three-dimensional simulations, the hybrid model successfully captures the temporal and spatial formation and alignment of vortices and the suppression of superconductivity under increasing magnetic fields, demonstrating both the accuracy and robustness of the proposed computational approach.

</details>


### [4] [A Non-compact Positivity-Preserving Scheme for Parabolic PDE via Conditional Expectation](https://arxiv.org/abs/2601.10977)
*Haoran Xu,Jie Ren,Xingye Yue*

Main category: math.NA

TL;DR: Novel non-compact positivity-preserving scheme for linear non-divergence parabolic equations using Feynman-Kac formula and wide stencil approximations, with robust boundary treatments achieving various convergence rates.


<details>
  <summary>Details</summary>
Motivation: Classical schemes often fail for anisotropic diffusion with mixed derivatives unless covariance matrix is diagonally dominated. Existing methods like BZ and semi-Lagrangian schemes suffer from accuracy loss at boundaries.

Method: Based on Feynman-Kac formula, solution expressed as conditional expectation of associated diffusion process. Uses wide stencil scheme instead of compact Markov chain approximations. Different boundary treatments: quad-tree stopping time schemes for Dirichlet (O(Δt¹ᐟ²) and O(Δt)), discrete specular reflection for Neumann (O(Δt¹ᐟ²)), modular wrapping for periodic (O(Δt)).

Result: Schemes are unconditionally stable and positive preserving. Numerical experiments confirm predicted L∞ convergence rates for all boundary conditions. Non-compact stencil leads to time step constraint Δt ∼ h.

Conclusion: Proposed framework provides effective treatment for anisotropic diffusion with mixed derivatives, overcoming limitations of classical schemes. Robust boundary handling avoids accuracy loss seen in existing methods, with explicit schemes (except uniform stopping time) that maintain positivity and stability.

Abstract: We propose a novel non-compact, positivity-preserving scheme for linear non-divergence form parabolic equations. Based on the Feynman-Kac formula, the solution is expressed as a conditional expectation of an associated diffusion process. Instead of using compact Markov chain approximations, we employ a wide stencil scheme to approximate the conditional expectation, ensuring consistency and positivity preservation. This method is effective for anisotropic diffusion with mixed derivatives, where classical schemes often fail unless the covariance matrix is diagonally dominated.
  A key feature of our framework is its robust treatment of boundary conditions, which avoids the accuracy loss commonly encountered in BZ and semi-Lagrangian schemes. For Dirichlet boundaries, we introduce (i) a quad-tree non-uniform stopping time scheme with O($Δt^{1/2}$) accuracy and (ii) a quad-tree uniform stopping time scheme with O($Δt$) accuracy. For Neumann boundaries, we use discrete specular reflection with O($Δt^{1/2}$) convergence, while periodic boundaries are treated using modular wrapping, achieving O($Δt$) accuracy. All analyses are conducted under the practical scaling $Δt \sim h$.
  Except for the uniform stopping time scheme, all schemes are explicit. The schemes are unconditionally stable and positive preserving, thanks to the probabilistic structure. To ensure consistency, a non-compact stencil is involved, which leads to the large time step constraint $Δt \sim h$. Numerical experiments confirm the predicted $L^\infty$ convergence rates for all types of boundary conditions.

</details>


### [5] [A model order reduction based adaptive parareal method for time-dependent partial differential equations](https://arxiv.org/abs/2601.10981)
*Xiaoying Dai,Miao Hu,Shuwei Shen*

Main category: math.NA

TL;DR: Proposes an adaptive parareal method using model order reduction to construct coarse propagators dynamically for time-dependent PDEs.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of parallel-in-time methods for long-term evolution problems by adaptively constructing coarse propagators using data from fine propagators.

Method: Combines parareal method with model order reduction: uses fine propagator data from each iteration to adaptively construct coarse propagators via MOR techniques.

Result: Applied to 3D advection-diffusion equations with Kolmogorov and ABC flows, showing good performance for long-term evolution problems.

Conclusion: The adaptive parareal method with model order reduction effectively simulates long-term time-dependent PDEs with improved efficiency.

Abstract: In this paper, we propose a model order reduction based adaptive parareal method for time-dependent partial differential equations. By using the data obtained by the fine propagator in each iteration of the plain parareal method together with some model order reduction technique, we construct the coarse propagator adaptively in each parareal iteration, and then obtain our adaptive parareal method. We apply this new method to solve some 3D time-dependent advection-diffusion equations with the Kolmogorov flow and the ABC flow. Numerical results show the good performance of our method in simulating long-term evolution problems.

</details>


### [6] [Exact Constraint Enforcement in Physics-Informed Extreme Learning Machines using Null-Space Projection Framework](https://arxiv.org/abs/2601.10999)
*Rishi Mishra,Smriti,Balaji Srinivasan,Sundararajan Natarajan,Ganapathy Krishnamurthi*

Main category: math.NA

TL;DR: NP-PIELM enforces boundary conditions exactly via null-space projection, eliminating penalty weights and preserving single-shot training efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional PIELMs use penalty terms for boundary conditions, resulting in approximate satisfaction that is sensitive to weight choices and propagates errors into the solution interior.

Method: Null-Space Projected PIELM (NP-PIELM) achieves exact constraint enforcement through algebraic projection in coefficient space, exploiting the geometric structure of the admissible coefficient manifold via null space decomposition of the boundary operator.

Result: The method eliminates penalty coefficients, dual variables, and problem-specific constructions while maintaining single-shot training efficiency. Numerical experiments validate the framework on elliptic and parabolic problems with complex geometries and mixed boundary conditions.

Conclusion: NP-PIELM provides a robust framework for exact boundary condition enforcement in physics-informed extreme learning machines, overcoming limitations of penalty-based approaches while preserving computational efficiency.

Abstract: Physics-informed extreme learning machines (PIELMs) typically impose boundary and initial conditions through penalty terms, yielding only approximate satisfaction that is sensitive to user-specified weights and can propagate errors into the interior solution. This work introduces Null-Space Projected PIELM (NP-PIELM), achieving exact constraint enforcement through algebraic projection in coefficient space. The method exploits the geometric structure of the admissible coefficient manifold, recognizing that it admits a decomposition through the null space of the boundary operator. By characterizing this manifold via a translation-invariant representation and projecting onto the kernel component, optimization is restricted to constraint-preserving directions, transforming the constrained problem into unconstrained least-squares where boundary conditions are satisfied exactly at discrete collocation points. This eliminates penalty coefficients, dual variables, and problem-specific constructions while preserving single-shot training efficiency. Numerical experiments on elliptic and parabolic problems including complex geometries and mixed boundary conditions validate the framework.

</details>


### [7] [B-spline-Based ALE-MFS Framework for Evolving Domains](https://arxiv.org/abs/2601.11041)
*Muhammad Ammad,Leevan Ling,Shu Ma*

Main category: math.NA

TL;DR: A B-spline based arbitrary Lagrangian-Eulerian method of fundamental solutions (ALE-MFS) for curvature-driven motion of 2D evolving domains, using meshless MFS for interior velocity computation and adaptive B-spline boundary reconstruction, with a posteriori error estimates and superior mesh quality compared to classical FEM approaches.


<details>
  <summary>Details</summary>
Motivation: To develop a robust, meshless method for simulating curvature-driven motion of evolving domains that avoids volumetric meshing, handles strongly nonconvex shapes and large deformations, and provides high-quality moving meshes for ALE-finite element methods.

Method: ALE-MFS combines arbitrary Lagrangian-Eulerian framework with method of fundamental solutions: boundary points track material motion, interior velocities computed via harmonic extension using MFS with sources on fixed auxiliary circle, boundary normals/curvature reconstructed with adaptive local B-spline scheme, and error estimation via hatmatrix LOOCV for both square collocation and zero-padded least-squares systems.

Result: Square collocation works for moderately complex geometries, while zero-padded least-squares significantly improves interior velocity regularity and pointwise transport accuracy for strongly nonconvex shapes. The method generates higher-quality moving meshes with larger minimum angles and slower mesh ratio growth than classical FEM mesh-motion strategies.

Conclusion: The ALE-MFS algorithm provides a practical, easily integrable alternative for challenging moving-interface simulations, offering robust handling of complex geometries, meshless computation, and superior mesh quality compared to traditional approaches.

Abstract: We develop and analyze a B-spline based arbitrary Lagrangian-Eulerian method of fundamental solutions (ALE-MFS) for curvature-driven motion of two-dimensional evolving domains. Boundary points move with the material to track the geometric flow, while interior points move within an ALE framework via a harmonic extension of the boundary velocity, computed by a meshless MFS with sources on a fixed auxiliary circle, thus avoiding volumetric meshing. Boundary normals and curvature are reconstructed by an adaptive local B-spline scheme that remains robust for strongly nonconvex shapes and large deformations. A posteriori error estimates are obtained from a hatmatrix formulation of leave-one-out cross-validation (LOOCV) for both square collocation and zero-padded least-squares systems, and are complemented by maximum principle indicators for harmonic problems. Numerical experiments on circular, star-shaped, and amoeba-like domains show that square collocation suffices for moderately complex geometries, while zero-padded least-squares significantly improves interior velocity regularity and pointwise transport accuracy for strongly nonconvex shapes, without altering the source or collocation sets. The ALE-MFS algorithm also generates high-quality moving meshes for ALE-finite element methods, with larger minimum angles and slower mesh ratio growth than classical FEM mesh-motion strategies, suggesting a practical and easily integrable alternative for challenging moving-interface simulations.

</details>


### [8] [An Adaptive Lagrangian B-Spline Framework for Point Cloud Manifold Evolution](https://arxiv.org/abs/2601.11051)
*Muhammad Ammad,Leevan Ling*

Main category: math.NA

TL;DR: Adaptive Lagrangian framework using localized B-spline interpolation for evolving point-cloud surfaces in 3D, enabling meshless evolution with high-order geometric estimates and adaptive refinement.


<details>
  <summary>Details</summary>
Motivation: To develop a meshless, adaptive method for evolving smooth surfaces represented by point-cloud data, overcoming limitations of traditional mesh-based approaches and enabling direct evolution from discrete samples.

Method: Constructs overlapping localized tensor-product B-spline patches from point clouds, uses analytic B-spline derivatives for high-order geometric estimates, implements conditioning-aware interpolation with Gauss-Seidel refinement, and employs adaptive knot insertion and point redistribution based on geometric error indicators.

Result: Demonstrates efficient and accurate reproduction of surface evolution phenomena including mean-curvature flow, anisotropic deformations, and coupled surface-field dynamics, showing the method's precision and versatility.

Conclusion: Localized B-spline methods provide precise and versatile tools for dynamic manifold approximation, offering a robust framework for geometric evolution of point-cloud surfaces with adaptive refinement capabilities.

Abstract: We extend our recent curve-evolution framework based on localized B-spline interpolation to present an adaptive Lagrangian framework for the geometric evolution of point-cloud data representing smooth, codimension-one surfaces in $\mathbb{R}^3$. The method constructs overlapping, localized tensor-product B-spline patches, enabling direct, meshless surface evolution from discrete samples. Within each patch, the differentiable B-spline representation yields analytic, high-order estimates of intrinsic geometric invariants, supporting curvature-driven and geometry-coupled flows. The organization of control points facilitates coherent updates of both surface samples and spline coefficients under intrinsic velocity fields. A conditioning-aware formulation of the local interpolation system, combined with a Gauss-Seidel refinement of control points, maintains interpolation quality throughout the evolution. Adaptive knot insertion and point redistribution, guided by geometric error indicators and local sampling density, preserve surface resolution and regularity during deformation. Numerical experiments demonstrate efficient and accurate reproduction of surface evolution phenomena, including mean-curvature flow, anisotropic deformations, and coupled surface-field dynamics, establishing localized B-spline methods as precise and versatile tools for dynamic manifold approximation.

</details>


### [9] [Numerical Treatment of Non-local Integral Operators in the Framework of Evolutionary Equations](https://arxiv.org/abs/2601.11132)
*Sebastian Franz,Sascha Trostorff*

Main category: math.NA

TL;DR: Paper develops theory and numerical methods for abstract differential equations with non-local integral operators, proving well-posedness and convergence, with simulation results.


<details>
  <summary>Details</summary>
Motivation: To study abstract differential equations that include non-local integral operators, which arise in various applications but require rigorous mathematical analysis for well-posedness and numerical approximation.

Method: Uses evolutionary equations theory to analyze abstract differential equations with non-local operators. Provides well-posedness conditions, develops numerical approximation methods, and proves convergence under conditions on the integral kernel and solution.

Result: Establishes well-posedness conditions for the equations, develops convergent numerical methods, and provides simulation results demonstrating the theoretical findings.

Conclusion: The paper successfully provides a theoretical framework for analyzing non-local differential equations, establishes conditions for well-posedness, develops convergent numerical methods, and validates the approach through simulations.

Abstract: Using the theory of evolutionary equations, we consider abstract differential equations including non-local integral operators. After providing a condition for the well-posedness of the addressed equation we consider a numerical method of approximating its solution. We provide convergence proofs under conditions on the kernel of the integral operator and the solution and finish the paper with some simulation results.

</details>


### [10] [Eigenvector-based acceleration strategies for gradient-type methods](https://arxiv.org/abs/2601.11145)
*Jean-Paul Chehab,Gaspard Kemlin,Marcos Raydan,Yousef Saad*

Main category: math.NA

TL;DR: The paper proposes strategies to accelerate gradient-type methods for convex optimization by relaxing optimal step lengths to avoid zigzagging and leveraging Lanczos method properties when search directions approach eigenvectors.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient methods like steepest descent and minimal residual suffer from the negative zigzag effect when using optimal step lengths, which slows convergence. The authors aim to develop acceleration techniques that overcome this limitation while maintaining convergence guarantees.

Method: The authors propose relaxing the traditional optimal step length in gradient methods to avoid the zigzag effect. This allows iterates to explore the entire space, occasionally causing search directions to approach eigenvectors of the Hessian matrix. Once a search direction approaches an eigenvector, they leverage properties of the Lanczos method to accelerate convergence toward the global minimizer.

Result: The proposed strategies successfully accelerate gradient-type methods for minimizing strictly convex quadratics and functions. By avoiding the zigzag effect and exploiting eigenvector approximations, the methods achieve faster convergence compared to traditional gradient approaches.

Conclusion: Relaxing optimal step lengths in gradient methods combined with Lanczos-based acceleration when search directions approach eigenvectors provides effective speedup strategies for convex optimization problems, overcoming limitations of traditional gradient approaches.

Abstract: Several strategies are described and analyzed to speed-up gradient-type methods when applied to the minimization of strictly convex quadratics and strictly convex functions. The proposed techniques focus on relaxing the traditional optimal step length associated with gradient methods, including the steepest descent (SD) and the minimal residual (MR) methods. Such a relaxation avoids the well-known negative zigzag effect and allows the iterates to move in the entire space which in turn implies that every so often the search direction approaches some eigenvector of the underlying Hessian matrix. The proposed speedups then rely on taking advantage of the properties of the Lanczos method once a search direction that approaches an eigenvector has been identified in order to accelerate the convergence towards the global minimizer. After analyzing the proposed strategies, we illustrate them on the global minimization of strictly convex functions.

</details>


### [11] [An efficient solver based on low-rank approximation and Neumann matrix series for unsteady diffusion-type partial differential equations with random coefficients](https://arxiv.org/abs/2601.11152)
*Yujun Zhu,Min Li,Yulan Ning,Ju Ming*

Main category: math.NA

TL;DR: A novel efficient solver for unsteady diffusion-type PDEs with random coefficients using generalized low-rank matrix approximation and Neumann series expansion to reduce computational cost.


<details>
  <summary>Details</summary>
Motivation: The computational challenge of repeatedly solving large-scale linear systems from spatial and temporal discretizations under uncertainty in diffusion-type PDEs with random coefficients.

Method: Proposes a generalized low-rank matrix approximation for stochastic stiffness matrices and approximates their inverses using Neumann matrix series expansion, transforming high-dimensional matrix inversion into low-dimensional matrix multiplications.

Result: The solver significantly reduces computational cost and storage requirements while maintaining high numerical accuracy, with error analysis provided. Applied successfully to unsteady stochastic diffusion equations and distributed optimal control problems.

Conclusion: The proposed method is feasible and effective for uncertainty quantification problems involving unsteady diffusion-type PDEs with random coefficients, offering computational efficiency without sacrificing accuracy.

Abstract: In this paper, we develop an efficient numerical solver for unsteady diffusion-type partial differential equations with random coefficients. A major computational challenge in such problems lies in repeatedly handling large-scale linear systems arising from spatial and temporal discretizations under uncertainty. To address this issue, we propose a novel generalized low-rank matrix approximation to represent the stochastic stiffness matrices, and approximate their inverses using the Neumann matrix series expansion. This approach transforms high-dimensional matrix inversion into a sequence of low-dimensional matrix multiplications. Therefore, the solver significantly reduces the computational cost and storage requirements while maintaining high numerical accuracy. The error analysis of the proposed solver is also provided. Finally, we apply the method to two classic uncertainty quantification problems: unsteady stochastic diffusion equations and the associated distributed optimal control problems. Numerical results demonstrate the feasibility and effectiveness of the proposed solver.

</details>


### [12] [Adaptive Randomized Extended Bregman-Kaczmarz Method for Combined Optimization Problems](https://arxiv.org/abs/2601.11157)
*Zeyu Dong,Aqin Xiao,Guojian Yin,Junfeng Yin*

Main category: math.NA

TL;DR: Proposes adaptive randomized averaging block extended Bregman-Kaczmarz (aRABEBK) method for solving inverse problems with data-fidelity and regularization terms, featuring automatic step size adjustment and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Combined optimization problems with data-fidelity and regularization terms are common in inverse problems, but existing methods require manual tuning of relaxation parameters and may converge slowly.

Method: Develops an adaptive randomized averaging block extended Bregman-Kaczmarz method that automatically adjusts iteration-wise relaxation parameters using residual information, allowing for more aggressive step sizes without manual tuning.

Result: Establishes convergence theory with expected linear convergence rate guarantees. Numerical experiments on synthetic and real datasets for sparse and minimum-norm least-squares problems show faster convergence and improved robustness compared to state-of-the-art methods.

Conclusion: The aRABEBK method provides an effective solution for inverse problems with automatic parameter adjustment, achieving superior performance over existing extended Kaczmarz and Bregman-Kaczmarz algorithms.

Abstract: Combined optimization problems that couple data-fidelity and regularization terms arise naturally in a wide range of inverse problems. In this paper, we study an adaptive randomized averaging block extended Bregman-Kaczmarz (aRABEBK) method for solving such problems. The proposed method incorporates iteration-wise relaxation parameters that are automatically adjusted using residual information, allowing for more aggressive step sizes without additional manual tuning. We establish a convergence theory for the proposed framework and derive expected linear convergence rate guarantees. Numerical experiments on both synthetic and real data sets for sparse and minimum-norm least-squares problems demonstrate that our aRABEBK method achieves faster convergence and improved robustness compared with state-of-the-art extended Kaczmarz and Bregman-Kaczmarz-type algorithms, including its nonadaptive counterpart.

</details>


### [13] [Discontinuous Galerkin schemes for multi-dimensional coupled hyperbolic systems](https://arxiv.org/abs/2601.11172)
*Niklas Kolbe,Siegfried Müller,Aleksey Sikstel*

Main category: math.NA

TL;DR: Novel Runge-Kutta discontinuous Galerkin schemes for coupled conservation law systems with sharp interfaces, using relaxation approach and local projection to avoid expensive nonlinear half-Riemann problems.


<details>
  <summary>Details</summary>
Motivation: Need efficient numerical schemes for coupled systems of conservation laws separated by fixed sharp interfaces, particularly for fluid-structure interaction problems where traditional methods require expensive nonlinear half-Riemann problem solutions.

Method: Derived from Jin-Xin relaxation approach with problem-specific interface coupling modification, using local projection instead of solving nonlinear half-Riemann problems. Higher-order time discretization achieved through strong stability preserving Runge-Kutta methods with asymptotic preserving implicit-explicit treatment.

Result: Developed novel class of Runge-Kutta discontinuous Galerkin schemes that efficiently handle coupled conservation law systems with sharp interfaces without requiring expensive nonlinear half-Riemann problem solutions.

Conclusion: The proposed relaxation-based approach provides efficient numerical schemes for coupled conservation law systems with sharp interfaces, demonstrated through application to multi-dimensional fluid-structure coupling problems.

Abstract: A novel class of Runge-Kutta discontinuous Galerkin schemes for coupled systems of conservation laws in multiple space dimensions that are separated by a fixed sharp interface is introduced. The schemes are derived from a relaxation approach and a local projection and do not require expensive solutions of nonlinear half-Riemann problems. The underlying Jin-Xin relaxation involves a problem specific modification of the coupling condition at the interface, for which a simple construction algorithm is presented. The schemes are endowed with higher order time discretization by means of strong stability preserving Runge-Kutta methods. These are derived from an asymptotic preserving implicit-explicit treatment of the coupled relaxation system taken to the discrete relaxation limit. In a case study the application to a multi-dimensional fluid-structure coupling problem employing the compressible Euler equations and a linear elastic model is discussed.

</details>


### [14] [A Machine-Learned Near-Well Model in OPM Flow](https://arxiv.org/abs/2601.11193)
*Peter von Schultzendorff,Tor Harald Sandve,Birane Kane,David Landa-Marbán,Jakub Wiktor Both,Jan Martin Nordbotten*

Main category: math.NA

TL;DR: Integration of neural networks into OPM Flow reservoir simulator enables hybrid modeling with automatic differentiation, demonstrated via a data-driven near-well model for CO2 storage applications.


<details>
  <summary>Details</summary>
Motivation: Hybrid reservoir simulation combining physics-based models with ML components offers high fidelity and fast inference, but requires tight integration with automatic differentiation frameworks for efficient nonlinear solvers, inverse problems, and optimization.

Method: First integration of neural networks into OPM Flow simulator: networks trained in TensorFlow and imported as native automatic differentiation functions, enabling seamless workflow integration. Applied to create a data-driven near-well model that learns Peaceman-like well indices from fine-scale ensemble simulations.

Result: Successfully implemented neural network integration framework in OPM Flow. The data-driven near-well model achieves high fidelity to fine-scale results at low computational cost for CO2 storage applications, demonstrating the framework's potential for hybrid modeling.

Conclusion: The OPM Flow-Neural Network framework enables efficient hybrid reservoir simulation by integrating ML components with automatic differentiation, overcoming limitations of traditional near-well models while maintaining computational efficiency.

Abstract: Recent advances in reservoir simulation increasingly utilize hybrid approaches that couple physics-based simulators with machine-learning (ML) components. ML components offer high fidelity to training data and fast inference, enabling efficient and accurate modeling of complex multi-scale or multi-physics phenomena. Modern reservoir simulators rely on automatic differentiation (AD) to support efficient and flexible strategies for nonlinear solvers, inverse problems, and optimization problems. Efficient hybrid modeling therefore requires tight integration of the ML components with the simulator's AD framework.
  We present the first integration of neural networks into the high-performance reservoir simulator OPM Flow. Networks are trained in TensorFlow and imported into OPM, where they are accessed as native AD functions. This presents an efficient framework for hybrid modeling and enables seamless integration in existing simulator workflows.
  As an application, we introduce a novel, data-driven near-well model. Near-well models are essential in reservoir simulation for accurately representing singular pressure gradients around wells. Commonly used are the Peaceman near-well model and its extensions, or local grid refinement around the wells. Peaceman-type models are limited to simplified flow regimes, whereas local grid refinement is computationally expensive. We address these limitations by training a neural network to infer a Peaceman-like well index from fine-scale ensemble simulations of the near-well region. It is then integrated into OPM Flow with the new framework. Tested on relevant examples for CO$_2$ storage, the method offers high fidelity to fine-scale results at low computational cost, demonstrating the potential of the OPM Flow-Neural Network framework for hybrid modeling.

</details>


### [15] [A Recovery-Based Error Indicator for Finite Difference Methods](https://arxiv.org/abs/2601.11308)
*Ferhat Sindy,Annalisa Buffa,Marco Picasso*

Main category: math.NA

TL;DR: A novel recovery-based error indicator for high-order Finite Difference Methods that uses post-processing and interpolation into polynomial Finite Element spaces to estimate gradient errors.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate error estimation method for high-order Finite Difference Methods that can handle complex problems including discontinuous coefficients and heterogeneous media.

Method: Interpolate Finite Difference grid values into polynomial Finite Element space, then apply a recovery-based error indicator with polynomial-preserving property to estimate gradient errors.

Result: Demonstrated performance and accuracy through numerical experiments on 2D Poisson problems, elliptic problems with discontinuous coefficients, and 2D/3D wave equations in homogeneous/heterogeneous media.

Conclusion: The proposed recovery-based error indicator effectively estimates gradient errors for high-order Finite Difference Methods across various problem types and complexities.

Abstract: A novel recovery-based error indicator for high-order Finite Difference Methods, based on post-processing of the Finite Difference values is presented. The values obtained on the Finite Difference grid are interpolated into a suitable polynomial Finite Element space. A recovery-based error indicator, with the polynomial-preserving property, is then applied to estimate the gradient error. The performance and accuracy of the proposed error indicator are demonstrated through several numerical experiments, including the two-dimensional Poisson problem solved using second- and fourth-order finite difference schemes. Additional experiments are conducted on elliptic problems with discontinuous coefficients, as well as on the two and three-dimensional wave equation in homogeneous media with second- and fourth-order finite differences, and in heterogeneous media with second-order finite differences.

</details>


### [16] [Constructing Orthonormal Rational Function Vectors with an application in Rational Approximation](https://arxiv.org/abs/2601.11317)
*Robbe Vermeiren*

Main category: math.NA

TL;DR: Two algorithms for constructing orthonormal rational function bases using pencil-based formulations with k-Hessenberg matrices, applied to rational approximation problems with exponential pole clustering near singularities.


<details>
  <summary>Details</summary>
Motivation: To develop robust methods for constructing orthonormal bases of rational function vectors for rational approximation problems, particularly for handling exponentially clustered poles near singularities, building on previous work on inverse generalized eigenvalue problems.

Method: 1) Extends pencil-based formulation of inverse generalized eigenvalue problem to rational vectors of arbitrary length k, using pairs of k-Hessenberg matrices to represent recurrence relations. 2) Develops an updating algorithm based on similarity transformations using rotations. 3) Derives a Krylov-type algorithm related to the rational Arnoldi method.

Result: Successfully recovers the optimal lightning + polynomial convergence rate for rational approximation of √z on [0,1] as demonstrated by Herremans, Huybrechs, and Trefethen (2023), showing robustness for handling exponentially clustered poles near singularities.

Conclusion: The proposed methods provide effective algorithms for constructing orthonormal rational function bases and demonstrate practical utility in rational approximation problems, particularly for handling challenging cases with poles clustered near singularities.

Abstract: We present two algorithms for constructing orthonormal bases of rational function vectors with respect to a discrete inner product, and discuss how to use them for a rational approximation problem. Building on the pencil-based formulation of the inverse generalized eigenvalue problem by Van Buggenhout et al.\ (2022), we extend it to rational vectors of arbitrary length $k$, where the recurrence relations are represented by a pair of $k$-Hessenberg matrices, i.e., matrices with possibly $k$ nonzero subdiagonals. An updating algorithm based on similarity transformations using rotations and a Krylov-type algorithm related to the rational Arnoldi method are derived. The performance is demonstrated on the rational approximation of $\sqrt{z}$ on $[0,1]$, where the optimal lightning + polynomial convergence rate of Herremans, Huybrechs, and Trefethen (2023) is successfully recovered. This illustrates the robustness of the proposed methods for handling exponentially clustered poles near singularities.

</details>


### [17] [Solving the Fisher nonlinear differential equations via Physics-Informed Neural Networks: A Comprehensive Retraining Study and Comparative Analysis with the Finite Difference Method](https://arxiv.org/abs/2601.11406)
*Ahmed Aberqi,Ahmed Miloudi*

Main category: math.NA

TL;DR: PINNs applied to solve 1D nonlinear Fisher-KPP equation, validated against analytical and FDM solutions, with focus on retraining strategies and error analysis.


<details>
  <summary>Details</summary>
Motivation: To demonstrate PINNs as a competitive alternative to traditional numerical methods for solving complex nonlinear PDEs like the Fisher-KPP equation, which models important reaction-diffusion phenomena in population dynamics and flame propagation.

Method: Standard PINN framework with neural network architecture, physics-informed loss function, and investigation of retraining strategies to optimize performance. Direct comparison with analytical solution and Finite Difference Method (FDM).

Result: PINNs accurately approximate the Fisher-KPP equation solution, with thorough quantitative error analysis showing efficacy. Study highlights critical aspects of model retraining and optimizer state management.

Conclusion: PINNs are a viable and competitive alternative to traditional numerical methods for solving nonlinear differential equations, with broader applications across scientific domains, though careful attention to retraining strategies is essential.

Abstract: Physics-Informed Neural Networks (PINNs) represent a groundbreaking paradigm in scientific computing, seamlessly integrating the robust framework of deep learning with fundamental physical laws. This paper meticulously applies the standard PINN framework to solve the challenging one-dimensional nonlinear Fisher-KPP equation, a critical model in reaction-diffusion dynamics describing phenomena such as population spread and flame propagation. We detail a comprehensive methodology, encompassing the neural network architecture, the physics-informed loss function, and an in-depth investigation into retraining strategies aimed at optimizing model performance. Our approach is rigorously validated through a direct comparison of the PINN solution against both the known analytical solution and a numerical solution derived from the Finite Difference Method (FDM). Through this work, we elucidate the intricate balance between model complexity, training efficiency, and accuracy. Results highlight the PINN's remarkable capability in accurately approximating the solution to this complex PDE, while also shedding light on the critical aspects and challenges of model retraining, particularly concerning the optimizer's state. This study provides a thorough quantitative error analysis, demonstrating the efficacy of PINNs as a viable and competitive alternative to traditional numerical methods for solving nonlinear differential equations, and discusses their broader applications across various scientific domains.

</details>


### [18] [Tensor field tomography with attenuation and refraction: adjoint operators for the dynamic case and numerical experiments](https://arxiv.org/abs/2601.11483)
*Lukas Vierus,Thomas Schuster,Bernadette Hahn*

Main category: math.NA

TL;DR: Tensor field tomography with refraction, attenuation, and time-dependence using attenuated ray transforms along geodesics; adjoint representations derived; numerical implementation shows integral methods outperform PDE-based approaches in efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive tensor field tomography framework that accounts for realistic physical phenomena including refraction, attenuation, and time-dependence, moving beyond simplified straight-line assumptions.

Method: Uses attenuated ray transforms along geodesic curves defined by refractive index; formulates as inverse source problem via transport equations; derives two adjoint representations; implements numerically with damped Landweber method with Nesterov acceleration; solves transport equations via viscosity approximation.

Result: Integral representation significantly outperforms PDE-based methods in computational efficiency while achieving comparable reconstruction accuracy; inclusion of refraction improves accuracy despite increased computational cost; noise analysis confirms benefits of refraction modeling.

Conclusion: Accounting for refraction in tensor field tomography models is worthwhile despite higher computational costs, as it improves reconstruction accuracy; integral-based adjoint methods offer superior computational efficiency compared to PDE-based approaches.

Abstract: This article is concerned with tensor field tomography in a fairly general setting, that takes refraction, attenuation and time-dependence of tensor fields into account. The mathematical model is given by attenuated ray transforms of the fields along geodesic curves corresponding to a Riemannian metric that is defined by the index of refraction. The data are given at the boundary tangent bundle of the domain and it is well-known that they can be characterized as boundary data of a transport equation turning tensor field tomography into an inverse source problem. This way the adjoint of the forward mapping can be computed using the integral representation or, equivalently, associated to a dual transport equation. The article offers and proves two different representations for the adjoint mappings both in the dynamic and static case. The numerical implementation is demonstrated and evaluated for static fields using the damped Landweber method with Nesterov acceleration applied to both, the integral and PDE-based formulations. The transport equations are solved using a viscosity approximation. The error analysis reveals that the integral representation significantly outperforms PDE-based methods in terms of computational efficiency while achieving comparable reconstruction accuracy. The impact of noise and deviations from straight-line trajectories are investigated confirming improved accuracy if refraction is taken into account. We conclude that the inclusion of refraction to the forward model pays in spite of increased numerical cost.

</details>


### [19] [Efficient error estimators for Generalized Nyström](https://arxiv.org/abs/2601.11493)
*Lorenzo Lazzarino,Katherine J. Pearce,Nathaniel Pritchard*

Main category: math.NA

TL;DR: Extension of leave-one-out error estimation framework to generalized Nyström decomposition for rectangular matrices


<details>
  <summary>Details</summary>
Motivation: Randomized algorithms in linear algebra need efficient ways to assess approximation accuracy without additional expensive matrix accesses. While recent work developed leave-one-out estimators for randomized SVD and standard Nyström, the generalized Nyström decomposition for rectangular matrices lacked such estimators.

Method: Extend leave-one-out framework to generalized Nyström decomposition by deriving three new leave-one-out error estimators specifically designed for this decomposition method.

Result: Three new leave-one-out error estimators for generalized Nyström decomposition, validated through numerical experiments to demonstrate their effectiveness.

Conclusion: Successfully extended the leave-one-out error estimation framework to generalized Nyström decomposition, providing efficient accuracy assessment for randomized methods on rectangular matrices without additional matrix accesses.

Abstract: Randomized algorithms in numerical linear algebra have proven to be effective in ameliorating issues of scalability when working with large matrices, efficiently producing accurate low-rank approximations. A key remaining challenge, however, is to efficiently assess the approximation accuracy of randomized methods without additional expensive matrix accesses. Recent work has addressed this issue by deriving fast leave-one-out error estimators for the randomized SVD and Nyström decomposition, enabling accurate error estimation with no additional matrix accesses. In this work, we extend the leave-one-out framework to the generalized Nyström decomposition, an approach that can be applied to general rectangular matrices. We do this by deriving three new leave-one-out error estimators and validating their effectiveness through numerical experiments.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [20] [Critical points of the two-dimensional Ambrosio-Tortorelli functional with convergence of the phase-field energy](https://arxiv.org/abs/2601.10875)
*Jean-François Babadjian,Martin Rakovsky,Rémy Rodiac*

Main category: math.AP

TL;DR: The paper extends previous results on convergence of critical points from the Ambrosio-Tortorelli functional to the Mumford-Shah functional in 2D, requiring only convergence of the phase-field energy to the length energy term rather than full energy convergence.


<details>
  <summary>Details</summary>
Motivation: Previous work showed that if the full Ambrosio-Tortorelli energy converges to the Mumford-Shah energy, then the first inner variation also converges, making the limit a critical point of Mumford-Shah. The authors want to weaken this requirement to only needing convergence of the phase-field energy to the length energy term.

Method: The authors consider a family of critical points of the Ambrosio-Tortorelli functional in 2D with uniform energy bound. They analyze the convergence properties as ε→0, focusing on the relationship between the phase-field energy term and the length energy in the Mumford-Shah functional.

Result: The authors prove that under the sole convergence of the phase-field energy to the length energy term (rather than requiring full energy convergence), the first inner variation still converges, making the limit u a critical point of the Mumford-Shah functional in the sense of inner variations.

Conclusion: In the 2D setting, convergence of critical points from Ambrosio-Tortorelli to Mumford-Shah can be established under weaker conditions than previously known - only requiring convergence of the phase-field energy to the length energy term rather than full energy convergence.

Abstract: We consider a family $\{(u_\varepsilon, v_\varepsilon)\}_{\varepsilon>0}$ of critical points of the Ambrosio-Tortorelli functional. Assuming a uniform energy bound, the sequence $\{(u_\varepsilon, v_\varepsilon)\}_{\varepsilon>0}$ converges in $L^2(Ω)$ to a limit $(u, 1)$ as $\varepsilon \to 0$, where $u$ is in $SBV^2(Ω)$. It was previously shown that if the full Ambrosio-Tortorelli energy associated to $(u_\varepsilon,v_\varepsilon)$ converges to the Mumford-Shah energy of $u$, then the first inner variation converges as well. In particular, $u$ is a critical point of the Mumford-Shah functional in the sense of inner variations. In this work, focusing on the two-dimensional setting, we extend this result under the sole convergence of the phase-field energy to the length energy term in the Mumford-Shah functional.

</details>


### [21] [Normalized solutions of Nehari-Pankov type to indefinite variational problems](https://arxiv.org/abs/2601.10941)
*Damien Galant,Tobias Weth*

Main category: math.AP

TL;DR: The paper develops an abstract framework for existence and multiplicity of solutions with prescribed norm to nonlinear equations, with applications to Schrödinger equations on graphs, biharmonic equations on tori, and various PDEs with Dirichlet boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To establish a general approach for finding solutions with prescribed norm (mass) to nonlinear equations of the form Au = λu + I'(u), where A is a self-adjoint operator with compact resolvent and I' is a superlinear term. This addresses the challenge of finding solutions with specific prescribed properties in various nonlinear PDE settings.

Method: Develops an abstract framework where solutions are detected as ground states of Nehari-Pankov type for the λ-dependent action functional. The key observation is that the H-norms of λ-dependent solution families form connected sets. Uses Weyl type estimates for spectral gaps, variational characterizations of eigenvalues, eigenfunction bounds, and analytic number theory to estimate the size of these connected sets.

Result: 1) Existence of infinitely many solutions to nonlinear Schrödinger equations on compact graphs with prescribed (arbitrarily large) mass, including mass-supercritical case. 2) Similar existence and multiplicity for biharmonic semilinear equations on 2-torus. 3) Multiple solutions with prescribed small mass for second and higher order equations in bounded domains with Dirichlet boundary conditions.

Conclusion: The abstract framework provides a unified approach to existence and multiplicity of solutions with prescribed norm across various nonlinear PDE settings. The method leverages spectral properties and variational techniques to establish connectedness of solution families and estimate their norm ranges, yielding concrete existence results in specific applications.

Abstract: We consider abstract nonlinear equations of the form $A u = λu + I'(u)$, where $A$ is a self-adjoint operator with compact resolvent on a Hilbert space $H$, $λ\in \mathbf{R}$ is a parameter, and $u \mapsto I'(u)$ is a superlinear term of variational nature. In this abstract setting, we develop a new approach to existence and multiplicity of solutions with prescribed norm in $H$. We then consider various applications of this approach. First, we obtain, under fairly general assumptions including the mass-supercritical case, the existence of infinitely many solutions to a class of nonlinear (time-independent) Schrödinger equations on a compact graph $\mathcal{G}$ with prescribed (arbitrarily large) mass. Moreover, we derive a similar existence and multiplicity result for a biharmonic semilinear equation in the $2$-torus. For a larger class of second order and higher order equations in a bounded domain with Dirichlet boundary conditions, we also show the existence of multiple solutions with prescribed small mass.
  The solutions we obtain are detected as ground states of Nehari-Pankov type for the associated $λ$-dependent action functional, where $λ$ varies in a spectral gap between sufficiently large eigenvalues of $A$. The key observation in this abstract framework is the fact that the $H$-norms of these $λ$-dependent solution families form connected sets. To estimate the size of these connected sets in specific settings, we use Weyl type estimates for the length of spectral gaps, variational characterizations of eigenvalues, bounds for associated eigenfunctions and a classical bound from analytic number theory.

</details>


### [22] [Optimal Trudinger-Moser inequalities on complete noncompact Riemannian manifolds: Revisit of the argument from the local inequalities to global ones](https://arxiv.org/abs/2601.10996)
*Jungang Li,Guozhen Lu*

Main category: math.AP

TL;DR: A short note clarifying part of Theorem 1.3 proof from a reference paper and providing alternative argument from local to global inequalities.


<details>
  <summary>Details</summary>
Motivation: To simplify and clarify the proof of Theorem 1.3 from a referenced paper [8], and to demonstrate an alternative approach for deriving global inequalities from local ones.

Method: The paper presents a short note with two main contributions: 1) providing a simpler clarification of part of the proof of Theorem 1.3 from reference [8], and 2) offering an alternative argument that transitions from local inequalities to global ones.

Result: The note successfully clarifies the proof of Theorem 1.3 and provides an alternative method for deriving global inequalities from local ones, though specific mathematical results are not detailed in the abstract.

Conclusion: This short note contributes to mathematical understanding by simplifying a complex proof and offering an alternative approach to connecting local and global inequalities, potentially making the concepts more accessible to readers.

Abstract: The main purpose of this short note, on the one hand, to is clarify some part of the proof of Theorem 1.3 in [8] in a simple way, and on the other hand, to give an alternative argument from local inequalities to global ones.

</details>


### [23] [Inverse Spectral Problem With Low Regularity Refractive Index](https://arxiv.org/abs/2601.11146)
*Kewen Bu,Youjun Deng,Yan Jiang,Kai Zhang*

Main category: math.AP

TL;DR: Radial refractive index n is not uniquely determined by special transmission eigenvalues alone, but becomes uniquely determined when supplemented with partial a priori information.


<details>
  <summary>Details</summary>
Motivation: To investigate whether a radial refractive index can be uniquely determined from spectral data (transmission eigenvalues), which is important for inverse scattering problems and non-destructive testing.

Method: First shows non-uniqueness for piecewise twice continuously differentiable functions using only special transmission eigenvalues. Then proves uniqueness for twice continuously differentiable functions (or continuously differentiable with Lipschitz derivative) when eigenvalues are supplemented with partial a priori information.

Result: 1) For piecewise twice continuously differentiable functions, n is NOT uniquely determined by special transmission eigenvalues alone. 2) For twice continuously differentiable functions (or continuously differentiable with Lipschitz derivative), n IS uniquely determined on [0,1] when eigenvalues are combined with partial a priori information.

Conclusion: Special transmission eigenvalues alone are insufficient for unique determination of radial refractive index, but uniqueness can be achieved by incorporating additional partial a priori information about the refractive index.

Abstract: This article investigates the unique determination of a radial refractive index n from spectral data. First, we demonstrate that for piecewise twice continuously differentiable functions, n is not uniquely determined by the special transmission eigenvalues associated with radially symmetric eigenfunctions. Subsequently we prove that if n \in M is twice continuously differentiable functions(or continuously differentiable functions with Lipschitz continuous derivative), then n is uniquely determined on [0,1] by all special transmission eigenvalues when supplemented by partial a priori information on the refractive index.

</details>


### [24] [On a Mullins-Sekerka model for the growth of active droplets modelling protocells: Stability analysis and numerical computations](https://arxiv.org/abs/2601.11155)
*Harald Garcke,Kei Fong Lam,Robert Nürnberg,Andrea Signori*

Main category: math.AP

TL;DR: Analysis of chemically active Mullins-Sekerka models showing droplet growth, instability, splitting, and shell-type formations with theoretical stability analysis and numerical simulations.


<details>
  <summary>Details</summary>
Motivation: Mullins-Sekerka models with chemical reactions can produce grow-divide cycles that serve as models for protocells and chemical compartments in living systems, which are fundamental for biological organization.

Method: Theoretical analysis of radially symmetric solutions, stability analysis in radial and planar situations, analysis of multilayered shell-type solutions, and development of a parametric finite element method that handles topological changes (splitting/merging).

Result: Existence of radially symmetric solutions proven, detailed stability analysis completed, numerical simulations verify theoretical findings and show complex dynamics including multiple instabilities, droplet splitting, and shell-type solution emergence.

Conclusion: Chemically active Mullins-Sekerka models exhibit rich dynamics including growth-division cycles and shell formations, providing mathematical framework for understanding protocell behavior and chemical compartmentalization in living systems.

Abstract: Mullins-Sekerka models with chemical reactions can lead to scenarios where droplets grow, become unstable, split, grow and undergo further division. These grow and division cycles have been proposed as a model for protocells and are believed to play a fundamental role in living systems by providing chemical compartments which are important in the organization of living systems. This paper analyses chemically active Mullins-Sekerka models. Existence of radially symmetric solutions is shown and a detailed stability analysis in radial as well as planar situations is given. In particular, we also analyze multilayered solutions leading to shell-type situations. Finally, we introduce a numerical method based on a parametric finite element approach that explicitly accounts for topological changes, thereby allowing for droplet splitting and merging. Several numerical simulations verify the findings of the theoretical stability analysis and show complex dynamical behavior, including multiple instabilities, splittings of droplets and appearance of shell-type solutions.

</details>


### [25] [Ergodic pairs for fractional Hamilton-Jacobi equations on bounded domains: large solutions](https://arxiv.org/abs/2601.11241)
*Alexander Quaas,Erwin Topp*

Main category: math.AP

TL;DR: Study of ergodic problem for viscous Hamilton-Jacobi equations with censored fractional Laplacian diffusion, focusing on gradient terms with scaling ≤ fractional order, using vanishing discount method.


<details>
  <summary>Details</summary>
Motivation: To extend ergodic theory to nonlocal fractional diffusion operators (censored fractional Laplacian) in bounded domains, particularly when gradient nonlinearities have scaling comparable to the fractional order, overcoming challenges from state-dependent operators where classical Laplacian invariance properties don't apply.

Method: Vanishing discount method: analyze approximated solutions to derive qualitative properties for the ergodic problem, including blow-up rates and characterization of ergodic constant. Focus on cases where gradient term scaling ≤ fractional diffusion order.

Result: Existence of ergodic pairs involving solutions that blow-up on domain boundary ∂Ω, with precise blow-up rates for solutions and characterization of ergodic constant.

Conclusion: Successfully extends ergodic theory to censored fractional Laplacian setting, providing existence results and qualitative properties despite challenges from state-dependent nonlocal operators, using vanishing discount method as key analytical tool.

Abstract: In this article, we study the ergodic problem associated to viscous Hamilton-Jacobi equation where the diffusion is governed by the censored fractional Laplacian, a nonlocal elliptic operator restricted to a bounded domain $Ω\subset \mathbb{R}^N$. We restrict ourselves to the case in which the nonlinear gradient term has a scaling less or equal than the fractional order of the diffusion. In similarity to its second-order counterpart, we provide existence of ergodic pairs involving solutions that blow-up on $\partial Ω$. We use the celebrated vanishing discount method, where the analysis of the approximated solutions have its own interest, leading to qualitative properties for the ergodic problem such as precise blow-up rates for the solution and characterization of the ergodic constant. The main difficulties arise from the state-dependency of the operator, from which the arguments of the local case based on well-known invariance properties of the Laplacian are not longer at disposal.

</details>


### [26] [Structured Deformations in Linearized Elasticity](https://arxiv.org/abs/2601.11333)
*Manuel Friedrich,José Matias,Elvira Zappale*

Main category: math.AP

TL;DR: Extends structured deformation theory to linearized elasticity with integral energy representation featuring bulk and surface contributions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between nonlinear elasticity and linearized elasticity in the context of structured deformations, providing a comprehensive energy framework that accounts for both bulk and surface effects.

Method: Uses two approaches: 1) direct approach via global relaxation method in BD (bounded deformation) space, and 2) approximation from nonlinear elastic energies of nonsimple materials.

Result: Develops integral representation for energy in linearized elasticity with structured deformations, featuring both bulk and surface contributions.

Conclusion: Successfully extends structured deformation theory to linearized elasticity, providing a unified energy framework that captures both bulk and surface effects through two complementary derivation methods.

Abstract: We extend the theory of structured deformations to the setting of linearized elasticity by providing an integral representation for the underlying energy that features bulk and surface contributions. Our derivation is obtained both via a direct approach by means of a global method for relaxation in BD and via an approximation from nonlinear elastic energies associated to {nonsimple} materials.

</details>


### [27] [Elastic Calderón Problem via Resonant Hard Inclusions: Linearisation of the N-D Map and Density Reconstruction](https://arxiv.org/abs/2601.11356)
*Huaian Diao,Mourad Sini,Ruixiang Tang*

Main category: math.AP

TL;DR: The paper develops a metamaterial-inspired method to reconstruct mass density in elastic media using resonant inclusions to create effective negative density, enabling Fourier-based reconstruction from boundary measurements.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of recovering mass density in elastic media from boundary measurements (Neumann-to-Dirichlet map), which is challenging due to the complexity of elastic wave equations and lack of direct reconstruction methods.

Method: Introduce periodic array of resonant high-density inclusions to create effective medium with uniform negative density shift. Use linearization around this negative background and test with complex geometric optics solutions to derive explicit reconstruction formula for Fourier transform of density.

Result: Prove convergence of Neumann-to-Dirichlet map to effective map with operator norm estimate. Derive explicit reconstruction formula for Fourier transform of density, providing global density recovery scheme.

Conclusion: The approach provides a metamaterial-inspired analytic framework for inverse coefficient problems in linear elasticity and demonstrates how nanoscale resonators can be leveraged in reconstruction algorithms.

Abstract: We study an elastic Calderon-type inverse problem: recover the mass density $ρ(x)$ in a bounded domain $Ω\subset\mathbb{R}^3$ from the Neumann-to-Dirichlet map associated with the isotropic Lamé system $\mathcal{L}_{λ,μ}u+ω^2ρ(x)u=0$. We introduce a constructive strategy that embeds a subwavelength periodic array of resonant high-density (hard) inclusions to create an effective medium with a uniform negative density shift. Specifically, we place a periodic cluster of inclusions of size $a$ and density $ρ_1\asymp a^{-2}$ strictly inside $Ω$. For frequencies $ω$ tuned to an eigenvalue of the elastic Newton (Kelvin) operator of a single inclusion, we show that as $a\to0$ and the number of inclusions $M\to\infty$, the Neumann-to-Dirichlet map $Λ_D$ converges to an effective map $Λ_{\mathcal{P}}$ corresponding to a background density shift $-\mathcal{P}^2$, with the operator norm estimate $\|Λ_D-Λ_{\mathcal{P}}\|\le Ca^α\mathcal{P}^6$ for some $α>0$ determined by the geometric scaling. Around this negative background we derive a first-order linearization of $Λ_{\mathcal{P}}$ in terms of $ρ$ and the Newton volume potential for the shifted Lamé operator. Testing the linearized relation with complex geometric optics solutions yields an explicit reconstruction formula for the Fourier transform of $ρ$, and hence a global density recovery scheme. The results provide a metamaterial-inspired analytic framework for inverse coefficient problems in linear elasticity and a concrete paradigm for leveraging nanoscale resonators in reconstruction algorithms.

</details>


### [28] [Homogenized moderately wrinkled shell theory from 3D Koiter's linear elasticity](https://arxiv.org/abs/2601.11384)
*Pedro Hernández-Llanos,Rajesh Mahadevan,Ravi Prakash*

Main category: math.AP

TL;DR: Derivation of periodically wrinkled shell models from 3D linear elasticity using two-scale convergence, with specific scaling parameter p=2 for the mid-surface periodic oscillations.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical models for shells with periodic surface wrinkles/oscillations, connecting microscopic periodic features to macroscopic shell behavior through rigorous asymptotic analysis.

Method: Two-scale convergence technique applied to 3D linear elasticity, with mid-surface defined as ψ(x₁,x₂) + ε²θ(x₁/ε,x₂/ε)a₃(x₁,x₂), where θ is periodic and p=2. Uses Koiter's shell model for strain energy.

Result: Different shell theories emerge depending on the scaling parameter ε>0 and p>1, with specific results for the p=2 case where periodic oscillations scale quadratically with ε.

Conclusion: Two-scale convergence provides rigorous framework for deriving periodically wrinkled shell models from 3D elasticity, with scaling parameter p determining the effective shell theory that emerges.

Abstract: In this paper we derive, by two$-$scale convergence, periodically wrinked shell models starting from three dimensional linear elasticity, depending of the behaviour of the small parameter $\varepsilon>0$ and $p>1$, differents theories appear. We assume that the mid-surface of the shell is given by $\displaystyle ψ(x_1,x_2)+\varepsilon^pθ\left(\frac{x_1}{\varepsilon},\frac{x_2}{\varepsilon}\right)\vect{a}_{3}(x_1,x_2)$, where $θ$ is $[0,1)^2$-periodic function and $p=2$. We also assume that the strain energy of the shell has the Koiter's model.

</details>


### [29] [Global $C^{1,α}$-Regularity for Musielak-Orlicz Equations in Divergence Form](https://arxiv.org/abs/2601.11495)
*Hlel Missaoui*

Main category: math.AP

TL;DR: Global C^{1,α} regularity established for bounded generalized solutions of elliptic equations with Musielak-Orlicz growth under Dirichlet/Neumann boundary conditions, extending previous results.


<details>
  <summary>Details</summary>
Motivation: To establish regularity theory for elliptic equations with non-standard growth conditions (Musielak-Orlicz type) that generalize many important special cases like variable exponent spaces, Orlicz spaces, and (p,q) growth situations.

Method: Analysis of elliptic equations in divergence form with Musielak-Orlicz growth conditions, focusing on the interplay between non-standard growth conditions and boundary behavior for generalized solutions.

Result: Proves global C^{1,α} regularity for bounded generalized solutions subject to Dirichlet or Neumann boundary conditions, extending and generalizing several important regularity results in special cases.

Conclusion: The paper successfully establishes comprehensive regularity results for elliptic equations with Musielak-Orlicz growth, highlighting new conditions that address the interaction between non-standard growth and boundary behavior in generalized settings.

Abstract: In this paper, we establish global $C^{1,α}$-regularity for bounded generalized solutions of elliptic equations in divergence form with Musielak-Orlicz growth and subject to Dirichlet or Neumann boundary conditions. In fact, our findings extend and generalize several important regularity results in cases of special attention such as variable exponent spaces, Orlicz spaces, and some $(p,q)$ situations. We also point out new conditions in the analysis that focus on the interplay between non-standard growth conditions and the boundary behavior in such generalized examples.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [30] [Vortex Solitons and Filamentation of Electromagnetic Beams in Relativistically Degenerate Plasmas](https://arxiv.org/abs/2601.10855)
*Nikolai Maltsev,Vazha I. Berezhiani*

Main category: physics.plasm-ph

TL;DR: Electromagnetic vortex beams form stable localized solitons with orbital angular momentum in relativistically degenerate plasmas, exhibiting azimuthal symmetry-breaking instabilities but maintaining topologically protected zero-intensity cores.


<details>
  <summary>Details</summary>
Motivation: To understand how electromagnetic vortex beams propagate and maintain stability in dense astrophysical plasmas, particularly for hard X-ray radiation applications in extreme environments like white dwarfs and neutron stars.

Method: Studied propagation and stability of electromagnetic vortex beams in relativistically degenerate plasmas through analysis of localized vortex solitons, examining both linear and nonlinear stability properties, and investigating symmetry-breaking instabilities.

Result: Vortex solitons exist but undergo azimuthal symmetry-breaking instabilities with growth rates dependent on beam power, propagation constant, and topological charge; they act as nonlinear attractors with finite basins of attraction while maintaining topologically protected zero-intensity cores.

Conclusion: Electromagnetic vortex beams can form stable structures in dense astrophysical plasmas with persistent topological protection of vortex cores, relevant for hard X-ray radiation propagation in extreme astrophysical environments.

Abstract: We study the propagation and stability of electromagnetic vortex beams in relativistically de generate plasmas. We show that such plasmas support localized vortex solitons carrying orbital angular momentum and analyze their linear and nonlinear stability. Vortex solitons undergo az imuthal symmetry-breaking instabilities whose growth rates depend on beam power, propagation constant, and topological charge, with the dominant mode determining the number of filaments formed during breakup. We further demonstrate that vortex solitons act as nonlinear attractors with a finite basin of attraction, while the vortex core remains topologically protected, maintaining a strictly zero field intensity at the beam center throughout the evolution. The results persist across a broad range of degeneracy parameters and are relevant to hard X-ray radiation propagating in dense astrophysical plasmas.

</details>


### [31] [Learning collision operators from plasma phase space data using differentiable simulators](https://arxiv.org/abs/2601.10885)
*Diogo D. Carvalho,Pablo J. Bilbao,Warren B. Mori,Luis O. Silva,E. Paulo Alves*

Main category: physics.plasm-ph

TL;DR: A method to learn collision operators from plasma phase space data using differentiable kinetic simulators and gradient-based optimization, achieving accurate results without prior time-scale assumptions.


<details>
  <summary>Details</summary>
Motivation: To develop a computational approach for inferring collision operators from plasma dynamics data that doesn't require prior assumptions about time-scales and can handle complex electromagnetic interactions.

Method: Combines a differentiable kinetic simulator (with differentiable Fokker-Planck solver) with gradient-based optimization to learn collision operators that best describe phase space dynamics from Particle-in-Cell simulation data.

Result: Learned operators are more accurate than particle-track-based estimates, require significantly less memory, and show excellent agreement with theoretical predictions for electrostatic scenarios in non-relativistic regimes.

Conclusion: Differentiable simulators provide a powerful and computationally efficient approach for inferring novel operators for complex plasma dynamics problems, including electromagnetic collisional dynamics and stochastic wave-particle interactions.

Abstract: We propose a methodology to infer collision operators from phase space data of plasma dynamics. Our approach combines a differentiable kinetic simulator, whose core component in this work is a differentiable Fokker-Planck solver, with a gradient-based optimisation method to learn the collisional operators that best describe the phase space dynamics. We test our method using data from two-dimensional Particle-in-Cell simulations of spatially uniform thermal plasmas, and learn the collision operator that captures the self-consistent electromagnetic interaction between finite-size charged particles over a wide variety of simulation parameters. We demonstrate that the learned operators are more accurate than alternative estimates based on particle tracks, while making no prior assumptions about the relevant time-scales of the processes and significantly reducing memory requirements. We find that the retrieved operators, obtained in the non-relativistic regime, are in excellent agreement with theoretical predictions derived for electrostatic scenarios. Our results show that differentiable simulators offer a powerful and computational efficient approach to infer novel operators for a wide rage of problems, such as electromagnetically dominated collisional dynamics and stochastic wave-particle interactions.

</details>


### [32] [Study of circular cross-section plasmas in HL-2A tokamak: MHD equilibrium, stability and operational \b{eta} limit](https://arxiv.org/abs/2601.11014)
*SHEN Yong,DONG Jiaqi,SHI Zhongbing,HE Hongda,ZHAO Kaijun,PENG Xiaodong,QU Hongpeng,LI Jia,SUN Aiping*

Main category: physics.plasm-ph

TL;DR: Study of MHD equilibrium and instability in circular cross-section tokamak plasmas on HL-2A, showing how safety factors and beta affect kink modes and establishing beta limits proportional to normalized current.


<details>
  <summary>Details</summary>
Motivation: Circular cross-section plasma is fundamental for magnetic confinement fusion, but understanding MHD equilibrium and instability in this basic configuration is crucial for tokamak operation and stability limits.

Method: Based on HL-2A limiter discharge experiments, investigating MHD equilibrium and instability through analysis of safety factors (q₀, qₐ), beta parameters, and their effects on kink modes.

Result: Internal kink mode (m/n=1/1) unstable at q₀=0.95; beta increase triggers external kink modes; qₐ>2 and q₀ slightly >1 stabilizes kink modes; maximum beta limit found to be β(max)~2.01I_N; HL-2A operational limit β_N^c~2.0; higher q₀ reduces beta limit to ~1.8 at q₀=1.3.

Conclusion: Safety factors and beta parameters critically determine MHD stability in circular tokamak plasmas, with specific operational windows for stability and established beta limits that guide tokamak operation and design.

Abstract: Circular cross-section plasma is the most basic form of tokamak plasma and the fundamental configuration for magnetic confinement fusion experiments. Based on the HL-2A limiter discharge experiments, the magnetohydrodynamic (MHD) equilibrium and MHD instability of circular cross-section tokamak plasmas are investigated in this work. The results show that when q_0=0.95, the internal kink mode of m/n=1/1 is always unstable. The increase in plasma \b{eta} (the ratio of thermal pressure to magnetic pressure) can lead to the appearance of external kink modes. The combination of axial safety factor q_0 and edge safety factor q_a determines the equilibrium configuration of the plasma and also affects the MHD stability of the equilibrium, but its growth rate is also related to the size of \b{eta}. Under the condition of q_a>2 and q_0 slightly greater than 1, the internal kink mode and surface kink mode can be easily stabilized. However the plasma becomes unstable again and the instability intensity increases as q_0 continues to increase when q_0 exceeds 1. As the poloidal beta (\b{eta}_p) increases, the MHD instability develops, the equilibrium configuration of MHD elongates laterally, and the Shafranov displacement increases, which in turn has the effect on suppressing instability. Calculations have shown that the maximum \b{eta} value imposed by the ideal MHD mode in a plasma with free boundary in tokamak experiments is proportional to the normalized current I_N (I_N=I_p (MA)/a(m)B_0 (T)), and the achievable maximum beta \b{eta}(max) is calibrated to be 2.01I_N,i.e. \b{eta}(max)~2.01I_N. The operational \b{eta} limit of HL-2A circular cross-section plasma is approximately \b{eta}_N^c~2.0. Too high a value of q_0 is not conducive to MHD stability and leads the \b{eta} limit value to decrease. When q_0=1.3, we obtain a maximum value of \b{eta}_N of approximately 1.8.

</details>


### [33] [A new class of special functions arising in plasma linear susceptibility tensor calculations](https://arxiv.org/abs/2601.11276)
*Roberto Ricci*

Main category: physics.plasm-ph

TL;DR: The paper introduces a new class of special functions related to Bessel, Anger and Weber functions, originally motivated by plasma physics calculations, and shows how they solve an inhomogeneous Bessel ODE with specific properties.


<details>
  <summary>Details</summary>
Motivation: The functions were originally motivated by linear susceptibility tensor calculations in hot, magnetized plasmas, where existing mathematical approaches led to computationally inefficient expressions involving infinite sums of Bessel functions with slow convergence.

Method: The authors show these functions are solutions of an inhomogeneous Bessel ODE with specified initial conditions and Nielsen's requirement. They derive recurrence relations, alternative representations using incomplete Anger-Weber functions, and a series expansion in terms of integer-order Bessel functions via the Jacobi-Anger formula.

Result: The functions admit a simple series expansion in terms of Bessel functions, but in plasma applications this leads to slowly-converging infinite sums when particle gyro-radius exceeds wavelength. Using recurrence properties, the authors derive a simpler expression for the linear susceptibility tensor that avoids this computational inconvenience.

Conclusion: The new class of special functions provides a mathematically rigorous foundation for plasma physics calculations, with recurrence properties enabling more efficient numerical evaluation of linear susceptibility tensors compared to traditional Bessel function expansions.

Abstract: We investigate some fundamental properties of a peculiar class of special functions strictly related to Bessel, Anger and Weber functions, whose introduction was originally motivated by linear susceptibility tensor calculations in a hot, magnetised plasma. We show that these functions are solutions of an inhomogeneous Bessel ODE, with specified initial conditions and a distinct right-hand-side term fulfilling the Nielsen's requirement. Beside deriving recurrence relations and an alternative representation involving incomplete Anger-Weber functions, we show that these functions admit a simple series expansion in terms of Bessel functions of integer order, obtained by resorting to the Jacobi-Anger formula. In plasma applications this eventually leads to expressions involving infinite sums of products of Bessel functions, not particularly apt to numerical evaluation ought to their slow convergence rate when the particle's gyro-radius is larger than the wavelength. By exploiting the previously determined recurrence properties of the new class of functions we present a particularly simple derivation of the linear susceptibility tensor that enables to avoid this inconvenience.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [34] [Specular differentiation in normed vector spaces and its application to nonsmooth convex optimization](https://arxiv.org/abs/2601.10950)
*Kiyuob Jung*

Main category: math.OC

TL;DR: Introduces specular differentiation as a generalization of Gâteaux and Fréchet differentiation for normed vector spaces, with applications to nonsmooth convex optimization.


<details>
  <summary>Details</summary>
Motivation: Classical differentiation methods (Gâteaux and Fréchet) are insufficient for handling nonsmooth functions in optimization problems. There's a need for a more general differentiation framework that can handle non-differentiable convex functions in higher-dimensional spaces.

Method: Develops specular differentiation as a generalization of existing differentiation concepts in normed vector spaces. Investigates theoretical properties including Quasi-Mean Value Theorem and Quasi-Fermat's Theorem. Creates three numerical optimization methods based on specular differentiation for nonsmooth convex functions.

Result: Theoretical framework established with fundamental properties proven. Numerical methods successfully minimize non-differentiable functions that classical optimization methods cannot handle, as demonstrated through experiments.

Conclusion: Specular differentiation provides a powerful generalization of classical differentiation concepts, enabling effective optimization of nonsmooth convex functions where traditional methods fail. The proposed numerical methods demonstrate practical utility for challenging optimization problems.

Abstract: This paper introduces specular differentiation, which generalizes Gâteaux and Fréchet differentiation in normed vector spaces. Fundamental theoretical properties of specular differentiation are investigated, including the Quasi-Mean Value Theorem and Quasi-Fermat's Theorem. As an application, three numerical methods using specular differentiation are devised to optimize nonsmooth convex functions in higher-dimensional Euclidean spaces. Numerical experiments demonstrate that the proposed methods are capable of minimizing non-differentiable functions that classical methods fail to minimize.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [35] [A numerical study on the effect of rolling friction on clogging of pores in particle-laden flows](https://arxiv.org/abs/2601.11121)
*Sagar G. Nayak,Zhenjiang You,Yuchen Dai,Geoff Wang,Prapanch Nair*

Main category: physics.flu-dyn

TL;DR: Study examines how rolling resistance affects particle clogging in porous media using DEM-IBM simulations, showing rolling friction influences jamming behavior at pore throats.


<details>
  <summary>Details</summary>
Motivation: Current models use rolling friction coefficient as lumped parameter for various micromechanical/hydrodynamic factors, but its direct influence on clogging behavior in porous media is not well understood.

Method: Developed DEM library coupled with open-source immersed boundary method (IBM) solver for pore/particle resolved simulations; performed direct numerical simulations (DNS) of dense suspension clogging at pore scale.

Result: Presented 3D validations for DEM library and DEM-IBM coupling; studied effect of rolling resistance on clogging at pore entry through simulations.

Conclusion: Rolling resistance significantly influences particle jamming and pore clogging behavior in porous media, providing insights for better clogging prediction models.

Abstract: Particulate matter in a fluid injected into a porous reservoir impairs its permeability spatio-temporally due to pore clogging. As particle volume fraction increases near the pore throats, inter-particle contact mechanics determine their jamming and subsequent pore clogging behavior. During contact of particles submerged in a fluid, in addition to sliding friction, a rolling resistance develops due to a several micromechanical and hydrodynamic factors. A coefficient of rolling friction is often used as a lumped parameter to characterize particle rigidity, particle shape, lubrication and fluid mediated resistance, however its direct influence on the clogging behavior is not well studied in literature. We study the effect of rolling resistance on the clogging behavior of a dense suspension at pore scale using direct numerical simulations (DNS). A discrete element method (DEM) library is developed and coupled with an open-source immersed boundary method (IBM) based solver to perform pore and particle resolved simulations. Several 3D validations are presented for the DEM library and the DEM-IBM coupling and the effect of rolling resistance on clogging at a pore entry is studied.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [36] [Walk based Laplacians for Modeling Diffusion on Complex Networks](https://arxiv.org/abs/2601.11338)
*Francesca Arrigo,Fabio Durastante*

Main category: cs.SI

TL;DR: Novel framework for diffusion modeling on networks using walk-based Laplacians with memory effects, including backtracking exclusion and downweighting, with efficient computational methods.


<details>
  <summary>Details</summary>
Motivation: To develop more flexible diffusion models for complex networks that incorporate memory effects and avoid limitations of standard Laplacian operators, particularly addressing backtracking behavior in network walks.

Method: Construct parametric family of walk-based Laplacian operators that count network traversals, with variants: (1) standard walk-based Laplacians, (2) nonbacktracking variants eliminating immediate reversals, and (3) backtrack-downweighted variants providing continuous interpolation. Use Krylov subspace methods for efficient computation.

Result: The operators extend standard Laplacian definitions while preserving key properties. Efficient algorithms enable application to large networks, with GPU acceleration. Numerical experiments on real-world networks validate modeling flexibility and computational efficiency.

Conclusion: The framework provides a flexible approach to diffusion modeling on complex networks with memory effects, offering computational efficiency for large-scale applications while maintaining mathematical rigor.

Abstract: We develop a novel framework for modeling diffusion on complex networks by constructing Laplacian-like operators based on walks around a graph. Our approach introduces a parametric family of walk-based Laplacians that naturally incorporate memory effects by excluding or downweighting backtracking trajectories, where walkers immediately revisit nodes. The framework includes: (i) walk-based Laplacians that count all traversals in the network; (ii) nonbacktracking variants that eliminate immediate reversals; and (iii) backtrack-downweighted variants that provide a continuous interpolation between these two regimes. We establish that these operators extend the definition of the standard Laplacian and also preserve some of its properties. We present efficient algorithms using Krylov subspace methods for computing them, ensuring applicability of our proposed framework to large networks. Extensive numerical experiments on real-world networks validate the modeling flexibility of our approach and demonstrate the computational efficiency of the proposed algorithms, including GPU acceleration.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [37] [Towards precision quantitative measurement of radiation reaction within the classical radiation-dominated regime](https://arxiv.org/abs/2601.10728)
*Minghao Ma,Ke Liu,Ge Zhou,Zhida Yang,Yulin Xin,Jiadong Yang,Pengfei Zhu,Yipeng Wu,Min Chen,Tongpu Yu,Wenchao Yan,Jie Zhang*

Main category: physics.optics

TL;DR: Proposes experimental scenario using petawatt laser colliding with MeV electron beam to study radiation reaction in classical radiation-dominated regime, with three key observables for validation.


<details>
  <summary>Details</summary>
Motivation: Radiation reaction lacks definitive experimental validation, especially for the transition from classical to quantum regime, requiring quantitative benchmarks.

Method: Collision of high-intensity petawatt-class laser with tens-of-MeV electron beam from LINAC, enabling access to classical radiation-dominated regime where radiation reaction dominates but quantum effects remain modest.

Result: Numerical simulations identify three key observables: 1) energy spectra measurement for quantum correction factor validation, 2) collision time delay control with charge-counting for intensity dependence mapping, 3) verification of 90° photon emission under recoil condition.

Conclusion: These experimental measurements will establish benchmarks for radiation reaction models across classical-to-quantum regime, providing critical insights into fundamental strong-field quantum electrodynamics.

Abstract: Radiation reaction (RR) is a fundamental yet incompletely validated process in laser-particle interactions, since it lacks quantitatively definitive experimental verifications, especially the transition from classical to quantum regime. Herein, we propose a novel experimental scenario for investigating radiation RR within the classical radiation-dominated regime (CRDR), via the collision of a high-intensity petawatt-class laser with a tens-of-MeV electron beam from a LINAC. This approach enables access to a distinct parameter regime wherein RR dominates electron dynamics while quantum effects remain modest. Numerical simulations demonstrate that three key observables exist for identifying the RR within this CRDR regime: (i) quantitative measurement of energy spectra to validate the quantum correction factor; (ii) control of the collision time delay with charge-counting to map intensity dependence of RR; and (iii) verification of large angle ($90^\circ$) photon emission under the recoil condition $2γ\gtrsim a_0$. These experimental measurements will establish the benchmarks for RR models spanning the classical-to-quantum regime, thereby providing critical insights into fundamental strong-field quantum electrodynamics.

</details>


### [38] [Generation and Enhancement of Persistent Nanoscale Magnetization in All-Dielectric Metasurfaces by Optically Injected and Localized Free Carriers](https://arxiv.org/abs/2601.11003)
*Shivaksh Rawat,Samyobrata Mukherjee,Gennady Shvets*

Main category: physics.optics

TL;DR: Time-varying dielectric metasurfaces enable frequency conversion and temporal scattering of metasurface-guided waves, generating quasistatic magnetic fields and residual magnetization.


<details>
  <summary>Details</summary>
Motivation: To explore how time-varying dielectric metasurfaces with sharp optical resonances can serve as temporal interfaces for manipulating metasurface-guided waves and generating persistent magnetic effects.

Method: Using analytical methods and electromagnetic simulations to study localized free-carrier generation in metasurfaces, enabling frequency conversion and temporal scattering of infrared metasurface-guided waves.

Result: Demonstrated frequency-shifted, time-refracted, and reflected infrared metasurface-guided waves; generated large, highly localized quasistatic magnetic fields within metasurfaces; showed residual nanoscale magnetization persists after wave departure; energy partitioning between scattered waves, carrier motion, and magnetic fields.

Conclusion: Time-varying dielectric metasurfaces provide a powerful platform for temporal wave manipulation and generation of persistent magnetic effects through free-carrier engineering, enabling new approaches to dynamic photonics and magneto-optical applications.

Abstract: Time-varying dielectric metasurfaces supporting sharp optical resonances with a non-trivial electromagnetic field distribution represent a unique platform for realizing temporal interfaces for metasurface-guided waves (MGWs). Rapidly changing metasurface resonance enables frequency conversion and temporal scattering of a concurrently propagating MGW. Using analytical methods and electromagnetic simulations, we demonstrate that localized free-carrier generation can be engineered to produce frequency-shifted, time-refracted, and reflected infrared MGWs. Furthermore, we demonstrate that such time interfaces can be utilized to generate large, highly localized quasistatic magnetic fields within the metasurfaces. The resulting nanoscale magnetization, supported by the residual circulating currents, persists after the departure of the time-scattered MGWs. We further demonstrate that the initial electromagnetic energy of the injected MGWs is partitioned between the time-reflected/refracted MGWs, residual motion of the free carriers, and a quasistatic magnetic field.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [39] [The rise and fall of stretched bond errors: Extending the analysis of Perdew-Zunger self-interaction corrections of reaction barrier heights beyond the LSDA](https://arxiv.org/abs/2601.11454)
*Yashpal Singh,Juan E Peralta,Koblar Alan Jackson*

Main category: physics.chem-ph

TL;DR: Self-interaction corrections (SIC) improve DFT barrier height predictions; orbital analysis shows stretched bond orbitals near transition states are major contributors; XC/H ratio identifies one-electron self-interaction error; SCAN meta-GGA may have reached best possible accuracy for semi-local functionals with Perdew-Zunger SIC.


<details>
  <summary>Details</summary>
Motivation: To understand how self-interaction corrections improve chemical reaction barrier height predictions in DFT, and to identify which orbitals contribute most to these corrections along reaction pathways.

Method: Orbital-by-orbital analysis using Fermi-Löwdin Orbital Self-Interaction Correction calculations on three semi-local DFAs (from Jacob's Ladder) for four reactions from BH76 benchmark set; tracking XC/H ratio (self-exchange-correlation to self-Hartree energy) along reaction pathways from reactants to transition states to products.

Result: Major SIC contributions come from stretched bond orbitals near transition states; XC/H ratio varies widely in practical DFAs (should be 1.0 for exact functional); largest XC/H values found in stretched/strongly lobed orbitals; differences in XC/H between configurations identify major SIC contributors; SCAN meta-GGA with Perdew-Zunger SIC may have reached optimal accuracy for semi-local functionals.

Conclusion: Self-interaction corrections significantly improve barrier height predictions by addressing errors in stretched bond orbitals near transition states; the XC/H ratio serves as a useful diagnostic for identifying one-electron self-interaction errors; SCAN meta-GGA with Perdew-Zunger SIC appears to achieve near-optimal accuracy for semi-local functionals in barrier height predictions.

Abstract: Incorporating self-interaction corrections (SIC) significantly improves chemical reaction barrier height predictions made using density functional theory methods. We present a detailed, orbital-by-orbital analysis of these corrections for three semi-local density functional approximations (DFAs) situated on the three lowest rungs of the Jacob's Ladder of approximations. The analysis is based on Fermi-Löwdin Orbital Self-Interaction Correction calculations performed at several steps along the reaction pathway from the reactants (R) to the transition state (TS) to the products (P) for four representative reactions selected from the BH76 benchmark set. For all three functionals, the major contribution to self-interaction corrections of the barrier heights can be traced to stretched bond orbitals that develop near the TS configuration. The magnitude of the ratio of the self-exchange-correlation energy to the self-Hartree energy (XC/H) for a given orbital is introduced as an indicator of one-electron self-interaction error. For the exact, but unknown density functional, XC/H = 1.0 for all orbitals, while for the practical DFAs studied here, XC/H spans a range of values. The largest values are obtained for stretched or strongly lobed orbitals. We show that significant differences in XC/H for corresponding orbitals in the R, TS, and P configurations can be used to identify the major contributors to the SIC of barrier heights and reaction energies. Based on such comparisons, we suggest that barrier height predictions made using the SCAN meta-generalized gradient approximation may have attained the best accuracy possible for a semi-local functional using the Perdew-Zunger SIC approach.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [40] [Temporal Complexity and Self-Organization in an Exponential Dense Associative Memory Model](https://arxiv.org/abs/2601.11478)
*Marco Cafiso,Paolo Paradisi*

Main category: nlin.AO

TL;DR: Dense Associative Memory models with exponential interactions exhibit complex temporal self-organization and scale-free behavior during learning, with critical regimes emerging over noise intensity intervals rather than single points.


<details>
  <summary>Details</summary>
Motivation: While Dense Associative Memory models have been studied for storage capacity, little attention has been given to their temporal self-organizing behavior during learning. The authors aim to investigate this using Temporal Complexity framework to understand how learning induces complex dynamics.

Method: The authors analyze a stochastic exponential DAM (SEDAM) model using Temporal Complexity framework, focusing on transition events between neural avalanche structures and coincidence structures. They systematically explore how TC indicators depend on control parameters (noise intensity and memory load).

Result: SEDAM exhibits regimes of complex intermittency with nontrivial temporal correlations and scale-free behavior, indicating spontaneous emergence of self-organizing dynamics. Critical regimes emerge in small noise intensity intervals (not single points), and the noise range needed for criticality slightly decreases with increasing memory load.

Conclusion: Temporal Complexity provides a complementary framework for understanding learning in neural systems, revealing a link between memory load and network self-organizing capacity. The findings support the extended criticality concept where critical behavior occurs over parameter intervals rather than isolated points.

Abstract: Dense Associative Memory (DAM) models generalize the classical Hopfield model by incorporating n-body or exponential interactions that greatly enhance storage capacity. While the criticality of DAM models has been largely investigated, mainly within a statistical equilibrium picture, little attention has been devoted to the temporal self-organizing behavior induced by learning. In this work, we investigate the behavior of a stochastic exponential DAM (SEDAM) model through the lens of Temporal Complexity (TC), a framework that characterizes complex systems by intermittent transition events between order and disorder and by scale-free temporal statistics. Transition events associated with birth-death of neural avalanche structures are exploited for the TC analyses and compared with analogous transition events based on coincidence structures. We systematically explore how TC indicators depend on control parameters, i.e., noise intensity and memory load. Our results reveal that the SEDAM model exhibits regimes of complex intermittency characterized by nontrivial temporal correlations and scale-free behavior, indicating the spontaneous emergence of self-organizing dynamics. These regimes emerge in small intervals of noise intensity values, which, in agreement with the extended criticality concept, never shrink to a single critical point. Further, the noise intensity range needed to reach the critical region, where self-organizing behavior emerges, slightly decreases as the memory load increases. This study highlights the relevance of TC as a complementary framework for understanding learning and information processing in artificial and biological neural systems, revealing the link between the memory load and the self-organizing capacity of the network.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs](https://arxiv.org/abs/2601.11259)
*Lorenzo Tomada,Federico Pichi,Gianluigi Rozza*

Main category: cs.LG

TL;DR: LD-GCN is an encoder-free GNN architecture for nonlinear MOR of parameterized PDEs that learns interpretable latent dynamics and enables time-extrapolation and zero-shot prediction.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based MOR methods fail to combine geometric inductive biases with interpretable latent behavior, often overlooking dynamics-driven features or disregarding spatial information.

Method: Proposes LD-GCN: a purely data-driven, encoder-free architecture that learns global low-dimensional representations of dynamical systems conditioned on inputs/parameters. Temporal evolution is modeled in latent space via time-stepping, with trajectories decoded onto geometrically parameterized domains using GNNs.

Result: Mathematically validated via universal approximation theorem for encoder-free architectures. Numerically tested on complex computational mechanics problems with physical/geometric parameters, including detection of bifurcating phenomena for Navier-Stokes equations.

Conclusion: LD-GCN addresses the gap in existing methods by combining geometric inductive biases with interpretable latent dynamics, enabling time-extrapolation, zero-shot prediction through latent interpolation, and enhanced interpretability for nonlinear MOR of parameterized PDEs.

Abstract: Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking dynamics-driven features or disregarding spatial information. In this work, we address this gap by introducing Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns a global, low-dimensional representation of dynamical systems conditioned on external inputs and parameters. The temporal evolution is modeled in the latent space and advanced through time-stepping, allowing for time-extrapolation, and the trajectories are consistently decoded onto geometrically parameterized domains using a GNN. Our framework enhances interpretability by enabling the analysis of the reduced dynamics and supporting zero-shot prediction through latent interpolation. The methodology is mathematically validated via a universal approximation theorem for encoder-free architectures, and numerically tested on complex computational mechanics problems involving physical and geometric parameters, including the detection of bifurcating phenomena for Navier-Stokes equations. Code availability: https://github.com/lorenzotomada/ld-gcn-rom

</details>


### [42] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: A novel data-driven latent space inference framework using paired autoencoders to handle observational inconsistencies in inverse problems, with learned mappings between latent spaces for regularized inversion.


<details>
  <summary>Details</summary>
Motivation: To address observational inconsistencies (partial, noisy, or out-of-distribution data) in inverse problems while maintaining consistency with underlying physical models, overcoming limitations of traditional methods that struggle with data corruption.

Method: Uses two autoencoders - one for parameter space and one for observation space - connected by learned mappings between their latent spaces. This enables surrogate regularized inversion and optimization in low-dimensional latent spaces, with ability to reconstruct corrupted data before parameter estimation.

Result: Produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of same architecture, especially in scenarios with data inconsistencies. Demonstrated effectiveness on medical tomography and geophysical seismic-waveform inversion.

Conclusion: The framework provides flexible, data-driven approach for inverse problems that handles observational inconsistencies while maintaining physical consistency, with broad applicability across scientific and engineering domains beyond the demonstrated imaging examples.

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [43] [On spectral interference of the short-time Fourier transform and its nonlinear variations](https://arxiv.org/abs/2601.10910)
*Shrikant Chand,James Nolen,Hau-Tieng Wu*

Main category: math.CA

TL;DR: The paper analyzes spectral interference in time-frequency representations, focusing on STFT with Gaussian windows and reassignment methods like synchrosqueezing. It quantifies frequency resolution limits, describes interference patterns, and develops theoretical frameworks to understand and mitigate distortion.


<details>
  <summary>Details</summary>
Motivation: Spectral interference severely distorts time-frequency representations in physical applications, creating misleading visualizations of signal components. Understanding when and why this occurs is crucial for accurate signal analysis.

Method: Theoretical analysis of STFT with Gaussian windows and nonlinear refinements like reassignment and synchrosqueezing transform. Uses two-component harmonic model, asymptotic analysis, Bargmann transform, Möbius geometry, and measure mapping perspective.

Result: Identifies critical frequency gap threshold for STFT resolution, describes interference bubble patterns, reveals canonical phase winding behavior, connects reassignment to holomorphic structures, and develops generalized synchrosqueezing framework to mitigate interference.

Conclusion: The paper provides comprehensive theoretical understanding of spectral interference, quantifies resolution limits, explains reassignment behavior through geometric frameworks, and offers generalized methods to improve time-frequency analysis in interference regimes.

Abstract: Spectral interference, the frequency counterpart of the beating phenomenon in the time domain, can severely distort time-frequency representations (TFRs) in physical applications. We study this phenomenon for the short-time Fourier transform (STFT) with a Gaussian window and for nonlinear refinements based on the reassignment method, with an emphasis on the synchrosqueezing transform (SST). Working with a two-component harmonic model, we quantify when STFT can (and cannot) resolve two nearby frequencies: a sharp transition occurs at a critical gap that scales inversely to kernel bandwidth and depends explicitly on the amplitude ratio. Below this threshold, the spectrogram ridges undergo bifurcation and form repeating time-frequency bubbles, which we describe asymptotically and, in the balanced-amplitude case, approximate closely by ellipses. We then analyze the STFT phase, showing a canonical winding behavior, and relate the complex-valued SST reassignment map to a holomorphic structure via the Bargmann transform. In the two-component setting the reassignment rule admits an explicit Mobius-geometry description, sending frequency lines to circular arcs in the complex plane. Finally, viewing SST and reassignment through a measure mapping perspective, we derive small-kernel asymptotics that explain when reassignment sharpens energy and when it produces distorted or misleading TFRs; we also introduce a generalized synchrosqueezing framework that isolates the role of STFT weighting and clarifies how alternative choices can mitigate interference in certain regimes.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [44] [NAVIS: A LAMMPS-Python framework for efficient computation of nanochannel velocity and thermal interfacial slip](https://arxiv.org/abs/2601.11391)
*Sleeba Varghese,Sobin Alosious,Jesper Schmidt Hansen,Billy Dean Todd*

Main category: cond-mat.soft

TL;DR: NAVIS is a LAMMPS-Python toolkit for computing hydrodynamic friction and thermal resistance at solid-fluid interfaces using equilibrium molecular dynamics.


<details>
  <summary>Details</summary>
Motivation: To provide a computational toolkit for studying interfacial friction and thermal transport in nanofluidic systems, which are crucial for practical applications but require specialized methods to measure.

Method: Equilibrium molecular dynamics (EMD) methods implemented as a LAMMPS-Python scripted toolkit, building on previous studies. The toolkit calculates linear response friction and thermal resistance at interfaces, with detailed instructions for water-graphene (hydrodynamic slip) and water-CNT (thermal slip) systems.

Result: A pedagogical framework and computational toolkit (NAVIS) that enables researchers to compute Navier friction coefficient and Kapitza thermal resistance at arbitrary solid-fluid interfaces using standard molecular dynamics software.

Conclusion: NAVIS provides a valuable resource for computational researchers studying interfacial phenomena in nanofluidic systems, facilitating efficient analysis of key factors for practical applications.

Abstract: We present NAVIS (NAnochannel Velocity and thermal Interfacial Slip), a LAMMPS-Python scripted toolkit for computing the Navier (hydrodynamic) friction coefficient and Kapitza (thermal) resistance at arbitrary solid-fluid interfaces. NAVIS is based on equilibrium molecular dynamics (EMD) methods for calculating the linear response friction and thermal resistance at the interface, as well as the corresponding velocity and temperature slips. The methodology is based on our previous studies (Hansen, et al., Phys. Rev. E 84, 016313 (2011); Varghese et al., J. Chem. Phys. 154, 184707 (2021); Alosious, et al., J. Chem. Phys. 151, 194502 (2019); Alosious, et al., Langmuir 37, 2355-2361 (2021)), and in this work we provide a pedagogical framework for the implementation of this toolkit on two systems: (i) a water-graphene system (for hydrodynamic slip) and (ii) a water-CNT system (for thermal slip). We provide detailed instructions for performing the EMD simulations using the LAMMPS package and processing the simulation outputs using Python modules to obtain the desired quantities of interest. We expect the toolkit to be useful for computational researchers studying interfacial friction and thermal transport, key factors for efficient and practical applications of nanofluidic systems.

</details>
