<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 39]
- [math.AP](#math.AP) [Total: 51]
- [physics.comp-ph](#physics.comp-ph) [Total: 10]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 11]
- [math-ph](#math-ph) [Total: 2]
- [math.SP](#math.SP) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.LO](#math.LO) [Total: 1]
- [math.FA](#math.FA) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 4]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.OA](#math.OA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [nucl-th](#nucl-th) [Total: 1]
- [math.DS](#math.DS) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [math.ST](#math.ST) [Total: 2]
- [nlin.PS](#nlin.PS) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Concatenated Matrix SVD: Compression Bounds, Incremental Approximation, and Error-Constrained Clustering](https://arxiv.org/abs/2601.11626)
*Maksym Shamrai*

Main category: math.NA

TL;DR: A framework for clustering matrices before SVD compression with guaranteed error bounds, enabling principled grouping decisions rather than heuristics.


<details>
  <summary>Details</summary>
Motivation: Current matrix compression via concatenation and SVD relies on heuristic grouping without theoretical guarantees on reconstruction error, leaving uncertainty about which matrices can be safely compressed together.

Method: Develop spectral bounds for concatenated matrices using Weyl-type monotonicity and incremental residual analysis, create efficient incremental SVD estimator, and propose three clustering algorithms with error control.

Result: Theoretical bounds on SVD reconstruction error for concatenated matrices, efficient estimator for dominant singular values without full matrix formation, and three clustering algorithms with speed-accuracy trade-offs.

Conclusion: Provides first principled framework for compression-aware matrix clustering with explicit error guarantees, enabling reliable SVD compression across diverse applications.

Abstract: Large collections of matrices arise throughout modern machine learning, signal processing, and scientific computing, where they are commonly compressed by concatenation followed by truncated singular value decomposition (SVD). This strategy enables parameter sharing and efficient reconstruction and has been widely adopted across domains ranging from multi-view learning and signal processing to neural network compression. However, it leaves a fundamental question unanswered: which matrices can be safely concatenated and compressed together under explicit reconstruction error constraints? Existing approaches rely on heuristic or architecture-specific grouping and provide no principled guarantees on the resulting SVD approximation error. In the present work, we introduce a theory-driven framework for compression-aware clustering of matrices under SVD compression constraints. Our analysis establishes new spectral bounds for horizontally concatenated matrices, deriving global upper bounds on the optimal rank-$r$ SVD reconstruction error from lower bounds on singular value growth. The first bound follows from Weyl-type monotonicity under blockwise extensions, while the second leverages singular values of incremental residuals to yield tighter, per-block guarantees. We further develop an efficient approximate estimator based on incremental truncated SVD that tracks dominant singular values without forming the full concatenated matrix. Therefore, we propose three clustering algorithms that merge matrices only when their predicted joint SVD compression error remains below a user-specified threshold. The algorithms span a trade-off between speed, provable accuracy, and scalability, enabling compression-aware clustering with explicit error control. Code is available online.

</details>


### [2] [Dirichlet Extremals for Discrete Plateau Problems in GT-Bezier Spaces via PSO](https://arxiv.org/abs/2601.11677)
*Muhammad Ammad,Md Yushalify Misro,Samia Bibi,Ahmad Ramli*

Main category: math.NA

TL;DR: The paper presents a two-level optimization method for constructing minimal-energy tensor-product surfaces using generalized trigonometric Bézier basis, reducing design freedom to a 4-parameter optimization problem solved by particle swarm optimization.


<details>
  <summary>Details</summary>
Motivation: To develop a discrete parametric Plateau problem solution that minimizes surface energy while interpolating boundary constraints, improving upon classical Bernstein-Bézier and other energy-based surface constructions.

Method: Uses generalized trigonometric Bézier basis with boundary interpolation via prescribed control net rows/columns. Interior control points determined by Dirichlet energy extremal principle, leading to symmetric linear system. Remaining 4 shape parameters optimized via particle swarm optimization. Also adapts method to hybrid tensor-product/bilinear Coons framework.

Result: The two-level procedure consistently decreases Dirichlet energy and often reduces surface area compared to classical Bernstein-Bézier Dirichlet patches, quasi-harmonic, and bending-energy constructions with identical boundary data.

Conclusion: The method successfully reduces design freedom to manageable optimization while producing minimal-energy surfaces, with potential applications in CAD and geometric modeling through both GT-Bézier and hybrid Coons patch frameworks.

Abstract: We study a discrete analogue of the parametric Plateau problem in a non-polynomial tensor-product surface spaces generated by the generalized trigonometric (GT)--Bézier basis. Boundary interpolation is imposed by prescribing the boundary rows and columns of the control net, while the interior control points are selected by a Dirichlet principle: for each admissible choice of Bézier basis shape parameters, we compute the unique Dirichlet-energy extremal within the corresponding GT--Bézier patch space, which yields a parameter-dependent symmetric linear system for the interior control net under standard nondegeneracy assumptions. The remaining design freedom is thereby reduced to a four-parameter optimization problem, which we solve by particle swarm optimization. Numerical experiments show that the resulting two-level procedure consistently decreases the Dirichlet energy and, in our tests, often reduces the realized surface area relative to classical Bernstein--Bézier Dirichlet patches and representative quasi-harmonic and bending-energy constructions under identical boundary control data. We further adapt the same Dirichlet-extremal methodology to a hybrid tensor-product/bilinear Coons framework, obtaining minimality-biased TB--Coons patches from sparse boundary specifications.

</details>


### [3] [Solving High-Dimensional PDEs Using Linearized Neural Networks](https://arxiv.org/abs/2601.11771)
*Tong Mao,Jinchao Xu,Xiaofeng Xu*

Main category: math.NA

TL;DR: Linearized shallow neural networks (RFM/ELM) for PDEs: variational methods suffer severe ill-conditioning, while collocation methods with robust solvers offer better stability and accuracy as neuron count increases.


<details>
  <summary>Details</summary>
Motivation: To understand the numerical behavior of linearized shallow neural networks (used in RFM and ELM) for solving PDEs, comparing variational (Galerkin) and collocation formulations to identify computational bottlenecks and stability issues.

Method: Numerical study comparing variational (Galerkin) and collocation formulations for linearized shallow neural networks with ReLU^k and tanh activations. Examines conditioning of linear systems, uses direct solvers for variational methods and robust least-squares solvers for collocation.

Result: Variational formulation produces severely ill-conditioned linear systems, limiting scalability even with direct solvers. Collocation methods with robust solvers show better numerical stability and achieve higher accuracy with increasing neurons. Random parameter sampling is unnecessary for high accuracy - deterministic schemes work for tanh activations.

Conclusion: Collocation methods with robust solvers are preferable over variational formulations for linearized neural networks in PDE solving due to better numerical stability and scalability. Random parameter sampling can be replaced by deterministic schemes without sacrificing accuracy.

Abstract: Linearized shallow neural networks that are constructed by fixing the hidden-layer parameters have recently shown strong performance in solving partial differential equations (PDEs). Such models, widely used in the random feature method (RFM) and extreme learning machines (ELM), transform network training into a linear least-squares problem. In this paper, we conduct a numerical study of the variational (Galerkin) and collocation formulations for these linearized networks. Our numerical results reveal that, in the variational formulation, the associated linear systems are severely ill-conditioned, forming the primary computational bottleneck in scaling the neural network size, even when direct solvers are employed. In contrast, collocation methods combined with robust least-squares solvers exhibit better numerical stability and achieve higher accuracy as we increase neuron numbers. This behavior is consistently observed for both ReLU$^k$ and $\tanh$ activations, with $\tanh$ networks exhibiting even worse conditioning. Furthermore, we demonstrate that random sampling of the hidden layer parameters, commonly used in RFM and ELM, is not necessary for achieving high accuracy. For ReLU$^k$ activations, this follows from existing theory and is verified numerically in this paper, while for $\tanh$ activations, we introduce two deterministic schemes that achieve comparable accuracy.

</details>


### [4] [Global Recovery from Local Data: Interior Nudging for 2D Navier-Stokes equations in a Physical Domain](https://arxiv.org/abs/2601.11831)
*Rui Fang,Ali Pakzad*

Main category: math.NA

TL;DR: Local data assimilation using only interior observations far from boundaries can achieve global flow field recovery comparable to full-domain assimilation.


<details>
  <summary>Details</summary>
Motivation: In practical data assimilation applications, sensor deployment is often constrained, making it crucial to determine if effective forecasting can be achieved with observations only in subregions, possibly far from boundaries.

Method: Extends continuous data assimilation framework to 2D incompressible Navier-Stokes equations with Dirichlet BCs. Uses rigorous mathematical analysis of convergence conditions for nudging parameter, spatial resolution, and observation region geometry. Computational validation via finite element methods over complex geometries.

Result: Theoretical proof shows assimilated solution converges globally to true solution when maximum distance from any domain point to observational subregion is bounded by constant multiple of ν¹ᐟ². Computational results show even greater robustness - synchronization achieved even beyond theoretical threshold. Local nudging performs comparably to full-domain assimilation, reaching machine precision accuracy. Observations near boundary found largely uninformative.

Conclusion: Full observability is not necessary for effective data assimilation. Carefully chosen interior observations, even far from boundaries, can suffice for global flow field recovery, offering practical advantages for sensor placement in constrained real-world applications.

Abstract: In many real-world applications of data assimilation (DA), the strategic placement of observers is crucial for effective and efficient forecasting. Motivated by practical constraints in sensor deployment, we show that global recovery of the flow field can be achieved using observations available only in a subregion of the domain, possibly far from the boundary. We focus on the two-dimensional incompressible Navier-Stokes equations posed in a bounded physical domain with Dirichlet boundary conditions. Building on the continuous data assimilation framework of Azouani, Olson, and Titi (2014), we rigorously prove that the assimilated solution converges globally to the true solution under suitable conditions on the nudging parameter, spatial resolution, and the geometry of the observation region, specifically, when the maximum distance from any point in the domain to the observational subregion is bounded by a constant multiple of \( ν^{1/2} \) (in terms of scaling). Our computational results, conducted via finite element methods over complex geometries, support the theoretical findings and reveal even greater robustness in practice. Specifically, synchronization with the true solution is achieved even when the observational subregion lies farther from the rest of the domain than the theoretical threshold permits. Across all three tested scenarios, the local nudging algorithm performs comparably to full-domain assimilation, reaching global accuracy up to machine precision. Interestingly, observational data near the boundary are found to be largely uninformative. This demonstrates that full observability is not necessary: carefully chosen interior observations, even far from the boundary, can suffice.

</details>


### [5] [A Separable and Asymptotic-Preserving Dynamical Low-Rank Method for the Vlasov--Poisson--Fokker--Planck System](https://arxiv.org/abs/2601.11900)
*Shiheng Zhang,Jingwei Hu*

Main category: math.NA

TL;DR: DLR method for VPFP system with conservative spatial discretization and time discretization handling stiff collisions, featuring first/second-order low-rank IMEX schemes with proven AP property.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for the Vlasov-Poisson-Fokker-Planck system that can handle stiff collisions while maintaining computational efficiency through low-rank approximations.

Method: Dynamical low-rank (DLR) method with: (i) conservative spatial discretization of Fokker-Planck operator factoring into velocity-only and space-only components for efficient low-rank projection, (ii) time discretization within DLR framework handling stiff collisions, proposing both first-order and second-order low-rank IMEX schemes.

Result: Proved asymptotic-preserving (AP) property for first-order scheme when field fluctuation is small. Numerical experiments demonstrate accuracy, robustness, and AP property at modest ranks.

Conclusion: The proposed DLR method provides an effective approach for VPFP system with efficient low-rank approximations, handling stiff collisions while maintaining accuracy and AP properties.

Abstract: We present a dynamical low-rank (DLR) method for the Vlasov--Poisson--Fokker--Planck (VPFP) system. Our main contributions are two-fold: (i) a conservative spatial discretization of the Fokker--Planck operator that factors into velocity-only and space-only components, enabling efficient low-rank projection, and (ii) a time discretization within the DLR framework that properly handles stiff collisions. We propose both first-order and second-order low-rank IMEX schemes. For the first-order scheme, we prove an asymptotic-preserving (AP) property when the field fluctuation is small. Numerical experiments demonstrate accuracy, robustness, and AP property at modest ranks.

</details>


### [6] [Phase-IDENT: Identification of Two-phase PDEs with Uncertainty Quantification](https://arxiv.org/abs/2601.11922)
*Edward L. Yang,Roy Y. He*

Main category: math.NA

TL;DR: Phase-IDENT is a method for identifying PDEs from noisy observations of dynamical systems with phase transitions, simultaneously identifying PDEs in each phase and reconstructing phase boundaries with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Many dynamical systems in fluid dynamics and materials science exhibit phase transitions, where different PDEs govern behavior in distinct regions separated by phase boundaries. Identifying these PDEs from noisy observations is challenging but important for understanding complex physical phenomena.

Method: Phase-IDENT simultaneously identifies underlying PDEs in each regime and reconstructs phase boundaries. It incorporates change point detection techniques to provide uncertainty quantification for detected boundaries, enhancing interpretability and robustness.

Result: Numerical experiments on various two-phase PDE systems under different noise levels demonstrate the effectiveness of the proposed approach in accurately identifying PDEs and reconstructing phase boundaries.

Conclusion: Phase-IDENT provides an effective framework for PDE identification in systems with phase transitions, offering simultaneous PDE identification, boundary reconstruction, and uncertainty quantification for enhanced interpretability in complex physical systems.

Abstract: We propose a novel method, Phase-IDENT, for identifying partial differential equations (PDEs) from noisy observations of dynamical systems that exhibit phase transitions. Such phenomena are prevalent in fluid dynamics and materials science, where they can be modeled mathematically as functions satisfying different PDEs within distinct regions separated by phase boundaries. Our approach simultaneously identifies the underlying PDEs in each regime and accurately reconstructs the phase boundaries. Furthermore, by incorporating change point detection techniques, we provide uncertainty quantification for the detected boundaries, enhancing the interpretability and robustness of our method. We conduct numerical experiments on a variety of two-phase PDE systems under different noise levels, and the results demonstrate the effectiveness of the proposed approach.

</details>


### [7] [A Survey on Spherical Designs: Existence, Numerical Constructions, and Applications](https://arxiv.org/abs/2601.11963)
*Congpei An,Xiaosheng Zhuang*

Main category: math.NA

TL;DR: Survey paper on spherical designs and their applications from a numerical analysis perspective, covering theoretical results, construction methods, and applications across multiple fields.


<details>
  <summary>Details</summary>
Motivation: Spherical designs represent a significant topic in point distributions on spheres with deep connections to various mathematical fields. The paper aims to provide a comprehensive survey from a numerical analysis viewpoint, highlighting both theoretical foundations and practical applications.

Method: The paper reviews existing literature on spherical designs, covering fundamental theoretical results, numerical construction methods (including optimization-based techniques and fast computational algorithms), and applied outcomes. It organizes the survey around key topics including existence proofs, construction techniques, and computational approaches.

Result: The survey synthesizes knowledge about spherical designs, demonstrating their mathematical elegance and practical utility across diverse applications including interpolation, numerical integration, hyperinterpolation, signal/image processing, and numerical solutions to PDEs and integral equations.

Conclusion: Spherical designs are mathematically rich structures with wide-ranging applications, particularly valuable from a numerical analysis perspective for their ability to provide efficient point distributions on spheres that preserve polynomial integration properties.

Abstract: This paper provides a survey of spherical designs and their applications, with a particular emphasis on the perspective of ``numerical analysis''. A set \(X_N\) of \(N\) points on the unit sphere \(\mathbb{S}^d\) is called a \textit{spherical \(t\)-design} if the average value of any polynomial of degree at most \(t\) over \(X_N\) equals its average over the entire sphere. Spherical designs represent one of the most significant topics in the study of point distributions on spheres. They are deeply connected to algebraic combinatorics, discrete geometry, differential geometry, approximation theory, optimization, coding theory, quantum physics, and other fields, which have led to the development of profound and elegant mathematical theories. This article reviews fundamental theoretical results, numerical construction methods, and applied outcomes related to spherical designs. Key topics covered include existence proofs, optimization-based construction techniques, fast computational algorithms, and applications in interpolation, numerical integration, hyperinterpolation, signal and image processing, as well as numerical solutions to partial differential and integral equations.

</details>


### [8] [The linearization approach to the Calderón problem revisited: reconstruction via the Born approximation](https://arxiv.org/abs/2601.11975)
*Carlos Castro,Fabricio Macià,Cristóbal Meroño,Daniel Sánchez-Mendoza*

Main category: math.NA

TL;DR: The paper studies linearization techniques for the Calderón inverse problem, proving exact linearization for radial conductivities and developing numerical algorithms for general conductivities using Born approximation.


<details>
  <summary>Details</summary>
Motivation: Linearization techniques are widely used for the Calderón inverse problem (reconstructing conductivity from Dirichlet-to-Neumann maps), but their theoretical foundation is not fully understood. The paper aims to provide rigorous analysis of linearization effectiveness.

Method: 1) Prove that any DtN map from radial conductivity admits exact representation as linearized DtN map with unique Born approximation. 2) Linearize on family of background conductivities including constant case. 3) Characterize Born approximation as solution of generalized moment problem. 4) Develop numerical algorithm for reconstructing Born approximation of general conductivity on unit disk. 5) Test resolution and robustness through numerical experiments.

Result: 1) Established rigorous foundation for linearization-based methods for radial conductivities. 2) Born approximation characterized as solution to generalized moment problem. 3) Developed numerical algorithm that works for general conductivities. 4) Numerical experiments demonstrate method's effectiveness. 5) Born approximation serves as starting point for full conductivity reconstruction.

Conclusion: Linearization techniques can be rigorously justified for radial conductivities, and the Born approximation approach provides a practical numerical method for general conductivities, offering both theoretical foundation and computational framework for solving the Calderón inverse problem.

Abstract: Linearization techniques are widely used in the analysis and numerical solution of the Calderón inverse problem, even if their theoretical basis is not fully understood. In this article, we study the effectiveness of linearization for reconstructing a conductivity from its Dirichlet-to-Neumann (DtN) map, combining rigorous analysis with numerical experiments. In particular, we prove that any DtN map arising from a radial conductivity in the unit ball of $\mathbb{R}^d$ admits an exact representation as a linearized DtN map for a uniquely determined integrable function, the Born approximation. We linearize on a family of background conductivities that includes the constant case, giving a rigorous foundation for linearization-based methods in this framework. We also characterize the Born approximation as a solution of a generalized moment problem. Since this moment problem is formally well-defined even for non-radial conductivities, we use it to develop a numerical algorithm to reconstruct the Born approximation of a general conductivity on the unit disk. We provide numerical experiments to test the resolution and robustness of the Born approximation in different situations. Finally, we show how it can be used as the starting point of an algorithm for reconstructing a conductivity from its DtN map.

</details>


### [9] [A Multi-Level Deep Framework for Deep Solvers of Partial Differential Equations](https://arxiv.org/abs/2601.12000)
*Yu Yang,Qiaolin He*

Main category: math.NA

TL;DR: A multi-level deep framework inspired by multigrid methods that uses adaptive sampling to concentrate training points in high-frequency regions and leverages neural network generalization to update PDEs across training levels.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of deep solvers for PDEs by addressing the challenge of capturing high-frequency components, inspired by the success of multigrid methods in traditional numerical analysis.

Method: A multi-level training framework with adaptive sampling that progressively concentrates training points in computational regions corresponding to high-frequency components, then uses neural network generalization to update PDEs for subsequent training levels based on results from all previous levels.

Result: The method's effectiveness is demonstrated through rigorous mathematical proofs and detailed numerical experiments, showing improved performance in solving PDEs with high-frequency components.

Conclusion: The proposed multi-level deep framework successfully addresses the challenge of capturing high-frequency components in PDE solutions by combining adaptive sampling with neural network generalization across multiple training levels.

Abstract: In this paper, inspired by the multigrid method, we propose a multi-level deep framework for deep solvers. Overall, it divides the entire training process into different levels of training. At each level of training, an adaptive sampling method proposed in this paper is first employed to obtain new training points, so that these points become increasingly concentrated in computational regions corresponding to high-frequency components. Then, the generalization ability of deep neural networks are utilized to update the PDEs for the next level of training based on the results from all previous levels. Rigorous mathematical proofs and detailed numerical experiments are employed to demonstrate the effectiveness of the proposed method.

</details>


### [10] [Streaming Operator Inference for Model Reduction of Large-Scale Dynamical Systems](https://arxiv.org/abs/2601.12161)
*Tomoki Koike,Prakash Mohan,Marc T. Henry de Frahan,Julie Bessac,Elizabeth Qian*

Main category: math.NA

TL;DR: Streaming OpInf enables efficient model reduction from sequential data streams using incremental SVD and recursive least squares, reducing memory by 99%+ while maintaining accuracy comparable to batch methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Operator Inference (OpInf) requires batch processing of all data simultaneously, which is memory-intensive for large-scale applications and doesn't support online model updates with new streaming data.

Method: Proposes Streaming OpInf that uses incremental SVD for adaptive basis construction and recursive least squares for streaming operator updates, enabling flexible combination of streaming algorithms for numerical linear algebra.

Result: Achieves accuracy comparable to batch OpInf while reducing memory requirements by over 99%, enabling dimension reductions exceeding 31,000x, with orders-of-magnitude faster predictions on benchmark problems and large-scale turbulent channel flow.

Conclusion: Streaming OpInf successfully addresses memory limitations of traditional batch OpInf, enables online model adaptation from streaming data, and provides efficient model reduction for large-scale applications.

Abstract: Projection-based model reduction enables efficient simulation of complex dynamical systems by constructing low-dimensional surrogate models from high-dimensional data. The Operator Inference (OpInf) approach learns such reduced surrogate models through a two-step process: constructing a low-dimensional basis via Singular Value Decomposition (SVD) to compress the data, then solving a linear least-squares (LS) problem to infer reduced operators that govern the dynamics in this compressed space, all without access to the underlying code or full model operators, i.e., non-intrusively. Traditional OpInf operates as a batch learning method, where both the SVD and LS steps process all data simultaneously. This poses a barrier to deployment of the approach on large-scale applications where dataset sizes prevent the loading of all data into memory at once. Additionally, the traditional batch approach does not naturally allow model updates using new data acquired during online computation. To address these limitations, we propose Streaming OpInf, which learns reduced models from sequentially arriving data streams. Our approach employs incremental SVD for adaptive basis construction and recursive LS for streaming operator updates, eliminating the need to store complete data sets while enabling online model adaptation. The approach can flexibly combine different choices of streaming algorithms for numerical linear algebra: we systematically explore the impact of these choices both analytically and numerically to identify effective combinations for accurate reduced model learning. Numerical experiments on benchmark problems and a large-scale turbulent channel flow demonstrate that Streaming OpInf achieves accuracy comparable to batch OpInf while reducing memory requirements by over 99% and enabling dimension reductions exceeding 31,000x, resulting in orders-of-magnitude faster predictions.

</details>


### [11] [Explicit symmetric low-regularity integrators for the semilinear Klein-Gordon equation](https://arxiv.org/abs/2601.12246)
*Zhirui Shen,Bin Wang*

Main category: math.NA

TL;DR: Symmetric low-regularity integrators for semilinear Klein-Gordon equation with improved convergence under relaxed regularity assumptions


<details>
  <summary>Details</summary>
Motivation: To develop symmetric numerical integrators for the semilinear Klein-Gordon equation that maintain good convergence properties even with low-regularity solutions, addressing the need for efficient and accurate long-term simulations

Method: Proposed a general symmetrization procedure to construct symmetric schemes from existing explicit (non-symmetric) integrators. Applied this to derive two novel symmetric schemes with error analysis showing optimal convergence in energy space under relaxed regularity

Result: Both integrators achieve optimal convergence orders in energy space under significantly relaxed regularity assumptions. First-order symmetric scheme shows improved convergence as solution regularity increases. Second-order symmetric scheme nearly preserves system energy over extended periods in numerical experiments

Conclusion: The symmetrization procedure successfully enables construction of symmetric low-regularity integrators for semilinear Klein-Gordon equation with improved convergence properties and long-term energy preservation

Abstract: This paper is concerned with the design and analysis of symmetric low-regularity integrators for the semilinear Klein-Gordon equation. We first propose a general symmetrization procedure that allows for the systematic construction of symmetric schemes from existing explicit (non-symmetric) integrators. Applying this procedure, we derive two novel schemes. Error analyses show that both integrators achieve their optimal convergence orders in the energy space under significantly relaxed regularity assumptions. Furthermore, the symmetry property ensures that the convergence order of a first-order symmetric scheme improves as the regularity of the exact solution increases. A numerical experiment demonstrates that the proposed second-order symmetric scheme nearly preserves the system energy over extended periods.

</details>


### [12] [Physics-informed machine learning for reconstruction of dynamical systems with invariant measure score matching](https://arxiv.org/abs/2601.12675)
*Yongsheng Chen,Suddhasattwa Das,Wei Guo,Xinghui Zhong*

Main category: math.NA

TL;DR: PINN-IMSM: A mesh-free framework using physics-informed neural networks with invariant measure score matching to reconstruct dynamical systems from unlabeled point-cloud data by learning score functions and solving score-based Fokker-Planck equations.


<details>
  <summary>Details</summary>
Motivation: To reconstruct dynamical systems from unlabeled point-cloud data that captures the system's invariant measure, overcoming limitations of mesh-based methods that suffer from the curse of dimensionality in high-dimensional problems.

Method: 1. Learn score function (gradient of log-density) directly from data via denoising score matching, bypassing explicit density estimation. 2. Embed learned score into physics-informed neural network (PINN) to reconstruct drift velocity field under score-based Fokker-Planck equation. 3. Recast as PDE-constrained optimization seeking minimal-energy velocity field. 4. Solve using stochastic augmented Lagrangian method.

Result: The framework accurately recovers invariant measures and reconstructs faithful dynamical behavior for problems up to five dimensions, demonstrated on Van der Pol oscillator, active swimmer in anharmonic trap, and chaotic Lorenz-63 and Lorenz-96 systems.

Conclusion: PINN-IMSM provides an effective mesh-free approach for reconstructing dynamical systems from point-cloud data, scaling to higher dimensions while addressing ill-posedness through PDE-constrained optimization with theoretical guarantees of unique solutions.

Abstract: In this paper, we develop a novel mesh-free framework, termed physics-informed neural networks with invariant measure score matching (PINN-IMSM), for reconstructing dynamical systems from unlabeled point-cloud data that capture the system's invariant measure. The invariant density satisfies the steady-state Fokker-Planck (FP) equation. We reformulate this equation in terms of its score function (the gradient of the log-density), which is estimated directly from data via denoising score matching, thereby bypassing explicit density estimation. This learned score is then embedded into a physics-informed neural network (PINN) to reconstruct the drift velocity field under the resulting score-based FP equation. The mesh-free nature of PINNs allows the framework to scale to higher dimensions, avoiding the curse of dimensionality inherent in mesh-based methods. To address the ill-posedness of high-dimensional inverse problems, we recast the problem as a PDE-constrained optimization that seeks the minimal-energy velocity field. Under suitable conditions, we prove that this problem admits a unique solution that depends continuously on the score function. The constrained formulation is solved using a stochastic augmented Lagrangian method. Numerical experiments on representative dynamical systems, including the Van der Pol oscillator, an active swimmer in an anharmonic trap, and the chaotic Lorenz-63 and Lorenz-96 systems, demonstrate that PINN-IMSM accurately recovers invariant measures and reconstructs faithful dynamical behavior for problems in up to five dimensions.

</details>


### [13] [Optimal Error Estimates of a Linearized Backward Euler Localized Orthogonal Decomposition for the Landau-Lifshitz Equation](https://arxiv.org/abs/2601.12734)
*Zetao Ma,Rui Du,Lei Zhang*

Main category: math.NA

TL;DR: A new spatial discretization method for simulating Landau-Lifshitz magnetization dynamics using Localized Orthogonal Decomposition with systematic error analysis.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable and efficient simulation technique for magnetization dynamics governed by the Landau-Lifshitz equation, addressing the need for accurate spatial discretization methods.

Method: Proposes a novel spatial discretization technique based on the Localized Orthogonal Decomposition (LOD) method, with systematic decomposition of overall discretization error into temporal and spatial components, and spatial error analysis within the LOD framework.

Result: Numerical examples validate the accuracy and approximation properties of the proposed scheme, demonstrating its effectiveness for simulating magnetization dynamics.

Conclusion: The LOD-based spatial discretization provides a reliable and efficient approach for Landau-Lifshitz equation simulations with systematic error control and validated accuracy.

Abstract: We introduce a novel spatial discretization technique for the reliable and efficient simulation of magnetization dynamics governed by the Landau-Lifshitz (LL) equation. The overall discretization error is systematically decomposed into temporal and spatial components. The spatial error analysis is conducted by formulating the LL equation within the framework of the Localized Orthogonal Decomposition (LOD) method. Numerical examples are presented to validate the accuracy and approximation properties of the proposed scheme.

</details>


### [14] [High-order Lagrange multiplier schemes for general Hamiltonian PDEs](https://arxiv.org/abs/2601.12776)
*Yonghui Bo,Yushun Wang*

Main category: math.NA

TL;DR: A Lagrange multiplier approach for constructing linearly implicit energy-preserving schemes of arbitrary order for Hamiltonian PDEs, offering broader applicability than auxiliary variable methods while preserving original energy exactly.


<details>
  <summary>Details</summary>
Motivation: Existing auxiliary variable methods for energy-preserving schemes require the nonlinear part of energy to be bounded from below, limiting their applicability. There's a need for methods that can handle more general Hamiltonian PDEs while preserving the original energy exactly.

Method: A novel Lagrange multiplier approach is introduced to construct linearly implicit energy-preserving schemes. Unlike auxiliary variable methods, this approach doesn't require the nonlinear energy part to be bounded from below. The method involves solving a nonlinear algebraic equation to determine the Lagrange multiplier, but maintains computational efficiency comparable to existing methods.

Result: The proposed schemes preserve the original energy exactly at both continuous and discrete levels (not a modified energy). Rigorous proofs are provided for energy conservation and numerical accuracy. Numerical experiments show the schemes are efficient, accurate, and structure-preserving, with computational cost generally not dominant despite needing to solve a nonlinear equation.

Conclusion: The Lagrange multiplier approach provides a more general framework for constructing energy-preserving schemes for Hamiltonian PDEs, overcoming limitations of auxiliary variable methods while maintaining computational efficiency and exact energy preservation.

Abstract: In this paper, we introduce a Lagrange multiplier approach to construct linearly implicit energy-preserving schemes of arbitrary order for general Hamiltonian PDEs. Unlike the widely used auxiliary variable methods, this novel approach does not require the nonlinear part of the energy to be bounded from below, thereby offering broader applicability. Moreover, this approach preserves the original energy exactly at both the continuous and discrete levels, as opposed to a modified energy preserved by the auxiliary variable methods. Rigorous proofs are provided for the energy conservation and numerical accuracy of all derived schemes. The trade-off for these advantages is the need to solve a nonlinear algebraic equation to determine the Lagrange multiplier. Nevertheless, numerical experiments show that the associated computational cost is generally not dominant, indicating that the new schemes retain computational efficiency comparable to the auxiliary variable-based schemes. Numerical results demonstrate the efficiency, accuracy, and structure-preserving properties of the proposed schemes.

</details>


### [15] [Graph Laplacian assisted regularization method under noise level free heuristic and statistical stopping rule](https://arxiv.org/abs/2601.12792)
*Harshit Bajpai,Ankik Kumar Giri*

Main category: math.NA

TL;DR: A graph-based regularization framework for solving ill-posed inverse problems with iterative graph Laplacian updates, using two stopping criteria and requiring no prior noise level knowledge.


<details>
  <summary>Details</summary>
Motivation: To address the solution of both linear and nonlinear ill-posed inverse problems with a flexible regularization approach that adapts to the evolving solution structure without requiring prior knowledge of noise levels.

Method: Develops a novel graph-based regularization framework where regularization is formulated through an iteratively updated graph Laplacian. Uses two stopping criteria: heuristic rule and statistical discrepancy principle (requiring averaged measurements from multiple observations). Starts with initial reconstruction from methods like Tikhonov, FBP, or TV, then iteratively refines reconstruction while dynamically updating the graph Laplacian to reflect solution structure changes.

Result: Provides detailed convergence analysis establishing stability and regularization properties under both stopping strategies. Numerical experiments on X-ray CT and phase retrieval CT demonstrate the method's effectiveness and robustness, with performance comparisons under both stopping rules.

Conclusion: The proposed graph-based regularization framework effectively solves ill-posed inverse problems by adaptively updating the regularization structure, works without prior noise knowledge, and shows strong performance in CT applications with both stopping criteria.

Abstract: In this work, we address the solution of both linear and nonlinear ill-posed inverse problems by developing a novel graph-based regularization framework, where the regularization term is formulated through an iteratively updated graph Laplacian. The proposed approach operates without prior knowledge of the noise level and employs two distinct stopping criteria namely, the heuristic rule and the statistical discrepancy principle. To facilitate the latter, we utilize averaged measurements derived from multiple repeated observations. We provide a detailed convergence analysis of the method in statistical prospective, establishing its stability and regularization properties under both stopping strategies. The algorithm begins with the computation of an initial reconstruction using any suitable techniques like Tikhonov regularization (Tik), filtered back projection (FBP) or total variation (TV), which is used as the foundation for generating the initial graph Laplacian. The reconstruction is made better step by step using an iterative process, during which the graph Laplacian is dynamically re-calibrated to reflect how the solution's structure is changing. Finally, we present numerical experiments on X-ray Computed Tomography (CT) and phase retrieval CT, demonstrating the effectiveness and robustness of the proposed method and comparing its reconstruction performance under both stopping rules.

</details>


### [16] [Two Frameworks and their Fourth Order Implicit Schemes for Time Discretization of Maxwell's Equations](https://arxiv.org/abs/2601.12793)
*Archana Arya,Kaushik Kalyanaraman*

Main category: math.NA

TL;DR: Energy-conserving fourth-order time discretizations for Maxwell's equations using compatible finite elements, with two strategies: spatial (replacing time derivatives with spatial ones via Maxwell's equations) and temporal (using higher-order finite differences).


<details>
  <summary>Details</summary>
Motivation: To develop high-order (fourth-order) time discretization methods for Maxwell's equations that preserve energy conservation properties, which is crucial for long-time simulations and physical accuracy in electromagnetic modeling.

Method: Two strategies: 1) Spatial strategy - Taylor expand solution in time, use Maxwell's equations to replace fourth-order time derivatives with higher-order spatial derivatives; 2) Temporal strategy - use higher-order finite difference schemes for time derivative terms in truncated Taylor approximation. Both use compatible de Rham finite element spaces for spatial discretization.

Result: Developed fourth-order time-accurate schemes for Maxwell's equations that are both stable and convergent, with energy conservation properties. Validated with numerical examples in ℝ².

Conclusion: Successfully developed two classes of fourth-order time discretization methods for Maxwell's equations that conserve energy, with proven convergence for semi- and full-discretizations. The strategies are generalizable to other linear/quasi-linear time-dependent PDE systems.

Abstract: Our work is about energy conserving fourth-order time discretizations of a three-field formulation of Maxwell's equations in conjunction with a spatial discretization using higher-order and compatible de Rham finite element spaces. Toward this end, we delineate two broad classes of strategies for general higher-order time discretizations which we term spatial and temporal strategies. We provide a description of these two strategies and develop fourth-order time accurate schemes in the context of our Maxwell's system. However, our description can be used to prescribe similar fourth- or even higher-order time-integration methods for any linear (or quasi-linear) system of time-dependent partial differential equations. Our organizing principle in our proposed two strategies is to Taylor expand the unknown solution in time by assuming sufficient regularity. Then, in the spatial strategy, we use Maxwell's equations themselves to replace the fourth-order time derivatives in an appropriately truncated Taylor expansion with corresponding higher-order spatial derivatives. On the other hand, in the temporal strategy, we simply use higher-order finite difference schemes for the various higher-order time derivative terms in the truncated Taylor approximation. In both cases, we then defer to a standard finite element exterior calculus manner of compatible discretization for the spatial component of the Maxwell's solution. For our proposed schemes corresponding to the two strategies, we show that they are both stable and convergent and provide some validating numerical examples in $\mathbb{R}^2$. Our main contributions are in the development of the fourth-order time discretization methods that are energy conserving using our two outlined strategies and proofs of their convergence for semi- and full-discretizations of our three-field system of Maxwell's equations.

</details>


### [17] [Data-Consistent Learning of Inverse Problems](https://arxiv.org/abs/2601.12831)
*Markus Haltmeier,Gyeongha Hwang*

Main category: math.NA

TL;DR: DC networks combine classical regularization with learned methods to create convergent regularization methods that are both theoretically sound and visually appealing.


<details>
  <summary>Details</summary>
Motivation: Inverse problems are ill-posed with non-uniqueness and instability issues. Classical regularization provides mathematical guarantees but lacks flexibility/visual quality, while learned methods produce good visuals but lack theoretical guarantees. There's a need to bridge this gap.

Method: DC networks enforce the measurement model within network architecture. Specifically, null-space networks combined with classical regularization as initial reconstruction define a convergent regularization method.

Result: The approach preserves theoretical reliability of classical schemes while leveraging expressive power of data-driven learning, yielding reconstructions that are both accurate and visually appealing.

Conclusion: DC networks address the limitations of both classical and learned reconstruction methods by creating a hybrid approach that maintains mathematical guarantees while achieving superior visual quality.

Abstract: Inverse problems are inherently ill-posed, suffering from non-uniqueness and instability. Classical regularization methods provide mathematically well-founded solutions, ensuring stability and convergence, but often at the cost of reduced flexibility or visual quality. Learned reconstruction methods, such as convolutional neural networks, can produce visually compelling results, yet they typically lack rigorous theoretical guarantees. DC (DC) networks address this gap by enforcing the measurement model within the network architecture. In particular, null-space networks combined with a classical regularization method as an initial reconstruction define a convergent regularization method. This approach preserves the theoretical reliability of classical schemes while leveraging the expressive power of data-driven learning, yielding reconstructions that are both accurate and visually appealing.

</details>


### [18] [A hierarchical splitting approach for N-split differential equations](https://arxiv.org/abs/2601.12878)
*Kevin Schäfers,Michael Günther*

Main category: math.NA

TL;DR: Hierarchical splitting approach for differential equations that constructs N-split methods from two-split methods, with analysis of convergence, error terms, and applications to multirate integration.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic framework for constructing splitting methods for N-split systems by leveraging existing two-split methods, enabling efficient numerical integration for complex differential equations with multiple time scales.

Method: Hierarchical splitting approach that recursively applies splitting methods for two-split systems to build methods for N-split systems. The approach is augmented with multiple time-stepping techniques and includes analysis of convergence order, error terms, self-adjointness, and computational order characterization.

Result: Theoretical analysis provides explicit formulas for leading-order error terms, conditions for increased convergence rates in multirate settings, and demonstrates computational efficiency through numerical simulations on rigid body equations and separable Hamiltonian systems with multirate potential.

Conclusion: Hierarchical splitting methods offer a promising framework at the intersection of geometric numerical integration and multirate integration, providing systematic construction of efficient integrators for complex differential equations with theoretical guarantees and practical computational benefits.

Abstract: We propose a hierarchical splitting approach to differential equations that provides a design principle for constructing splitting methods for $N$-split systems by iteratively applying splitting methods for two-split systems. We analyze the convergence order, derive explicit formulas for the leading-order error terms, and investigate self-adjointness. Moreover, we discuss compositions of hierarchical splitting methods in detail. We further augment the hierarchical splitting approach with multiple time-stepping techniques, turning the class into a promising framework at the intersection of geometric numerical integration and multirate integration. In this context, we characterize the computational order of a multirate integrator and establish conditions on the multirate factors that guarantee an increased convergence rate in practical computations up to a certain step size. Finally, we design several hierarchical splitting methods and perform numerical simulations for rigid body equations and a separable Hamiltonian system with multirate potential, confirming the theoretical findings and showcasing the computational efficiency of hierarchical splitting methods.

</details>


### [19] [Machine Learning for highly oscillatory differential equations](https://arxiv.org/abs/2601.12907)
*Maxime Bouchereau*

Main category: math.NA

TL;DR: Neural networks replace heavy pre-computations in solving highly oscillatory differential equations, combined with micro-macro techniques for efficient numerical solutions.


<details>
  <summary>Details</summary>
Motivation: Highly oscillatory differential equations in multi-scale problems are analytically intractable. Existing numerical methods require heavy pre-computations from averaging theory, creating computational bottlenecks.

Method: Use neural networks to approximate vector fields needed for pre-computations, then combine with micro-macro techniques to solve oscillatory problems efficiently.

Result: The approach is demonstrated through numerical simulations, showing it can effectively handle oscillatory differential equations.

Conclusion: Machine learning (neural networks) can successfully replace traditional heavy pre-computations in solving oscillatory differential equations, enabling more efficient numerical solutions when combined with micro-macro techniques.

Abstract: Highly oscillatory differential equations, commonly encountered in multi-scale problems, are often too complex to solve analytically. However, several numerical methods have been developed to approximate their solutions. Although these methods have shown their efficiency, the first part of the strategy often involves heavy pre-computations from averaging theory. In this paper, we leverage neural networks (machine learning) to approximate the vector fields required by the pre-computations in the first part, and combine this with micro-macro techniques to efficiently solve the oscillatory problem. We illustrate our work by numerical simulations.

</details>


### [20] [An iterative approach to a fluid-rigid body interaction problem](https://arxiv.org/abs/2601.13004)
*Charles M. Elliott,Thomas Sales*

Main category: math.NA

TL;DR: Short-time existence of strong solutions for 3D fluid-rigid body interaction with small density ratio, using iterative approach on prescribed evolution domains.


<details>
  <summary>Details</summary>
Motivation: To establish existence of solutions for incompressible fluid-rigid body interaction problems in three dimensions, which is challenging due to the moving boundary and coupling between Navier-Stokes equations and rigid body motion.

Method: Introduces an iterative approach based on solving a sequence of related problems on domains with prescribed evolution. The method couples incompressible Navier-Stokes equations with ODEs governing rigid body motion, using no-slip boundary conditions on the rigid body boundary.

Result: Proves short-time existence of strong solutions when the relative density ratio (fluid density/body density) is sufficiently small. Numerical experiments in 2D demonstrate the necessity of this smallness assumption.

Conclusion: The iterative approach successfully establishes existence results for fluid-rigid body interaction problems under small density ratio conditions, and the method has potential applications in numerical methods for moving boundary problems.

Abstract: We study a novel approach for the existence of solutions to an incompressible fluid-rigid body interaction problem in three dimensions. Our approach introduces an iteration based on a sequence of related problems posed on domains with prescribed evolution. In particular we prove the short-time existence of strong solutions to a system coupling the incompressible Navier--Stokes equations to the ordinary differential equations governing the motion of a rigid body, with no slip boundary conditions on the boundary of the rigid body, provided that the relative density $\fracρ{ρ_B}$, is sufficiently small. We also discuss the use of our iterative approach in numerical methods for the moving boundary problem, and complement this with some numerical experiments in two dimensions which demonstrate the necessity of the smallness assumption on $\fracρ{ρ_B}$.

</details>


### [21] [Solving Generalized Lyapunov Equations with guarantees: application to the Model Reduction of Switched Linear Systems](https://arxiv.org/abs/2601.13039)
*Mattia Manucci,Benjamin Unger*

Main category: math.NA

TL;DR: Efficient approximation of generalized Lyapunov equations with error guarantees for model order reduction of switched linear systems, overcoming limitations of classical balanced truncation through piecewise balanced reduction.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by applications in model order reduction of switched linear systems, where generalized Lyapunov equations play a central role. Classical balanced-truncation error estimates for SLS are not viable due to restrictive assumptions requiring exact satisfaction of LMIs by numerically computed GLE solutions.

Method: Proposes piecewise balanced reduction (PBR) framework for SLS, based on solving multiple GLEs and constructing piecewise constant time-varying projection matrices. The method extends standard balanced-truncation error bounds to account for inexact LMI solutions and piecewise constant projections.

Result: Develops rigorous, computable error guarantees for GLE approximations and demonstrates how PBR formulation controls errors from inexact LMI solutions. The approach accounts for influence of piecewise constant time-varying projection matrices, making it applicable to a broad class of SLS.

Conclusion: PBR approach overcomes limitations of classical balanced truncation for SLS, providing a flexible framework with controlled error bounds that accounts for numerical inaccuracies in GLE solutions and piecewise constant projections, validated through numerical experiments.

Abstract: We present an efficient strategy to approximate the solutions of large-scale generalized Lyapunov equations (GLEs) with rigorous, computable error guarantees. This work is motivated by applications in model order reduction (MOR) of switched linear systems (SLS) in control form, where GLEs play a central role. We analyze how inaccuracies in the numerical solution of GLEs propagate through the MOR procedure and affect the accuracy and reliability of the reduced order model. Furthermore, the classical balanced-truncation error estimate for SLS is neither theoretically nor practically viable, as they rely on restrictive assumptions requiring several requiring several linear matrix inequalities (LMI) to be satisfied exactly by numerically computed solutions of the GLEs. To overcome these limitation, we propose a new MOR framework for SLS, called piecewise balanced reduction (PBR). The method is based on solving multiple GLEs and the construction of projection matrices that are piecewise constant in time to appropriately balance and subsequently reduce the SLS. We extend the standard balanced-truncation error bounds and demonstrate that the PBR formulation allows us to control the error arising from the inexact LMI. In addition, our new error bound accounts for the influence of the piecewise constant time-varying projection matrices. Altogether, this renders the PBR approach for SLS applicable to a broad and flexible class of SLS. Numerical experiments are provided to corroborate our theoretical results.

</details>


### [22] [Stochastic Gradient Descent for Nonlinear Inverse Problems in Banach Spaces](https://arxiv.org/abs/2601.13110)
*Bangti Jin,Zeljko Kereta,Yuxin Xia*

Main category: math.NA

TL;DR: SGD for nonlinear inverse problems in Banach spaces with iterative regularization analysis


<details>
  <summary>Details</summary>
Motivation: SGD is widely used in ML but lacks theoretical analysis for nonlinear inverse problems in infinite-dimensional Banach spaces. Need to understand its convergence and regularization properties for such problems.

Method: Apply SGD to solve nonlinear inverse problems in Banach spaces, analyze through iterative regularization framework. Prove convergence under general assumptions, establish regularizing properties with a priori stopping rules, derive convergence rates under conditional stability assumptions.

Result: Proved almost sure convergence to minimum distance solution, showed regularizing property in expectation, established convergence rates for both exact and noisy data under conditional stability assumptions.

Conclusion: SGD provides effective iterative regularization method for nonlinear inverse problems in Banach spaces with theoretical guarantees and practical performance demonstrated on tomography applications.

Abstract: Stochastic gradient descent (SGD) and its variants are widely used and highly effective optimization methods in machine learning, especially for neural network training. By using a single datum or a small subset of the data, selected randomly at each iteration, SGD scales well to problem size and has been shown to be effective for solving large-scale inverse problems. In this work, we investigate SGD for solving nonlinear inverse problems in Banach spaces through the lens of iterative regularization. Under general assumptions, we prove almost sure convergence of the iterates to the minimum distance solution and show the regularizing property in expectation under an a priori stopping rule. Further, we establish convergence rates under the conditional stability assumptions for both exact and noisy data. Numerical experiments on Schlieren tomography and electrical impedance tomography are presented to show distinct features of the method.

</details>


### [23] [Towards Matrix-Free Patch Smoothers for the Stokes Problem: Evaluating Local p-Multigrid Solvers](https://arxiv.org/abs/2601.13230)
*Michał Wichrowski*

Main category: math.NA

TL;DR: A matrix-free, iterative p-multigrid approach for vertex-patch smoothers in Stokes equations shows Braess-Sarazin preconditioners are robust, with single iteration achieving convergence rates comparable to exact solvers.


<details>
  <summary>Details</summary>
Motivation: Vertex-patch smoothers are effective for robust geometric multigrid convergence in Stokes equations with high-order finite elements, but their practical efficiency is limited by the computational cost of solving local saddle-point problems, especially when explicit matrix factorizations are not feasible.

Method: The paper explores a fully iterative, matrix-free-compatible approach using p-multigrid techniques for local patch solves. It evaluates different local solver configurations including Braess-Sarazin and block-triangular preconditioners.

Result: Numerical experiments show the Braess-Sarazin approach is particularly resilient. A single iteration of the local solver yields global convergence rates comparable to those obtained with exact local solvers, even on distorted meshes and with large viscosity jumps.

Conclusion: The proposed matrix-free iterative approach with Braess-Sarazin preconditioning provides an efficient alternative to exact local solves for vertex-patch smoothers in Stokes equations, maintaining robustness across challenging conditions.

Abstract: Vertex-patch smoothers offer an effective strategy for achieving robust geometric multigrid convergence for the Stokes equations, particularly in the context of high-order finite elements. However, their practical efficiency is often limited by the computational cost of solving the local saddle-point problems, especially when explicit matrix factorizations are not feasible. We explore a fully iterative, matrix-free-compatible approach to the local patch solve using $p$-multigrid techniques. We evaluate different local solver configurations: Braess-Sarazin and block-triangular preconditioners. Our numerical experiments suggest that the Braess-Sarazin approach is particularly resilient. We find that a single iteration of the local solver yields global convergence rates comparable to those obtained with exact local solvers, even on distorted meshes and in the presence of large viscosity jumps.

</details>


### [24] [Deep Neural networks for solving high-dimensional parabolic partial differential equations](https://arxiv.org/abs/2601.13256)
*Wenzhong Zhang,Zhenyuan Hu,Wei Cai,George EM Karniadakis*

Main category: math.NA

TL;DR: This review paper provides a tutorial introduction to neural-network-based methods for solving high-dimensional parabolic PDEs, organized around three main paradigms to overcome the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: High-dimensional PDEs suffer from the curse of dimensionality, making classical grid-based methods impractical beyond a few dimensions. Deep neural networks offer a promising mesh-free alternative for approximating PDE solutions in tens to thousands of dimensions.

Method: The paper organizes methods into three paradigms: (1) PDE residual-based approaches (including physics-informed neural networks), (2) stochastic methods from Feynman-Kac and backward stochastic differential equations, and (3) hybrid derivative-free random difference approaches to reduce computational cost of derivatives in high dimensions.

Result: The methods demonstrate scalability, effectiveness, and accuracy on benchmark problems including Hamilton-Jacobi-Bellman and Black-Scholes equations in up to 1000 dimensions, showing neural networks can overcome the curse of dimensionality for high-dimensional PDEs.

Conclusion: Neural-network-based methods provide promising alternatives to classical PDE solvers for high-dimensional problems, but open challenges remain for developing reliable and scalable solvers, with future directions needed to address current limitations.

Abstract: The numerical solution of high dimensional partial differential equations (PDEs) is severely constrained by the curse of dimensionality (CoD), rendering classical grid--based methods impractical beyond a few dimensions. In recent years, deep neural networks have emerged as a promising mesh free alternative, enabling the approximation of PDE solutions in tens to thousands of dimensions. This review provides a tutorial--oriented introduction to neural--network--based methods for solving high dimensional parabolic PDEs, emphasizing conceptual clarity and methodological connections. We organize the literature around three unifying paradigms: (i) PDE residual--based approaches, including physicsinformed neural networks and their high dimensional variants; (ii) stochastic methods derived from Feynman--Kac and backward stochastic differential equation formulations; and (iii) hybrid derivative--free random difference approaches designed to alleviate the computational cost of derivatives in high dimensions. For each paradigm, we outline the underlying mathematical formulation, algorithmic implementation, and practical strengths and limitations. Representative benchmark problems--including Hamilton--Jacobi--Bellman and Black--Scholes equations in up to 1000 dimensions --illustrate the scalability, effectiveness, and accuracy of the methods. The paper concludes with a discussion of open challenges and future directions for reliable and scalable solvers of high dimensional PDEs.

</details>


### [25] [A Scalable Sequential Framework for Dynamic Inverse Problems via Model Parameter Estimation](https://arxiv.org/abs/2601.13347)
*Aryeh Keating,Mirjeta Pasha*

Main category: math.NA

TL;DR: Memory-efficient Kalman filter framework for dynamic CT reconstruction with minimal hyperparameter tuning, integrating motion models and expectation-maximization for parameter estimation.


<details>
  <summary>Details</summary>
Motivation: Large-scale dynamic inverse problems are ill-posed due to model complexity and high dimensionality. Classical regularization is infeasible due to memory requirements, requiring sequential online methods. Need for memory-efficient framework for dynamic CT reconstruction with minimal hyperparameter tuning.

Method: Prior-informed, dimension-reduced Kalman filter with smoothing. Integrates regularized motion models with expectation-maximization strategies for estimating state transition dynamics and error covariances within Kalman filtering framework.

Result: Demonstrated effectiveness through numerical experiments on limited-angle and single-shot CT problems. Shows improvements in reconstruction accuracy, memory efficiency, and computational cost.

Conclusion: Proposed framework addresses practical deployment challenges of Kalman filtering for dynamic CT reconstruction, enabling memory-efficient sequential processing with automatic parameter estimation.

Abstract: Large-scale dynamic inverse problems are often ill-posed due to model complexity and the high dimensionality of the unknown parameters. Regularization is commonly employed to mitigate ill-posedness by incorporating prior information and structural constraints. However, classical regularization formulations are frequently infeasible in this setting due to prohibitive memory requirements, necessitating sequential methods that process data and state information online, eliminating the need to form the full space-time problem. In this work, we propose a memory-efficient framework for reconstructing dynamic sequences of undersampled images from computerized tomography data that requires minimal hyperparameter tuning. The approach is based on a prior-informed, dimension-reduced Kalman filter with smoothing. While well suited for dynamic image reconstruction, practical deployment is challenging when the state transition model and covariance parameters must be initialized without prior knowledge and estimated in a single pass. To address these limitations, we integrate regularized motion models with expectation-maximization strategies for the estimation of state transition dynamics and error covariances within the Kalman filtering framework. We demonstrate the effectiveness of the proposed method through numerical experiments on limited-angle and single-shot computerized tomography problems, highlighting improvements in reconstruction accuracy, memory efficiency, and computational cost.

</details>


### [26] [Sparse Identification of Nonlinear Distributed-Delay Dynamics via the Linear Chain Trick](https://arxiv.org/abs/2601.13536)
*Mohammed Alanazi,Majid Bani-Yaghoub*

Main category: math.NA

TL;DR: Extends SINDy framework to identify distributed-delay differential equations using Linear Chain Trick, enabling joint inference of governing equations, mean delay, and delay distribution dispersion from time-series data.


<details>
  <summary>Details</summary>
Motivation: Existing SINDy extensions can recover discrete delay differential equations but cannot capture distributed delays that naturally arise in biological, physical, and engineering systems. Distributed delays are common in real-world systems but current methods lack the ability to identify them.

Method: Extends SINDy by incorporating the Linear Chain Trick (LCT), which provides a finite-dimensional ordinary differential equation representation of distributed memory effects. This allows SINDy to operate in an augmented state space using conventional sparse regression while maintaining interpretability of delayed influences through the chain trick.

Result: The method accurately reconstructs distributed delay dynamics, remains robust under noise and sparse sampling conditions. Successfully demonstrated on several models including logistic growth model and Hes1-mRNA gene regulatory network model.

Conclusion: Provides a transparent, data-driven approach for discovering nonlinear systems with distributed-delay, enabling joint inference of governing equations, mean delay, and delay distribution dispersion from time-series data.

Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) framework has been frequently used to discover parsimonious differential equations governing natural and physical systems. This includes recent extensions to SINDy that enable the recovery of discrete delay differential equations, where delay terms are represented explicitly in the candidate library. However, such formulations cannot capture the distributed delays that naturally arise in biological, physical, and engineering systems. In the present work, we extend SINDy to identify distributed-delay differential equations by incorporating the Linear Chain Trick (LCT), which provides a finite-dimensional ordinary differential equation representing the distributed memory effects. Hence, SINDy can operate in an augmented state space using conventional sparse regression while preserving a clear interpretation of delayed influences via the chain trick. From time-series data, the proposed method jointly infers the governing equations, the mean delay, and the dispersion of the underlying delay distribution. We numerically verify the method on several models with distributed delay, including the logistic growth model and a Hes1--mRNA gene regulatory network model. We show that the proposed method accurately reconstructs distributed delay dynamics, remains robust under noise and sparse sampling, and provides a transparent, data-driven approach for discovering nonlinear systems with distributed-delay.

</details>


### [27] [A hybrid numerical method for a microscopic and macroscopic traffic flow model](https://arxiv.org/abs/2601.13541)
*Yuanhong Wu,Shuzhi Liu,Qinglong Zhang*

Main category: math.NA

TL;DR: A traffic flow model combining microscopic follow-the-leader principles with macroscopic constraints, reformulating ARZ models to describe realistic fundamental diagrams, with Riemann problem solutions and 2D extensions validated numerically.


<details>
  <summary>Details</summary>
Motivation: To develop a traffic flow model that bridges microscopic follow-the-leader behavior with macroscopic flow constraints, creating a more realistic representation of traffic dynamics that can capture fundamental diagrams observed in real traffic.

Method: Introduces a microscopic follow-the-leader model with density/velocity constraints, reformulates classical ARZ models using advected variables (velocity offset p and relative velocity u), derives elementary waves, solves Riemann problems, extends to 2D, and validates with hybrid Godunov-Glimm numerical simulations.

Result: Successfully developed a traffic flow model with conservative macroscopic formulation, derived elementary waves, solved Riemann problems for theoretical consistency, extended to 2D, and verified performance through numerical simulations showing realistic fundamental diagrams.

Conclusion: The proposed model effectively bridges microscopic and macroscopic traffic flow descriptions, provides theoretical consistency through Riemann problem solutions, and demonstrates practical applicability through successful 1D and 2D numerical simulations.

Abstract: In this paper, we introduce a traffic flow model based on a microscopic follow-the-leader model, while enforcing maximal constraints on the density and velocity of the flow. The related macroscopic model can be represented in conservative formulation. By introducing an advected variable up with the flow, where p is the velocity offset, and u is the relative velocity, we reformulate the classical Aw-Rascle-Zhang (ARZ) model and the modified Aw-Rascle model to describe a realistic fundamental diagrams. The elementary waves are derived, and the Riemann problem is solved to validate the model's theoretical consistency. We further extend to a two-dimensional model. Numerical simulations are given for both one-and two-dimensional case by using the hybrid Godunov-Glimm scheme to verify the model's performance.

</details>


### [28] [Nonlinear fractional-periodic boundary value problems with Hilfer fractional derivative: existence and numerical approximations of solutions](https://arxiv.org/abs/2601.13584)
*Niels Goedegebure,Kateryna Marynets*

Main category: math.NA

TL;DR: This paper develops analytical and numerical methods for solving boundary value problems with Hilfer fractional derivatives, including fractional-periodic boundary conditions, using perturbation techniques and Bernstein splines.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for analytical solutions to boundary value problems with Hilfer fractional derivatives, which generalize Riemann-Liouville and Caputo operators. It tackles the challenge of singular solutions near t=0 and develops numerical methods to approximate these solutions.

Method: The authors use perturbation of initial value problems with enforced boundary conditions. They construct converging solutions in weighted continuous function spaces to overcome singularities. For numerical approximation, they implement Bernstein splines and prove convergence with criteria and asymptotic rates. They also apply grid search to match original non-perturbed systems.

Result: The paper proves existence conditions for analytical solutions and develops a convergent numerical method using Bernstein splines. Numerical examples show empirical convergence matching theoretical bounds, with the method capable of approximating singular solution behavior and handling nonlinear problems.

Conclusion: The developed analytical and numerical framework successfully solves boundary value problems with Hilfer fractional derivatives, overcoming singularity challenges and providing convergent approximations for both linear and nonlinear problems.

Abstract: We prove conditions for existence of analytical solutions for boundary value problems with the Hilfer fractional derivative, generalizing the commonly used Riemann-Liouville and Caputo operators. The boundary values, referred to in this paper as fractional-periodic, are fractional integral conditions generalizing recurrent solution values for the non-Caputo case of the Hilfer fractional derivative. Analytical solutions to the studied problem are obtained using a perturbation of the corresponding initial value problem with enforced boundary conditions. In general, solutions to the boundary value problem are singular for $t\downarrow 0$. To overcome this singularity, we construct a sequence of converging solutions in a weighted continuous function space. We present a Bernstein splines-based implementation to numerically approximate solutions. We prove convergence of the numerical method, providing convergence criteria and asymptotic convergence rates. Numerical examples show empirical convergence results corresponding with the theoretical bounds. Moreover, the method is able to approximate the singular behavior of solutions and is demonstrated to converge for nonlinear problems. Finally, we apply a grid search to obtain correspondence to the original, non-perturbed system.

</details>


### [29] [Optimizing Parallel Schemes with Lyapunov Exponents and kNN-LLE Estimation](https://arxiv.org/abs/2601.13604)
*Mudassir Shams,Andrei Velichko,Bruno Carpentieri*

Main category: math.NA

TL;DR: A unified analytical-data-driven framework for identifying and reducing instabilities in inverse parallel root-finding schemes using Lyapunov exponent analysis and adaptive parameter selection.


<details>
  <summary>Details</summary>
Motivation: Inverse parallel schemes for solving nonlinear systems exhibit complex dynamical behavior (from contraction to chaos) depending on parameters and initial states, requiring better tools to identify and mitigate these instabilities.

Method: Combines theoretical stability/bifurcation analysis with computational micro-series pipeline using kNN-driven estimation of local largest Lyapunov exponent (LLE) from solver trajectories, plus Lyapunov-informed parameter selection strategy.

Result: Close agreement between theoretical stability diagrams and empirical Lyapunov profiles; adaptive parameter selection significantly improves solver robustness; micro-series Lyapunov analysis proves practical for constructing self-stabilizing schemes.

Conclusion: Establishes micro-series Lyapunov analysis as an interpretable tool for stabilizing root-finding algorithms and opens avenues for extending diagnostics to higher-dimensional or noisy problems.

Abstract: Inverse parallel schemes remain indispensable tools for computing the roots of nonlinear systems, yet their dynamical behavior can be unexpectedly rich, ranging from strong contraction to oscillatory or chaotic transients depending on the choice of algorithmic parameters and initial states. A unified analytical-data-driven methodology for identifying, measuring, and reducing such instabilities in a family of uni-parametric inverse parallel solvers is presented in this study. On the theoretical side, we derive stability and bifurcation characterizations of the underlying iterative maps, identifying parameter regions associated with periodic or chaotic behavior. On the computational side, we introduce a micro-series pipeline based on kNN-driven estimation of the local largest Lyapunov exponent (LLE), applied to scalar time series derived from solver trajectories. The resulting sliding-window Lyapunov profiles provide fine-grained, real-time diagnostics of contractive or unstable phases and reveal transient behaviors not captured by coarse linearized analysis. Leveraging this correspondence, we introduce a Lyapunov-informed parameter selection strategy that identifies solver settings associated with stable behavior, particularly when the estimated LLE indicates persistent instability. Comprehensive experiments on ensembles of perturbed initial guesses demonstrate close agreement between the theoretical stability diagrams and empirical Lyapunov profiles, and show that the proposed adaptive mechanism significantly improves robustness. The study establishes micro-series Lyapunov analysis as a practical, interpretable tool for constructing self-stabilizing root-finding schemes and opens avenues for extending such diagnostics to higher-dimensional or noise-contaminated problems.

</details>


### [30] [Improving the local solution of the DG predictor of the ADER-DG method for solving systems of ordinary differential equations and its applicability to systems of differential-algebraic equations](https://arxiv.org/abs/2601.13908)
*I. S. Popov*

Main category: math.NA

TL;DR: Improved ADER-DG method with local DG predictor achieves one higher convergence order for ODE systems while maintaining continuity at grid nodes, with rigorous proofs and applications to DAEs.


<details>
  <summary>Details</summary>
Motivation: To enhance the ADER-DG numerical method by improving local numerical solutions for solving initial value problems of ODE systems, achieving higher accuracy and smoothness while maintaining the method's structural advantages.

Method: Proposes an improved local numerical solution for ADER-DG method with local DG predictor. Uses rigorous proofs based on ε-embedding method to demonstrate applicability to DAE systems. Maintains original ADER-DG structure while achieving higher convergence orders.

Result: Improved solution shows convergence orders one higher than original ADER-DG, with continuity at grid nodes. Empirical results confirm expected convergence orders across various error norms. Method maintains superconvergence and stability properties while providing higher accuracy and smoothness.

Conclusion: The improved ADER-DG method successfully enhances local numerical solutions without structural changes, achieving higher convergence orders and smoothness while preserving all original advantages, with rigorous theoretical foundation and practical validation.

Abstract: Improved local numerical solution for the ADER-DG numerical method with a local DG predictor for solving the initial value problem for a first-order ODE system is proposed. The improved local numerical solution demonstrates convergence orders of one higher than the convergence order of the local numerical solution of the original ADER-DG numerical method and has the property of continuity at grid nodes. Rigorous proofs of the approximation orders of the local numerical solution and the improved local numerical solution are presented. Obtaining the proposed improved local numerical solution does not require significant changes to the structure of the ADER-DG numerical method. Therefore, all conclusions regarding the convergence orders of the numerical solution at grid nodes, the resulting superconvergence, and the high stability of the ADER-DG numerical method remain unchanged. A wide range of applications of the ADER-DG numerical method is presented for solving specific initial value problems for ODE systems for a wide range of polynomial degrees. The obtained results provide strong confirmation for the developed rigorous theory. The improved local numerical solution is shown to exhibit both higher accuracy and improved smoothness and point-wise comparability. Empirical convergence orders of all individual numerical solutions were calculated for a wide range of error norms, which well agree with the expected convergence orders. The rigorous proof, based on the $ε$-embedding method, of the applicability of the ADER-DG numerical method with a local DG predictor to solving DAE systems is presents.

</details>


### [31] [Direct Finite-Time Contraction (Step-Log) Profiling--Driven Optimization of Parallel Schemes for Nonlinear Problems on Multicore Architectures](https://arxiv.org/abs/2601.13637)
*Mudassir Shams,Andrei Velichko,Bruno Carpentieri*

Main category: math.NA

TL;DR: A third-order parallel Weierstrass-type solver with data-driven parameter tuning using contraction profiling for improved stability and convergence.


<details>
  <summary>Details</summary>
Motivation: High-order parallel iterative methods for nonlinear problems are sensitive to internal parameters and lack reproducible tuning procedures. Classical parameter selection methods are problem-dependent and computationally expensive, motivating lightweight data-driven alternatives.

Method: Proposes a parameterized single-step bi-parametric parallel Weierstrass-type scheme with third-order convergence, plus a training-free tuning framework using Direct finite-time contraction (step-log) profiling. The approach extracts Lyapunov-like contraction information from solver trajectories via step norms and step-log ratios, aggregates profiles over micro-launch ensembles, and ranks parameters using two compact scores: stability minimum S_min and stability moment S_mom.

Result: Numerical results show consistent improvements in convergence rate, stability, and robustness across diverse nonlinear test problems.

Conclusion: The proposed profiling-based strategy establishes an efficient and reproducible alternative to classical parameter tuning methods for parallel iterative solvers.

Abstract: Efficient computation of all distinct solutions of nonlinear problems is essential in many scientific and engineering applications. Although high-order parallel iterative schemes offer fast convergence, their practical performance is often limited by sensitivity to internal parameters and the lack of reproducible tuning procedures. Classical parameter selection tools based on analytical conditions and dynamical-system diagnostics can be problem-dependent and computationally demanding, which motivates lightweight data-driven alternatives.
  In this study, we propose a parameterized single-step bi-parametric parallel Weierstrass-type scheme with third-order convergence together with a training-free tuning framework based on Direct finite-time contraction (step-log) profiling. The approach extracts Lyapunov-like finite-time contraction information directly from solver trajectories via step norms and step-log ratios, aggregates the resulting profiles over micro-launch ensembles, and ranks parameter candidates using two compact scores: the stability minimum S_min and the stability moment S_mom. Numerical results demonstrate consistent improvements in convergence rate, stability, and robustness across diverse nonlinear test problems, establishing the proposed profiling-based strategy as an efficient and reproducible alternative to classical parameter tuning methods.

</details>


### [32] [Nonlinear compressive reduced basis approximation : when Taylor meets Kolmogorov](https://arxiv.org/abs/2601.13712)
*Joubine Aghili,Hassan Ballout,Yvon Maday,Christophe Prud'homme*

Main category: math.NA

TL;DR: The paper analyzes model reduction for multi-parameter PDEs, showing that when Kolmogorov N-width decays slowly, linear complexity measures must be replaced with Gelfand width concepts, requiring nonlinear mappings beyond quadratic forms for efficient approximation.


<details>
  <summary>Details</summary>
Motivation: Traditional reduced-order models with O(N³) complexity become prohibitive when N must be large due to slow decay of Kolmogorov N-width in multi-parameter PDEs. New approaches are needed to overcome this "Kolmogorov Barrier" for efficient parameter-dependent PDE approximation.

Method: The paper investigates nonlinear model reduction methods based on Gelfand width concepts, decomposing N coordinates into n free variables and n̄ dependent variables with nonlinear mappings. It provides rigorous analysis of local sensing number, showing n = p (parameter dimension) is optimal locally, with p ≤ n ≤ p + k being more robust for wide parameter ranges.

Result: Analysis reveals that quadratic mappings are insufficient beyond local approximations, and more expressive nonlinear mappings (including machine learning approaches) become necessary. The work establishes theoretical foundations for nonlinear reduction strategies to overcome Kolmogorov Barrier limitations.

Conclusion: The paper provides a theoretical framework for nonlinear model reduction in multi-parameter PDEs, demonstrating the limitations of quadratic mappings and the need for more expressive nonlinear approaches. It highlights the importance of further research to develop effective strategies that can overcome the Kolmogorov Barrier for efficient high-dimensional parameter approximation.

Abstract: This paper investigates model reduction methods for efficiently approximating the solution of parameter-dependent PDEs with a multi-parameter vector $\vecμ \in \mathbb{R}^p$. In cases where the Kolmogorov $N$-width decays fast enough, it is effective to approximate the solution as a sum of $N$ separable terms, each being the product of a parameter-dependent coefficient and a space-dependent function. This leads to reduced-order models with $N$ degrees of freedom and complexity of order ${\mathcal O}(N^3)$.
  However, when the $N$-width decays slowly, $N$ must be large to achieve acceptable accuracy, making cubic complexity prohibitive. The linear complexity measure in terms of Kolmogorov width must be replaced by the Gelfand width, with its associated sensing number. Recent nonlinear approaches based on this notion decompose the $N$ coordinates into two groups: $n$ free variables and $\overline{n}$ dependent variables, where the latter are nonlinear functions of the former ($N= n+\overline n$). Several works have focused on cases where these $\overline{n}$ functions are homogeneous quadratic forms of the $n$ variables, with optimization strategies for choosing $n$ given a target accuracy.
  A rigorous analysis of the local sensing number is carried out, showing that $n = p$ is optimal and appropriate, at least locally, around a reference point. In practical scenarios involving wide parameter ranges, the condition $p\le n \le p + k$ (with $k$ small) is valid and more robust from continuity arguments. Additionally, the assumption of a quadratic mapping, while justified in a local sense, becomes insufficient. More expressive nonlinear mappings-including those using machine learning-become necessary. This work contributes a theoretical foundation for such strategies and highlights the need for further investigations to push back the Kolmogorov Barrier.

</details>


### [33] [A Hybridizable Discontinuous Galerkin Method for the non--local Camassa--Holm--Kadomtsev--Petviashvili equation](https://arxiv.org/abs/2601.13800)
*Mukul Dwivedi,Ruben Gutendorf,Andreas Rupp*

Main category: math.NA

TL;DR: A hybridizable discontinuous Galerkin method for 2D Camassa-Holm-Kadomtsev-Petviashvili equation using Cartesian meshes with tensor-product polynomials, localizing non-local operator via auxiliary variable, proving energy stability and O(h^{k+1/2}) convergence.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for the challenging 2D Camassa-Holm-Kadomtsev-Petviashvili equation that can handle its non-local operator and accurately resolve both smooth solutions and peaked solitary waves (peakons).

Method: Hybridizable discontinuous Galerkin method on Cartesian meshes with tensor-product polynomial spaces, treating x and y derivatives separately. The non-local operator ∂_x^{-1}u_y is localized through an auxiliary variable v satisfying v_x = u_y, enabling efficient element-by-element computations.

Result: Proved energy stability of the semi-discrete scheme and derived O(h^{k+1/2}) convergence in space. Numerical experiments validate theoretical results and demonstrate accurate resolution of both smooth solutions and peaked solitary waves (peakons).

Conclusion: The developed hybridizable discontinuous Galerkin method provides an effective approach for solving the 2D Camassa-Holm-Kadomtsev-Petviashvili equation with proven stability, convergence, and capability to handle challenging features like peakons.

Abstract: This paper develops a hybridizable discontinuous Galerkin method for the two-dimensional Camassa--Holm--Kadomtsev--Petviashvili equation. The method employs Cartesian meshes with tensor-product polynomial spaces, enabling separate treatment of \(x\) and \(y\) derivatives. The non-local operator \(\partial_{x}^{-1}u_{y}\) is localized through an auxiliary variable \(v\) satisfying \(v_x = u_y\), allowing efficient element-by-element computations. We prove energy stability of the semi-discrete scheme and derive \(\mathcal{O}(h^{k+1/2})\) convergence in space. Numerical experiments validate the theoretical results and demonstrate the method's capability to accurately resolve smooth solutions and peaked solitary waves (peakons).

</details>


### [34] [Multi-Trace Müller Boundary Integral Equation for Electromagnetic Scattering by Composite Objects](https://arxiv.org/abs/2601.13823)
*Van Chien Le,Kristof Cools*

Main category: math.NA

TL;DR: Extension of Müller equation to composite dielectric scattering using global multi-trace method with Stratton-Chu representation, yielding well-conditioned second-kind operator system.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and efficient boundary integral formulation for time-harmonic electromagnetic scattering by composite dielectric objects that remains well-conditioned on dense meshes and at low frequencies without additional stabilization.

Method: Extends classical Müller equation to composite structures via global multi-trace method using Stratton-Chu representation in complementary regions (extinction property). Uses Petrov-Galerkin discretization with Rao-Wilton-Glisson trial functions and Buffa-Christiansen test functions.

Result: The formulation yields a block system composed entirely of second-kind operators. Linear systems remain well-conditioned on dense meshes and at low frequencies without additional stabilization, reducing computational costs for matrix-vector multiplications and iterative solving.

Conclusion: The proposed method provides an accurate and computationally efficient approach for electromagnetic scattering by composite dielectric objects, with numerical experiments demonstrating accuracy in computing field traces and derived quantities.

Abstract: This paper introduces a boundary integral equation for time-harmonic electromagnetic scattering by composite dielectric objects. The formulation extends the classical Müller equation to composite structures through the global multi-trace method. The key ingredient enabling this extension is the use of the Stratton-Chu representation in complementary region, also known as the extinction property, which augments the off-diagonal blocks of the interior representation operator. The resulting block system is composed entirely of second-kind operators. A Petrov-Galerkin (mixed) discretization using Rao-Wilton-Glisson trial functions and Buffa-Christiansen test functions is employed, yielding linear systems that remain well conditioned on dense meshes and at low frequencies without the need for additional stabilization. This reduces computational costs associated with matrix-vector multiplications and iterative solving. Numerical experiments demonstrate the accuracy of the method in computing field traces and derived quantities.

</details>


### [35] [Numerical solution of Smoluchowski coagulation equation combined with Ostwald ripening](https://arxiv.org/abs/2601.14011)
*Robert T. Zaks,Sergey A. Matveev,Margarita A. Nikishina,Dmitri V. Alexandrov*

Main category: math.NA

TL;DR: Numerical study confirms that particle systems undergoing simultaneous coagulation and Ostwald ripening evolve toward a universal particle-volume distribution regardless of initial conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the concluding stage of phase transformation where both coagulation (particle merging) and Ostwald ripening (growth of larger particles at expense of smaller ones) occur simultaneously, and to verify the existence of universal particle-volume distributions in such systems.

Method: Developed a computationally efficient numerical algorithm based on low-rank matrices to solve the integro-differential system of Smoluchowski-type kinetic and mass balance equations. Compared numerical solutions for different initial particle-volume distributions with theoretical universal distribution functions.

Result: Numerical calculations confirm that particulate ensembles tend to approach the universal particle-volume distribution asymptotically after sufficiently long time, regardless of initial particle-volume distribution. The low-rank matrix algorithm proved computationally efficient for solving the complex integro-differential equations.

Conclusion: The study demonstrates the universal behavior of particle systems undergoing simultaneous coagulation and Ostwald ripening, with systems evolving toward a predictable asymptotic distribution independent of initial conditions, which has important implications for materials science and phase transformation processes.

Abstract: The processes of simultaneous coagulation and Ostwald ripening of particles in the concluding stage of phase transformation are considered. We solve the integro-differential system of Smoluchowski-type kinetic and mass balance equations using a computationally efficient numerical algorithm based on low-rank matrices. We compare our numerical solutions for different initial particle-volume distributions with the universal distribution function for combined coagulation and Ostwald ripening. Our calculations confirm the tendency of a particulate ensemble to the universal particle-volume distribution to be approached asymptotically after a sufficiently long time, no matter what the initial particle-volume distribution might be.

</details>


### [36] [On the optimal shape parameter for kernel methods: Sharp direct and inverse statements](https://arxiv.org/abs/2601.14070)
*Tizian Wenzel,Gabriele Santin*

Main category: math.NA

TL;DR: The paper establishes a theoretical framework linking optimal RBF shape parameter selection to superconvergence phenomena for finitely smooth Sobolev kernels.


<details>
  <summary>Details</summary>
Motivation: The search for optimal shape parameters in RBF kernel approximation has been an unresolved research problem for decades, with no clear theoretical understanding of how approximation regimes, kernel regularity, and parameter choices interact.

Method: The authors leverage recently established theory on sharp direct, inverse and saturation statements for kernel-based approximation to create a theoretical framework. They analyze finitely smooth Sobolev kernels, covering practical radial kernels including those from machine learning.

Result: The framework links optimal shape parameter selection to superconvergence phenomena, elucidating the interaction between approximation regimes, kernel regularity, and parameter choices.

Conclusion: The work provides theoretical clarification for a decades-old unresolved question about RBF shape parameter optimization by connecting it to superconvergence and establishing clear relationships between approximation properties and kernel characteristics.

Abstract: The search for the optimal shape parameter for Radial Basis Function (RBF) kernel approximation has been an outstanding research problem for decades. In this work, we establish a theoretical framework for this problem by leveraging a recently established theory on sharp direct, inverse and saturation statements for kernel based approximation. In particular, we link the search for the optimal shape parameter to superconvergence phenomena. Our analysis is carried out for finitely smooth Sobolev kernels, thereby covering large classes of radial kernels used in practice, including those emerging from current machine-learning methodologies. Our results elucidate how approximation regimes, kernel regularity, and parameter choices interact, thereby clarifying a question that has remained unresolved for decades.

</details>


### [37] [From big q-Jacobi and Chebyshev polynomials to exponential-reproducing subdivision: new identities](https://arxiv.org/abs/2601.14189)
*Leonard Peter Bos,Lucia Romani,Alberto Viscardi*

Main category: math.NA

TL;DR: New identities for Chebyshev and q-Jacobi polynomials enable closed-form expressions for minimum-support interpolating subdivision schemes reproducing exponential powers.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical identities that can be applied to improve subdivision schemes in computer graphics and geometric modeling.

Method: Derive new mathematical identities for Chebyshev polynomials of the first kind and big q-Jacobi polynomials through theoretical analysis.

Result: Achieved closed-form expressions for Laurent polynomials that identify minimum-support interpolating subdivision schemes reproducing finite sets of integer powers of exponentials.

Conclusion: The derived polynomial identities provide practical benefits for constructing efficient subdivision schemes with minimal support while maintaining interpolation properties for exponential functions.

Abstract: In this paper we derive new identities satisfied by Chebyshev polynomials of the first kind and big q-Jacobi polynomials. An immediate benefit of the derived identities is the achievement of closed-form expressions for the Laurent polynomials that identify minimum-support interpolating subdivision schemes reproducing finite sets of integer powers of exponentials.

</details>


### [38] [Local electrical impedance tomography via projections](https://arxiv.org/abs/2601.14198)
*A. Jääskeläinen,A. Vavilov,J. Toivanen,A. Hänninen,V. Kolehmainen,N. Hyvönen*

Main category: math.NA

TL;DR: A method for local electrical impedance tomography reconstruction that eliminates nuisance conductivity changes outside the region of interest using orthogonal projection of measurements and forward map.


<details>
  <summary>Details</summary>
Motivation: In electrical impedance tomography (EIT), conductivity changes outside the region of interest (ROI) can interfere with accurate local reconstruction. This is particularly problematic in medical applications like stroke monitoring where physiological variations in surrounding tissues (e.g., scalp conductivity changes) can mask or distort the signal from the ROI.

Method: The method uses the Jacobian matrix of the forward map at an initial conductivity guess. The Jacobian is partitioned into ROI and nuisance (outside ROI) parts. The key innovation is projecting both measurements and forward map onto the orthogonal complement of left-hand singular vectors of a weighted nuisance Jacobian. This weighting can account for finite element discretization or prior conductivity information outside ROI. The inverse problem is then solved using only the projected relation, reconstructing conductivity only in the ROI.

Result: The method successfully demonstrated functionality on experimental data from a head-shaped water tank. It effectively reconstructed conductivity changes in the ROI (mimicking hemorrhagic stroke growth) while eliminating interference from conductivity changes outside ROI (mimicking physiological scalp variations).

Conclusion: The proposed projection-based method enables local EIT reconstruction that is robust to conductivity variations outside the region of interest, which is particularly valuable for medical applications where surrounding tissue changes can interfere with monitoring specific pathological conditions.

Abstract: This paper introduces a method for approximately eliminating the effect that conductivity changes outside the region of interest have in electrical impedance tomography, allowing to form a local reconstruction in the region of interest only. The method considers the Jacobian matrix of the forward map, i.e., of the map that sends the discretized conductivity to the electrode measurements, at an initial guess for the conductivity. The Jacobian matrix is divided columnwise into two parts: one corresponding to the region of interest and a nuisance Jacobian corresponding to the rest of the domain. The leading idea is to project both the electrode measurements and the forward map onto the orthogonal complement of the span of a number of left-hand singular vectors for a suitably weighted nuisance Jacobian. The weighting can, e.g., account for the element sizes in a finite element discretization or to prior information on the conductivity outside the region of interest. The inverse problem is then solved by considering the projected relation between the measurements and the forward map, only reconstructing the conductivity in the region of interest. The functionality of the method is demonstrated by applying a reconstruction algorithm that combines lagged diffusivity iteration and total variation regularization to experimental data. In particular, data from a head-shaped water tank is considered, with the conductivity change in the region of interest mimicking growth of a hemorrhagic stroke and the changes outside the region of interest imitating physiological variations in the conductivity of the scalp.

</details>


### [39] [Convergence analysis and a novel Lagrange multiplier partitioned method for fluid-poroelastic interaction](https://arxiv.org/abs/2601.14201)
*Amy de Castro,Hyesuk Lee*

Main category: math.NA

TL;DR: A partitioned method using Lagrange multipliers to decouple Stokes-Biot system with parallel solution of fluid and poroelastic structure subproblems.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient partitioned solution method for the coupled Stokes-Biot system that enables parallel computation by decoupling the fluid and poroelastic structure subproblems.

Method: Monolithic formulation with Lagrange multipliers enforcing interface conditions, finite element discretization, Schur complement algorithm with efficient preconditioner, using multipliers as Neumann boundary conditions to decouple subproblems.

Result: Established convergence of approximation, developed efficient preconditioner enabling parallel solution, validated theoretical error estimates through numerical experiments.

Conclusion: The proposed partitioned method effectively decouples Stokes and Biot equations for parallel solution while maintaining accuracy through Lagrange multiplier interface enforcement.

Abstract: We propose a partitioned method for the monolithic formulation of the Stokes-Biot system that incorporates Lagrange multipliers enforcing the interface conditions. The monolithic system is discretized using finite elements, and we establish convergence of the resulting approximation. A Schur complement based algorithm is developed together with an efficient preconditioner, enabling the fluid and poroelastic structure subproblems to be decoupled and solved independently at each time step. The Lagrange multipliers approximate the interface fluxes and act as Neumann boundary conditions for the subproblems, yielding parallel solution of the Stokes and Biot equations. Numerical experiments demonstrate the effectiveness of the proposed algorithm and validate the theoretical error estimate.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [40] [Poisson semigroup and the Gruet formula for the heat kernels on spaces of constant curvature](https://arxiv.org/abs/2601.11596)
*Mohamed Vall Ould Moustapha*

Main category: math.AP

TL;DR: New methods for deriving explicit formulas for Poisson and heat equations on Euclidean, spherical, and hyperbolic spaces, including new Gruet formulas for Euclidean and spherical spaces and a new elementary derivation for hyperbolic space.


<details>
  <summary>Details</summary>
Motivation: To develop unified methods for obtaining explicit formulas for Poisson and heat equations across different constant curvature spaces (Euclidean, spherical, hyperbolic), addressing the need for systematic approaches to these fundamental PDEs in differential geometry.

Method: Develops new mathematical methods for deriving explicit formulas for Poisson and heat semigroups on constant curvature spaces. The approach provides unified techniques applicable to Euclidean (R^n), spherical (S^n), and hyperbolic (H^n) spaces, with particular focus on obtaining Gruet formulas for heat kernels.

Result: Obtains new Gruet formulas for heat kernels in Euclidean and spherical spaces (R^n and S^n), which are novel contributions. Also provides a new elementary method to derive the classical Gruet formula for the heat semigroup kernel on hyperbolic space (H^n), offering a simpler derivation than existing approaches.

Conclusion: The paper presents unified methods for analyzing Poisson and heat equations on constant curvature spaces, yielding both new results (Gruet formulas for Euclidean and spherical spaces) and improved derivations (elementary method for hyperbolic space formula), advancing the mathematical understanding of fundamental PDEs in differential geometry.

Abstract: This paper is concerned with the Poisson and heat equations on spaces of constant curvature. More explicitly we provide new methods for obtaining old and new explicit formulas for the Poisson and heat semigroups on the Euclidean, spherical and hyperbolic spaces $\R^n$, $§^n$ and $\H^n$ . We obtain the Gruet formula for the heat kernels in Euclidean and spherical spaces $\R^n$ and $§^n$, which are new and we provide a new elementary method to derive the classical Gruet formula Gruet\cite{Gruet} for the kernel of the heat semigroup on the hyperbolic space $\H^n$.

</details>


### [41] [Construction of a Gibbs measure for the zonal Dirac equation](https://arxiv.org/abs/2601.11730)
*Anne-Sophie de Suzzoni,Cyril Malézé*

Main category: math.AP

TL;DR: A framework for constructing Gibbs measures for the Dirac equation on a sphere with Hartree-type nonlinearity, proving existence of weak solutions with Gibbs measure law preserved over time.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical tools for studying the Dirac equation with nonlinear interactions on curved manifolds (specifically the sphere), which has applications in quantum field theory and relativistic quantum mechanics.

Method: Construct Gibbs measures for the Dirac equation on the sphere with Hartree-type nonlinearity using a zonal model (analogous to spherically symmetric models), then prove existence of weak solutions via compactness arguments.

Result: Successfully built a Gibbs measure for the model and proved existence of a random variable that serves as a weak solution to the Dirac equation with its law being the Gibbs measure at all times.

Conclusion: The framework provides rigorous mathematical construction of Gibbs measures for nonlinear Dirac equations on curved manifolds, establishing existence of solutions that preserve statistical equilibrium properties over time.

Abstract: We propose a framework to construct Gibbs measures for the Dirac equation. We consider the Dirac equation on the sphere with a "Hartree-type" nonlinearity. We consider a zonal model, that is the analog of a spherically symmetric model but on the sphere. We build a Gibbs measure for this model. With a compactness argument, we prove the existence of a random variable that is a weak solution to the Dirac equation and whose law is the Gibbs measure at all times.

</details>


### [42] [On large periodic traveling surface waves in porous media](https://arxiv.org/abs/2601.11800)
*Huy Q. Nguyen,Noah Stevenson*

Main category: math.AP

TL;DR: First non-perturbative construction of large traveling surface waves in viscous fluids without surface tension, showing existence of connected solution set from quiescent to large amplitudes with possible breakdown scenarios.


<details>
  <summary>Details</summary>
Motivation: To study large traveling surface waves in viscous fluids governed by Darcy's law without surface tension effects, addressing the lack of non-perturbative constructions for such free boundary problems.

Method: Use Riemann mapping to reformulate 2D free boundary problem as 1D fully nonlinear pseudodifferential equation, discover hidden ellipticity, and apply global implicit function theorem to construct connected solution set.

Result: Constructed connected set of traveling waves containing both quiescent solution and large amplitude members; found that either solutions exist for arbitrarily large amplitude or finite number of breakdown scenarios occur.

Conclusion: First successful non-perturbative construction of large traveling surface waves in viscous fluids without surface tension, establishing existence of connected solution branches with possible breakdown mechanisms.

Abstract: We study large traveling surface waves within a two-dimensional finite depth, free boundary, homogeneous, incompressible and viscous fluid governed by Darcy's law. The fluid is bound by a gravitational force to a flat rigid bottom and meets an atmosphere of constant pressure at the top with its free surface, where it does not experience any capillarity effects. Additionally, the fluid is subject to a fixed, but arbitrarily selected, forcing data profile with variable amplitude. We use the Riemann mapping to equivalently reformulate the resulting two-dimensional free boundary problem as a single one-dimensional fully nonlinear pseudodifferential equation for a function describing the domain's geometry. By discovering a hidden ellipticity in the reformulated equation, we are able to import a global implicit function theorem to construct a connected set of traveling waves, containing both the quiescent solution and large amplitude members. We find that either solutions continue to exist for arbitrarily large data amplitude or else one of a finite number of meaningful breakdown scenarios must occur. This work stands as the first non perturbative construction of large traveling surface waves in any free boundary viscous fluid without surface tension.

</details>


### [43] [Topological and Purely Topological Alignment Dynamics](https://arxiv.org/abs/2601.11828)
*Trevor M. Leslie,Jan Peszek*

Main category: math.AP

TL;DR: The paper studies the Euler Alignment system with topological interactions, proving global classical solution existence for regular protocols and analyzing long-term behavior for both regular and singular purely topological interactions.


<details>
  <summary>Details</summary>
Motivation: To understand collective behavior systems with topological interaction protocols that depend on both Euclidean distance and mass distribution between agents, extending beyond traditional distance-based interactions.

Method: For regular interaction protocols, prove sufficient conditions for global classical solutions using conserved quantity nonnegativity. For purely topological interactions, decouple the system into autonomous velocity equation in mass coordinates and scalar conservation law with time-dependent flux.

Result: Established existence conditions for global classical solutions with regular protocols. For purely topological interactions, showed system decoupling and analyzed long-time behavior for both regular and singular protocols.

Conclusion: Topological interactions in Euler Alignment systems lead to mathematically tractable structures, with global solution existence under certain conditions and interesting decoupling properties for purely topological cases, enabling analysis of long-term dynamics.

Abstract: We study the Euler Alignment system of collective behavior, equipped with `topological' interaction protocols, which were introduced to the mathematical literature by Shvydkoy and Tadmor. Interactions subject to these protocols may depend on both the Euclidean distance between agents and on the mass distribution between them -- the `topological' component. When the interaction protocol is regular, we prove sufficient conditions for the existence of global-in-time classical solutions, related to the initial nonnegativity of a conserved quantity of the system. The remainder of our results explore the case where the interactions are `purely' topological and the interactions do not depend on the Euclidean distance. We show that in this case, the system decouples into an autonomous velocity equation in mass coordinates together with a scalar conservation law with time-dependent flux determined by the velocity. We analyze the long-time behavior for the dynamics associated to both regular and singular protocols.

</details>


### [44] [Weighted fractional ultrahyperbolic diffusion on geometrically deformed domains](https://arxiv.org/abs/2601.11851)
*Gustavo Dorrego*

Main category: math.AP

TL;DR: The paper derives a fundamental solution for a weighted space-time fractional ultrahyperbolic operator that decouples medium density from geometric deformation, revealing a geometry-independent drift mechanism.


<details>
  <summary>Details</summary>
Motivation: Standard fractional models on manifolds often conflate geometric anisotropy with medium heterogeneity, creating rigidity in modeling anomalous transport in complex, structurally deformed media.

Method: Using a novel spectral approach based on the Weighted Fourier Transform to derive the fundamental solution for a weighted space-time fractional ultrahyperbolic operator $(-\Box_{φ,ω})^β$, obtaining the Green's function in closed form via the Fox H-function.

Result: Successfully decouples medium density from geometric deformation, revealing a geometry-independent drift mechanism driven purely by medium inhomogeneity, providing a unified and computable framework.

Conclusion: The approach overcomes rigidity in standard fractional models by providing explicit separation of geometric and medium properties, offering a powerful framework for analyzing anomalous transport in complex deformed media.

Abstract: Standard fractional models on manifolds often conflate geometric anisotropy with medium heterogeneity. In this Letter, we overcome this rigidity by deriving the fundamental solution for a weighted space-time fractional ultrahyperbolic operator, denoted by $(-\Box_{φ,ω})^β$. Using a novel spectral approach based on the Weighted Fourier Transform, we explicitly \textbf{decouple the medium density from the geometric deformation}. A crucial finding is the emergence of a \textbf{geometry-independent drift mechanism} driven purely by the inhomogeneity of the medium. The Green's function is obtained in closed form via the Fox H-function, providing a unified and computable framework for anomalous transport in complex, structurally deformed media.

</details>


### [45] [Global weak solutions to the isentropic compressible Navier--Stokes equations with vacuum and unbounded density in a half-plane under Dirichlet boundary conditions](https://arxiv.org/abs/2601.11852)
*Shuai Wang,Xin Zhong*

Main category: math.AP

TL;DR: Global existence of weak solutions to compressible Navier-Stokes equations on half-plane with vacuum and Dirichlet boundary conditions under small initial energy.


<details>
  <summary>Details</summary>
Motivation: Extend previous discontinuous solution frameworks (Hoff, Perepelitsa) to handle vacuum states and unbounded density on unbounded domains with Dirichlet boundary conditions.

Method: Green function method with new estimates leveraging equation structure and half-plane geometry; develops intermediate-regularity class between Lions-Feireisl and Hoff frameworks.

Result: First global weak solution result in Hoff's framework on unbounded domain with both Dirichlet boundary conditions and far-field vacuum; solutions admit unbounded densities.

Conclusion: Develops natural extension of Hoff's theory overcoming two key obstructions: lack of global control of effective viscous flux from far-field vacuum and absence of boundary-induced regularity gains in no-slip setting.

Abstract: We establish the global existence of a class of weak solutions to the isentropic compressible Navier--Stokes equations in a half-plane with Dirichlet boundary conditions, allowing for vacuum both in the interior and at infinity, under a suitably small initial total energy. The solutions constructed here admit unbounded densities and lie in an intermediate regularity regime between the finite-energy weak solutions of Lions--Feireisl and the framework of Hoff. This result generalizes previous works of Hoff (Comm. Pure Appl. Math. 55 (2002), pp. 1365--1407) and Perepelitsa (Arch. Ration. Mech. Anal. 212 (2014), pp. 709--726) concerning discontinuous solutions by allowing vacuum states and unbounded density. Our analysis relies on the Green function method and new estimates involving the specific structure of the equations and the geometry of the half-plane. To the best of our knowledge, this is the first result concerning global weak solutions within Hoff's framework on an unbounded domain that simultaneously accommodates Dirichlet boundary conditions and far-field vacuum. The intermediate-regularity class developed here may be viewed as a natural extension of Hoff's theory, precisely tailored to overcome the two corresponding obstructions: the lack of global space-time control of the effective viscous flux arising from far-field vacuum and the absence of boundary-induced regularity gains in the no-slip setting.

</details>


### [46] [Lowest eigenvalues and higher order elliptic differential operators](https://arxiv.org/abs/2601.11882)
*David Raske*

Main category: math.AP

TL;DR: The paper shows that many fourth-order elliptic operators on Riemannian manifolds have sign-changing eigenfunctions associated with their lowest eigenvalues, contrary to typical expectation.


<details>
  <summary>Details</summary>
Motivation: While it's known that some elliptic operators can have sign-changing eigenfunctions for their lowest eigenvalues, the paper investigates how common this property is among elliptic differential operators on Riemannian manifolds.

Method: The author analyzes elliptic operators of the form Δ_g^2 - div_g(T-λ_2 g^{-1})d, where T is a negative semi-definite (2,0)-tensor field on M, g^{-1} is the inverse metric tensor, and λ_2 is the second lowest eigenvalue of -Δ_g.

Result: The paper proves that these specific fourth-order elliptic operators have the property that there exists a sign-changing eigenfunction associated with their lowest eigenvalue.

Conclusion: The result suggests that there are many fourth-order elliptic operators with sign-changing eigenfunctions for their lowest eigenvalues, indicating this property is not rare among such operators.

Abstract: Let $(M,g)$ be a closed, smooth Riemannian manifold of dimension $m \geq 1$. It is not difficult to produce an example of an elliptic differential operator on $(M,g)$ that has the property that there exists a sign-changing eigenfunction that is associated with the lowest eigenvalue. Indeed, $Δ_g^2 + λ_2 Δ_g$ does the job, where $Δ_g:=div_g \nabla_g$. and where $λ_2$ is the second lowest eigenvalue of the operator $-Δ_g$. The question that remains is how rare are elliptic differential operators whose lowest eigenvalue has this property. In this paper, the author proves that elliptic operators of the form $Δ_g^2 - div_g(T-λ_2 g^{-1}) d$, where $T$ is a negative semi-definite $(2,0)$-tensor field on $M$, and where $g^{-1}$ is the inverse metric tensor, have the property that there exists a sign-changing eigenfunction that is associated with the lowest eigenvalue of the operator. This suggests that there are a lot of fourth-order elliptic operators with the property that there exists a sign-changing eigenfunction that is associated with the lowest eigenvalue of the operator.

</details>


### [47] [Microscopic derivation of a one-dimensional lubrication model with roughness](https://arxiv.org/abs/2601.11999)
*Aline Lefebvre-Lepot,Muhammed Ali Mehmood,Charlotte Perrin,Ewelina Zatorska*

Main category: math.AP

TL;DR: Derivation of hydrodynamic model for inertial particles with hard cores, interacting through lubrication and repulsive forces, with convergence proof to macroscopic model encoding congestion effects.


<details>
  <summary>Details</summary>
Motivation: Extend previous work by Lefebvre-Lepot and Maury on non-inertial particles with only lubrication forces to include inertial effects and roughness, developing a more realistic model for particle systems with congestion.

Method: Derive hydrodynamic model for spherical inertial particles with hard cores, incorporating lubrication forces and pairwise repulsive forces from thin rough layers of reduced permeability. Prove convergence as particle number increases (size decreases) to macroscopic hydrodynamic model.

Result: Successfully extend previous work to include inertial effects and roughness, proving convergence to macroscopic model where congestion effects are encoded in macroscopic interaction forces depending on local critical density transported by flow.

Conclusion: Developed comprehensive hydrodynamic model for inertial particles with congestion effects, extending previous non-inertial models and providing rigorous convergence proof for systems with both inertial effects and surface roughness.

Abstract: We derive a hydrodynamic model for the motion of inertial particles with a spherical hard core, interacting through lubrication forces and pairwise repulsive forces. The repulsion arises from the assumption that each particle is surrounded by a thin rough layer of reduced permeability. We prove that, as the number of particles tends to infinity (and their size tends to 0), the microscopic dynamics converges to a macroscopic hydrodynamic model in which congestion effects are encoded directly into the macroscopic interaction forces, depending on a local critical density transported by the flow. In particular, we extend the work of Lefebvre-Lepot and Maury where non-inertial particles, submitted to only a lubrication force were considered, and present the convergence proof when inertial effects and roughness are taken into account.

</details>


### [48] [Boojums in Liquid Crystals Around a Colloid](https://arxiv.org/abs/2601.12065)
*Yuchen Huang,Yong Yu*

Main category: math.AP

TL;DR: Study of Landau-de Gennes theory for nematic liquid crystals around a spherical colloid, showing boojum defects form at poles under axial symmetry and Lyuksyutov constraint.


<details>
  <summary>Details</summary>
Motivation: To understand defect formation in nematic liquid crystals around spherical colloids, specifically characterizing boojum disclinations that appear at the poles under certain constraints.

Method: Using Landau-de Gennes theory in one constant limit, analyzing exterior domain of spherical colloid with Rapini-Papoular surface potential and homogeneous far-field conditions, under axially symmetric ansatz and Lyuksyutov constraint.

Result: Energy minimizers exhibit boojum disclinations at both poles of the colloid, with local structure of these boojum defects characterized mathematically.

Conclusion: Boojum defects are stable configurations at spherical colloid poles under the studied conditions, providing insight into defect formation in nematic liquid crystal systems with colloidal inclusions.

Abstract: We study the Landau-de Gennes theory in the one constant limit. The bulk domain is the exterior of a spherical colloid. A Rapini-Papoular surface potential is imposed on the colloid surface, supplemented by a homogeneous far-field condition at spatial infinity. Under the axially symmetric ansatz and the Lyuksyutov constraint, we show that energy minimizers exhibit boojum disclinations at the two poles of the colloid. The local structure of these boojum disclinations is also characterized.

</details>


### [49] [Sharpness of the Osgood Criterion for the Continuity Equation with Divergence-free Vector Fields](https://arxiv.org/abs/2601.12096)
*Roberto Colombo,Anuj Kumar*

Main category: math.AP

TL;DR: Constructs divergence-free velocity fields with non-Osgood moduli of continuity that exhibit non-uniqueness in ODE flows and continuity equations, introducing parallelization and a new fixed-point framework.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that non-uniqueness in ODE flows and continuity equations can occur for divergence-free velocity fields with moduli of continuity that fail the Osgood condition, extending beyond isolated trajectories to sets of positive measure.

Method: Introduces two novel ideas: (1) "parallelization" where velocity fields consist of simultaneous motion across multiple nested spatial scales at each time, and (2) a new fixed-point framework inspired by Bruè, Colombo and Kumar that incorporates parallelization to construct anomalous solutions.

Result: Constructed divergence-free velocity fields v ∈ C_tC^ω_x with non-Osgood moduli ω that admit at least two distinct flow maps for ODEs on sets of positive Lebesgue measure, and two distinct solutions to continuity equations that are absolutely continuous with respect to Lebesgue measure for almost every time.

Conclusion: Non-uniqueness in fluid dynamics can occur robustly for divergence-free velocity fields with non-Osgood moduli of continuity, affecting sets of positive measure rather than isolated trajectories, with parallelization and the new fixed-point framework being crucial technical innovations.

Abstract: For any modulus of continuity $ω$ that fails the Osgood condition, we construct a divergence-free velocity field $v \in C_t C^ω_x$ for which the associated ODE admits at least two distinct flow maps. In other words, non-uniqueness does not occur merely for a single or even finitely many trajectories, but instead on a set of initial conditions $E$ of positive Lebesgue measure. In fact, the set $E$ has full measure inside a cube where the construction is supported. Moreover, we also construct a divergence-free velocity field $v \in C_{t}C^ω_x$ for which the associated continuity equation admits two distinct solutions $μ^1$ and $μ^2$ which are absolutely continuous with respect to Lebesgue measure for almost every time, and start from the same initial datum $\bar μ\ll \mathscr{L}^{d}$. Our construction introduces two novel ideas: (i) We introduce the notion of "parallelization", where at each time, the velocity field consists of simultaneous motion across multiple nested spatial scales. This differs from most explicit constructions in the literature on mixing or anomalous dissipation, where the velocity on different scales acts at separate times. This is crucial to cover the whole class of non-Osgood moduli of continuity. (ii) Inspired by a recent work of Bruè, Colombo and Kumar, we develop a new fixed-point framework that naturally incorporates the parallelization mechanism. This framework allows us to construct anomalous solutions of the continuity equation that belong to $L^1(\mathbb{R}^d)$ a.e. in time.

</details>


### [50] [Higher integrability of solutions to elliptic equations under additional sign constraints](https://arxiv.org/abs/2601.12100)
*Stefan Schiffer*

Main category: math.AP

TL;DR: The paper presents two higher integrability results: (1) a version of Müller's theorem for maps with nonnegative determinant, and (2) improved regularity for p-Laplace solutions with sign constraints on partial derivatives, using Lipschitz truncation methods.


<details>
  <summary>Details</summary>
Motivation: Elliptic equations often exhibit hidden regularity where solutions have higher integrability than a priori known. The paper aims to establish such higher integrability results under additional sign constraints on specific terms, which can further sharpen the regularity improvement.

Method: The authors use Lipschitz truncation techniques. For the first result, they apply standard Lipschitz truncation. For the second result, they develop a new variation called "asymmetric Lipschitz truncation" to handle solutions to the p-Laplace equation with sign constraints on partial derivatives.

Result: Two main results: (1) A version of Müller's higher integrability theorem for maps u ∈ W^{1,n} with det(∇u) ≥ 0 (or det_-(∇u) ∈ L log L). (2) Higher integrability for (very weak) solutions to the p-Laplace equation where negative parts of partial derivatives have higher integrability than positive parts.

Conclusion: The paper successfully establishes higher integrability results under sign constraints using Lipschitz truncation methods, with the development of asymmetric Lipschitz truncation providing a new technical tool for handling p-Laplace equations with asymmetric derivative conditions.

Abstract: Solutions to elliptic equations often exhibit higher regularity properties such as \emph{higher integrability}. That is, for instance, a solution $u$ to a system that a priori only satisfies $ u \in W^{1,r}$ is more regular and even in the Sobolev space $W^{1,s}$ for some $s>r$. Under additional constraints of the sign of specific terms such as $(\partial_i u)$ this improvement of regularity can be sharpened further.
  In this work, we consider two examples of such higher integrability results: First, we show a version of Müller's result on the higher integrability of the determinant for maps $u \in W^{1,n} $ such that $\mathrm{det}(\nabla u) \geq 0$ (or $ \mathrm{det}_-(\nabla u) \in L \log L$). Second, we consider (very weak) solutions to the $p$-Laplace equation that satisfy sign constraints for their partial derivatives, i.e. that $(\partial_i u)_- $ is of higher integrability than $(\partial_i u)_+$. To prove our results, we use the method of Lipschitz truncation; for the second example we further develop a variation of this technique, the \emph{asymmetric} Lipschitz truncation.

</details>


### [51] [Navier slip effects in micropolar thin-film flow: a rigorous derivation of Reynolds-type models](https://arxiv.org/abs/2601.12125)
*María Anguiano,Igor Pažanin,Francisco J. Suárez-Grau*

Main category: math.AP

TL;DR: Analysis of micropolar fluid flow in thin 3D domains with Navier slip boundary conditions, identifying three slip regimes and deriving reduced systems and Reynolds-type equations.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of incompressible micropolar fluids in thin three-dimensional domains under Navier slip boundary conditions, particularly how slip effects influence thin-film flow dynamics.

Method: Rescaling governing equations followed by rigorous asymptotic analysis as film thickness tends to zero, considering friction coefficient dependent on small parameter. Identification of three slip regimes based on slip coefficient scaling.

Result: Derived three distinct regimes: perfect slip, partial slip, and no-slip. For each regime, obtained corresponding reduced micropolar systems and explicit expressions for velocity and microrotation fields, leading to generalized Reynolds-type equations for pressure.

Conclusion: Slip effects significantly impact micropolar thin-film flow, with different slip regimes leading to distinct reduced systems and pressure equations, providing a comprehensive framework for analyzing such flows in thin domains.

Abstract: We study the stationary flow of incompressible micropolar fluid in a thin three-dimensional domain under Navier slip boundary condition for the velocity and no-spin condition for microrotation. After rescaling the governing equations, we perform a rigorous asymptotic analysis as the film thickness tends to zero, considering a friction coefficient dependent on the small parameter. According to the scaling of the slip coefficient, we identify three distinct regimes: perfect slip, partial slip, and no-slip. For each regime, we derive the corresponding reduced micropolar system and obtain explicit expressions for the velocity and microrotation fields. This leads to a generalized Reynolds-type equation for the pressure, highlighting the impact of slip effects on the micropolar thin-film flow.

</details>


### [52] [Fractional Semilinear Equations on Hyperbolic Spaces](https://arxiv.org/abs/2601.12140)
*Jianxiong Wang*

Main category: math.AP

TL;DR: Study of semilinear equations with fractional Laplacian on hyperbolic space, deriving Green's function via Fourier analysis and proving symmetry/nonexistence results using moving planes method.


<details>
  <summary>Details</summary>
Motivation: The fractional Laplacian on hyperbolic space lacks conformal covariance (unlike in conformally compact Einstein manifolds), motivating explicit analysis of its properties and solution behavior.

Method: Employ Helgason-Fourier analysis to derive explicit Green's function, then apply direct method of moving planes to integral equation form, plus develop maximum principles on hyperbolic space.

Result: Explicit derivation of Green's function with asymptotic behaviors, symmetry of solutions established, nonexistence of positive solutions in critical/subcritical cases.

Conclusion: The paper provides fundamental analysis tools for fractional Laplacian on hyperbolic space, establishing key properties and solution behaviors through Fourier analysis and geometric methods.

Abstract: We study a semilinear equation involving the fractional Laplacian on the hyperbolic space $\mathbb{H}^n$. Unlike in conformally compact Einstein manifolds, the fractional Laplacian on $\mathbb{H}^n$ does not enjoy conformal covariance. By employing Helgason-Fourier analysis, we explicitly derive the Green's function of the fractional Laplacian on $\mathbb{H}^n$ and study its asymptotic behaviors. We then apply a direct method of moving planes to the integral form of the equation, establishing symmetry of solutions and nonexistence of positive solutions in the critical and subcritical cases, respectively. In addition, we develop several maximum principles on hyperbolic space.

</details>


### [53] [Sobolev inequalities for nonlinear Dirichlet forms](https://arxiv.org/abs/2601.12192)
*Ralph Chill,Burkhard Claus*

Main category: math.AP

TL;DR: The paper establishes equivalence between Sobolev inequalities and isocapacitary inequalities for nonlinear Dirichlet forms and their associated capacities.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the fundamental relationship between two important types of inequalities in functional analysis and potential theory, specifically in the context of nonlinear Dirichlet forms.

Method: The authors use analytical methods to prove equivalence between Sobolev-type inequalities and isocapacitary inequalities for a broad class of nonlinear Dirichlet forms, their associated Dirichlet spaces, and capacities.

Result: The paper successfully establishes the equivalence between Sobolev inequalities and isocapacitary inequalities in the specified context of nonlinear Dirichlet forms.

Conclusion: Sobolev inequalities and isocapacitary inequalities are equivalent for a large class of nonlinear Dirichlet forms, providing important theoretical connections between different analytical frameworks.

Abstract: In this short note we show an equivalence between Sobolev type inequalities and so called isocapacitary inequalities in the context of a large class of nonlinear Dirichlet forms, their associated Dirichlet spaces and their associated capacities.

</details>


### [54] [Time-asymptotic stability of composite waves of degenerate Oleinik shock and rarefaction for non-convex conservation laws with Cattaneo's law](https://arxiv.org/abs/2601.12216)
*Yuxi Hu,Ran Song*

Main category: math.AP

TL;DR: The paper proves time-asymptotic stability of composite waves (degenerate Oleinik shock + rarefaction wave) for a 1D conservation law with non-convex flux and artificial heat flux under Cattaneo's law, given small wave strength and initial perturbations.


<details>
  <summary>Details</summary>
Motivation: To understand the large-time behavior of solutions to hyperbolic conservation laws with non-convex flux and heat conduction governed by Cattaneo's law, which is important for modeling physical systems with finite propagation speeds.

Method: Uses Oleinik entropy condition, a-contraction method with time-dependent shifts, and weighted energy estimates to analyze stability of composite waves.

Result: Demonstrates time-asymptotic stability of composite waves combining degenerate Oleinik shocks and rarefaction waves under small wave strength and initial perturbation conditions.

Conclusion: The composite wave structure is stable for the studied hyperbolic system, providing rigorous mathematical foundation for long-term behavior of such conservation laws.

Abstract: This paper examines the large-time behavior of solutions to a one-dimensional conservation law featuring a non-convex flux and an artificial heat flux term regulated by Cattaneo's law, forming a 2$\times$2 system of hyperbolic equations. Under the conditions of small wave strength and sufficiently small initial perturbations, we demonstrate the time-asymptotic stability of a composite wave that combines a degenerate Oleinik shock and a rarefaction wave. The proof utilizes the Oleinik entropy condition, the a-contraction method with time-dependent shifts, and weighted energy estimates.

</details>


### [55] [Stabilization of arbitrary structures in a three-dimensional doubly degenerate nutrient taxis system](https://arxiv.org/abs/2601.12218)
*De-Ji-Xiang-Mao,Ai Huang,Yifu Wang*

Main category: math.AP

TL;DR: The paper proves global existence and convergence to equilibrium for a doubly degenerate nutrient taxis system in 3D domains, with specific parameter ranges for the degenerate diffusion exponent.


<details>
  <summary>Details</summary>
Motivation: To study the global dynamics of a doubly degenerate nutrient taxis system with degenerate diffusion and chemotaxis mechanisms, which presents mathematical challenges due to the doubly degenerate structure.

Method: Developed a novel class of functional inequalities to handle the doubly degenerate diffusion mechanism. Used these tools to establish global existence of continuous weak solutions for specific parameter ranges in 3D domains.

Result: For α ∈ (3/2, 19/12), the system admits global continuous weak solutions for sufficiently regular initial data. Solutions converge to equilibrium (u∞, 0) as t→∞, with non-homogeneous limiting profiles when initial signal concentration v0 is sufficiently small and u0 is not constant.

Conclusion: The doubly degenerate nutrient taxis system exhibits global well-posedness and convergence to equilibrium for specific parameter ranges, with novel functional inequality techniques enabling the analysis of the challenging doubly degenerate structure.

Abstract: The doubly degenerate nutrient taxis system \begin{equation}\label {0.1} \left\{ \begin{aligned} &u_{t}=\nabla \cdot (uv\nabla u)-χ\nabla \cdot (u^αv\nabla v)+\ell uv,&x\in Ω,\, t>0,\\ & v_{t}=Δv-uv,&x\in Ω,\, t>0,\\ \end{aligned} \right. \end{equation} is considered under zero-flux boundary conditions in a smoothly bounded domain $Ω\subset\mathbb{R}^3$ where $α>0,χ>0$ and $\ell> 0$. By developing a novel class of functional inequalities to address the challenges posed by
  the doubly degenerate diffusion mechanism in \eqref{0.1}, it is shown that for $α\in(\frac{3}{2},\frac{19}{12})$, the associated initial-boundary value problem admits a global continuous weak solution for sufficiently regular initial data. Furthermore, in an appropriate topological setting, this solution converges to an equilibrium $(u_\infty, 0)$ as $t\rightarrow \infty$. Notably, the limiting profile $u_{\infty}$ is non-homogeneous when the initial signal concentration $v_0$ is sufficiently small, provided the initial data $u_0$ is not identically constant.

</details>


### [56] [An optimal boundary control approach to the Cherrier-Escobar problem](https://arxiv.org/abs/2601.12232)
*Cheikh Birahim Ndiaye,Abdul-Malik Saiid*

Main category: math.AP

TL;DR: Optimal boundary control for conformal Laplacian/Robin operator on Riemannian manifolds with boundary; optimal controls equal optimal states and minimize Cherrier-Escobar functional, inducing conformal metrics with zero scalar curvature and constant mean curvature.


<details>
  <summary>Details</summary>
Motivation: To study optimal boundary control problems associated with the boundary obstacle problem for conformal operators on compact Riemannian manifolds with boundary, and understand the relationship between optimal controls, optimal states, and conformal geometry.

Method: Analysis of optimal boundary control problem for conformal Laplacian and conformal Robin operator on n-dimensional compact Riemannian manifolds with boundary (n≥3). Uses Cherrier-Escobar invariant, shows optimal controls equal optimal states when invariant is positive, proves optimal controls minimize Cherrier-Escobar functional.

Result: When Cherrier-Escobar invariant is positive: optimal controls equal their optimal states; optimal controls minimize Cherrier-Escobar functional, inducing conformal metrics with zero scalar curvature and constant mean curvature; existence of optimal control under Aubin type assumption. For standard unit ball: sharp Sobolev trace inequality; standard bubbles are only optimal controls and equal to optimal states.

Conclusion: Optimal boundary controls for conformal operators on Riemannian manifolds with boundary are deeply connected to conformal geometry - they equal optimal states, minimize Cherrier-Escobar functional, and induce special conformal metrics. For the unit ball, standard bubbles are the unique optimal controls.

Abstract: We study an optimal boundary control problem associated to the boundary obstacle problem for the couple conformal Laplacian and conformal Robin operator on n-dimensional compact Riemannian manifolds with boundary and with n\geq 3. When the Cherrier-Escobar invariant of the compact Riemannian manifold with boundary is positive, we show that the optimal controls are equal to their associated optimal states. Moreover, we show that the optimal controls are minimizers of the Cherrier-Escobar functional, and hence induce conformal metrics with zero scalar curvature and constant mean curvature. Furthermore, we show the existence of an optimal control under an Aubin type assumption. For the standard unit ball, we derive a sharp Sobolev trace type inequality and prove that the standard bubbles-namely conformal factor of metrics conformal to the standard one with zero scalar curvature and constant mean curvature -- are the only optimal controls and hence equal to their associated optimal states.

</details>


### [57] [Representation theorems for nonvariational solutions of the Helmholtz equation](https://arxiv.org/abs/2601.12335)
*M. Lanza de Cristoforis*

Main category: math.AP

TL;DR: The paper develops layer potential methods for solving Helmholtz equation Dirichlet/Neumann problems in bounded domains, focusing on α-Hölder continuous solutions that lack classical normal derivatives and may have infinite Dirichlet integrals.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of acoustic layer potentials to handle solutions of the Helmholtz equation that don't fit the classical variational framework - specifically α-Hölder continuous solutions that may lack classical normal derivatives at boundaries and have infinite Dirichlet integrals.

Method: Uses acoustic layer potential theory for multiply connected bounded domains with C^{max{1,m},α} regularity. Develops integral representation theorems using single layer acoustic potentials, focusing on the case m=0 where solutions may not have classical normal derivatives.

Result: Establishes solutions to both Dirichlet and Neumann problems for Helmholtz equation in interior and exterior domains using layer potentials. Proves integral representation theorem for solutions in terms of single layer acoustic potential, extending results to non-variational settings.

Conclusion: The paper successfully extends acoustic layer potential methods to handle α-Hölder continuous solutions of Helmholtz equation that fall outside classical variational theory, providing representation theorems for solutions lacking classical normal derivatives and with infinite Dirichlet integrals.

Abstract: We consider a possibly multiply connected bounded open subset $Ω$ of ${\mathbb{R}}^n$ of class $C^{\max\{1,m\},α}$ for some $m\in {\mathbb{N}}$, $α\in]0,1[$ and we plan to solve both the Dirichlet and the Neumann problem for the Helmholtz equation in $Ω$ and in the exterior of $Ω$ in terms of acoustic layer potentials. Then we turn to prove an integral representation theorem solutions of the Helmholtz equation in terms of a single layer acoustic potential. The main focus of the paper is on $α$-Hölder continuous solutions which may not have a classical normal derivative at the boundary points of $Ω$ and that may have an infinite Dirichlet integral around the boundary of $Ω$\, \textit{i.e.}, case $m=0$. Namely for solutions that do not belong to the classical variational setting.

</details>


### [58] [Asymptotic Behavior of the Principal Eigenvalue Problems with Large Divergence-Free Drifts](https://arxiv.org/abs/2601.12342)
*Yujin Guo,Yuan Lou,Hongfei Zhang*

Main category: math.AP

TL;DR: The paper studies the principal eigenvalue problem with large divergence-free drift, proving convergence of eigenpairs as drift coefficient α→∞ and investigating refined limiting profiles.


<details>
  <summary>Details</summary>
Motivation: To address an open question from Berestycki, Hamel, and Nadirashvili (2005) about the behavior of principal eigenpairs for eigenvalue problems with large divergence-free drifts, specifically understanding how such drifts affect the principal eigenvalue and eigenfunction.

Method: Analyzes the principal eigenvalue problem with drift term -2α∇m(x)·∇φ where m is harmonic and divergence-free. Uses analytical methods to prove convergence of eigenpairs as α→∞ and investigates refined limiting profiles through asymptotic analysis.

Result: Proves convergence of principal eigenpair (λα, φ) as α→∞ for divergence-free drifts where m is harmonic and has no first integral in H₀¹(Ω). Provides refined limiting profiles showing visible effects of large divergence-free drifts on the eigenpair.

Conclusion: The study resolves a special case of the open question about large divergence-free drifts, demonstrating that the principal eigenpair converges and revealing detailed asymptotic behavior that quantifies how large drifts influence the eigenvalue problem.

Abstract: In this paper, we consider the following principal eigenvalue problem with a large divergence-free drift: \begin{equation}\label{0.1} -\varepsilonΔφ-2α\nabla m(x)\cdot\nabla φ+V(x)φ=λ_αφ \,\ \text{in}\, \ H_0^1(Ω),\tag{0.1} \end{equation} where the domain $Ω\subset \mathbb{R}^N (N\ge 1)$ is bounded with smooth boundary $\partialΩ$, the constants $\varepsilon>0$ and $α>0$ are the diffusion and drift coefficients, respectively, and $m(x)\in C^{2}(\barΩ)$, $V (x)\in C^γ(\barΩ)~(0<γ<1)$ are given functions. For a class of divergence-free drifts where $m$ is a harmonic function in $Ω$ and has no first integral in $H_{0}^{1}(Ω)$, we prove the convergence of the principal eigenpair $(λ_α, φ)$ for (0.1) as $α\rightarrow+\infty$, which addresses a special case of the open question proposed in [H. Berestycki, F. Hamel and N. Nadirashvili, CMP, 2005]. Moreover, we further investigate the refined limiting profiles of the principal eigenpair $(λ_α, φ)$ for (0.1) as $α\rightarrow+\infty$, which display the visible effects of the large divergence-free drifts on the principal eigenpair $(λ_α, φ)$.

</details>


### [59] [Classification of the structures of stable radial solutions for semilinear elliptic equations in $\bf R^N$](https://arxiv.org/abs/2601.12350)
*Yasuhito Miyamoto,Yūki Naito*

Main category: math.AP

TL;DR: The paper analyzes stability of radial solutions for semilinear elliptic equations with supercritical nonlinearities, providing classification, existence criteria, and connections to singular solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of radial solutions for semilinear elliptic equations with supercritical nonlinearities, which is important for understanding solution structures and behavior in higher dimensions.

Method: The authors study the semilinear elliptic equation Δu + f(u) = 0 in R^N (N ≥ 3) with supercritical nonlinearity f. They analyze stability through the limits of f'(u)F(u) as u → 0 or ∞, where F(u) = ∫_u^∞ 1/f(t)dt, and classify solution structures with respect to stability.

Result: The paper provides a complete classification of solution structures with respect to stability of radial solutions. It establishes criteria for existence and nonexistence of stable radial solutions based on limits of f'(u)F(u). It also shows the relationship between existence of singular stable solutions and the solution structure.

Conclusion: The stability analysis of radial solutions for supercritical semilinear elliptic equations can be systematically characterized through limits of f'(u)F(u), providing comprehensive understanding of solution structures and their stability properties, including connections to singular solutions.

Abstract: We study the stability of radial solutions of the semilinear elliptic equation $Δu +f(u)=0$ in ${\bf R^N}$, where $N \geq 3$ and $f$ is a general superciritical nonlinearity. We give a classification of the solution structures with respect to the stability of radial solutions, and establish criteria for the existence and nonexistence of stable radial solutions in terms of the limits of $f'(u)F(u)$ as $u \to 0$ or $\infty$, where $F(u) = \int^{\infty}_u 1/f(t)dt$. Furthermore, we show the relation between the existence of singular stable solutions and the solution structure.

</details>


### [60] [Time-fractional nonlinear evolution equations with time-dependent constraints](https://arxiv.org/abs/2601.12352)
*Yoshihito Nakajima*

Main category: math.AP

TL;DR: Develops abstract theory for time-fractional gradient flows with time-dependent convex functionals in Hilbert spaces, proving existence of strong solutions and applying to nonlinear parabolic equations on moving domains.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical framework for time-fractional gradient flow equations with time-dependent convex functionals, extending classical gradient flow theory to fractional calculus settings with applications to evolving domain problems.

Method: Develops Gronwall-type lemmas for nonlinear Volterra integral inequalities and fractional chain-rule formulae to prove existence of strong solutions to time-fractional abstract evolution equations governed by time-dependent subdifferential operators.

Result: Proves existence of strong solutions for time-fractional gradient flow equations with time-dependent convex functionals, and applies the abstract theory to initial-boundary value problems for time-fractional nonlinear parabolic equations on moving domains.

Conclusion: Establishes a comprehensive abstract theory for time-fractional gradient flows with time-dependent convex functionals, providing mathematical tools and results applicable to nonlinear parabolic equations on evolving domains.

Abstract: This article is devoted to presenting an abstract theory of time-fractional gradient flow equations for time-dependent convex functionals in real Hilbert spaces. The main results are concerned with the existence of strong solutions to time-fractional abstract evolution equations governed by time-dependent subdifferential operators. To prove these results, Gronwall-type lemmas for nonlinear Volterra integral inequalities and fractional chain-rule formulae are developed. Moreover, the obtained abstract results are applied to the initial-boundary value problem for time-fractional nonlinear parabolic equations on moving domains.

</details>


### [61] [Localization and interpolation of parabolic $L^p$ Neumann problems](https://arxiv.org/abs/2601.12429)
*Martin Dindoš,Linhan Li,Jill Pipher*

Main category: math.AP

TL;DR: The paper establishes solvability of the Neumann problem in atomic Hardy spaces for parabolic operators with bounded measurable time-dependent coefficients, using localization estimates and interpolation techniques.


<details>
  <summary>Details</summary>
Motivation: To extend solvability results for parabolic equations with Neumann boundary conditions beyond standard L^p spaces to more general function spaces like atomic Hardy spaces, particularly for operators with bounded measurable time-dependent coefficients.

Method: First proves a localization estimate for local solutions to parabolic equations with zero Neumann data, assuming solvability of L^p Neumann and L^{p'} Dirichlet problems for the adjoint operator. Then uses this estimate to establish solvability in atomic Hardy spaces and obtain interpolation of L^p Neumann problem solvability.

Result: Successfully proves solvability of the Neumann problem in atomic Hardy spaces for parabolic operators with bounded measurable time-dependent coefficients, and obtains interpolation results for L^p Neumann problem solvability.

Conclusion: The paper provides important extensions of boundary value problem solvability for parabolic equations to more general function spaces, with applications to interpolation theory and analysis of operators with time-dependent coefficients.

Abstract: We show a localization estimate for local solutions to the parabolic equation $-\partial_t u+\mbox{div} (A\nabla u)=0$ with zero Neumann data, assuming that the $L^p$ Neumann problem and $L^{p'}$ Dirichlet problem for the adjoint operator are solvable in a Lipschitz cylinder for some $p\in(1,\infty)$. Using this result, we establish the solvability of the Neumann problem in the atomic Hardy space for parabolic operators with bounded, measurable, time-dependent coefficients, and hence obtain the interpolation of solvability of the $L^p$ Neumann problem.

</details>


### [62] [Rigidity results in multi-bubble dynamics for non-radial energy-critical heat equation](https://arxiv.org/abs/2601.12517)
*Kihyun Kim,Frank Merle*

Main category: math.AP

TL;DR: Classification of multi-bubble dynamics for energy-critical nonlinear heat equations in dimensions N≥7, identifying three scenarios with different universal blow-up speeds based on scaling relationships and bubble positions.


<details>
  <summary>Details</summary>
Motivation: To understand the classification of asymptotic behaviors in multi-bubble dynamics for energy-critical nonlinear heat equations without symmetry constraints, particularly focusing on how multiple solitons interact and concentrate over time.

Method: Analyze multi-bubble dynamics assuming each bubble is given by scalings and translations of ±W (ground state) with non-colliding conditions. Consider J≥2 solitons and identify three different scenarios based on scaling relationships and bubble positions.

Result: Three classification scenarios: 1) When one scaling dominates, one bubble stabilizes while others concentrate with speed t^{-2/(N-6)}; 2) Under non-degenerate position conditions, all bubbles concentrate with speed t^{-1/(N-4)}; 3) In degenerate but not too degenerate cases, all bubbles concentrate with newly discovered speed t^{-1/(N-3)}. Theorem covers ≤4 bubbles with example constructions.

Conclusion: First classification result in non-radial multi-bubble dynamics where scales, positions, and signs all play nontrivial roles, revealing three distinct universal blow-up speeds depending on scaling relationships and geometric configurations of bubbles.

Abstract: This paper concerns the classification of asymptotic behaviors in multi-bubble dynamics for the energy-critical nonlinear heat equations in large dimensions $N\geq7$ without symmetry. This multi-bubble dynamics appears naturally at least for a sequence of times in view of soliton resolution. We assume each bubble is given by the scalings and translations of $\pm W$ with (localized) non-colliding conditions for a sequence of times, where $W$ is the ground state. The case of one soliton was previously established and in particular there is no blow-up. We consider the case of $J\geq2$ solitons, where we expect only infinite-time blow-up.
  We are able to identify three different scenarios, where we have a continuous-in-time resolution with an unexpected universal blow-up speed. The first one is when one scaling is much larger than the others. In this case, one bubble does not concentrate (hence stabilize) and the other bubbles concentrate with the universal blow-up speed $t^{-2/(N-6)}$ together with strong sign constraints. Next, assuming we are not in the first scenario, we establish a non-degenerate condition on the positions of bubbles to obtain that all bubbles concentrate with the universal blow-up speed $t^{-1/(N-4)}$. The last case we consider is a degenerate, but not too much degenerate, scenario. Here again, we obtain that all bubbles concentrate with the universal blow-up speed $t^{-1/(N-3)}$. This last rate has not been discovered before. Our theorem covers the case of four or less bubbles and we provide the construction of examples. To our knowledge, this is the first classification result in the non-radial multi-bubble dynamics, where both the scales, positions, and signs enter the dynamics nontrivially.

</details>


### [63] [A class of non-cylindrical domains for parabolic equations](https://arxiv.org/abs/2601.12609)
*Alberto Domínguez Corella,Jorge Rivera-Noriega*

Main category: math.AP

TL;DR: The paper introduces a class of non-cylindrical domains suitable for Dirichlet-type parabolic problems, with mixed Lipschitz boundary regularity, using an implicit function theorem approach.


<details>
  <summary>Details</summary>
Motivation: To define appropriate non-cylindrical domains for posing and solving Dirichlet-type problems for parabolic equations like the heat equation, where traditional cylindrical domains may be insufficient.

Method: Develops a class of domains with mixed Lipschitz boundary regularity and uses an adequate version of the implicit function theorem tailored for functions with this regularity.

Result: Establishes that the introduced class of domains is equivalent in type to domains previously considered by other researchers in the field.

Conclusion: Provides a well-defined framework for studying parabolic equations on non-cylindrical domains with mixed Lipschitz boundaries, connecting to existing literature on similar domain classes.

Abstract: We present a class of non-cylindrical domains where Dirichlet-type problems for parabolic equations, such as the heat equation, can be posed and solved. The regularity for the boundary of this class of domains is a mixed Lipschitz condition, as described in the bulk of the paper. The main tool is an adequate version of the implicit function theorem for functions with this kind of regularity. It is proved that the class introduced herein is of the same type as domains previously considered by several authors.

</details>


### [64] [A Landau-de Gennes Type Theory for Cholesteric-Helical Smectic-Smectic C* Liquid Crystal Phase Transitions](https://arxiv.org/abs/2601.12653)
*Apala Majumdar,Baoming Shi,Dawei Wu,Jingmin Xia,Lei Zhang*

Main category: math.AP

TL;DR: Mathematical analysis of a modified Landau-de Gennes theory for temperature-driven phase transitions between cholesteric, helical smectic, and smectic C* liquid crystal phases, establishing existence of minimizers and convergence in asymptotic limits.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical understanding of temperature-driven phase transitions in complex liquid crystal systems, specifically the transitions between cholesteric, helical smectic, and smectic C* phases, which are important for both fundamental physics and practical applications in display technologies.

Method: Develops a modified Landau-de Gennes theory coupling tensor-valued nematic order parameter with real-valued smectic layer modulation. Uses mathematical analysis to establish existence of energy minimizers in 3D with Dirichlet conditions, analyzes asymptotic limits (Oseen-Frank limit and dominant elastic constants limit), and applies stability analysis and bifurcation theory to study phase transitions.

Result: Proves existence of energy minimizers in 3D. Shows convergence to Landau-de Gennes bulk energy minimizer in Oseen-Frank limit and to classical helical director profile in dominant elastic constants limit. Derives complete sequence of symmetry-breaking transitions: cholesteric → helical smectic → smectic C* with decreasing temperature. Results supported by numerical simulations.

Conclusion: The modified Landau-de Gennes theory successfully models complex temperature-driven phase transitions in liquid crystals, with rigorous mathematical foundations. The analysis provides complete understanding of symmetry-breaking transitions and establishes convergence properties in important asymptotic limits, offering theoretical framework for studying complex liquid crystal phases.

Abstract: We present a rigorous mathematical analysis of a modified Landau-de Gennes (LdG) theory modeling temperature-driven phase transitions between cholesteric, helical smectic, and smectic C* phases. This model couples a tensor-valued order parameter (nematic orientational order) with a real-valued order parameter (smectic layer modulation). We establish the existence of energy minimizers of the modified LdG energy in three dimensions, subject to Dirichlet conditions, and rigorously analyze the energy minimizers in two asymptotic limits. First, in the Oseen--Frank limit, we show that the global minimizer strongly converges to a minimizer of the Landau-de Gennes bulk energy. Second, in the limit of dominant elastic constants, we prove that the global minimizers converge to a classical helical director profile. Finally, through stability analysis and bifurcation theory, we derive the complete sequence of symmetry-breaking transitions with decreasing temperature-from the cholesteric phase (with in-plane twist and no layering) to an intermediate helical smectic phase (with in-plane twist and layering), and ultimately to the smectic C* phase (with out-of-plane twist and layering). These theoretical results are supported by numerical simulations.

</details>


### [65] [On a class of logarithmic Schrödinger equations via perturbation method](https://arxiv.org/abs/2601.12732)
*Chen Huang,Zhipeng Yang*

Main category: math.AP

TL;DR: New perturbative variational method developed to solve logarithmic Schrödinger equation with potential growing at infinity, proving existence and multiplicity of nontrivial weak solutions despite lack of C¹-smoothness.


<details>
  <summary>Details</summary>
Motivation: The logarithmic Schrödinger equation arises in quantum mechanics and nonlinear optics, but the associated functional lacks C¹-smoothness due to the logarithmic nonlinearity, making standard variational methods inapplicable. The potential V(x) grows at infinity, creating a compact embedding framework.

Method: Developed a new perturbative variational approach to handle the non-smoothness of the functional. The method combines perturbation techniques with variational arguments to overcome the lack of differentiability caused by the logarithmic term u log u².

Result: Proved the existence and multiplicity of nontrivial weak solutions to the logarithmic Schrödinger equation under the assumption that V(x) ∈ C(ℝᴺ) and V(x) → +∞ as |x| → ∞.

Conclusion: The new perturbative variational method successfully addresses the technical challenge of non-smooth functionals in logarithmic Schrödinger equations, providing a framework for proving existence and multiplicity results for such problems with potentials growing at infinity.

Abstract: In this paper, we consider the following logarithmic Schrödinger equation \[ -Δu + V(x)u = u \log u^{2} \quad \text{in }\ \mathbb{R}^{N}. \] Assuming that \(V(x)\in C(\mathbb{R}^{N})\) and \(V(x)\to+\infty\) as \(|x|\to\infty\), we develop a new perturbative variational approach to overcome the lack of \(C^{1}\)-smoothness of the associated functional and prove the existence and multiplicity of nontrivial weak solutions.

</details>


### [66] [A Sharp Global Boundedness Result for Keller--Segel--(Navier--)Stokes Systems with Rapid Diffusion and Saturated Sensitivities](https://arxiv.org/abs/2601.12733)
*Minh Le*

Main category: math.AP

TL;DR: The paper establishes global existence and boundedness results for Keller-Segel-(Navier-)Stokes systems in 2D and 3D with specific conditions on chemotactic sensitivity functions.


<details>
  <summary>Details</summary>
Motivation: To understand the interplay between chemotaxis and fluid dynamics, particularly how fluid coupling affects the blow-up behavior of Keller-Segel systems. The goal is to determine conditions under which global bounded solutions exist despite the potential for blow-up in pure chemotaxis models.

Method: Analysis of the coupled Keller-Segel-(Navier-)Stokes PDE system with no-flux boundary conditions for cell density and chemical concentration, and Dirichlet boundary conditions for fluid velocity. The approach involves establishing regularity estimates and using functional analytic techniques to prove global existence and boundedness under specific conditions on the chemotactic sensitivity function S.

Result: 1) In 2D with κ=1 (Navier-Stokes) and S(ξ)→0 as ξ→∞, global classical solutions exist and remain uniformly bounded. 2) In 3D with κ=0 (Stokes) and S(ξ) decaying like (ξ+1)^{-α} with α>1/3, global bounded classical solutions exist. These results are optimal given known blow-up thresholds for pure chemotaxis systems.

Conclusion: Fluid coupling can prevent blow-up in Keller-Segel systems when chemotactic sensitivity decays sufficiently fast. The critical decay rates (S→0 in 2D, α>1/3 in 3D) match known blow-up thresholds for pure chemotaxis, showing optimality. This demonstrates how fluid transport mechanisms can regularize chemotactic aggregation.

Abstract: We investigate the Keller--Segel--(Navier--)Stokes system posed in a smooth bounded domain \(Ω\subset \mathbb{R}^N\) with \(N = 2,3\): \begin{equation*} \begin{cases} n_t + u \cdot \nabla n = Δn - \nabla \cdot \big( n S(n)\nabla c \big), \\[2mm] u \cdot \nabla c = Δc - c + n, \\[2mm] u_t + κ(u \cdot \nabla) u = Δu - \nabla P + n \nabla φ, \\[2mm] \nabla \cdot u = 0, \end{cases} \end{equation*} where \(κ\in \left \{0,1 \right \} \), the given gravitational potential \(φ\in W^{2, \infty}(Ω)\), and the chemotactic sensitivity function \(S \in C^2([0,\infty))\).
  Under no-flux boundary conditions for \(n\) and \(c\), together with the Dirichlet boundary condition for \(u\), we show that, provided the initial data satisfy suitable regularity assumptions, the following results hold: \begin{itemize}
  \item If \(N = 2\), \(κ= 1\), and the sensitivity function satisfies
  \(\lim_{ξ\to \infty} S(ξ) = 0\), then the Keller--Segel--Navier--Stokes system admits
  a global classical solution that remains uniformly bounded in time.
  \item If \(N = 3\), \(κ= 0\), and \(S\) satisfies
  \[
  |S(ξ)| \le K_S (ξ+ 1)^{-α} \quad \text{for all } ξ\ge 0,
  \]
  with some constants \(K_S > 0\) and \(α> \frac{1}{3}\), then the
  Keller--Segel--Stokes system possesses a global bounded classical solution. \end{itemize} Our results are optimal, since it is well established that, in the absence of fluid effects, blow-up can occur when $S \equiv \mathrm{const}$ in two dimensions, or when $α< \tfrac{1}{3}$ in three dimensions.

</details>


### [67] [Sub-wavelength resonances in two-dimensional multi-layer elastic media](https://arxiv.org/abs/2601.12821)
*Yan Jiang,Hongyu Liu,Fanbo Sun,Yajuan Wang*

Main category: math.AP

TL;DR: The paper studies sub-wavelength resonances in 2D elastic media with high-contrast Lamé parameters and density, developing mathematical foundations and formulas for resonance frequencies in nested layer structures.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize sub-wavelength resonances in elastic media with high material contrasts, which has applications in wave manipulation, metamaterials, and elastic wave control.

Method: Uses layer potential techniques combined with asymptotic analysis to derive formulas for resonance frequencies, applies spectral properties to solve eigenvalue problems, and validates with numerical experiments.

Result: Proves invertibility of the leading-order operator, derives original formula for sub-wavelength resonance frequencies controlled by 3N×3N matrices (3N resonances for N-nested layers), shows O(ω⁻²) field enhancement, computes explicit expressions for disk geometries.

Conclusion: The paper establishes mathematical foundations for analyzing sub-wavelength resonances in high-contrast elastic media, providing theoretical formulas and numerical validation for resonance behavior in nested layer structures.

Abstract: In this paper, we focus on the sub-wavelength resonances in two-dimensional elastic media characterized by high contrasts in both Lamé parameters and density. Our contributions are fourfold. First, it is proved that the operator $\hat{\mathbf{S}}_{\partial D}^ω$, which serves as a leading order approximation to $\mathbf{S}_{\partial D}^ω$ as $ω\rightarrow0$, is invertible in the space $\mathcal{L}(L^{2}\left(\partial D)^{2},H^{1}(\partial D)^{2}\right)$. Second, based on layer potential techniques in combination with asymptotic analysis, we derive an original formula for the leading-order terms of sub-wavelength resonance frequencies, which are controlled by the determinant of the $3N \times 3N$ matrices. Specifically, there are $3N$ resonance frequencies within an $N$-nested layer structure. In addition, the scattering field exhibits an enhancement coefficient on the order of $\mathcal{O}(ω^{-2})$ as the incident frequency $ω$ approaches the resonance frequency. Third, by applying spectral properties to solve the corresponding eigenvalue problem, we compute the quantitative expressions for sub-wavelength resonance frequencies within a disk. Finally, some numerical experiments are provided to illustrate theoretical results and demonstrate the existence of the sub-wavelength resonance modes.

</details>


### [68] [Traveling waves for monostable reaction-diffusion-convection equations with discontinuous density-dependent coefficients](https://arxiv.org/abs/2601.12869)
*Pavel Drábek,Soyeun Jung,Eunkyung Ko,Michaela Zahradníková*

Main category: math.AP

TL;DR: This paper studies wave propagation in scalar reaction-diffusion-convection equations with p-Laplacian diffusion and monostable reaction, introducing non-smooth traveling wave profiles to handle discontinuous diffusion and piecewise continuous convection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a framework for analyzing wave propagation in equations with p-Laplacian-type diffusion that can handle discontinuities, degenerations, and singularities in the diffusion coefficient, as well as piecewise continuous convective velocity, which are not covered by classical smooth traveling wave theory.

Method: The authors introduce a new concept of non-smooth traveling wave profiles and use comparison arguments for an equivalent non-Lipschitz first-order ODE. They formulate sufficient conditions for existence/non-existence and analyze how convection affects minimal wave speed compared to the convection-free problem.

Result: The paper establishes existence and non-existence conditions for generalized traveling wave solutions, analyzes how convective velocity affects minimal wave speed, and provides asymptotic analysis of wave profiles under power-type assumptions on diffusion and reaction terms.

Conclusion: The proposed framework successfully handles challenging cases with discontinuous diffusion and piecewise continuous convection, providing a generalized approach to traveling wave analysis for p-Laplacian reaction-diffusion-convection equations with monostable reaction.

Abstract: This paper concerns wave propagation in a class of scalar reaction-diffusion-convection equations with $p$-Laplacian-type diffusion and monostable reaction. We introduce a new concept of a non-smooth traveling wave profile, which allows us to treat discontinuous diffusion with possible degenerations and singularities at 0 and 1, as well as only piecewise continuous convective velocity. Our approach is based on comparison arguments for an equivalent non-Lipschitz first-order ODE. We formulate sufficient conditions for the existence and non-existence of these generalized solutions and discuss how the convective velocity affects the minimal wave speed compared to the problem without convection. We also provide brief asymptotic analysis of the profiles, for which we need to assume power-type behavior of the diffusion and reaction terms.

</details>


### [69] [A functional inequalities approach for the field-road diffusion model with (symmetric) nonlinear exchanges](https://arxiv.org/abs/2601.12909)
*Matthieu Alfaro,Claire Chainais-Hillairet,Flore Nabet*

Main category: math.AP

TL;DR: A new functional inequalities approach proves exponential decay of relative entropy for field-road diffusion models, ensuring convergence to stationary states determined by initial mass.


<details>
  <summary>Details</summary>
Motivation: To analyze field-road diffusion models (coupled parabolic PDEs on different dimensional sets) and establish convergence properties to stationary states.

Method: A direct functional inequalities approach to prove exponential decay of relative entropy.

Result: Exponential decay of relative entropy is established, leading to convergence of solutions to stationary states determined by initial total mass.

Conclusion: The functional inequalities approach successfully demonstrates convergence properties for field-road diffusion models, with stationary states uniquely determined by initial mass conservation.

Abstract: In this note, we consider the so-called field-road diffusion model in a bounded domain, consisting of two parabolic PDEs posed on sets of different dimensions and coupled through (symmetric) nonlinear exchange terms. We propose a new and rather direct functional inequalities approach to prove the exponential decay of a relative entropy, and thus the convergence of the solution towards the stationary state selected by the total mass of the initial datum.

</details>


### [70] [Sharp lower bound for the Monge-Ampère torsion on convex sets](https://arxiv.org/abs/2601.12915)
*Francesco Salerno*

Main category: math.AP

TL;DR: The paper establishes bounds on the ratio between Monge-Ampère torsion deficit and geometric deficit from Alexandrov-Fenchel inequality for convex sets near balls.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between two different geometric deficits: the Monge-Ampère torsion deficit (measuring deviation from optimal torsion functional value) and the geometric deficit from Alexandrov-Fenchel inequality, particularly for convex sets close to balls.

Method: Uses shape derivative technique to analyze the ratio between the two deficits for families of open, bounded convex sets of class C² that smoothly converge to a ball.

Result: Proves that the ratio between the Monge-Ampère torsion deficit and the geometric deficit is bounded from below by a dimensional constant, and also bounded from above by a constant, for convex sets near balls.

Conclusion: The ratio between these two geometric deficits is uniformly bounded both from above and below by constants, establishing a controlled relationship between different measures of deviation from spherical geometry.

Abstract: The \emph{Monge-Ampère} torsion deficit of an open, bounded convex set $Ω\subset\R^n$ of class $C^2$ is the normalized gap between the value of the torsion functional evaluated on $Ω$ and its value on the ball with the same $(n-1)$-quermassintegral as $Ω$. Using the technique of the \emph{shape derivative}, we prove that the ratio between this deficit and to a geometric deficit arising from the \emph{Alexandrov-Fenchel inequality}, for any given family of open, bounded convex sets of $\R^n$ ($n\geq2$) of class $C^2$, smoothly converging to a ball, is bounded from below by a dimensional constant. We also show that this ratio is always bounded from above by a constant.

</details>


### [71] [Bernstein type gradient estimate for system of weighted local heat equations with potential term](https://arxiv.org/abs/2601.12992)
*Sujit Bhattacharyya*

Main category: math.AP

TL;DR: Bernstein-type gradient estimates for two systems of local weighted heat equations with potentials on weighted Riemannian manifolds, covering all cases with linear/exponential potentials on static/evolving manifolds.


<details>
  <summary>Details</summary>
Motivation: To provide gradient estimates for heat-type equations with potentials on weighted manifolds, partially resolving a problem raised by Bhattacharyya et al.

Method: Derives Bernstein-type gradient estimates by considering all possible combinations: linear potential, exponential potential, static manifold, and evolving manifold.

Result: Complete set of gradient estimates for two systems of local weighted heat equations with potentials on weighted Riemannian manifolds.

Conclusion: The work provides comprehensive gradient estimates that partially resolve the problem raised by Bhattacharyya et al., covering all relevant cases of potentials and manifold types.

Abstract: In this article we provide Bernstein type gradient estimates for two system of local weighted heat type equations with potentials on a weighted Riemannian manifold. We derive all possible cases considering linear potential, exponential potential, combining with static manifold and evolving manifold. This work partially resolved the problem raised by Bhattacharyya et al. in \cite{SB-1}.

</details>


### [72] [Optimal existence of weak solutions for the generalised Navier-Stokes-Voigt equations](https://arxiv.org/abs/2601.13051)
*Ankit Kumar,Hermenegildo Borges de Oliveira,Manil T. Mohan*

Main category: math.AP

TL;DR: Existence and uniqueness of weak solutions for incompressible generalized Navier-Stokes-Voigt equations with power-law rheology for p > 2d/(d+2) in dimensions d=2,3.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for generalized Navier-Stokes-Voigt equations with power-law viscosity, which model complex fluids with memory effects and non-Newtonian behavior, extending classical fluid dynamics theory.

Method: Using Gelfand triple framework and Aubin-Dubinskii lemma to obtain strong convergence of approximate solutions, proving existence and uniqueness of weak solutions through functional analysis techniques.

Result: Proved existence of weak solutions for p ∈ (2d/(d+2), ∞) in dimensions d=2,3, and demonstrated uniqueness of these weak solutions for the same parameter range.

Conclusion: The optimal range p > 2d/(d+2) is established for existence and uniqueness of weak solutions to generalized Navier-Stokes-Voigt equations, with the Gelfand triple framework being essential for the proof.

Abstract: In this study, we investigate the incompressible generalised Navier-Stokes-Voigt equations within a bounded domain $Ω\subset \mathbb{R}^d$, where $d \geq 2$. The governing momentum equation is expressed as: \begin{align*} \partial_t(\boldsymbol{v} - κΔ\boldsymbol{v}) + \nabla \cdot (\boldsymbol{v} \otimes \boldsymbol{v}) + \nabla π- ν\nabla \cdot \big( |\mathbf{D}(\boldsymbol{v})|^{p-2} \mathbf{D}(\boldsymbol{v}) \big) = \boldsymbol{f}. \end{align*} Here, for $d \in \{2,3\}$, $\boldsymbol{v}$ represents the velocity field, $π$ denotes the pressure, and $\boldsymbol{f}$ is the external forcing term. The constants $κ$ and $ν$ correspond to the relaxation time and kinematic viscosity, respectively. The parameter $p \in (1, \infty)$ characterizes the fluid's flow behavior, and $\mathbf{D}(\boldsymbol{v})$ denotes the symmetric part of the velocity gradient $\nabla \boldsymbol{v}$. For the power-law exponent $p \in \big( \frac{2d}{d+2}, \infty \big)$, we establish the existence of a weak solution to the generalised Navier-Stokes-Voigt equations. Furthermore, we demonstrate that the weak solution is unique for the same range of the exponent $p$. The optimality of our results lies in the framework's use of a Gelfand triple, which allows the Aubin-Dubinskii lemma to yield strong convergence of approximate solutions, essential for existence and valid precisely for $p > \frac{2d}{d+2}$.

</details>


### [73] [Wasserstein geometry of nonnegative measures on finite Markov chains I: Gradient flow](https://arxiv.org/abs/2601.13073)
*Qifan Mao,Xinyu Wang,Xiaoping Xue*

Main category: math.AP

TL;DR: The paper develops a Benamou-Brenier type transportation metric for measures on Markov chains, establishing a Riemannian structure and showing that generalized heat equations with sources are gradient flows of discrete entropy.


<details>
  <summary>Details</summary>
Motivation: To provide a coherent geometric interpretation of generalized diffusion equations with source terms and clarify the role of Benamou-Brenier formulation in discrete optimal transport for nonnegative measures.

Method: Develops a Benamou-Brenier type transportation metric on finite reversible Markov chains, endowing the space of measures with Riemannian structure. Uses this geometric framework to identify generalized heat equations with sources as gradient flows of discrete entropy.

Result: Establishes that generalized heat equations with sources are gradient flows of discrete entropy in this Riemannian framework. Proves exponential convergence to unique equilibrium using local Łojasiewicz inequality.

Conclusion: The Benamou-Brenier formulation provides a coherent geometric framework for discrete optimal transport of nonnegative measures and offers a geometric interpretation of generalized diffusion equations with source terms.

Abstract: We investigate a Benamou--Brenier type transportation metric for nonnegative measures on a finite reversible Markov chain, which endows the space of measures with a Riemannian structure. Using this geometric framework, we identify a generalized heat equation with source as the gradient flow of the discrete entropy. Moreover, by means of a local Łojasiewicz inequality, we prove exponential convergence of the flow to a unique equilibrium. Our results clarify the role of the Benamou--Brenier formulation in discrete optimal transport for nonnegative measures and provide a coherent geometric interpretation of generalized diffusion equations with source terms.

</details>


### [74] [Wasserstein geometry of nonnegative measures on finite Markov chains II: Geodesic and duality formulae](https://arxiv.org/abs/2601.13080)
*Qifan Mao,Xinyu Wang,Xiaoping Xue*

Main category: math.AP

TL;DR: The paper studies a Benamou-Brenier-type transportation metric on nonnegative measures over finite reversible Markov chains, revealing geodesics have full support and establishing Kantorovich duality.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric structure and duality properties of transportation metrics on measure spaces over Markov chains, particularly those combining transport and source costs in nonconservative continuity equations.

Method: Defines a metric through dynamic formulation with transport and source costs along solutions of nonconservative continuity equations, where mass variation occurs along a fixed positive reference direction. Analyzes geodesic structure and establishes Kantorovich-type duality via Hamilton-Jacobi subsolutions.

Result: Geodesics exhibit non-locality with full support almost everywhere; source terms follow characteristic temporal profile (creation then decay); metric bounds shift-transport distance above; establishes Kantorovich duality formula characterizing the metric.

Conclusion: The transportation metric reveals fundamental geometric properties of measure spaces over Markov chains, with geodesics showing universal support properties and the duality framework providing analytical characterization of optimal transport structure.

Abstract: In this paper, we investigate the geodesic structure and the associated Kantorovich-type duality for a Benamou-Brenier-type transportation metric defined on the space of nonnegative measures over a finite reversible Markov chain. The metric is introduced through a dynamic formulation that combines transport and source costs along solutions of a nonconservative continuity equation, where mass variation is constrained to occur along a fixed strictly positive reference direction. We show that geodesics associated with this metric exhibit a non-locality property: almost every time, they are supported on the whole state space, independently of the choice of endpoints. Moreover, along optimal curves, the source term displays a characteristic temporal profile, with mass creation occurring at early times and subsequent decay as the curve approaches the target measure. As an application of this property, we compare our metric with the shift-transport distance and prove that the latter is always bounded above by our metric. Finally, we establish a Kantorovich-type duality formula in terms of Hamilton-Jacobi subsolutions, which provides a characterization of the metric and highlights the role of the momentum associated with geodesic curves.

</details>


### [75] [On the splitting of Neumann eigenvalues in perforated domains](https://arxiv.org/abs/2601.13129)
*Veronica Felli,Lorenzo Liverani,Roberto Ognibene*

Main category: math.AP

TL;DR: Generic splitting of Neumann Laplacian eigenvalues under singular domain perturbations by small holes.


<details>
  <summary>Details</summary>
Motivation: To understand how eigenvalues of the Neumann Laplacian behave under singular domain perturbations, specifically when small holes are excised from the domain. This addresses fundamental questions in spectral theory about eigenvalue multiplicity and stability under geometric perturbations.

Method: The authors consider domain perturbations by excising a small spherical hole shrinking to an interior point. They prove that eigenvalue splitting is generic when the hole center is outside a set of Hausdorff dimension N-1. The proof relies on establishing an asymptotic expansion for perturbed eigenvalues in terms of the scaling parameter.

Result: The main result shows that splitting of multiple eigenvalues is generic: if the hole center is located outside a set of Hausdorff dimension N-1 and the radius is sufficiently small, multiple eigenvalues split into branches of lower multiplicity. The asymptotic expansion generalizes previous results, being valid for holes of arbitrary shape in dimensions N≥3.

Conclusion: Singular domain perturbations by small holes generically cause multiple eigenvalues of the Neumann Laplacian to split into eigenvalues of lower multiplicity, with the splitting occurring for almost all hole positions. The asymptotic expansion provides a powerful tool for analyzing such spectral perturbations.

Abstract: We address the problem of splitting of eigenvalues of the Neumann Laplacian under singular domain perturbations. We consider a domain perturbed by the excision of a small spherical hole shrinking to an interior point. Our main result establishes that the splitting of multiple eigenvalues is a generic property: if the center of the hole is located outside a set of Hausdorff dimension $N-1$ and the radius is sufficiently small, multiple eigenvalues split into branches of lower multiplicity. The proof relies on the validity of an asymptotic expansion for the perturbed eigenvalues in terms of the scaling parameter. Such an asymptotic formula is of independent interest and generalizes previous results; notably, in dimension $N\geq 3$, it is valid for holes of arbitrary shape.

</details>


### [76] [PDE aspects of the dynamical optimal transport in the Lorentzian setting](https://arxiv.org/abs/2601.13167)
*Nicola Gigli,Felix Rott,Matteo Zanardini*

Main category: math.AP

TL;DR: The paper establishes a Lorentzian version of the Benamou-Brenier formula by extending optimal transport theory from Riemannian manifolds to spacetimes, replacing the continuity equation with a causal continuity inequality.


<details>
  <summary>Details</summary>
Motivation: To extend the equivalence between static and dynamic formulations of optimal transport from Riemannian manifolds to spacetimes, bridging Wasserstein geometry with PDEs in relativistic contexts.

Method: Investigate optimal transport on spacetimes by transitioning from the continuity equation to a 'causal continuity inequality' that accounts for relativistic causality constraints.

Result: Obtain a Lorentzian version of the Benamou-Brenier formula, establishing the equivalence between static and dynamic formulations of optimal transport in spacetime settings.

Conclusion: The paper successfully extends optimal transport theory to spacetimes, providing a relativistic framework that maintains the crucial link between Wasserstein geometry and PDEs through a modified continuity inequality approach.

Abstract: One of the crucial features of optimal transport on Riemannian manifolds is the equivalence of the `static', original, formulation of the problem and of the `dynamic' one, based on the study of the continuity equation. This furnishes the key link between Wasserstein geometry and PDEs that has found so many applications in the last 20 years.
  In this paper we investigate this kind of equivalence on spacetimes. At the PDE level, this requires to transition from the continuity equation to a suitable `continuity inequality', to which we shall refer to as `causal continuity inequality'. As a direct consequence of our findings we obtain a Lorentzian version of the celebrated Benamou--Brenier formula.

</details>


### [77] [Onsager's Mean Field Theory of Vortex Flows with Singular Sources: Blow-Up and Concentration without Quantization](https://arxiv.org/abs/2601.13192)
*Daniele Bartolucci,Paolo Cosentino,Lina Wu*

Main category: math.AP

TL;DR: Generalization of mean field theory for turbulent Euler flows with point singularities, analyzing a new "blow up and concentration without quantization" phenomenon where concentration mass can take continuous values.


<details>
  <summary>Details</summary>
Motivation: Motivated by Onsager's statistical mechanics description of turbulent Euler flows with point singularities, and building on previous mean field theory work by Caglioti et al. (1995).

Method: Generalization of mean field theory, proving equivalence of statistical ensembles, and careful analysis of a new blow up phenomenon with non-standard pointwise estimates.

Result: Identification of "blow up and concentration without quantization" phenomenon where concentration mass can take values in a full interval of real numbers, lying between classical blow up-concentration-quantization and blow up without concentration.

Conclusion: Complete description of allowed asymptotic profiles through careful analysis of pointwise estimates in this non-standard context, advancing understanding of singular behavior in turbulent Euler flows.

Abstract: Motivated by the Onsager statistical mechanics description of turbulent Euler flows with point singularities, we make a first step in the generalization of the mean field theory in [Caglioti, Lions, Marchioro, Pulvirenti; Comm. Math. Phys. (1995)]. On one side we prove the equivalence of statistical ensembles, on the other side we are bound to the analysis of a new blow up phenomenon, which we call "blow up and concentration without quantization", where the mass associated with the concentration is allowed to take values in a full interval of real numbers. This singular behavior may be regarded as lying between the classical blow up-concentration-quantization and the blow up without concentration phenomenon first proposed in [Lin, Tarantello; C.R. Math. Acad. Sci. Paris (2016)]. A careful analysis is needed to generalize known pointwise estimates in this non standard context, resulting in a complete description of the allowed asymptotic profiles.

</details>


### [78] [Asymptotic Stability of Rarefaction Waves for the Hyperbolized Navier-Stokes-Fourier System](https://arxiv.org/abs/2601.13193)
*Yuxi Hu,Mengran Yuan,Jie Zhang*

Main category: math.AP

TL;DR: Proves global existence and asymptotic stability of rarefaction waves for a compressible fluid system with Maxwell-Cattaneo relaxation laws, showing convergence to Euler rarefaction waves.


<details>
  <summary>Details</summary>
Motivation: To study the asymptotic stability of rarefaction waves in a compressible fluid system where classical Newtonian viscosity and Fourier heat conduction are replaced by Maxwell's law (for viscosity) and Cattaneo's law (for heat conduction), which introduces finite signal propagation speeds and generalizes the Navier-Stokes-Fourier equations.

Method: Considers the Cauchy problem in Lagrangian coordinates with initial data connecting two constant states via a rarefaction wave of the corresponding Euler system. Uses a combination of the relative entropy method and standard energy estimates to prove the main results.

Result: Proves that for sufficiently small initial perturbations and wave strength, the relaxation system admits a unique global solution, and this solution converges uniformly to the background rarefaction wave as time approaches infinity.

Conclusion: The Maxwell-Cattaneo relaxation system exhibits stable rarefaction wave behavior similar to classical systems, with solutions converging to Euler rarefaction waves asymptotically, demonstrating the mathematical well-posedness of this generalized fluid model.

Abstract: This paper investigates the asymptotic stability of rarefaction waves for a one-dimensional compressible fluid system, where the Newton's law of viscosity and Fourier's law of heat conduction are replaced by Maxwell's law and Cattaneo's law, respectively. The system, which generalizes the classical Navier-Stokes-Fourier equations, features finite signal propagation speeds. We consider the Cauchy problem in Lagrangian coordinates with initial data connecting two different constant states via a rarefaction wave of the corresponding Euler system. Our main result proves that, provided the initial perturbation and wave strength are sufficiently small, the relaxation system admits a unique global solution. Furthermore, this solution converges uniformly to the background rarefaction wave as time approaches infinity. The proof is established through a combination of the relative entropy method and usual energy estimates.

</details>


### [79] [A Harnack-type inequality for a perturbed singular Liouville Equation](https://arxiv.org/abs/2601.13212)
*Daniele Bartolucci,Paolo Cosentino,Lina Wu*

Main category: math.AP

TL;DR: Harnack-type inequality for sequences of solutions to perturbed Liouville equations with singular coefficients, motivated by Onsager's statistical mechanics of turbulent Euler flows.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by Onsager's statistical mechanics description of turbulent Euler flows with point singularities, which leads to analyzing sequences of solutions to perturbed Liouville equations.

Method: Analyze sequences of solutions to the perturbed Liouville equation -Δv_n = (ε_n² + |x|²)^{α_n} V_n(x) e^{v_n} with parameters ε_n→0⁺, α_n→α_∞∈(-1,1), and V_n satisfying uniform bounds and local uniform convergence.

Result: Obtain a Harnack-type inequality for sequences of solutions to the perturbed Liouville equation, which provides uniform control on the solutions despite the singular behavior as ε_n→0 and x→0.

Conclusion: The Harnack-type inequality establishes important regularity properties for sequences of solutions to singular Liouville-type equations, with applications to statistical mechanics descriptions of turbulent flows with point singularities.

Abstract: Motivated by the Onsager statistical mechanics description of turbulent Euler flows with point singularities, we obtain a Harnack-type inequality for sequences of solutions of the following perturbed Liouville equation, \begin{equation}\nonumber
  -Δv_n=\left({ε_n^2+|x|^2}\right)^{α_n}V_n(x)e^{\displaystyle v_n} \qquad\text{in} \,\,\, Ω, \end{equation} where $ε_n\to0^+$, $α_n\toα_\infty\in(-1,1)$, $Ω$ is a bounded domain in $\mathbb{R}^2$ containing the origin and $V_n$ satisfies, \begin{equation}\nonumber
  0<a\leq V_n\leq b<+\infty, \,\, V_n\in C^{0}(Ω), \,\,V_n\to V \,\, \text{locally uniformly in}\,\,Ω. \end{equation}

</details>


### [80] [Analytic spectral perturbation theory for a high-contrast Maxwell operator](https://arxiv.org/abs/2601.13408)
*Robert V. Kohn,Raghavendra Venkatraman*

Main category: math.AP

TL;DR: Spectral perturbation theory for Maxwell operator in cavities with high-contrast core-shell structures, analyzing analytic dependence on contrast parameter and geometry effects.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties of time-harmonic Maxwell operator in cavities with high-contrast dielectric structures, particularly how resonances depend on geometry and contrast parameter in degenerate limiting regimes.

Method: Develop spectral theory for degenerate Maxwell systems using novel operator-theoretic reformulation, prove complex-analytic dependence on contrast parameter δ, analyze asymptotic expansions for spherical inclusions, and construct examples showing geometry sensitivity.

Result: Established analytic spectral dependence on δ near δ=0, identified conditions for geometry-independent leading-order asymptotics in spherical cases, and constructed examples showing shell geometry sensitivity even in symmetric settings.

Conclusion: The work clarifies mechanisms behind geometry-invariance of resonances in high-contrast Maxwell systems and explains their robustness under small complex perturbations, providing fundamental understanding of spectral behavior in degenerate regimes.

Abstract: We study analytic spectral perturbation theory for the time-harmonic Maxwell operator in a perfectly electrically conducting cavity containing a high-contrast core--shell structure. The dielectric permittivity equals $1$ in a bounded inclusion and a small complex parameter $δ$ in the surrounding shell. The limit $δ\to 0$ corresponds to an infinite-contrast regime and leads to a degenerate Maxwell system. Despite this degeneracy, we develop a detailed spectral theory for the limiting problem for general Lipschitz inclusions and shells.
  Using a novel operator-theoretic reformulation, we prove complex-analytic dependence of the spectrum on $δ$ in a neighborhood of $δ= 0$. When the inclusion is a ball, we analyze the asymptotic expansion of eigenvalues and identify conditions under which the leading-order term is independent of the geometry of the surrounding shell. We also construct examples of resonances for which the leading-order asymptotics depend sensitively on the shell geometry, even in this symmetric setting. These results clarify the mechanisms underlying geometry-invariance of resonances in high-contrast Maxwell systems and explain their robustness under small complex perturbations.

</details>


### [81] [Stabilization of an incompressible fluid-elastic structure system using a vacuum bubble](https://arxiv.org/abs/2601.13430)
*B. Ingimarson,I. Kukavica,W. S. Ożański*

Main category: math.AP

TL;DR: The paper proves a priori estimates for fluid-structure interaction with a vacuum bubble, showing bubble presence stabilizes the system enabling global existence and exponential decay for small data.


<details>
  <summary>Details</summary>
Motivation: To analyze the interaction between elastic bodies and incompressible fluids in 3D curved domains, particularly when the fluid contains a vacuum bubble, and understand how this bubble affects system stability.

Method: Mathematical analysis proving a priori estimates for the PDE system modeling fluid-structure interaction with Navier-Stokes equations for the fluid, damped wave equation for the elastic body, and curved free interface.

Result: The vacuum bubble stabilizes the system by providing control of the pressure function's average, leading to global existence and exponential decay of smooth solutions for small initial data.

Conclusion: Vacuum bubbles play a crucial stabilizing role in fluid-structure interaction problems, enabling long-time existence and decay properties that might not hold without the bubble.

Abstract: We prove a priori estimates for the system of partial differential equations modeling the interaction between an elastic body and an incompressible fluid in a 3D curved domain. The fluid is governed by the incompressible Navier-Stokes equations and contains a bubble whose interior is a vacuum. The elastic body is described by a damped wave equation, and interaction with the fluid takes place along a free interface whose initial domain is curved. We show that the presence of the vacuum bubble stabilizes the system in the sense that it provides control of the average of the pressure function, and hence allows global existence and exponential decay of smooth solutions for small data.

</details>


### [82] [A priori estimates and exact solvability for non-coercive stochastic control equations](https://arxiv.org/abs/2601.13444)
*Maria Luísa Pasinato,Boyan Sirakov*

Main category: math.AP

TL;DR: First explicit a priori and regularity estimates for Dirichlet problem of Hamilton-Jacobi-Bellman operators with opposite-sign principal half-eigenvalues, plus exact multiplicity results (0,1,2 solutions) depending on valuation function.


<details>
  <summary>Details</summary>
Motivation: To establish explicit a priori and regularity estimates for solutions of Dirichlet problems for Hamilton-Jacobi-Bellman operators from stochastic control, particularly when principal half-eigenvalues have opposite signs. Also to study solution multiplicity for such fully nonlinear equations.

Method: Analysis of Hamilton-Jacobi-Bellman operators from stochastic control, focusing on cases where principal half-eigenvalues have opposite signs. Uses techniques from fully nonlinear elliptic equations and eigenvalue analysis.

Result: First explicit a priori and regularity estimates established. When negative eigenvalue is not too negative, the Dirichlet problem can have exactly two, one, or zero solutions depending on the valuation function. This provides exact multiplicity results.

Conclusion: Novel exact multiplicity result for fully nonlinear equations, generalizing the Ambrosetti-Prodi theorem to Hamilton-Jacobi-Bellman equations. Provides foundational estimates and classification of solution existence for these important stochastic control problems.

Abstract: We establish, for the first time, explicit a priori and regularity estimates for solutions of the Dirichlet problem for Hamilton-Jacobi-Bellman operators from stochastic control, whose principal half-eigenvalues have opposite signs. In addition, if the negative eigenvalue is not too negative, the problem can have exactly two, one or zero solutions, depending on the valuation function. This is a novel exact multiplicity result for fully nonlinear equations, which also yields a generalization of the Ambrosetti-Prodi theorem to such equations.

</details>


### [83] [On the radius of analyticity and Gevrey regularity for the Boltzmann equation](https://arxiv.org/abs/2601.13560)
*Wei-Xi Li,Lvqiao Liu,Hao Wang*

Main category: math.AP

TL;DR: The paper establishes sharp short-time and global-in-time estimates on analyticity radius and Gevrey regularity for the non-cutoff Boltzmann equation with hard potentials in perturbative settings.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of solutions to the non-cutoff Boltzmann equation for hard potentials, particularly focusing on analytic and Gevrey regularity in perturbative regimes.

Method: Combines hypoelliptic estimates with macro-micro decomposition to analyze mild solutions of the Boltzmann equation.

Result: Establishes sharp short-time estimates on analyticity radius and Gevrey regularity, and obtains global-in-time radius estimates in Gevrey space.

Conclusion: The non-cutoff Boltzmann equation for hard potentials exhibits strong regularity properties with precise estimates on analyticity and Gevrey regularity in perturbative settings.

Abstract: This paper investigates the non-cutoff Boltzmann equation for hard potentials in a perturbative setting. We first establish a sharp short-time estimate on the radius of analyticity and Gevrey regularity of mild solutions. Furthermore, we obtain a global-in-time radius estimate in Gevrey space. The proof combines hypoelliptic estimates with the macro-micro decomposition.

</details>


### [84] [Exact solution of the (2+1)-dimensional damping forcing coupled Burgers equation by using Darboux transformation](https://arxiv.org/abs/2601.13634)
*Prasanta Chatterjee,Nanda Kanan Pal,Dipan Saha,Santanu Raut*

Main category: math.AP

TL;DR: The paper studies a (2+1)-dimensional damping forcing coupled Burgers equation, establishes its Lax pair, derives N-fold Darboux transformation, and obtains wave solutions including solitary and periodic waves, with graphical analysis of damping/forcing effects.


<details>
  <summary>Details</summary>
Motivation: To investigate a modified version of the coupled Burgers equation with damping and forcing terms in (2+1) dimensions, which extends the classical Burgers equation to include physical effects like dissipation and external forcing.

Method: Established the Lax pair for the equation, derived N-fold Darboux transformation using the Lax pair, and applied one-fold and two-fold Darboux transformations to obtain wave solutions.

Result: Obtained various wave solutions including solitary wave solutions and periodic wave solutions, and graphically demonstrated the impact of damping and forcing terms on these solutions.

Conclusion: Successfully analyzed the (2+1)-dimensional damping forcing coupled Burgers equation using integrability methods, providing explicit solutions and demonstrating how damping and forcing terms affect wave propagation characteristics.

Abstract: In this article, we investigate the (2+1)-dimensional damping forcing coupled Burgers equation, which is obtain by adding damping and forcing terms from couple Burgers equation. The Lax pair of the (2+1)-dimensional damping forcing coupled Burgers equation is established. With the help of Lax pair, we derive the $N$-fold Darboux transformation of (2+1)-dimensional damping forcing coupled Burgers equation. Using one fold and two fold Darboux transformation, we demonstrated some wave solutions including solitary wave solution and periodic wave solution. The impact of damping and forcing terms in solitary wave solution and periodic solution is graphically demonstrated.

</details>


### [85] [Sharp Quantitative Forms of the Hardy Inequality on Cartan-Hadamard Manifolds via Sobolev-Lorentz Embeddings](https://arxiv.org/abs/2601.13750)
*Avas Banerjee,Debdip Ganguly,Prasun Roychowdhury*

Main category: math.AP

TL;DR: The paper proves a quantitative Hardy inequality on Riemannian models with centered isoperimetric inequality, establishes optimal Sobolev-Lorentz embeddings on Cartan-Hadamard manifolds, and shows correspondence between Hardy deficits on manifolds and Euclidean spaces.


<details>
  <summary>Details</summary>
Motivation: To extend classical Hardy inequality results from Euclidean spaces to curved Riemannian manifolds, particularly establishing quantitative bounds and understanding how curvature affects extremal behavior.

Method: Uses symmetrization techniques on manifolds combined with a novel Jacobian-type transformation that compares volume growth, level sets, and gradient terms between Euclidean and manifold geometries.

Result: Proves a quantitative Hardy inequality on Riemannian models satisfying centered isoperimetric inequality, generalizes Cianchi-Ferone's result to curved spaces, establishes optimal Sobolev-Lorentz embedding on Cartan-Hadamard manifolds, and shows correspondence between Hardy deficits.

Conclusion: The framework provides sharp control over functionals and reveals curvature's influence on extremal behavior, successfully extending Euclidean results to curved spaces with quantitative precision.

Abstract: In this article, we investigate the quantitative form of the classical Hardy inequality. In our first result, we prove the following quantitative bound under the assumption that the $\mathbb{M}^N$ is a Riemannian model satisfying the centered isoperimetric inequality: We prove that $$ \|\nabla_g u\|^2_{L^{2}(\mathbb{M}^N)} - \frac{(N-2)^2}{4}\left\|\frac{u}{r(x)}\right\|^2_{L^2(\mathbb{M}^N)} \geq C [\mbox{dist}(u, Z)]^{\frac{4N}{N-2}}\left\|\frac{u}{r(x)}\right\|^2_{L^2(\mathbb{M}^N)},$$ for every real-valued weakly differentiable function $u$ on $\mathbb{M}^N$ such that $|\nabla_g u| \in L^2(\mathbb{M}^N)$ and $u$ decays to zero at infinity. Here $r(x) = d_g(x,x_0)$ denotes the geodesic distance from a fixed pole $x_0,$ the set $Z$ represents the family of virtual extremals, and the distance is understood in an appropriate generalized Lorentz-type space. Our approach is built on the symmetrization technique on manifolds, combined with a novel Jacobian-type transformation that provides a precise way for comparing volume growth, level sets, and gradient terms across the two geometries of Euclidean and manifold settings. When coupled with symmetrization, this framework yields sharp control over the relevant functionals and reveals how the underlying curvature influences extremal behavior. Our result generalizes the seminal result of Cianchi-Ferone [Ann. Inst. H. Poincaré C Anal. Non Linéaire 25 (2008)] to the curved spaces. Moreover, building upon this transformation, we succeed in extending Sobolev-Lorentz embedding-classically formulated in the Euclidean setting to the broader framework of Cartan-Hadamard models and we establish an optimal Sobolev-Lorentz embedding in this geometric setting. Finally, we establish a quantitative correspondence between the Hardy deficit on the manifold and an appropriate weighted Hardy deficit in Euclidean space, showing that each controls the other.

</details>


### [86] [Existence and regularity of minimizers for a variational problem of species population density](https://arxiv.org/abs/2601.13771)
*Pu-Zhao Kow,Masato Kimura,Hiroshi Ohtsuka*

Main category: math.AP

TL;DR: The paper studies variational problems for population density models in nonhomogeneous environments, analyzing free boundary problems, establishing existence of global solutions, and examining spatial saturation patterns.


<details>
  <summary>Details</summary>
Motivation: Motivated by models of species population density in nonhomogeneous environments, the research aims to understand spatial patterns of population saturation and the mathematical structure of solutions to these variational problems.

Method: The authors first analyze local minimizers and saturated regions from a free boundary perspective. They then compare the original problem with a radially symmetric minimization problem, study its properties, and use analytic examples of radially symmetric solutions along with numerical simulations.

Result: The paper establishes the existence and structure of global solutions, illustrates theoretical results with analytic examples and numerical simulations, and provides insight into spatial saturation patterns in population models.

Conclusion: The research successfully analyzes population density variational problems, establishes solution existence and structure, but highlights an unresolved question regarding the quasiconcavity of minimizers as a direction for future work.

Abstract: We study a variational problem motivated by models of species population density in a nonhomogeneous environment. We first analyze local minimizers and the structure of the saturated region (where the population attains its maximal density) from a free boundary perspective. By comparing the original problem with a radially symmetric minimization problem and studying its properties, we then establish the existence and structure of a global solution. Analytic examples of radially symmetric solutions and numerical simulations illustrate the theoretical results and provide insight into spatial saturation patterns in population models. We further highlight an unresolved question regarding the quasiconcavity of minimizers.

</details>


### [87] [Analytic description of the moving moisture front in soils](https://arxiv.org/abs/2601.13833)
*Bettina Detmann,Chiara Gavioli,Pavel Krejčí,Yanyan Zhang*

Main category: math.AP

TL;DR: Analytical upper bounds for moisture front propagation speed in soils derived from Richards equation, with explicit criterion for gravity vs capillarity dominance.


<details>
  <summary>Details</summary>
Motivation: To rigorously analyze moisture propagation in soils, which occurs at finite speeds observable in both natural settings and laboratory tests, and to understand the competition between gravity and capillarity effects.

Method: Derivation of analytical upper bounds for moisture front propagation speed under gravity using the Richards equation with compactly supported initial data, including existence/uniqueness proofs for degenerate Richards equation on whole space.

Result: Explicit criterion describing competition between gravity and capillarity: if capillarity dominates, wet regions remain wet indefinitely; if gravity dominates, moisture travels downward with asymptotically bounded speed. Numerical simulations confirm theoretical predictions and match experimental observations.

Conclusion: The study provides rigorous analytical bounds for moisture propagation speeds, clarifies the gravity-capillarity competition mechanism in soils, and validates the theoretical framework through numerical simulations that align with experimental data.

Abstract: The fact that moisture propagates in soils at a finite speed is confirmed by natural everyday experience as well as by controlled laboratory tests. In this text, we rigorously derive analytical upper bounds for the speed of moisture front propagation under gravity for the solution to the Richards equation with compactly supported initial data. The main result is an explicit criterion describing a competition between gravity and capillarity, where the dominant effect is determined by the characteristics of the soil. If capillarity prevails, the initially wet regions remain wet for all times, while if gravity is dominant, moisture travels downward at a speed that is asymptotically bounded from below and above. As a by-product, we prove the existence and uniqueness of a solution to an initial value problem for the degenerate Richards equation on the whole space. Numerical simulations based on the proposed model confirm the theoretical predictions, with results that closely match experimental observations.

</details>


### [88] [Wiener Algebras Methods for Liouville Theorems on the Stationary Navier-Stokes System](https://arxiv.org/abs/2601.13916)
*Nicolas Lerner*

Main category: math.AP

TL;DR: The paper proves Liouville theorems for stationary Navier-Stokes equations by establishing sufficient conditions on the low-frequency part of solutions using Wiener algebra properties and classical singular integrals.


<details>
  <summary>Details</summary>
Motivation: To establish Liouville-type results (vanishing theorems) for stationary Navier-Stokes equations, which describe incompressible fluid flow, by analyzing the low-frequency behavior of solutions.

Method: Uses properties of classical singular integrals with respect to Wiener algebras to analyze the low-frequency part of solutions, establishing sufficient conditions for Liouville theorems.

Result: Proves Liouville theorems for stationary Navier-Stokes systems by providing sufficient conditions on the low-frequency component of solutions.

Conclusion: The approach using Wiener algebras and singular integrals provides effective sufficient conditions for establishing Liouville theorems in stationary Navier-Stokes theory.

Abstract: We prove some Liouville theorems for the stationary Navier-Stokes system for incompressible fluids. We provide some sufficient conditions on the low frequency part of the solution, using some properties of classical singular integrals with respect to Wiener algebras.

</details>


### [89] [On large periodic traveling wave solutions to the free boundary Stokes and Navier-Stokes equations](https://arxiv.org/abs/2601.14085)
*Seyed Abdolhamid Banihashemi,Huy Q. Nguyen*

Main category: math.AP

TL;DR: Existence and stability of large-amplitude periodic traveling waves for viscous fluid layers with free boundaries under gravity, surface tension, and external stress.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of finite-depth viscous fluid layers with free boundaries under combined effects of gravity, surface tension, and external traveling stress, which models various physical phenomena like wind-driven waves or external forcing.

Method: Analyze nonlocal normal-stress to normal-Dirichlet operators for Stokes and Navier-Stokes equations in Sobolev-regular domains. Use functional analytic techniques to prove existence via fixed-point arguments and stability via spectral analysis.

Result: For any isotropic stress tensor with periodic profile, there exists a locally unique periodic traveling wave solution (possibly large amplitude). These solutions are asymptotically stable for dynamic free boundary Stokes equations.

Conclusion: The paper establishes rigorous existence and stability results for traveling wave solutions in viscous free boundary problems, providing mathematical foundation for understanding wave phenomena in fluid layers under external forcing.

Abstract: We study the free boundary problem for a finite-depth layer of viscous incompressible fluid in arbitrary dimension, modeled by the Stokes or Navier-Stokes equations. In addition to the gravitational field acting in the bulk, the free boundary is acted upon by surface tension and an external stress tensor posited to be in traveling wave form. We prove that for any isotropic stress tensor with periodic profile, there exists a locally unique periodic traveling wave solution, which can have large amplitude. Moreover, we prove that the constructed traveling wave solutions are asymptotically stable for the dynamic free boundary Stokes equations. Our proofs rest on the analysis of the nonlocal normal-stress to normal-Dirichlet operators for the Stokes and Navier-Stokes equations in domains of Sobolev regularity.

</details>


### [90] [The nonlinear Steklov problem in outward cuspidal domains](https://arxiv.org/abs/2601.14186)
*Pier Domenico Lamberti,Alexander Ukhlov*

Main category: math.AP

TL;DR: Study of nonlinear Steklov eigenvalue problems in outward cuspidal domains, establishing variational characterization of first nontrivial eigenvalue and existence of weak solutions via compact trace embeddings.


<details>
  <summary>Details</summary>
Motivation: To extend Steklov eigenvalue theory to geometrically challenging domains with cusps, where standard Sobolev embeddings may fail, requiring weighted trace analysis.

Method: Leverage compactness of weighted trace embeddings in outward cuspidal domains to establish variational framework for nonlinear Steklov problems.

Result: Obtain variational characterization of first nontrivial eigenvalue and prove existence of corresponding weak solution in these singular domains.

Conclusion: The weighted trace approach enables rigorous treatment of Steklov eigenvalue problems in cuspidal domains, extending classical results to geometrically singular settings.

Abstract: In this article, we consider the nonlinear Steklov eigenvalue problem in outward cuspidal domains. Using the compactness of the weighted trace embedding we obtain the variational characterization of the first non-trivial eigenvalue and prove the existence of a corresponding weak solution.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [91] [Multi-Scale Negative Coupled Information Systems (MNCIS): A Unified Spectral Topology Framework for Stability in Turbulence, AI, and Biology](https://arxiv.org/abs/2601.11594)
*Pengyue Hou*

Main category: physics.comp-ph

TL;DR: The paper proposes a unified Multi-Scale Negative Coupled Information System (MNCIS) framework with Adaptive Spectral Negative Coupling (ASNC) to prevent spectral gap collapse in complex dynamical systems across hydrodynamics, AI, and biological physics.


<details>
  <summary>Details</summary>
Motivation: Complex dynamical systems often suffer from recurrent structural instability where the spectral gap collapses, leading to low-dimensional "Zero-Mode Attractors" like spectral pile-up or over-smoothing. This instability appears across diverse fields from turbulence to neural networks to biological pattern formation.

Method: Generalizes the MNCIS framework with an active topological operator called Adaptive Spectral Negative Coupling (ASNC), which functions as a state-dependent high-pass filter that penalizes entropy accumulation at spectral boundaries. Validated through three implementations: 1) 3D Navier-Stokes turbulence with ASNC as adaptive sub-grid scale model, 2) Graph Neural Networks with ASNC as parameter-free topological constraint, 3) Reaction-diffusion morphogenesis systems.

Result: 1) Stabilizes inviscid limit in turbulence while preserving Kolmogorov -5/3 inertial range without artificial hyper-viscosity. 2) Enables training of ultra-deep 64-layer GNNs without residual connections while maintaining perfectly stationary feature variance on ogbn-arxiv benchmark. 3) Stabilizes Turing patterns against diffusive washout in high-entropy regimes.

Conclusion: The MNCIS framework provides a base-independent topological condition for distinguishing viable complex systems from those collapsing into thermal equilibrium, bridging physical stability and information persistence across multiple domains.

Abstract: Complex dynamical systems frequently encounter a recurrent structural instability: the collapse of the spectral gap, driving the system toward a low-dimensional "Zero-Mode Attractor" (e.g., spectral pile-up or over-smoothing). Building upon recent global well-posedness estimates [Hou, arXiv:2601.00638], this work generalizes the Multi-Scale Negative Coupled Information System (MNCIS) framework. We postulate that global stability requires an active topological operator -- Adaptive Spectral Negative Coupling (ASNC) -- functioning as a state-dependent high-pass filter that penalizes entropy accumulation at spectral boundaries. We validate this unified framework via three implementations:(1) Hydrodynamics: In 3D Navier-Stokes turbulence ($N=256^3$), ASNC acts as a global-enstrophy adaptive sub-grid scale (SGS) model, stabilizing the inviscid limit and preserving the Kolmogorov $-5/3$ inertial range without artificial hyper-viscosity.(2) Artificial Intelligence: Addressing Over-smoothing in Graph Neural Networks (GNNs), we implement ASNC as a parameter-free topological constraint. Unlike baselines (e.g., DeepGCNs) relying on dense residual connections to bypass signal decay, our framework enables the training of ultra-deep 64-layer networks without residual connections, maintaining perfectly stationary feature variance ($σ^2 \equiv 1.0$) on the ogbn-arxiv benchmark. (3) Biological Physics: In reaction-diffusion morphogenesis, it stabilizes Turing patterns against diffusive washout in high-entropy regimes. Our results suggest that the MNCIS framework provides a base-independent topological condition for distinguishing viable complex systems from those collapsing into thermal equilibrium, bridging physical stability and information persistence.

</details>


### [92] [Level set-based topology optimization of micropolar solids under thermo-mechanical loading](https://arxiv.org/abs/2601.11607)
*Mayank Shekhar,Ayyappan Unnikrishna Pillai,Subhayan De,Mohammad Masiur Rahaman*

Main category: physics.comp-ph

TL;DR: Novel level set-based topology optimization for micropolar solids under thermo-mechanical loading that captures size effects through micropolar theory.


<details>
  <summary>Details</summary>
Motivation: Need for accurate topology optimization methods that account for size effects in materials with microstructural features under combined thermal and mechanical loading conditions.

Method: Level set-based topology optimization incorporating micropolar theory to include microstructural length-scale information, implemented using open-source Julia libraries Gridap.jl and GridapTopOpt.jl.

Result: Demonstrated effectiveness through 2D benchmark problems, showing significant influence of microstructures (via micropolar parameters) and temperature on optimized topologies.

Conclusion: The proposed thermo-mechanical micropolar formulation is necessary for materials with pronounced non-local effects, providing accurate topology optimization for size-dependent solids.

Abstract: We propose a novel level set-based topology optimization for micropolar solids subjected to thermo-mechanical loading. To capture the size effects, we have incorporated the microstructural length-scale information into the level set-based topology optimization method by adopting a micropolar theory. The proposed non-local topology optimization method can provide accurate topology optimization for size-dependent solids under thermo-mechanical loading. We have demonstrated the effectiveness of the proposed method through a few representative two-dimensional benchmark problems. The numerical results reveal the substantial influence of underlying micro-structures, incorporated in the model through micropolar parameters, and temperature on topology optimization, highlighting the necessity of the proposed thermo-mechanical micropolar formulation for materials with pronounced non-local effects. For the numerical implementation of the proposed model, we have used open-source finite element libraries, \texttt{Gridap.jl}, and \texttt{GridapTopOpt.jl}, available in Julia, to ensure transparency and reproducibility of the reported computational results.

</details>


### [93] [Padé Approximation and Partition Function Zeros](https://arxiv.org/abs/2601.12018)
*R. G. M. Rodrigues*

Main category: physics.comp-ph

TL;DR: Padé approximation reduces computational cost for Fisher zeros analysis in phase transitions, enabling reliable study of anisotropic Heisenberg (XY) model with fewer polynomial terms.


<details>
  <summary>Details</summary>
Motivation: Fisher zeros are theoretically important for phase transitions but require density of states knowledge, limiting practical use. Alternative EPD and MGF methods have convergence issues in the 2D anisotropic Heisenberg (XY) model.

Method: Introduce Padé approximation to systematically reduce number of zeros needed in Fisher, EPD, and MGF formulations without accuracy loss. Combines Fisher zeros (no convergence algorithm needed) with Padé approximation for reliable XY model analysis.

Result: Applications to 2D Ising and XY models show substantial reductions in polynomial degree and computation time while preserving accurate critical temperature estimates.

Conclusion: Padé approximation enables efficient and reliable Fisher zeros analysis for phase transitions, overcoming previous computational limitations for challenging models like the XY model.

Abstract: Fisher zeros play a central role in the theoretical understanding of phase transitions. However, their computation requires knowledge of the density of states, which limits their practical applicability. Alternative approaches based on the Energy Probability Distribution (EPD) and Moment Generating Function (MGF) alleviate the computational cost but suffer from convergence issues in the two-dimensional \textbf{anisotropic Heisenberg} model (XY model). In this work, we introduce a Padé approximation to systematically reduces the number of zeros required in the Fisher, EPD, and MGF formulations without loss of accuracy. Moreover, since the Fisher zeros formulation does not rely on a convergence algorithm, their combination with a Padé approximation enables a reliable analysis of the XY model while significantly reducing computational cost. Applications to the two-dimensional Ising and XY models demonstrate substantial reductions in polynomial degree and computation time while preserving accurate estimates of the critical temperature.

</details>


### [94] [Efficient O(N^1.5) Electronic Structure Computation of Million-Atom Systems](https://arxiv.org/abs/2601.12098)
*Zichong Zhang,Shuze Zhu*

Main category: physics.comp-ph

TL;DR: A new tight-binding framework reduces computational complexity from O(N³) to O(N¹·⁵) for quantum materials, enabling large-scale simulations of moiré superlattices including magic-angle twisted bilayer graphene with millions of atoms.


<details>
  <summary>Details</summary>
Motivation: Conventional electronic structure methods have O(N³) scaling that limits exploration of quantum phenomena in complex materials like moiré superlattices, preventing study of large systems and ultra-low twist-angle regimes.

Method: Developed a high-performance tight-binding framework that transforms the Hamiltonian into real symmetric form and combines Sylvester's inertia law with LDL decomposition to achieve O(N¹·⁵) scaling.

Result: The method solves magic-angle twisted bilayer graphene in minutes on a laptop, scales to 1.5 million atoms within days on a workstation, and reveals robust flat bands persisting down to 0.09 degree twist angles with strain relaxation.

Conclusion: This framework bridges DFT accuracy with large-scale quantum simulation, enabling systematic data-driven exploration of mesoscale quantum materials previously inaccessible to computational methods.

Abstract: The exploration of quantum phenomena in complex materials such as moiré superlattices is limited by the O(N^3) scaling of conventional electronic structure methods. Here we introduce a high-performance tight-binding framework that reduces the complexity to O(N^1.5) by transforming the Hamiltonian into a real symmetric form and combining Sylvester's inertia law with LDL decomposition. This approach enables efficient band structure calculations for large systems: solving magic angle twisted bilayer graphene in minutes on a laptop and scaling to 1.5 million atoms within days on a workstation. We apply it to the previously inaccessible ultra-low twist-angle regime (less than 0.16 degree) with mechanical strain relaxation and find robust flat bands persisting down to 0.09 degree. Our framework bridges density functional theory accuracy with large-scale quantum simulation, opening a route to systematic data-driven exploration of mesoscale quantum materials.

</details>


### [95] [Hitchhiker's guide to second-generation Car-Parrinello ab-initio molecular dynamics](https://arxiv.org/abs/2601.12191)
*Thomas D. Kühne*

Main category: physics.comp-ph

TL;DR: Practical guide for implementing a Car-Parrinello-like Born-Oppenheimer molecular dynamics method in CP2K/Quickstep code, demonstrated on liquid water simulations.


<details>
  <summary>Details</summary>
Motivation: To provide practical implementation guidance for a previously proposed Car-Parrinello-like Born-Oppenheimer molecular dynamics approach, making it accessible for actual computational applications.

Method: Implementation guide for the Car-Parrinello-like Born-Oppenheimer molecular dynamics method within the CP2K/Quickstep code framework, focusing on practical computational details rather than theoretical foundations.

Result: Demonstration of the method's application to liquid water simulations at ambient conditions, showing practical feasibility and implementation workflow.

Conclusion: The paper provides a practical implementation guide that enables researchers to apply the Car-Parrinello-like Born-Oppenheimer molecular dynamics method using CP2K/Quickstep code for realistic simulations like liquid water.

Abstract: In a recent letter [T. D. Kühne, M. Krack, F. Mohamed and M. Parrinello, Phys. Rev. Lett. 98, 066401 (2007)], we outlined a new Car-Parrinello-like approach to Born-Oppenheimer molecular dynamics. Here, we provide a guide to performing actual calculations using our method and demonstrate this on liquid water at ambient conditions. We do not go into methodological details beyond those necessary for applying this approach, but focus on practical details pertinent to our particular implementation within the CP2K/Quickstep code [T. D. Kühne et al., J. Chem. Phys. 152, 194103 (2020)].

</details>


### [96] [Chaotic Dynamics and Bifurcation Analysis of the Hindmarsh-Rose Neuron Model with Blue-Sky Catastrophe under Magnetic Field Influence](https://arxiv.org/abs/2601.13267)
*Ram Pravesh Yadav,Hirdesh K. Pharasi,R. K. Brojen Singh,Anirban Chakraborti*

Main category: physics.comp-ph

TL;DR: Magnetic field feedback transforms Hindmarsh-Rose neuron dynamics, creating nonmonotonic transitions from regular spiking to chaotic bursting to structured irregular patterns based on coupling strength.


<details>
  <summary>Details</summary>
Motivation: To understand how electromagnetic feedback affects neuronal dynamics, particularly in systems exhibiting blue-sky catastrophes, and explore magnetic coupling as a mechanism for controlling instability and chaos in neuronal systems.

Method: Introduce magnetic flux variable that couples nonlinearly to membrane potential in Hindmarsh-Rose neuron model; analyze using interspike-interval bifurcation analysis, compute largest Lyapunov exponent via Wolf algorithm, and support with Poincaré sections and time-series analysis.

Result: Magnetic coupling strength shows nonmonotonic effects: weak coupling preserves regular spiking/bursting, intermediate coupling promotes chaotic bursting, strong coupling yields structured irregular dynamics; electromagnetic feedback robustly reshapes firing patterns and bifurcation structure.

Conclusion: Electromagnetic feedback serves as a tunable mechanism for controlling instability and chaos in slow-fast neuronal systems, with magnetic coupling strength determining distinct dynamical regimes.

Abstract: We investigate the impact of magnetic-field-induced feedback on the dynamics of a Hindmarsh-Rose neuron model exhibiting a blue-sky catastrophe. By introducing a magnetic flux variable that couples nonlinearly to the membrane potential, we demonstrate that electromagnetic effects profoundly reshape neuronal firing patterns and bifurcation structure. Interspike-interval bifurcation analysis reveals a nonmonotonic dependence on the magnetic coupling strength, with weak coupling preserving regular spiking and bursting, intermediate coupling promoting chaotic bursting, and strong coupling yielding structured irregular dynamics. These transitions are quantitatively characterized using the largest Lyapunov exponent computed via the Wolf algorithm and supported by Poincaré sections and time-series analysis. Our results establish electromagnetic feedback as a robust and tunable mechanism for controlling instability and chaos in slow-fast neuronal systems.

</details>


### [97] [Refined Gradient-Based Temperature Optimization for the Replica-Exchange Monte-Carlo Method](https://arxiv.org/abs/2601.13542)
*Tatsuya Miyata,Shunta Arai,Satoshi Takabe*

Main category: physics.comp-ph

TL;DR: Proposes a refined online temperature selection method for replica-exchange Monte-Carlo that enforces physical constraints via reparameterization, optimizes temperatures via gradient descent using acceptance rate variance as loss function, and outperforms previous methods without hyperparameter tuning issues.


<details>
  <summary>Details</summary>
Motivation: The replica-exchange Monte-Carlo (RXMC) method struggles with temperature selection, which critically affects sampling efficiency for multi-modal distributions. Existing methods don't properly enforce physical constraints like monotonic temperature ordering, and recent policy gradient approaches require careful hyperparameter tuning.

Method: Extends gradient-based optimization framework with reparameterization to strictly enforce monotonic ordering of inverse temperatures. Defines variance of acceptance rates between adjacent replicas as loss function, estimates gradient using differential information from sampling process, and optimizes temperatures via gradient descent.

Result: Method successfully achieves uniform acceptance rates and reduces round-trip times across temperature space in benchmark spin systems (2D ferromagnetic Ising model, 2D ferromagnetic XY model, 3D Edwards-Anderson model). Outperforms policy gradient methods by avoiding hyperparameter tuning issues while preventing constraint violations.

Conclusion: The proposed refined online temperature selection method effectively optimizes RXMC temperature selection while enforcing physical constraints, offering practical advantages over existing approaches without the hyperparameter tuning challenges of recent policy gradient methods.

Abstract: The replica-exchange Monte-Carlo (RXMC) method is a powerful Markov-chain Monte-Carlo algorithm for sampling from multi-modal distributions, which are challenging for conventional methods. The sampling efficiency of the RXMC method depends highly on the selection of the temperatures, and finding optimal temperatures remains a challenge. In this study, we propose a refined online temperature selection method by extending the gradient-based optimization framework proposed previously. Building upon the existing temperature update approach, we introduce a reparameterization technique to strictly enforce physical constraints, such as the monotonic ordering of inverse temperatures, which were not explicitly addressed in the original formulation. The proposed method defines the variance of acceptance rates between adjacent replicas as a loss function, estimates its gradient using differential information from the sampling process, and optimizes the temperatures via gradient descent. We demonstrate the effectiveness of our method through experiments on benchmark spin systems, including the two-dimensional ferromagnetic Ising model, the two-dimensional ferromagnetic XY model, and the three-dimensional Edwards-Anderson model. Our results show that the method successfully achieves uniform acceptance rates and reduces round-trip times across the temperature space. Furthermore, our proposed method offers a significant advantage over recently proposed policy gradient method that require careful hyperparameter tuning, while simultaneously preventing the constraint violations that destabilize optimization.

</details>


### [98] [Efficient local classification of parity-based material topology](https://arxiv.org/abs/2601.13598)
*Stephan Wong,Ichitaro Yamazaki,Chris Siefert,Iain Duff,Terry A. Loring,Alexander Cerjan*

Main category: physics.comp-ph

TL;DR: Real-space framework for classifying parity-based Z₂ topology in aperiodic systems using spectral localizer and scalable sparse Pfaffian sign computation.


<details>
  <summary>Details</summary>
Motivation: Topological classification of aperiodic materials is challenging because conventional momentum-space approaches require translational symmetry, which is absent in quasicrystals and other non-periodic systems.

Method: Uses spectral localizer framework with direct computation of Pfaffian sign for large sparse skew-symmetric matrices. Develops scalable sparse factorization algorithm for reliable sign determination without requiring translational symmetry, spectral gaps, or gapped auxiliary operators.

Result: Successfully identifies quantum spin Hall effect in quasicrystalline class AII systems (including gapless heterostructures) and diagnoses fragile topology in large C₂T-symmetric photonic quasicrystals.

Conclusion: Spectral localizer combined with efficient sparse numerical methods provides unified, robust tool for diagnosing parity-based topological phases in aperiodic electronic, photonic, and acoustic materials where conventional band theory fails.

Abstract: Although the classification of crystalline materials can be generally handled by momentum-space-based approaches, topological classification of aperiodic materials remains an outstanding challenge, as the absence of translational symmetry renders such conventional approaches inapplicable. Here, we present a numerically efficient real-space framework for classifying parity-based $\mathbb{Z}_2$ topology in aperiodic systems based on the spectral localizer framework and the direct computation of the sign of a Pfaffian associated with a large sparse skew-symmetric matrix. Unlike projector-based or momentum-space-based approaches, our method does not rely on translational symmetry, spectral gaps in the Hamiltonian's bulk, or gapped auxiliary operators such as spin projections, and instead provides a local, energy-resolved topological invariant accompanied by an intrinsic measure of topological protection. A central contribution of this work is the development of a scalable sparse factorization algorithm that enables the reliable determination of the Pfaffian's sign for large sparse matrices, making the approach practical to realistic physical materials. We apply this framework to identify the quantum spin Hall effect in quasicrystalline class AII systems, including gapless heterostructures, and to diagnose fragile topology in a large $C_2 \mathcal{T}$-symmetric photonic quasicrystal. Overall, our results demonstrate that the spectral localizer, combined with efficient sparse numerical methods, provides a unified and robust tool for diagnosing parity-based topological phases in aperiodic electronic, photonic, and acoustic materials where conventional band-theoretic indexes are inapplicable.

</details>


### [99] [An efficient treatment of heat-flux boundary conditions in GSIS for rarefied gas flows](https://arxiv.org/abs/2601.13870)
*Yanbing Zhang,Ruifeng Yuan,Liyan Luo,Lei Wu*

Main category: physics.comp-ph

TL;DR: GSIS method improves efficiency of heat-flux boundary conditions in rarefied gas simulations by using Maxwellian distribution to estimate incident flux and ensuring boundary consistency between kinetic and macroscopic solvers.


<details>
  <summary>Details</summary>
Motivation: Heat-flux boundary conditions are computationally expensive in rarefied gas flow simulations because wall-reflected gas temperature and density must be determined dynamically during computation, creating convergence bottlenecks.

Method: Proposed method within GSIS framework: Boltzmann kinetic equation solved deterministically in outer loop, macroscopic synthetic equations solved in inner loop. For macroscopic boundary flux at each inner iteration, incident increment estimated using Maxwellian distribution, and reflected contribution obtained from boundary conditions consistent with kinetic solver.

Result: Method retains fast-converging and asymptotic-preserving properties of GSIS while significantly reducing iterations needed to determine wall-reflected gas parameters. Numerical simulations show good agreement with DSMC method and substantial efficiency gains over conventional iterative schemes.

Conclusion: The proposed boundary treatment method effectively addresses convergence bottlenecks in heat-flux boundary conditions for rarefied gas simulations, maintaining accuracy while achieving significant computational efficiency improvements.

Abstract: Heat-flux boundary conditions are challenging to implement efficiently in rarefied gas flow simulations because the wall-reflected gas temperature and density must be determined dynamically during the computation. This paper aims to tackle this problem within the general synthetic iterative scheme (GSIS), where the Boltzmann kinetic equation is solved deterministically in an outer loop and macroscopic synthetic equations are solved in an inner loop. To avoid kinetic-macroscopic boundary-flux mismatch and the resulting convergence bottlenecks, for the macroscopic boundary flux at every inner iteration, the incident increment is estimated using a Maxwellian distribution, and then the reflected contribution is obtained by boundary conditions consistent with those in the kinetic solver. In addition to retaining the fast-converging and asymptotic-preserving properties of GSIS, the proposed method significantly reduces the iterations required to determine the wall-reflected gas parameters. Numerical simulations of rarefied gas flows in and around a 3D nozzle, a 2D adiabatic cylinder, and a 2D annular heat-transfer configuration show good agreement with the direct simulation Monte Carlo method, while achieving substantial efficiency gains over conventional iterative schemes.

</details>


### [100] [Gradient-based optimization of exact stochastic kinetic models](https://arxiv.org/abs/2601.14183)
*Francesco Mottes,Qian-Ze Zhu,Michael P. Brenner*

Main category: physics.comp-ph

TL;DR: Differentiable stochastic simulation using straight-through Gumbel-Softmax enables gradient-based optimization for parameter inference and inverse design in stochastic kinetic systems.


<details>
  <summary>Details</summary>
Motivation: Stochastic kinetic models are essential for biological, chemical, and physical systems with discrete events and small populations, but parameter inference and inverse design are challenging due to non-differentiable discrete reaction events in Stochastic Simulation Algorithm trajectories.

Method: Straight-through Gumbel-Softmax estimation that maintains exact stochastic simulations in forward pass while approximating gradients through continuous relaxation in backward pass only.

Result: Robust parameter inference in stochastic gene expression, accurately recovering kinetic rates of telegraph promoter models from both moment statistics and full steady-state distributions across diverse parameter regimes. Applicable to inverse design in stochastic thermodynamics, characterizing Pareto-optimal trade-offs between non-equilibrium currents and entropy production.

Conclusion: The ability to efficiently differentiate through exact stochastic simulations provides foundation for systematic inference and rational design across domains governed by continuous-time Markov dynamics.

Abstract: Stochastic kinetic models describe systems across biology, chemistry, and physics where discrete events and small populations render deterministic approximations inadequate. Parameter inference and inverse design in these systems require optimizing over trajectories generated by the Stochastic Simulation Algorithm, but the discrete reaction events involved are inherently non-differentiable. We present an approach based on straight-through Gumbel-Softmax estimation that maintains exact stochastic simulations in the forward pass while approximating gradients through a continuous relaxation applied only in the backward pass. We demonstrate robust performance on parameter inference in stochastic gene expression, accurately recovering kinetic rates of telegraph promoter models from both moment statistics and full steady-state distributions across diverse and challenging parameter regimes. We further demonstrate the method's applicability to inverse design problems in stochastic thermodynamics, characterizing Pareto-optimal trade-offs between non-equilibrium currents and entropy production. The ability to efficiently differentiate through exact stochastic simulations provides a foundation for systematic inference and rational design across the many domains governed by continuous-time Markov dynamics.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [101] [A Novel Numerical Algorithms Optimization Method with Machine Learning Frameworks: Application on Real-time Plasmas Equilibrium Reconstruction in EXL-50U Spherical Torus](https://arxiv.org/abs/2601.12378)
*G. H. Zheng,S. F. Liu,X. Gu,Y. P. Zhang,J. Li,Y. Liu,X. C. Lun,L. Xing,J. G. Chen,Z. Y. Chen,Y. Yu,D. Guo,Z. Y. Yang,H. S. Xie,X. M. Song,Y. J. Shi,EXL-50U Team*

Main category: physics.plasm-ph

TL;DR: PTEFIT: A real-time plasma reconstruction algorithm using PyTorch/TensorRT optimization for tokamak fusion, achieving 0.268ms inference time at 129×129 resolution and enabling feedback control on EXL-50U.


<details>
  <summary>Details</summary>
Motivation: To develop a real-time plasma reconstruction algorithm for tokamak fusion that combines performance, flexibility, and usability by leveraging modern machine learning frameworks' advantages for numerical algorithm optimization.

Method: Proposes a novel optimization method using PyTorch and TensorRT frameworks, exploiting their modularity, low development threshold, and automatic tuning capabilities to create the PTEFIT algorithm for plasma reconstruction.

Result: Successfully deployed on EXL-50U spherical tokamak with 0.268ms average inference time per time slice at 129×129 resolution, enabling real-time feedback control of plasma maximum radial position and isoflux control.

Conclusion: The design philosophy has significant potential to accelerate GPU parallel computing development and optimization, and can be extended to other numerical algorithms beyond plasma reconstruction.

Abstract: This work proposes for the first time a novel optimization method for numerical algorithms, which takes advantages of machine learning frameworks PyTorch and TensorRT, leveraging their modularity, low development threshold, and automatic tuning characteristics to achieve a real-time plasmas reconstruction algorithm called PTEFIT as an application in tokamak-based controlled fusion that combines performance, flexibility, and usability. The algorithm has been deployed and routinely operated on the EXL-50U spherical tokamak, with an average inference time of only 0.268ms per time slice at $129\times 129$ resolution, and has successfully driven feedback control of the maximum radial position of plasmas and isoflux control. We believe that its design philosophy has sufficient potential to accelerate development and optimization in GPU parallel computing, and is expected to be extended to other numerical algorithms.

</details>


### [102] [Plasmoid formation via competing lower-hybrid drift and Kelvin-Helmholtz instabilities: A hybrid kinetic-gyrokinetic simulation study](https://arxiv.org/abs/2601.12466)
*S. Thatikonda,F. N. De Oliveira-Lopes,A. Mustonen,K. Pommois,D. Told,F. Jenko*

Main category: physics.plasm-ph

TL;DR: LHDI can suppress KHI in thin current sheets with strong density gradients, preventing classical KH vortices and instead generating plasmoids through inverse cascade from kinetic to fluid scales.


<details>
  <summary>Details</summary>
Motivation: To understand how microturbulence from lower-hybrid drift instability (LHDI) interacts with and potentially regulates large-scale Kelvin-Helmholtz instability (KHI) in thin current sheets, particularly in space plasma boundary layers like the solar wind-magnetosphere interface.

Method: Used hybrid kinetic-gyrokinetic model-based Super Simple Vlasov (ssV) code with fully kinetic ions and drift-kinetic electrons to simulate Harris-type current sheets and velocity shear layers with strong cross-field density gradients.

Result: In thin current sheets with strong density gradients, LHDI develops rapidly at sheet edges, nonlinearly merges into larger-scale magnetic islands before KHI can evolve, and suppresses classical KH vortices. In thicker sheets or weaker gradients, KHI dominates and produces expected rolled-up vortices.

Conclusion: LHDI-induced turbulence acts as both seed and regulator of plasmoid-generating instabilities, mediating cross-scale energy transfer. Microturbulence can govern large-scale magnetic topology during collisionless reconnection in space plasma boundary layers.

Abstract: We investigate the nonlinear formation of plasmoids in 2D low-beta current sheets through the interplay between the Kelvin-Helmholtz instability (KHI) and the lower-hybrid drift instability (LHDI). Using a hybrid kinetic-gyrokinetic model-based Super Simple Vlasov (ssV) code with fully kinetic ions and drift-kinetic electrons, we simulate Harris-type current sheets and velocity shear layers with strong cross-field density gradients. Our central hypothesis is that steep density gradients drive LHDI, which can grow faster than KHI and initiate an inverse cascade from kinetic to fluid scales, potentially suppressing KHI. Our simulations confirm that, in thin current sheets, LHDI develops rapidly at the sheet edges and nonlinearly merges into larger-scale magnetic islands before KHI can evolve. These LHDI-driven structures distort the velocity shear and suppress classical KH vortices. In contrast, for thicker current sheets or weaker density gradients, KHI dominates and produces the expected rolled-up vortices and associated plasmoids. These findings demonstrate that LHDI-induced turbulence can act as both a seed and a regulator of plasmoid-generating instabilities, mediating cross-scale energy transfer. This mechanism is relevant to thin boundary layers in space plasmas, such as the solar wind magnetosphere interface, and suggests that microturbulence can govern large-scale magnetic topology during collisionless reconnection.

</details>


### [103] [SPARC Tokamak Error Field Expectations and Physics-Based Correction Coil Design](https://arxiv.org/abs/2601.12469)
*N. C. Logan,C. E. Myers,R. Sweeney,C. Paz-Soldan,M. Pharr,N. Leuthold,M. Nickerson,J. Halpern,I. Stewart*

Main category: physics.plasm-ph

TL;DR: Non-axisymmetric magnetic coils designed for SPARC tokamak to correct error fields and suppress edge localized modes using plasma response optimization.


<details>
  <summary>Details</summary>
Motivation: To enable stable operation of SPARC (a compact high-field tokamak) in new high-field regimes without error field induced locked modes, which can disrupt plasma confinement.

Method: Utilized GPEC's multi-modal plasma response representation to optimize geometric coupling between 3D coil arrays and desired plasma responses. Designed coils to couple to plasma-amplified kink driving core resonances, balanced construction tolerances against correction requirements.

Result: Developed physics-driven coil designs that provide confidence for operating SPARC without error field induced locked modes, with maximum allowable error fields projected using empirical scaling consistent with MHD modeling.

Conclusion: The designed non-axisymmetric magnetic field coils enable efficient error field correction and edge localized mode suppression in SPARC, supporting stable operation in high-field tokamak regimes.

Abstract: Non-axisymmetric magnetic field coils have been designed to provide efficient error field correction and suppress edge localized modes in SPARC - a compact high-field tokamak that is presently under construction at Commonwealth Fusion Systems. These designs utilize the Generalized Perturbed Equilibrium Code's (GPEC's) representation of the multi-modal, non-axisymmetric plasma response to optimize the geometric coupling between 3D coil arrays and the desired core or edge plasma response. Error field correction coils are designed to couple to the plasma-amplified kink that dominates the drive of core resonances. The maximum allowable error field is projected to SPARC using an empirical scaling that is consistent with linear and nonlinear MHD modeling expectations. Asymmetric construction and assembly tolerances are then balanced against the corresponding kA-turns needed for correction to levels below the allowable limit. These physics-driven coil designs provide confidence in our ability to operate SPARC in new high field tokamak regimes without error field induced locked modes.

</details>


### [104] [Dimensional Analysis Approach to Experiments in Z pinch Devices](https://arxiv.org/abs/2601.12692)
*Miguel Cárdenas*

Main category: physics.plasm-ph

TL;DR: The paper proposes that Z-pinch discharges can be characterized by three dimensionless parameters forming a 3D surface containing all macroscopic physical information, and outlines a method for estimating plasma temperature.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive framework for understanding Z-pinch discharge behavior through dimensionless parameters that capture all macroscopic physical information, addressing practical challenges in implementing this approach.

Method: The authors propose using three dimensionless parameters arranged as a 3D surface to characterize Z-pinch discharges. They analyze practical implementation challenges and then outline a specific method for estimating plasma temperature in these discharges.

Result: The paper establishes that Z-pinch discharge behavior can be completely described by three dimensionless parameters forming a 3D surface, and provides a practical approach for plasma temperature estimation given implementation challenges.

Conclusion: Z-pinch discharges can be fully characterized by three dimensionless parameters forming a comprehensive 3D surface, and despite practical implementation difficulties, a feasible method for plasma temperature estimation can be developed.

Abstract: The physical behavior of discharges in Z pinch devices can be completely deciphered in terms of only three dimensionless parameters. These parameters can be arranged in a way that draw a surface in 3D space. This surface compiles all the accessible information on the macroscopic physical behavior of each possible Z pinch discharge. We analyze the practical problems the drawing of this surface encounters and in view of the situation, we devote the remainder of the article to outline a feasible method for estimating the plasma temperature in Z pinch discharges.

</details>


### [105] [Oxygen atom density and kinetics in intermediate-pressure radiofrequency capacitively-coupled plasmas in pure O2](https://arxiv.org/abs/2601.13067)
*Shu Zhang,Andrey Volynets,Garrett A. Curley,Jean-Paul Booth*

Main category: physics.plasm-ph

TL;DR: Study of oxygen RF plasmas using laser spectroscopy to measure O atom densities and temperatures, revealing complex pressure-dependent dissociation behavior and recombination mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand oxygen atom production and loss mechanisms in RF capacitively coupled plasmas, particularly how pressure and RF power affect dissociation and recombination processes.

Method: Used single mode laser cavity ringdown spectroscopy at 630 nm to measure absolute O atom densities and translational temperatures. Conducted time-resolved measurements in afterglow of pulse-modulated plasmas to probe recombination processes.

Result: At high pressures (≥267 Pa), O atom mole fraction increases with RF power and decreases with pressure (max 15%). At lower pressures (67-133 Pa), dissociation shows distinct maxima with power before decreasing. Surface recombination dominates at low pressures, enhanced by ion bombardment. Time-resolved data revealed O⁻ ion density and ozone formation in afterglow.

Conclusion: Pressure-dependent dissociation behavior explained by different recombination mechanisms: surface recombination (ion-enhanced) dominates at low pressures, while gas phase mechanisms dominate at high pressures. High power at low pressures suggests transition to plasma mode with fewer high-energy electrons.

Abstract: We have studied radiofrequency capacitively coupled plasmas in pure O2 using single mode laser cavity ringdown spectroscopy of oxygen atoms at 630 nm. The absolute atom densities and translational temperatures were determined over a range of pressures and RF power . At pressures of 267 Pa and above, the O atom mole fraction increases with RF power and decreases with pressure, reaching a maximum of 15 percent. However, at 133 and 67 Pa it passes through a distinct maximum with power before decreasing significantly. The atom recombination processes are probed by time resolved measurements in the afterglow of pulse modulated plasmas. At 133 and 67 Pa the atom loss is dominated by surface recombination, and we see clear evidence that this rate is increased by energetic ion bombardment, in agreement with a study from Bill Graham group. This effect partially explains the observed decrease in dissociation at high RF power. The time-resolved results also allow the O negative ion density to be determined and indicate the creation of ozone in the afterglow. At 133 Pa, the trends with RF power of the O2 dissociation, O negative ion density and gas temperature suggest a transition at high power to a plasma mode with fewer high energy electrons. At higher pressures gas phase recombination mechanisms become dominant, however gas convection driven by gas cooling in the afterglow makes it complex to analyse the time-resolved data.

</details>


### [106] [Electromagnetic ghosts in pair plasmas](https://arxiv.org/abs/2601.13175)
*Maxim Lyutikov*

Main category: physics.plasm-ph

TL;DR: Weakly nonlinear counter-propagating EM pulses in pair plasma create long-lasting localized electromagnetic structures called "ghosts" through density fluctuations that trap waves, similar to Anderson localization.


<details>
  <summary>Details</summary>
Motivation: To understand the formation and persistence of localized electromagnetic structures in pair plasmas from colliding EM pulses, and how guide magnetic fields affect this process.

Method: Analysis of weakly nonlinear (a₀ ≪ 1) counter-propagating EM pulses in pair plasma, examining wave trapping by density fluctuations created through beating between pulses, comparing to random plasma density grating and Anderson-like wave localization.

Result: Collisions create long-surviving electromagnetic "ghosts" - waves trapped by random large density fluctuations from pulse beating. Structures persist for mesoscale times with slow EM energy leakage through high-density walls. Large guide magnetic fields (ω_B ≥ few ω) suppress ghost formation.

Conclusion: Counter-propagating EM pulses in pair plasma can generate persistent localized electromagnetic structures through density fluctuation trapping, but strong guide magnetic fields inhibit this ghost formation process.

Abstract: Collisions of two weakly nonlinear, $a_0 \ll 1$, counter-propagating EM pulses in pair plasma leave behind a long-surviving collection of localized waves, {\it an electromagnetic ghost}. Waves are trapped (localized) by the random large density fluctuations created by the beat between the pulses. The process is similar to random plasma density grating and/or Anderson-like wave localization. Structures survive for long, mesoscale times, while the EM energy slowly bleeds through high density walls of the density trap. Large guide magnetic field, $ω_B \geq $ few $ω$, suppresses the formation of the ghosts.

</details>


### [107] [Learning time-dependent and integro-differential collision operators from plasma phase space data using differentiable simulators](https://arxiv.org/abs/2601.13377)
*Diogo D. Carvalho,Luis O. Silva,E. P. Alves*

Main category: physics.plasm-ph

TL;DR: Differentiable kinetic simulators learn time-varying collision operators from PIC simulation data, outperforming particle track statistics.


<details>
  <summary>Details</summary>
Motivation: Modeling collisional and stochastic wave-particle dynamics in far-from-equilibrium plasmas is challenging due to complex, evolving stochastic processes.

Method: Extend differentiable kinetic simulators with plasma diagnostics to learn time-varying collision operators, and introduce integro-differentiable operator formulation to probe relevant terms.

Result: Both approaches recover operators that accurately reproduce plasma phase space dynamics and are more accurate than particle track statistics estimates.

Conclusion: Differentiable simulators show potential for inferring collision operators when no closed-form solutions exist or deviations from theory are expected.

Abstract: Collisional and stochastic wave-particle dynamics in plasmas far from equilibrium are complex, temporally evolving, stochastic processes which are challenging to model. In this work, we extend previous methods coupling differentiable kinetic simulators and plasma phase space diagnostics to learn collision operators that account for time-varying background distributions. We also introduce a more general integro-differentiable operator formulation to probe relevant terms in the collision operator. To validate the proposed methodology we use data generated by self-consistent electromagnetic Particle-in-Cell simulations. We show that both approaches recover operators that can accurately reproduce the plasma phase space dynamics while being more accurate than estimates based on particle track statistics. These results further demonstrate the potential of using differentiable simulators to infer collision operators for scenarios where no closed form solution exists or deviations from existing theory are expected.

</details>


### [108] [Quasi-linear approach of bi-Kappa distributed electrons with dynamic $κ$ parameter. EMEC instability](https://arxiv.org/abs/2601.13888)
*Pablo S Moya,Roberto E Navarro,Marian Lazar,Peter H Yoon,Rodrigo A López,Stefaan Poedts*

Main category: physics.plasm-ph

TL;DR: This paper develops an improved quasi-linear theory that self-consistently models the temporal evolution of the kappa parameter during plasma instabilities, going beyond previous approaches that kept kappa constant.


<details>
  <summary>Details</summary>
Motivation: Previous quasi-linear theories for plasmas with Kappa-type velocity distributions treated the kappa parameter as constant during plasma dynamics, which is unrealistic since the suprathermal particle population should evolve along with the thermal core. The authors aim to develop a more realistic model that captures the coupled evolution of both components.

Method: The authors propose a new quasi-linear modeling approach that couples the equation for kurtosis (fourth-order moment) with temporal variations of temperature components. This allows the kappa parameter to self-consistently vary during plasma dynamics, relaxing the constraint that low-energy core electrons and suprathermal tails evolve independently. The case study focuses on electron-cyclotron (EMEC) instability driven by anisotropic bi-Kappa electrons.

Result: The model reveals that kappa typically decreases during instability saturation, indicating suprathermalization where wave fluctuations energize suprathermal electrons. VDs evolve toward quasi-Maxwellian shapes (kappa increases) only in low-beta regimes with initial kappa > 5. Instability-driven relaxation only partially resolves temperature anisotropy, as waves generally further energize suprathermal electrons.

Conclusion: The new quasi-linear theory provides a more realistic description of plasma dynamics with Kappa distributions by self-consistently modeling kappa evolution. The approach reveals that suprathermalization (kappa decrease) predominates over thermalization, with important implications for understanding wave-particle interactions and energy transfer in space and laboratory plasmas.

Abstract: In recent years, significant progress has been made in the velocity-moment-based quasi-linear (QL) theory of waves and instabilities in plasmas with nonequilibrium velocity distributions (VDs) of the Kappa (or $κ$) type. However, the temporal variation of the parameter $κ$, which quantifies the presence of suprathermal particles, is not fully captured by such a QL analysis, and typically $κ$ remains constant during plasma dynamics. We propose a new QL modeling that goes beyond the limits of a previous approach, realistically assuming that the quasithermal core cannot evolve independently of energetic suprathermals. The case study is done on the electron-cyclotron (EMEC) instability generated by anisotropic bi-Kappa electrons with $A=T_\perp/T_\parallel > 1$ ($\parallel, \perp$ denoting directions with respect to the background magnetic field). The parameter $κ$ self-consistently varies through the QL equation of kurtosis (fourth-order moment) coupled with temporal variations of the temperature components, relaxing the constraint on the independence of the low-energy (core) electrons and suprathermal high-energy tails of VDs. The results refine and extend previous approaches. A clear distinction is made between regimes that lead to a decrease or an increase in the $κ$ parameter with saturation of the instability. What predominates is a decrease in $κ$, i.e., an excess of suprathermalization, which energizes suprathermal electrons due to self-generated wave fluctuations. Additionally, we found that VDs can evolve toward a quasi-Maxwellian shape (as $κ$ increases) primarily in regimes with low beta and initial kappa values greater than five. Instability-driven relaxation only partially resolves temperature anisotropy in bi-Kappa electron VDs, as wave fluctuations generally act to further energize suprathermal electrons.

</details>


### [109] [Reduction of SAXS Signal due to Doppler Broadening Induced Loss of Coherence](https://arxiv.org/abs/2601.13905)
*Thomas Kluge,Uwe Hernandez Acosta,Klaus Steiniger,Ulrich Schramm,Thomas E. Cowan*

Main category: physics.plasm-ph

TL;DR: Doppler broadening in laser-heated plasmas degrades SAXS coherence, reducing signal intensity and affecting parameter retrieval accuracy, especially for seeded XFELs or high temperatures.


<details>
  <summary>Details</summary>
Motivation: To understand how Doppler-induced spectral broadening affects small-angle X-ray scattering (SAXS) coherence in laser-heated plasmas, and quantify its impact on parameter retrieval accuracy for different XFEL configurations.

Method: Analytical and numerical study of Doppler broadening effects on SAXS signals, applied to two benchmark geometries: single density steps (wires) and periodic gratings. Parameter space mapping for current SASE and self-seeded XFEL configurations.

Result: Doppler effects lower Bragg-peak heights and broaden widths for gratings, while only affecting overall scaling with q for isolated steps. Effects remain manageable for SASE bandwidths (<10% error) but become dominant error source for seeded configurations or above-keV temperatures.

Conclusion: Provides criteria for when Doppler broadening must be included in SAXS analysis and offers a method to infer electron temperature directly from coherence-loss signatures, with practical implications for density-gradient retrieval and interface-sharpness measurements.

Abstract: We present an analytical and numerical study of how Doppler-induced spectral broadening in laser-heated plasmas degrades the coherence of small-angle X-ray scattering (SAXS) signals, and show that the resulting loss of temporal coherence reduces the SAXS intensity. Applying this formalism to two benchmark geometries - single density steps (wires) and periodic gratings -- we obtain analytic estimates. For gratings, finite coherence simultaneously lowers Bragg-peak heights and broadens their widths, whereas for isolated steps only the overall scaling with q affected. We map the parameter space relevant to current SASE and self-seeded XFELs, revealing that Doppler effects remain managable for the trieval of geometry parameters (less than few 10 % error) for SASE bandwidths but become the dominant error source in seeded configurations or above-keV temperatures. Practical consequences for density-gradient retrieval and interface-sharpness measurements are quantified. The results supply clear criteria for when Doppler broadening must be included in SAXS data analysis and offer a route to infer electron temperature directly from coherence-loss signatures.

</details>


### [110] [The Geometry of Flux Surfaces with Quasi-Poloidal Symmetry](https://arxiv.org/abs/2601.13980)
*Rishin Madan,Wrick Sengupta,Elizabeth J. Paul,Mohammed Haque,Richard Nies,Amitava Bhattacharjee*

Main category: physics.plasm-ph

TL;DR: A novel framework transforms quasi-poloidal magnetic field analysis from 3D to 2D, enabling efficient optimization and theoretical understanding of these desirable plasma confinement configurations.


<details>
  <summary>Details</summary>
Motivation: Quasi-poloidal magnetic fields have excellent plasma confinement properties (no radial drift, zero Pfirsch-Schlüter current, reduced transport) but cannot be analyzed using standard near-axis expansion methods, creating a major theoretical gap.

Method: Developed a novel framework that simplifies finding quasi-poloidal flux surfaces from a 3D problem to a 2D problem, applicable to both toroidal configurations and asymmetric magnetic mirrors. This 2D formulation enables efficient optimization and theoretical analysis.

Result: The reduced model successfully identifies classes of QP flux surfaces (including naturally flat mirrors), validates against numerically optimized QP equilibria, and explains observed features like cusps, high mirror ratios, and narrow pinch points in these equilibria.

Conclusion: The new 2D framework provides an effective theoretical tool for understanding and optimizing quasi-poloidal magnetic fields, overcoming previous limitations of near-axis expansion methods and explaining key features observed in numerical equilibria.

Abstract: Quasi-poloidal (QP) magnetic fields have desirable properties for confining plasma: no radial drift of guiding centres (with positive implications for neoclassical transport), zero Pfirsch-Schlüter current, a lower level of damping for poloidal flows, leading to reduced anomalous transport, and possible stability benefits. Despite their attractive properties, QP fields are not amenable to the near-axis expansion, a major theoretical tool for understanding toroidal fields. In this paper, we provide a novel framework for defining and understanding QP flux surfaces. This framework relies on a simplification that transforms the task of finding a quasi-poloidal flux surface from a 3D problem to a 2D problem. This simplification also applies to asymmetric magnetic mirrors with desirable properties. We sketch how this 2D problem can form the basis of an efficient optimisation problem for finding QP flux surfaces. We leverage this 2D problem for theoretical understanding: for instance, we identify one class of QP flux surfaces that are naturally flat mirrors (Velasco et al. 2023). The reduced model is validated against numerically optimised QP equilibria. We further utilise the reduced model to explain the prevalence of cusps, high mirror ratios, and narrow pinch points in these numerical equilibria.

</details>


### [111] [XFEL Imaging Techniques for High Energy Density and Inertial Fusion Energy Research at HED-HiBEF](https://arxiv.org/abs/2601.14028)
*Alejandro Laso Garcia,Mikhail Mishchenko,Victorien Bouffetier,Gabriel Perez-Callejo,Karen Appel,Alexey Arefiev,Carsten Baehtz,Erik Brambrink,Mihail Cernaianu,Domenico Doria,Tobias Dornheim,Gillis M. Dyer,Nicolas Fefeu,Eric Galtier,Thomas Gawne,Petru V. Ghenuche,Sebastian Goede,Johannes Hagemann,Marie-Luise Herbert,Hauke Höppner,Lingen Huang,Oliver Humphries,Mae Jones,Dimitri Khaghani,Thomas Kluge,Jayanath Koliyadu,Dominik Kraus,Hae Ja Lee,Julian Lütgert,Mikako Makita,Jean-Paul Naedler,Bob Nagler,Motoaki Nakatsutsumi,Quynh Nguyen,Alexander Pelka,Thomas R. Preston,Chong Bing Qu,Sripati V. Rahul,Lisa Randolph,Ronald Redmer,Martin Rehwald,Hans G. Rinderknecht,Angel Rodriguez-Fernandez,Joao J. Santos,Ulrich Schramm,Michal Smid,Cornelius Strohm,Jergus Strucka,Minxue Tang,Patrik Vagovic,Milenko Vescovi,Long Yang,Karl Zeil,Ulf Zastrau,Thomas E. Cowan,Toma Toncian*

Main category: physics.plasm-ph

TL;DR: The HED-HiBEF imaging platform at European XFEL combines XFEL with high-intensity lasers for high-resolution imaging of extreme physics phenomena.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced imaging platform for studying high energy density physics and fusion-related research with exceptional spatial and temporal resolution.

Method: Combines European XFEL beam with ReLaX (high-intensity short-pulse laser) and DiPOLE-100X (high-energy nanosecond-pulse laser) to create an integrated imaging platform.

Result: Achieved spatial resolution better than 500 nm and temporal resolution around 50 fs. Demonstrated applications include blast waves, converging cylindrical shocks in aluminum, resonant absorption measurements in copper, and planar shocks in polystyrene.

Conclusion: The HED-HiBEF platform enables unprecedented studies of extreme physics phenomena and has potential for further enhancement when combined with kJ-class lasers.

Abstract: The imaging platform developed at the High Energy Density - Helmholtz International Beamline for Extreme Fields (HED-HiBEF) instrument at the European XFEL and its applications to high energy density and fusion related research are presented. The platform combines the XFEL beam with the high-intensity short-pulse laser ReLaX and the high-energy nanosecond-pulse laser DiPOLE-100X. The spatial resolution is better than 500 nm and the temporal resolution of the order of 50 fs. We show examples of blast waves and converging cylindrical shocks in aluminium, resonant absorption measurements of specific charged states in copper with ReLaX and planar shocks in polystyrene material generated by DiPOLE-100X. We also discuss the possibilities introduced by combining this imaging platform with a kJ-class laser.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [112] [Hamiltonian hydrodynamic reductions of one-dimensional Vlasov equations](https://arxiv.org/abs/2601.13746)
*Rayan Oufar,Cristel Chandre*

Main category: math-ph

TL;DR: The paper shows that various Hamiltonian closures of the 1D Vlasov-Poisson equation can be unified into a single parametric form expressed in normal variables, revealing a common polynomial structure.


<details>
  <summary>Details</summary>
Motivation: To establish structural connections between different Hamiltonian fluid reductions of the 1D Vlasov-Poisson equation and provide a unified framework for describing these reduced hydrodynamic forms.

Method: Using hydrodynamic Poisson bracket framework to identify fundamental normal variables from Casimir invariants analysis, then applying this framework to analyze established Hamiltonian closures including multi-delta distribution and waterbag models.

Result: All analyzed closures lead to a unified parametric form: when expressed in normal variables, the parameterization is polynomial and of the same degree, uniquely generated from the μ₂ moment (a cubic polynomial in normal variables).

Conclusion: The structural connection between different physical models offers a path toward a more unified and simplified description of 1D Vlasov-Poisson dynamics through reduced hydrodynamic forms with arbitrary number of fluid variables.

Abstract: We investigate Hamiltonian fluid reductions of the one-dimensional Vlasov-Poisson equation. Our approach utilizes the hydrodynamic Poisson bracket framework, which allows us to systematically identify fundamental normal variables derived from the analysis of the Casimir invariants of the resulting Poisson bracket. This framework is then applied to analyze several well-established Hamiltonian closures of the onedimensional Vlasov equation, including the multi-delta distribution and the waterbag models. Our key finding is that all of these seemingly distinct closures consistently lead to the formulation of a unified form of parametric closures: When expressed in terms of the identified normal variables, the parameterization across all these closures is revealed to be polynomial and of the same degree. All these parametric closures are uniquely generated from one of the moments, called $μ$2, a cubic polynomial in the normal variables. This result establishes a structural connection between these different physical models, offering a path toward a more unified and simplified description of the one-dimensional Vlasov-Poisson dynamics through its reduced hydrodynamic forms with an arbitrary number of fluid variables.

</details>


### [113] [Covariant tomography of fields](https://arxiv.org/abs/2601.13261)
*Radosław Antoni Kycia*

Main category: math-ph

TL;DR: The paper introduces "covariant tomography" - a geometric framework for reconstructing interior classical fields from boundary data by decomposing differential forms and using parallel transport equations.


<details>
  <summary>Details</summary>
Motivation: To solve the Inverse Boundary Value Problem (IBVP) for classical fields, enabling reconstruction of parallelly transformed fields within a region based on known boundary data, which has applications in field tomography and gauge theory.

Method: Geometric decomposition of differential forms into exact and antiexact components, formulation of parallel transport equation via homotopy operator, and three extension techniques (radial, heat equation, harmonic) to map boundary values into interior.

Result: Developed a systematic framework for identifying realizability of boundary values, providing solutions for both current and gauge field tomography, with demonstrated applications in low-dimensional spaces and electromagnetic potential reconstruction in ℝ³.

Conclusion: The covariant tomography approach offers a powerful geometric method for solving IBVP problems, with extension choice affecting current regularity, and has practical applications in field reconstruction problems.

Abstract: This paper addresses the Inverse Boundary Value Problem (IBVP) for classical fields, specifically focusing on the recovery of parallelly transformed fields within a region based on known boundary data. We introduce a local solution framework, termed "covariant tomography," that uses geometric decomposition to reconstruct interior fields and currents within star-shaped open subsets. The core of our approach involves decomposing differential forms into exact and antiexact components, enabling the formulation of the parallel transport equation via a homotopy operator. We examine three primary extension techniques - radial, heat equation, and harmonic - to map boundary values into the interior, noting that the choice of extension directly influences the regularity of the resulting currents. The proposed methodology provides a systematic way to identify the realizability of boundary values and offers solutions for both current and gauge field tomography. Finally, we demonstrate the utility of this framework through illustrative examples in low-dimensional spaces and electromagnetic potential reconstruction in $\mathbb{R}^{3}$.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [114] [Boundary Delocalization and Spectral Packets for Dirichlet Eigenfunctions](https://arxiv.org/abs/2601.11605)
*Anton Alexa*

Main category: math.SP

TL;DR: Boundary delocalization principle for high-frequency Dirichlet eigenfunctions on convex domains, excluding persistent boundary concentration by comparing individual eigenmodes to spectral packets.


<details>
  <summary>Details</summary>
Motivation: To understand boundary behavior of high-frequency eigenfunctions on convex domains and exclude persistent boundary concentration, which is important for understanding eigenfunction localization and quantum chaos phenomena.

Method: Uses Rellich identity for mode-to-packet estimates and boundary local Weyl law for multi-mode bias exclusion. Compares boundary energies of single eigenfunctions to packet sums over frequency windows of sublinear size N_k = o(k).

Result: Establishes boundary delocalization principle that excludes persistent boundary concentration at individual eigenmode level compared to spectral packets. The comparison is independent of eigenvalue monotonicity and stable under eigenvalue crossings.

Conclusion: High-frequency Dirichlet eigenfunctions on smooth strictly convex domains cannot concentrate persistently at the boundary when compared to spectral packets, providing a fundamental delocalization property for such eigenfunctions.

Abstract: We establish a boundary delocalization principle for high-frequency Dirichlet eigenfunctions on smooth strictly convex domains. The main result excludes persistent boundary concentration at the level of individual eigenmodes when compared to short spectral packets of sublinear length. Quantitatively, we compare boundary energies of single eigenfunctions to packet sums over frequency windows of size N_k = o(k), without asserting any asymptotic gain in magnitude. The main mode-to-packet estimate relies only on the Rellich identity. For the multi-mode bias exclusion we additionally use the boundary local Weyl law to obtain a packet zero-mean cancellation estimate. This mode-to-packet comparison is independent of eigenvalue monotonicity and is stable under eigenvalue crossings.

</details>


### [115] [Directional Ballistic Transport in Quantum Waveguides](https://arxiv.org/abs/2601.13471)
*Adam Black,David Damanik,Peter Kuchment,Tal Malinovitch,Giorgio Young*

Main category: math.SP

TL;DR: Schrödinger operators with mixed periodic/compactly supported potentials exhibit surface states with directional ballistic transport - ballistic in periodic directions, absent in compact directions.


<details>
  <summary>Details</summary>
Motivation: To understand transport properties of quantum systems with mixed boundary conditions (periodic in some directions, compactly supported in others) and characterize how surface states behave in such anisotropic geometries.

Method: Developed a Floquet theory to capture analytic variation of surface states, reformulated eigenvalue problem for surface states as a Fredholm problem using Dirichlet-to-Neumann map.

Result: Surface states exhibit directional ballistic transport: ballistic transport occurs in periodic directions while transport is absent in compactly supported directions.

Conclusion: The mixed boundary conditions create anisotropic transport behavior in surface states, with clear directional dependence determined by the geometry of the potential's periodicity.

Abstract: We study the transport properties of Schrödinger operators on $\mathbb{R}^d$ with potentials that are periodic in some directions and compactly supported in the others. Such systems are known to produce surface states that are weakly confined near the support of the potential. We show that a natural set of surface states exhibits directional ballistic transport, characterized by ballistic transport in the periodic directions and its absence in the others. To prove this, we develop a Floquet theory that captures the analytic variation of surface states. The main idea consists of reformulating the eigenvalue problem for surface states as a Fredholm problem via the Dirichlet-to-Neumann map.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [116] [Reorienting off-path Nudged Elastic Bands (RONEB) via Minimum Mode Following](https://arxiv.org/abs/2601.12630)
*Rohit Goswami,Miha Gunde,Hannes Jónsson*

Main category: physics.chem-ph

TL;DR: RONEB is a hybrid transition state search algorithm combining NEB's double-ended nature with Min-Mode Following efficiency, achieving ~46% reduction in computational cost compared to standard CI-NEB.


<details>
  <summary>Details</summary>
Motivation: Existing transition state search methods have limitations: double-ended methods like NEB are computationally expensive and can stagnate on flat/rough surfaces, while single-ended eigenmode-following techniques are efficient but cannot be constrained between specific states.

Method: RONEB (Reorienting Off-path Nudged Elastic Bands) integrates double-ended NEB with single-ended Min-Mode Following acceleration. It uses stability based on path optimization history, relative force triggering, and alignment-based back-off penalty to dynamically decouple climbing image from elastic band constraints.

Result: Benchmarking against CI-NEB shows median reduction in gradient calls of 46.3% on Baker-Chan test set and 28% reduction across 59 metallic rearrangement mechanisms in surface diffusion tests.

Conclusion: RONEB establishes itself as a highly effective tool for high-throughput automated chemical discovery by significantly reducing computational costs while maintaining transition state search accuracy.

Abstract: Accurate determination of transition states remains central to understanding reaction kinetics. Double-ended methods like the Nudged Elastic Band (NEB) ensure relevant transition states and paths, but incur high computational costs and suffer stagnation on flat or rough potential energy surfaces. Conversely, single-ended eigenmode-following techniques offer efficiency but cannot often be constrained between specific states. Here, we present the Reorienting Off-path Nudged Elastic Bands (RONEB), an adaptive hybrid algorithm that integrates the double ended nature of the NEB with the acceleration of single ended Min-Mode Following methods. RONEB provides stability based on the history of the path optimization, relative force triggering, and an alignment-based back-off penalty to dynamically decouple the climbing image from the elastic band constraints. We benchmark the method against the standard Climbing Image NEB (CI-NEB) across the Baker-Chan transition state test set using the PET-MAD machine-learned potential and the OptBench Pt(111) heptamer island surface diffusion set. A Bayesian analysis of the performance data quantifies a median reduction in gradient calls of 46.3% [95% CrI: -54.7%, -36.9%] relative to the baseline, while surface diffusion tests reveal a 28% reduction across 59 metallic rearrangement mechanisms. These results establish RONEB as a highly effective tool for high-throughput automated chemical discovery.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [117] [High Field Diamond Magnetometry Towards Tokamak Diagnostics](https://arxiv.org/abs/2601.13413)
*S. M. Graham,C. J. Stephen,A. J. Newman,A. M. Edmonds,M. L. Markham,G. W. Morley*

Main category: physics.app-ph

TL;DR: Demonstration of fiber-coupled nitrogen vacancy center magnetometry operating in high magnetic fields up to 1.2 T, achieving sensitivities suitable for tokamak fusion diagnostics.


<details>
  <summary>Details</summary>
Motivation: Nitrogen vacancy centers in diamond are radiation-hard and could serve as magnetometers for tokamak fusion power diagnostics, which require operation in high magnetic fields (≥1 T).

Method: Used fiber-coupled ensemble nitrogen vacancy centers with optically detected magnetic resonance (ODMR) measurements at magnetic fields up to 1.2 T.

Result: Achieved sensitivities of approximately 240-600 nT/√Hz in 10-150 Hz range for non-degenerate alignments, and 110 nT/√Hz for near-⟨111⟩ field alignments.

Conclusion: Demonstrated that NVC magnetometers can operate in the high magnetic field environment relevant to tokamak fusion diagnostics, showing promising sensitivity performance.

Abstract: Nitrogen vacancy centres (NVC) in diamond have been widely used for near-dc magnetometry. The intrinsic properties of diamonds make them potential candidates for tokamak fusion power diagnostics, where radiation-hard magnetometers will be essential for efficient control. An NVC magnetometer placed in a tokamak will need to operate within a $\geq$ 1 T magnetic field. In this work, we demonstrate fibre-coupled ensemble NVC optically detected magnetic resonance (ODMR) and magnetometry measurements at magnetic fields up to 1.2 T. Sensitivities of approximately 240 to 600 nT/$\sqrt{\textrm{Hz}}$ and 110 nT/$\sqrt{\textrm{Hz}}$ are achieved in a (10-150) Hz frequency range, for non-degenerate and near-$\langle$111$\rangle$ field alignments respectively.

</details>


### [118] [Two-dimensional FrBD friction models for rolling contact: extension to linear viscoelasticity](https://arxiv.org/abs/2601.13818)
*Luigi Romano*

Main category: physics.app-ph

TL;DR: Extends distributed rolling contact FrBD framework to linear viscoelasticity using Generalized Maxwell and Kelvin-Voigt rheological models, resulting in 2(n+1) hyperbolic PDEs that capture complex relaxation phenomena.


<details>
  <summary>Details</summary>
Motivation: To advance the FrBD (Friction Bristle Dynamics) paradigm by enabling unified and systematic treatment of linear viscoelasticity in distributed rolling contact systems, capturing complex relaxation phenomena that originate from viscoelastic behaviors.

Method: Extends distributed rolling contact FrBD framework using classic derivative Generalized Maxwell and Kelvin-Voigt rheological representations of bristle elements. Develops three distributed formulations of increasing complexity by specifying analytical expressions for transport and rigid relative velocity. Analyzes well-posedness and passivity for linear variants.

Result: Dynamics described by system of 2(n+1) hyperbolic PDEs that capture complex relaxation phenomena. Well-posedness and passivity properties hold for any physically meaningful parametrization. Numerical experiments illustrate steady-state characteristics and transient relaxation effects.

Conclusion: The findings substantially advance the FrBD paradigm by enabling unified and systematic treatment of linear viscoelasticity, providing rigorous theoretical foundations and practical modeling capabilities for viscoelastic friction dynamics.

Abstract: This paper extends the distributed rolling contact FrBD framework to linear viscoelasticity by considering classic derivative Generalised Maxwell and Kelvin-Voigt rheological representations of the bristle element. With this modelling approach, the dynamics of the bristle, generated friction forces, and internal deformation states are described by a system of 2(n+1) hyperbolic partial differential equations (PDEs), which can capture complex relaxation phenomena originating from viscoelastic behaviours. By appropriately specifying the analytical expressions for the transport and rigid relative velocity, three distributed formulations of increasing complexity are introduced, which account for different levels of spin excitation. For the linear variants, well-posedness and passivity are analysed rigorously, showing that these properties hold for any physically meaningful parametrisation. Numerical experiments complement the theoretical results by illustrating steady-state characteristics and transient relaxation effects. The findings of this paper substantially advance the FrBD paradigm by enabling a unified and systematic treatment of linear viscoelasticity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [119] [Privacy-Preserving Black-Box Optimization (PBBO): Theory and the Model-Based Algorithm DFOp](https://arxiv.org/abs/2601.11570)
*Pengcheng Xie*

Main category: cs.CR

TL;DR: DFOp is a new derivative-free solver for privacy-preserving black-box optimization with encrypted objective functions, featuring novel quadratic model updating and differential privacy mechanisms.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for solving unconstrained privacy-preserving black-box optimization problems where objective functions are transformed/encrypted, and aims to bridge the gap between derivative-free optimization and privacy preservation.

Method: Proposes DFOp, a derivative-free solver with a new updating formula for quadratic model functions, specifically designed for problems with encrypted objective functions F_k(x). Includes two differentially private noise-adding mechanisms for privacy protection.

Result: DFOp demonstrates better performance than compared algorithms in numerical experiments. The paper provides convergence analysis for solving transformed/encrypted objective functions and analyzes the transformation's impact on model functions.

Conclusion: DFOp is the first derivative-free solver capable of solving black-box optimization with step-encryption and privacy-preserving problems exactly, addressing the open question about combining derivative-free optimization with privacy.

Abstract: This paper focuses on solving unconstrained privacy-preserving black-box optimization (PBBO), its corresponding least Frobenius norm updating of quadratic models, and the differentially privacy mechanisms for PBBO. Optimization problems with transformed/encrypted objective functions aim to minimize F(x), which is encrypted/transformed/encrypted to F_k(x) as the output at the k-th iteration. A new derivative-free solver named DFOp, with its implementation, is proposed in this paper, which has a new updating formula for the quadratic model functions. The convergence of DFOp for solving problems with transformed/encrypted objective functions is given. Other analyses, including the new model updating formula and the analysis of the transformation's impact to model functions are presented. We propose two differentially private noise-adding mechanisms for privacy-preserving black-box optimization. Numerical results show that DFOp performs better than compared algorithms. To the best of our knowledge, DFOp is the first derivative-free solver that can solve black-box optimization problems with step-encryption and privacy-preserving black-box problems exactly, which also tries to answer the open question about the combination of derivative-free optimization and privacy.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [120] [Symmetric preparation of systems](https://arxiv.org/abs/2601.12458)
*Nils Dencker*

Main category: math.CA

TL;DR: Generalizes Weierstrass and Malgrange preparation theorems to symmetric matrix-valued functions, proving symmetric preparation for analytic and smooth symmetric systems vanishing to first order.


<details>
  <summary>Details</summary>
Motivation: The classical Weierstrass and Malgrange preparation theorems are fundamental tools in singularity theory and analysis, but they are limited to scalar-valued functions. There is a need to extend these results to symmetric matrix-valued functions, which arise naturally in many applications involving symmetric systems and matrix analysis.

Method: The paper develops a symmetric matrix-valued generalization of the classical preparation theorems. The approach involves extending the techniques from scalar-valued functions to symmetric matrix-valued functions, focusing on systems that vanish to first order. The method likely involves careful analysis of symmetric matrix structures and their analytic/smooth properties.

Result: Successfully proves symmetric preparation theorems for both analytic and smooth symmetric matrix-valued systems that vanish of first order. This provides a powerful tool for analyzing symmetric matrix functions near singular points, analogous to the classical preparation theorems for scalar functions.

Conclusion: The paper establishes a significant extension of fundamental preparation theorems to the symmetric matrix-valued case, opening new possibilities for analyzing symmetric systems in analysis, singularity theory, and applications where matrix-valued functions naturally occur.

Abstract: In this paper we generalize the Weierstrass and Malgrange preparation theorems to the symmetric matrix valued case, proving symmetric preparation of analytic and smooth symmetric systems that vanish of first order.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [121] [Evidence of energy conversion in weakly collisional plasma during an interplanetary coronal mass ejection](https://arxiv.org/abs/2601.12476)
*Omkar Dhamane,Anil Raghav,Simone Benella,Kishor Kumbhar,Raffaella D'Amicis,Oreste Pezzi,Utkarsh Sharma,Ashok Silwal,Panini Maurya,Mirko Stumpo,Kalpesh Ghag,Ajay Kumar,Mohit Shah,Mariyam Karari,Lynn B. Wilson,Jia Huang,Daniele Telloni*

Main category: astro-ph.SR

TL;DR: Analysis of two turbulent intervals within an ICME magnetic cloud reveals coexistence of left-handed Alfvén ion-cyclotron waves and right-handed fast magnetosonic/whistler waves due to evolving plasma anisotropies and marginal stability conditions.


<details>
  <summary>Details</summary>
Motivation: To investigate rare intervals of enhanced turbulent fluctuations within ICME magnetic clouds and characterize the wave populations present, as such intervals are typically less frequent in these regions.

Method: Spectral analysis and plasma instability analysis using ion-scale normalized magnetic helicity and polarization properties with respect to background magnetic field B0, with observations from Wind spacecraft during 8-9 June 2000 ICME event.

Result: First interval shows left-handed circularly polarized waves; second interval shows persistent left-handed waves plus additional high-frequency right-handed waves propagating parallel to B0. Left-handed fluctuations are Alfvén ion-cyclotron waves, right-handed are fast magnetosonic/whistler waves.

Conclusion: ICME plasma accesses resonance conditions supporting multiple ion-scale wave modes. Evolving anisotropies and approach to marginal stability allow coexistence of AIC-like and FM/W-like fluctuations, with enhanced electron heating favoring FM/W growth and strengthening density-magnetic field correlations.

Abstract: Intervals of enhanced turbulent fluctuations are typically less frequent within the magnetic cloud region of an interplanetary coronal mass ejection (ICME). We investigate two such intervals inside an ICME observed by the \textit{Wind} spacecraft on 8--9 June 2000 and characterize their associated wave populations. We focus on spectral analysis and plasma instability analysis, using ion-scale normalized magnetic helicity and polarization properties with respect to the background magnetic field $B_0$. In the first interval, the ion-scale normalized magnetic helicity shows a left-handed circularly polarized signature. In the second interval, the left-handed signature persists and an additional high-frequency right-handed population appears. The propagation is approximately parallel to $B_0$. The left-handed fluctuations are compatible with Alfvén ion-cyclotron (AIC) waves, while the right-handed fluctuations are consistent with fast magnetosonic/whistler (FM/W) waves. The ICME plasma accesses resonance conditions that support multiple ion-scale wave modes. Evolving anisotropies in the plasma and the approach to marginal stability allow the coexistence of AIC-like and fast-magnetosonic/whistler-like fluctuations, with enhanced electron heating favoring the growth of the FM/W contribution and strengthening the density--magnetic-field magnitude correlation.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [122] [Koopman Spectral Computation Beyond The Reflexive Regime: Endpoint Solvability Complexity Index And Type-2 Links](https://arxiv.org/abs/2601.12044)
*Christopher Sorg*

Main category: math.LO

TL;DR: The paper analyzes computational complexity of Koopman operator spectral problems in SCI framework, showing different oracle models for L¹ vs L∞ cases, constructing decision problems for SCI lower bounds, and connecting to Type-2/Weihrauch theory.


<details>
  <summary>Details</summary>
Motivation: To understand the computational complexity (Solvability Complexity Index) of computing the approximate point spectrum of Koopman operators acting on L^p spaces, particularly addressing differences between reflexive (1<p<∞) and non-reflexive (p=1,∞) cases, and to develop tools for establishing SCI lower bounds.

Method: Uses information-based framework of towers of algorithms (SCI theory) with point evaluation oracles for the map F and quadrature access to the measure ω. Proves uniform finite-dimensional quadrature compatibility to bring L¹ case into same oracle model as reflexive cases. Constructs prototype family of decision problems (Ξ_m) realizing prescribed finite tower heights for SCI lower bounds. Places results in broader computational landscape of Type-2/Weihrauch theory.

Result: Shows how L¹ case can be handled similarly to reflexive cases through uniform quadrature compatibility, while L∞ case fundamentally differs due to non-separability. Provides reusable reduction source for future SCI lower bounds via constructed decision problems. Establishes connections between SCI framework and Type-2/Weihrauch theory for Koopman operator spectral computation.

Conclusion: The computational complexity of Koopman operator spectral problems varies significantly between different L^p spaces, with L¹ requiring special treatment but being manageable, while L∞ presents fundamentally different challenges. The constructed decision problems provide valuable tools for establishing SCI lower bounds, and the connections to Type-2/Weihrauch theory offer broader computational context.

Abstract: We study the Solvability Complexity Index (SCI) of Koopman operator spectral computation in the information-based framework of towers of algorithms. Given a compact metric space $(\mathcal{X},d)$ with a finite Borel measure $ω$ on $\mathcal{X}$ and a continuous nonsingular map $F:\mathcal{X}\to \mathcal{X}$, our focus is the Koopman operator $\mathcal{K}_F$ acting on $L^p(\mathcal{X},ω)$ for $p\in\{1,\infty\}$ for the computational problem \[ Ξ_{σ_{\mathrm{ap}}}(F) :=σ_{\mathrm{ap}}\!\bigl(\mathcal{K}_F\bigr), \] with input access given by point evaluations of $F\mapsto F(x)$ (and fixed quadrature access to $ω$).
  We clarify how the $L^1$ case can be brought into the same oracle model as the reflexive regime $1<p<\infty$ by proving a uniform finite-dimensional quadrature compatibility, while highlighting the fundamentally different role played by non-separability at $p=\infty$.
  Beyond Koopman operators, we also construct a prototype family of decision problems $(Ξ_m)_{m\in\mathbb N}$ realizing prescribed finite tower heights, providing a reusable reduction source for future SCI lower bounds. Finally, we place these results deeper in the broader computational landscape of Type-2/Weihrauch theory.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [123] [A Functorial Approach to Multi-Space Interpolation with Function Parameters](https://arxiv.org/abs/2601.12572)
*Thomas Lamby,Samuel Nicolay*

Main category: math.FA

TL;DR: Extension of interpolation theory to multiple spaces using functional parameters, maintaining functorial framework with applications to Sobolev and Besov spaces.


<details>
  <summary>Details</summary>
Motivation: To generalize interpolation theory beyond the traditional two-space setting, enabling systematic handling of multiple spaces while preserving mathematical structure and functorial properties.

Method: Employ functional parameters within a fully functorial framework to extend interpolation theory to more than two spaces, ensuring stability under operations like powers and convex combinations.

Result: Successfully constructs generalized intermediate spaces and demonstrates that interpolating multiple generalized Sobolev spaces yields generalized Besov spaces, providing explicit tools for multi-parameter interpolation.

Conclusion: The framework offers both theoretical robustness and practical relevance for handling complex interpolation problems involving multiple spaces, with applications in functional analysis and PDE theory.

Abstract: We introduce an extension of interpolation theory to more than two spaces by employing a functional parameter, while retaining a fully functorial and systematic framework. This approach allows for the construction of generalized intermediate spaces and ensures stability under natural operations such as powers and convex combinations. As a significant application, we demonstrate that the interpolation of multiple generalized Sobolev spaces yields a generalized Besov space. Our framework provides explicit tools for handling multi-parameter interpolation, highlighting both its theoretical robustness and practical relevance.

</details>


### [124] [The nonlinear estimates on quantum Besov space](https://arxiv.org/abs/2601.11934)
*Deyu Chen,Guixiang Hong*

Main category: math.FA

TL;DR: The paper studies boundedness estimates of superposition operators with non-smooth symbols on quantum Besov spaces, generalizing previous results and resolving a conjecture about equivalence of quantum Besov space descriptions.


<details>
  <summary>Details</summary>
Motivation: Superposition operators are crucial for nonlinear analysis and well-posedness theory of nonlinear equations. The paper aims to extend boundedness estimates to non-smooth symbols on quantum Besov spaces, which has applications in noncommutative PDEs.

Method: The authors investigate superposition operators on quantum Besov spaces using techniques involving quantum chain rule and nonlinear interpolation. They prove boundedness estimates for operators with non-smooth symbols.

Result: The main results include boundedness estimates that significantly generalize McDonald's results for infinitely differentiable symbols. As a byproduct, they prove the equivalence of two descriptions of quantum Besov spaces, resolving a conjecture from previous work.

Conclusion: The paper establishes important boundedness properties for superposition operators with non-smooth symbols on quantum Besov spaces, with applications to noncommutative PDE well-posedness theory, while also resolving a fundamental conjecture about quantum Besov space descriptions.

Abstract: The superposition operators have been widely studied in nonlinear analysis, which are essential for the well-posedness theory of nonlinear equations. In this paper, we investigate the boundedness estimates of superposition operators with non-smooth symbols on quantum Besov spaces, which significantly generalize McDonald's results \cite{McNLE} for infinitely differentiable symbols and have rich applications in the well-posedness theory of noncommutative PDEs. As a byproduct, we prove the equivalence of the two descriptions of quantum Besov spaces, resolving the conjecture proposed in \cite[Remark 3.16]{McNLE}. The new ingredients in the proof also involve quantum chain rule and nonlinear interpolation.

</details>


### [125] [Characterizations of Lorentz Type Sobolev Multiplier Spaces and Their Preduals](https://arxiv.org/abs/2601.12206)
*Keng Hao Ooi*

Main category: math.FA

TL;DR: Characterizations of Sobolev multiplier spaces of Lorentz type, their preduals, and applications to boundedness of local Hardy-Littlewood maximal function.


<details>
  <summary>Details</summary>
Motivation: To provide comprehensive characterizations of Sobolev multiplier spaces of Lorentz type and their preduals, which are important function spaces in harmonic analysis and partial differential equations.

Method: Using block decomposition techniques and Köthe dual analysis to study the structure of these function spaces and their preduals.

Result: Several characterizations of Sobolev multiplier spaces of Lorentz type and their preduals are established, including block decomposition and Köthe dual properties.

Conclusion: The developed theory enables justification of the boundedness of the local Hardy-Littlewood maximal function on these spaces, demonstrating the practical utility of the characterizations.

Abstract: We provide several characterizations of Sobolev multiplier spaces of Lorentz type and their preduals. Block decomposition and Köthe dual of such preduals are discussed. As an application, the boundedness of local Hardy-Littlewood maximal function on these spaces will be justified.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [126] [Pollutant-induced changes in fish pigmentation and spatial patterns](https://arxiv.org/abs/2601.12801)
*Pranali Roy Chowdhury,Tian Xu Wang,Abbey MacDonald,Keith B. Tierney,Hao Wang*

Main category: q-bio.QM

TL;DR: Pollutants disrupt fish pigmentation patterns by altering pigment cell interactions, leading to transitions between stripes, spots, and mixed patterns that can be permanent with prolonged exposure.


<details>
  <summary>Details</summary>
Motivation: Pigmentation abnormalities in fish serve as biomarkers for environmental contamination, but the underlying mechanisms remain poorly understood. The study aims to understand how pollutants influence pigment cell self-organization and pattern formation.

Method: Used a continuum reaction-diffusion-advection framework with nonlocal Morse-type kernels to model short- and long-range interactions between melanophores and xanthophores. Investigated how perturbations to adhesion/repulsion strengths affect pattern formation.

Result: Perturbations to cell-cell interaction strengths drive transitions between stripes, spots, and mixed patterns, reproducing mutant phenotypes. Homotypic interactions are particularly sensitive to contamination. Early exposure effects may be recoverable, but prolonged exposure leads to sustained pigment loss. In growing fish, contamination affects stripe formation rate, number, and pigmentation levels.

Conclusion: The study provides mechanistic insight into how environmental contaminants disrupt fish pigmentation patterns by altering pigment cell interactions, explaining experimentally observed abnormalities and their potential permanence depending on exposure duration.

Abstract: Pigmentation abnormalities, ranging from hypo- to hyperpigmentation, can serve as biomarkers of developmental disruption in fish exposed to environmental contaminants. However, the mechanistic pathways underlying these alterations remain poorly understood. Studies have shown that pattern formation in fish development requires specific pigment cell interactions. Motivated by experimental observations of pigmentation alterations following contaminant exposure, we investigate how pollutants influence pigment cell self-organization using a continuum reaction-diffusion-advection framework. The model incorporates nonlocal Morse-type kernels to describe short- and long-range interactions among melanophores and xanthophores. Our results show that perturbations to the strengths of adhesion or repulsion can drive transitions between stripes, spots, and mixed patterns, reproducing phenotypes characteristic of fish pigmentation mutants. In particular, homotypic interactions are sensitive to contamination, leading to pronounced changes in melanophore density and resulting pigmentation patterns. Time-dependent simulations indicate that pigment changes from early short-term contaminant exposure may be recoverable, whereas prolonged exposure can lead to sustained pigment loss. In a growing fish, contaminant-induced changes in cell-cell interactions directly influence stripe formation rate, stripe number, and pigmentation levels. Overall, our study provides insight into the mechanistic link between experimentally observed pigmentation alterations and the changes in spatial patterns of adult fish.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [127] [The Harnack inequality without convexity for curve shortening flow](https://arxiv.org/abs/2601.13767)
*Arjun Sobnack,Peter M. Topping*

Main category: math.DG

TL;DR: A new Harnack inequality for curve shortening flow without convexity assumption, applied to show delayed parabolic regularity and settling behavior of polar graphical flows.


<details>
  <summary>Details</summary>
Motivation: Hamilton's 1995 Harnack inequality for mean curvature flow requires convexity. This paper aims to develop an alternative Harnack inequality that works without convexity assumptions, specifically for one-dimensional mean curvature flow (curve shortening flow).

Method: Proves a new Harnack inequality for curve shortening flow that doesn't require convexity. Applies this inequality to: 1) show explicit time when wild initial curves become graphical, 2) analyze settling behavior of polar graphical flows with radial ends, and 3) relate to Hamilton's inequality for convex flows.

Result: 1) Provides explicit time bound for when curve shortening flow evolution of initially wild curves becomes graphical (delayed parabolic regularity). 2) Gives estimates for how polar graphical flows with radial ends settle to expanding solutions. 3) Shows connection to Hamilton's inequality through pointwise curvature estimates in convex case.

Conclusion: The paper establishes a convexity-free Harnack inequality for curve shortening flow, demonstrating its utility in proving delayed regularity phenomena and analyzing flow behavior, while connecting to classical results in the convex setting.

Abstract: In 1995, Hamilton introduced a Harnack inequality for convex solutions of the mean curvature flow. In this paper we prove an alternative Harnack inequality for curve shortening flow, i.e. one-dimensional mean curvature flow, that does not require any assumption of convexity. For an initial proper curve in the plane whose ends are radial lines but which is otherwise arbitrarily wild, we use the Harnack inequality to give an explicit time by which the curve shortening flow evolution must become graphical. This gives a new instance of delayed parabolic regularity. The Harnack inequality also gives estimates describing how a polar graphical flow with radial ends settles down to an expanding solution. Finally, we relate our Harnack inequality to Hamilton's by identifying a pointwise curvature estimate implied by both Harnack inequalities in the special case of convex flows.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [128] [Coupled two-phase flow and surfactant/PFAS transport in porous media with angular pores: From pore-scale physics to Darcy-scale modeling](https://arxiv.org/abs/2601.11721)
*Sidian Chen,Bo Guo,Tianyuan Zheng*

Main category: physics.flu-dyn

TL;DR: A new modeling framework for two-phase surfactant-laden flow in porous media that incorporates pore angularity and interfacial tension-wettability coupling, overcoming limitations of traditional Leverett J-function approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional Darcy-type models using Leverett J-function idealize porous media as cylindrical tubes and decouple interfacial tension and wettability effects, limiting their ability to represent real pore geometries and coupled surfactant effects.

Method: Developed a modeling framework that explicitly incorporates pore angularity and interfacial tension-wettability coupling. Derived two-phase flow properties for angular pores, upscaled across pore size distributions, and formulated closed-form expressions integrated into coupled flow-transport models.

Result: Revealed nonlinear, nonmonotonic dependence of two-phase flow properties on pore angularity, pore size distribution, and interfacial tension. Simulations showed pore angularity strongly controls water flow, interfacial area, and PFAS retention, while surfactant-induced flow effects on PFAS leaching are generally minor under typical conditions.

Conclusion: The proposed framework provides a more physically grounded approach for modeling two-phase surfactant-laden flow and transport in porous media by addressing limitations of traditional Leverett J-function methods.

Abstract: Two-phase surfactant-laden flow and transport in porous media are central to many natural and engineering applications. Surfactants alter two-phase flow by modifying interfacial tension and wettability, while two-phase flow controls surfactant transport pathways and interfacial adsorption. These coupled processes are commonly modeled using Darcy-type two-phase flow equations combined with advection--dispersion--adsorption transport equations, with capillary pressure--saturation relationships scaled by the Leverett $J$-function. However, the Leverett $J$-function idealizes porous media as bundles of cylindrical tubes and decouples interfacial tension and wettability, limiting its ability to represent angular pore geometries and interfacial tension--wettability coupling effects. We present a modeling framework that explicitly incorporates pore angularity and interfacial tension--wettability coupling into Darcy-scale surfactant-laden flow and transport models. Two-phase flow properties are derived for angular pores, upscaled across pore size distributions, and formulated as explicit and closed-form expressions. These upscaled relationships are integrated into a coupled flow--transport model to simulate transient two-phase flow and surfactant transport. Results reveal a nonlinear and nonmonotonic dependence of two-phase flow properties on pore angularity, pore size distribution, and interfacial tension. Example simulations of water flow and PFAS migration in unsaturated soils indicate that surfactant-induced flow effects on PFAS leaching are generally minor under typical conditions, whereas pore angularity strongly controls water flow, interfacial area, and PFAS retention. Overall, the proposed framework provides a more physically grounded approach for modeling two-phase surfactant-laden flow and transport in porous media.

</details>


### [129] [Explicit and Implicit Finite Difference Solvers Implemented in JAX for Shock Wave Physics](https://arxiv.org/abs/2601.12204)
*Avinash Potluri,Arturo Rodriguez,Taylor N. Garcia,Chelsea M. Caballero,Katrina I. Sanchez,Payal Helambe,Vineeth V. Kumar,Francisco O. Aguirre Ortega*

Main category: physics.flu-dyn

TL;DR: Developed explicit/implicit finite-difference solvers for 1D Burgers equation using JAX framework to model shock dynamics, achieving portability across CPUs/GPUs/TPUs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To create a simplified analogue of Navier-Stokes equations for studying shock formation, propagation, and dissipation in computational fluid dynamics, while establishing a reproducible dataset for benchmarking CFD solvers and training machine learning models.

Method: Implemented explicit (forward Euler) and implicit (BTCS) finite-difference solvers for 1D Burgers viscous equation using Finite-JAX framework, with upwind scheme for convective terms and central scheme for diffusive terms, under periodic and Dirichlet boundary conditions with CFL stability criteria.

Result: Explicit scheme accurately captures shocks under strict time-step constraints, while implicit formulation provides greater stability and accuracy at higher computational cost. JAX maintains fidelity while achieving portability across different hardware (CPUs, GPUs, TPUs).

Conclusion: The FiniteJAX implementation enhances solver portability, scalability, and performance, establishing a reproducible dataset for CFD benchmarking and machine learning model training for nonlinear transport and impact-driven phenomena.

Abstract: Shock dynamics and nonlinear wave propagation are fundamental to computational fluid dynamics (CFD) and high-speed flow modeling. In this study, we developed explicit and implicit finite-difference solvers for the one-dimensional Burgers viscous equation to model shock formation, propagation, and dissipation. The governing equation, which incorporates convective and diffusive effects, serves as a simplified analogue of the Navier-Stokes equations. Using the Finite-JAX framework, each solver is implemented with upwind and central finite-difference schemes for the convective and diffusive terms, respectively. Time integration is performed using explicit forward Euler and implicit backward-time central space (BTCS) schemes under periodic and Dirichlet boundary conditions. Stability is ensured by the Courant-Friedrichs-Lewy (CFL) criteria for the convective and diffusive components. Numerical experiments quantify the accuracy, convergence, and real-time performance of JAX across CPUs, GPUs, and TPUs, demonstrating that JAX maintains fidelity while achieving portability. The results show that the explicit scheme captures impact accurately under strict time-step constraints, while the implicit formulation provides greater stability and accuracy at a higher computational cost. Taken together, these results establish a reproducible dataset for benchmarking CFD solvers and training machine learning models for nonlinear transport and impact-driven phenomena. Our new implementation of FiniteJAX enhances the portability, scalability, and performance of solvers based on the JAX framework developed by Google DeepMind.

</details>


### [130] [Unified multifractal description of longitudinal and transverse intermittency in fully developed turbulence](https://arxiv.org/abs/2601.12528)
*Dhawal Buaria*

Main category: physics.flu-dyn

TL;DR: The paper develops a unified multifractal framework that jointly describes longitudinal and transverse velocity increments and gradients in turbulent flows, revealing that transverse gradient scaling depends on mixed longitudinal-transverse structure functions.


<details>
  <summary>Details</summary>
Motivation: Existing multifractal descriptions of turbulence intermittency focus mainly on longitudinal components, despite evidence that transverse components exhibit distinct and stronger intermittency. There's a need for a unified framework that captures both components.

Method: Developed a unified multifractal framework that jointly prescribes longitudinal and transverse velocity increments and extends to gradients. Derived explicit relations linking inertial-range scaling exponents of structure functions to moments of velocity gradients in dissipation range.

Result: Revealed that longitudinal gradient scaling is solely prescribed by longitudinal structure functions (as traditionally expected), but transverse gradient scaling is prescribed by mixed longitudinal-transverse structure functions. Validation with high-resolution DNS of isotropic turbulence at Taylor-scale Reynolds number up to 1300 showed excellent agreement.

Conclusion: The framework provides a more complete and predictive description of turbulence intermittency that is faithful to the underlying dynamics, addressing the previously overlooked distinct behavior of transverse components.

Abstract: Small-scale intermittency is a defining feature of fully developed fluid turbulence, marked by rare and extreme fluctuations of velocity increments and gradients that defy mean-field descriptions. Existing multifractal descriptions of intermittency focus primarily on longitudinal increments and gradients, despite mounting evidence that transverse components exhibit distinct and stronger intermittency. Here, we develop a unified multifractal framework that jointly prescribes longitudinal and transverse velocity increments, and extends to gradients. We derive explicit relations linking inertial-range scaling exponents of structure functions to moments of velocity gradients in dissipation range. Our results reveal that longitudinal gradient scaling is solely prescribed by longitudinal structure functions, as traditionally expected; however, transverse gradient scaling is prescribed by mixed longitudinal-transverse structure functions. Validation with high-resolution direct numerical simulations of isotropic turbulence, at Taylor-scale Reynolds number up to $1300$ demonstrates excellent agreement, paving way for a more complete and predictive description of intermittency faithful to the underlying turbulence dynamics.

</details>


### [131] [Rigid Body Dynamics in Ambient Fluids](https://arxiv.org/abs/2601.13971)
*Marcel Padilla,Aviv Segall,Olga Sorkine-Hornung*

Main category: physics.flu-dyn

TL;DR: A novel framework for rigid body dynamics in fluids/air that accurately predicts object motion without CFD simulations, using added mass and generalized flow separation models.


<details>
  <summary>Details</summary>
Motivation: Current methods for simulating rigid body dynamics in ambient media (air/water) either rely on computationally expensive CFD simulations or use heuristic models that fail to capture complex phenomena like falling plate behaviors, Magnus effect, and football flight dynamics.

Method: Computes added mass of fluid and replaces heuristic lift/drag models with generalized estimates of flow separation and dynamic pressure. The algorithm is simple to implement, robust, doesn't require specialized integrators, and integrates seamlessly into existing physics engines.

Result: First method in rigid body dynamics context to reproduce full range of falling plate behaviors (fluttering, tumbling, chaotic, steady modes), Magnus effect, and American football flight dynamics (tight spiral pass paradox). Enables real-time simulation.

Conclusion: The framework provides an accurate, efficient alternative to CFD for rigid body dynamics in ambient media, capturing complex fluid-structure interaction phenomena while being practical for real-time applications in physics engines.

Abstract: We present a novel framework for rigid body dynamics in ambient media, such as air or water, enabling accurate motion prediction of objects without requiring computational fluid dynamics simulations. Our method computes the added mass of the fluid and replaces heuristic models for shape-dependent lift and drag with a generalized estimate of flow separation and dynamic pressure. Our method is the first within the rigid body dynamics context to reproduce the full range of falling plate behaviors: fluttering, tumbling, chaotic and steady modes, as well as phenomena such as the Magnus effect and the flight dynamics of an American football (tight spiral pass paradox). The resulting algorithm is simple to implement, robust, does not rely on specialized integrators and incorporates seamlessly into existing physics engines for real-time simulation.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [132] [Deterministic and probabilistic neural surrogates of global hybrid-Vlasov simulations](https://arxiv.org/abs/2601.12614)
*Daniel Holmberg,Ivan Zaitsev,Markku Alho,Ioanna Bouri,Fanni Franssila,Haewon Jeong,Minna Palmroth,Teemu Roos*

Main category: physics.space-ph

TL;DR: Graph-based ML emulators achieve 100x speedup for hybrid-Vlasov simulations of solar wind-magnetosphere interaction while maintaining physical accuracy and providing uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Hybrid-Vlasov simulations are computationally expensive even in 5D (2D + 3V), making real-time applications impractical. There's a need for faster alternatives that can capture ion-kinetic effects in solar wind-magnetosphere interactions while maintaining physical consistency.

Method: Used graph neural networks on 2D spatial grids (670k cells) to learn from four 5D Vlasiator simulations with varying ion densities. Developed two models: deterministic Graph-FM and probabilistic Graph-EFM with latent variables. Incorporated divergence penalty for magnetic field physical consistency and continuous ranked probability score for ensemble calibration.

Result: ML emulators achieved >100x speedup on single GPU vs 100 CPUs for original simulations. Both models accurately predicted future plasma states and captured magnetospheric responses across different runs. Probabilistic model provided well-calibrated uncertainty estimates.

Conclusion: Graph-based machine learning offers a viable path to make computationally expensive hybrid-Vlasov simulations tractable for real-time applications while providing forecast uncertainty quantification, bridging the gap between accuracy and computational efficiency.

Abstract: Hybrid-Vlasov simulations resolve ion-kinetic effects for modeling the solar wind-magnetosphere interaction, but even 5D (2D + 3V) simulations are computationally expensive. We show that graph-based machine learning emulators can learn the spatiotemporal evolution of electromagnetic fields and lower order moments of ion velocity distribution in the near-Earth space environment from four 5D Vlasiator runs performed with identical steady solar wind conditions. The initial ion number density is systematically varied, while the grid spacing is held constant, to scan the ratio of the characteristic ion skin depth to the numerical grid size. Using a graph neural network architecture operating on the 2D spatial simulation grid comprising 670k cells, we demonstrate that both a deterministic forecasting model (Graph-FM) and a probabilistic ensemble forecasting model (Graph-EFM) based on a latent variable formulation are capable of producing accurate predictions of future plasma states. A divergence penalty is incorporated during training to encourage divergence-freeness in the magnetic fields and improve physical consistency. For the probabilistic model, a continuous ranked probability score objective is added to improve the calibration of the ensemble forecasts. When trained, the emulators achieve more than two orders of magnitude speedup in generating the next time step relative to the original simulation on a single GPU compared to 100 CPUs for the Vlasiator runs, while closely matching physical magnetospheric response of the different runs. These results demonstrate that machine learning offers a way to make hybrid-Vlasov simulation tractable for real-time use while providing forecast uncertainty.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [133] [The Hamilton-Jacobi Equation and its Application to Nonlinear Beam Dynamics: Comparison of Approaches](https://arxiv.org/abs/2601.13739)
*Stephan I. Tzenov*

Main category: physics.acc-ph

TL;DR: The paper examines Hamilton-Jacobi equation applications in mechanical systems, reveals nontrivial differences between forward and backward twist maps in kick approximations, and studies statistical properties of particle beams under sextupole influence.


<details>
  <summary>Details</summary>
Motivation: To explore the rarely used Hamilton-Jacobi equation for finding mechanical system trajectories and deriving symplectic maps, while addressing overlooked differences between forward and backward twist maps in kick approximations, and to study statistical properties of particle beams under nonlinear sextupole effects.

Method: Utilizes Hamilton-Jacobi equation for trajectory analysis, employs kick approximation of Hamilton's equations in interaction representation to derive generalized one-turn twist maps, compares forward and backward map formulations, and studies statistical properties of particle beam density distributions under sextupole influence.

Result: Reveals nontrivial difference between forward and backward twist maps depending on whether nonlinear kick precedes or follows one-period rotation - a distinction typically overlooked in specialized literature. Provides analysis of statistical behavior and density distribution of particle beams under isolated sextupole influence.

Conclusion: The Hamilton-Jacobi equation provides elegant solutions for mechanical systems, but careful attention must be paid to the ordering of nonlinear kicks and rotations in twist map derivations, as this leads to fundamentally different forward vs. backward maps. The statistical analysis of particle beams under sextupole effects contributes to understanding nonlinear dynamics in accelerator physics.

Abstract: The rarely used Hamilton-Jacobi equation has been utilized as an elegant way to find the trajectories of mechanical systems and to derive symplectic maps. Further, the exact solution in kick approximation of Hamilton's equations of motion in interaction representation is written as a generalized one-turn twist map.
  One can imagine that the nonlinear kick comes first, followed by the one-period rotation along the machine circumference, or a second alternative in which the one-period rotation occurs before the kick. There is a difference in the result of solving Hamilton's equations between the two cases, which is expressed in obtaining a standard forward twist map in the first case, or alternatively a backward map in the second one. This nontrivial and intuitively unclear peculiarity is usually ignored/overlooked in practically all specialized references on the topic.
  Finally, the statistical properties and the behavior of the density distribution of a particle beam in configuration space under the influence of an isolated sextupole have been studied.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [134] [Tree tensor network states represent low-energy states faithfully](https://arxiv.org/abs/2512.20215)
*Thomas Barthel*

Main category: quant-ph

TL;DR: TTNS approximation error can be bounded using Schmidt spectra or Rényi entanglement entropies, and conversely, bounds on TTNS bond dimensions needed for specific accuracy can be derived.


<details>
  <summary>Details</summary>
Motivation: Extend previous results for matrix product states to tree tensor network states (TTNS), providing theoretical understanding of TTNS approximation capabilities and requirements.

Method: Theoretical analysis using Schmidt spectra and Rényi entanglement entropies to bound TTNS approximation errors, extending methods previously developed for matrix product states.

Result: Established bounds connecting TTNS approximation error to entanglement properties, showing efficient TTNS approximations exist for tree lattices when α<1 Rényi entanglement entropies obey area law for single-branch cuts.

Conclusion: TTNS can efficiently approximate quantum states with area-law entanglement, particularly relevant for ground and low-energy states of certain gapped systems on tree lattices.

Abstract: Extending corresponding results for matrix product states [Verstraete and Cirac, PRB 73, 094423 (2006); Schuch et al. PRL 100, 030504 (2008)], it is shown how the approximation error of tree tensor network states (TTNS) can be bounded using Schmidt spectra or Rényi entanglement entropies of the target quantum state. Conversely, one obtains bounds on TTNS bond dimensions needed to achieve a specific approximation accuracy. For tree lattices, the result implies that efficient TTNS approximations exist if $α<1$ Rényi entanglement entropies for single-branch cuts obey an area law, as in ground and low-energy states of certain gapped systems.

</details>


### [135] [The table maker's quantum search](https://arxiv.org/abs/2601.13306)
*Stefanos Kourtis*

Main category: quant-ph

TL;DR: Quantum search algorithm computes rounding hardness for elementary functions, achieving asymptotic speedup over classical methods for periodic functions.


<details>
  <summary>Details</summary>
Motivation: Determining the minimum working precision needed to compute elementary functions with correct rounding is computationally intensive. Classical algorithms struggle with this problem, especially for periodic functions over large intervals.

Method: The paper uses quantum search algorithms to compute rounding hardness. For exponential-related functions, quantum search runs in time $\tilde O(2^{n/2} \log (1/δ))$ to determine the minimum precision needed for correct rounding of n-bit floating-point inputs with probability $1-δ$.

Result: Quantum search provides asymptotic speedup over classical algorithms for periodic elementary functions in large binades. The algorithm successfully computes rounding hardness with high probability guarantees.

Conclusion: Quantum computing offers significant advantages for determining rounding hardness of elementary functions, particularly for periodic functions where it achieves asymptotic speedup over classical approaches.

Abstract: We show that quantum search can be used to compute the hardness to round an elementary function, that is, to determine the minimum working precision required to compute the values of an elementary function correctly rounded to a target precision of $n$ digits for all possible precision-$n$ floating-point inputs in a given interval. For elementary functions $f$ related to the exponential function, quantum search takes time $\tilde O(2^{n/2} \log (1/δ))$ to return, with probability $1-δ$, the hardness to round $f$ over all $n$-bit floating-point inputs in a given binade. For periodic elementary functions in large binades, standalone quantum search yields an asymptotic speedup over the best known classical algorithms and heuristics.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [136] [Multiscale Prediction of Polymer Relaxation Dynamics via Computational and Data-Driven Methods](https://arxiv.org/abs/2601.12942)
*Nguyen T. T. Duyen,Ngo T. Que,Anh D. Phan*

Main category: cond-mat.soft

TL;DR: Multiscale modeling combining MD simulations, ML, and ECNLE theory predicts polymer glass transition dynamics with good experimental agreement.


<details>
  <summary>Details</summary>
Motivation: To develop a practical and scalable tool for predicting polymer dynamic behavior, especially when experimental data are limited, by integrating computational methods.

Method: Integrated multiscale approach: 1) Molecular dynamics simulations and machine learning to predict glass transition temperatures (Tg) of four polymers, 2) Using predicted Tg as inputs to Elastically Collective Nonlinear Langevin Equation (ECNLE) theory to compute temperature-dependent relaxation times, diffusion coefficients, and dynamic fragility.

Result: Simulation-predicted Tg values show good quantitative agreement with experimental data. ML slightly overestimates Tg but yields dynamic fragility values close to experimental fragilities. Overall ECNLE calculations using these inputs agree well with broadband dielectric spectroscopy results.

Conclusion: The integrated multiscale modeling approach provides an effective tool for predicting polymer dynamic behavior, offering practical value for systems with limited experimental data.

Abstract: We present a multiscale modeling approach that integrates molecular dynamics simulations, machine learning, and the Elastically Collective Nonlinear Langevin Equation (ECNLE) theory to investigate the glass transition dynamics of polymer systems. The glass transition temperatures (Tg) of four representative polymers are estimated using simulations and machine learning model trained on experimental datasets. These predicted Tg values are used as inputs to the ECNLE theory to compute the temperature dependence of structural relaxation times and diffusion coefficients, and the dynamic fragility. The Tg values predicted from simulations show good quantitative agreement with experimental data. While machine learning tends to slightly overestimate Tg, the resulting dynamic fragility values remain close to experimental fragilities. Overall, ECNLE calculations using these inputs agree well with broadband dielectric spectroscopy results. Our integrated approach provides a practical and scalable tool for predicting the dynamic behavior of polymers, particularly in systems where experimental data are limited.

</details>


<div id='math.OA'></div>

# math.OA [[Back]](#toc)

### [137] [Abstract maximal hypoellipticity and applications](https://arxiv.org/abs/2601.13741)
*Omar Mohsen*

Main category: math.OA

TL;DR: The paper proves an abstract maximal hypoellipticity theorem showing operators are maximally hypoelliptic iff their principal symbol is left invertible, and applies it to resolve several major conjectures.


<details>
  <summary>Details</summary>
Motivation: To establish a general framework for maximal hypoellipticity that unifies and resolves various known results and conjectures in the literature, including the Rockland conjecture and Helffer-Nourrigat conjecture.

Method: Develops an abstract calculus with natural assumptions and uses C*-algebra theory (specifically Type I C*-algebras) to prove the main theorem connecting maximal hypoellipticity to left invertibility of principal symbols.

Result: Proves that in the abstract calculus, an operator is maximally hypoelliptic if and only if its principal symbol is left invertible. This result implies several important theorems: regularity for elliptic operators, Helffer-Nourrigat's resolution of Rockland conjecture, Rodino's theorem on product manifolds, and resolution of the Helffer-Nourrigat conjecture.

Conclusion: The abstract maximal hypoellipticity theorem provides a unifying framework that captures and extends multiple important results in the field, with further applications (like the microlocal Helffer-Nourrigat conjecture) to be addressed in a sequel paper.

Abstract: We prove an abstract theorem of maximal hypoellipticy showing that in an abstract calculus under some natural assumptions, an operator is maximally hypoelliptic if and only if its principal symbol is left invertible. We then show that our theorem implies various known results in the literature like regularity theorem for elliptic operators, Helffer and Nourrigat's resolution of the Rockland conjecture, Rodino's theorem on regularity of operators on products of manifolds, and our resolution of the Helffer-Nourrigat conjecture. Other examples like our resolution of the microlocal Helffer-Nourrigat conjecture will be given in a sequel to this paper.
  Our arguments are based on the theory of $C^*$-algebras of Type I.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [BladeSDF : Unconditional and Conditional Generative Modeling of Representative Blade Geometries Using Signed Distance Functions](https://arxiv.org/abs/2601.13445)
*Ashish S. Nair,Sandipp Krishnan Ravi,Itzel Salgado,Changjie Sun,Sayan Ghosh,Liping Wang*

Main category: cs.LG

TL;DR: The paper introduces a domain-specific implicit generative framework using DeepSDF for turbine blade geometry that enables performance-aware, manufacturable 3D design generation with high reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: Address critical gaps in performance-aware modeling and manufacturable design generation for turbine blades, going beyond traditional 2D-guided or unconstrained 3D pipelines.

Method: Uses DeepSDF with continuous signed distance function (SDF) representation to reconstruct/generate smooth geometries. Creates interpretable near-Gaussian latent space aligned with blade parameters. Includes neural network mapping engineering descriptors (like maximum directional strains) to latent codes for performance-informed generation.

Result: Achieves high reconstruction fidelity with surface distance errors within 1% of maximum blade dimension. Demonstrates robust generalization to unseen designs. Enables controlled exploration and unconditional synthesis through interpolation and Gaussian sampling.

Conclusion: The framework offers a practical and interpretable solution for data-driven turbine blade modeling and concept generation by integrating constraints, objectives, and performance metrics.

Abstract: Generative AI has emerged as a transformative paradigm in engineering design, enabling automated synthesis and reconstruction of complex 3D geometries while preserving feasibility and performance relevance. This paper introduces a domain-specific implicit generative framework for turbine blade geometry using DeepSDF, addressing critical gaps in performance-aware modeling and manufacturable design generation. The proposed method leverages a continuous signed distance function (SDF) representation to reconstruct and generate smooth, watertight geometries with quantified accuracy. It establishes an interpretable, near-Gaussian latent space that aligns with blade-relevant parameters, such as taper and chord ratios, enabling controlled exploration and unconditional synthesis through interpolation and Gaussian sampling. In addition, a compact neural network maps engineering descriptors, such as maximum directional strains, to latent codes, facilitating the generation of performance-informed geometry. The framework achieves high reconstruction fidelity, with surface distance errors concentrated within $1\%$ of the maximum blade dimension, and demonstrates robust generalization to unseen designs. By integrating constraints, objectives, and performance metrics, this approach advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a practical and interpretable solution for data-driven turbine blade modeling and concept generation.

</details>


### [139] [Semidefinite Programming for Quantum Channel Learning](https://arxiv.org/abs/2601.12502)
*Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin*

Main category: cs.LG

TL;DR: Quantum channel reconstruction from classical data using Semidefinite Programming (SDP) with convex optimization, achieving high fidelity with low Kraus rank channels.


<details>
  <summary>Details</summary>
Motivation: To reconstruct quantum channels from classical experimental data, which is important for quantum information processing and characterization of quantum systems.

Method: Use Semidefinite Programming (SDP) to optimize fidelity expressed as ratio of quadratic forms with respect to Choi matrix representation of quantum channels.

Result: SDP solvers successfully reconstruct various quantum channels with Kraus rank typically less than few percent of maximum, showing low-rank channels suffice for experimental data.

Conclusion: Low Kraus rank quantum channels are typically sufficient for describing experimental data, enabling efficient classical computation of quantum channel transformations.

Abstract: The problem of reconstructing a quantum channel from a sample of classical data is considered. When the total fidelity can be represented as a ratio of two quadratic forms (e.g., in the case of mapping a mixed state to a pure state, projective operators, unitary learning, and others), Semidefinite Programming (SDP) can be applied to solve the fidelity optimization problem with respect to the Choi matrix. A remarkable feature of SDP is that the optimization is convex, which allows the problem to be efficiently solved by a variety of numerical algorithms. We have tested several commercially available SDP solvers, all of which allowed for the reconstruction of quantum channels of different forms. A notable feature is that the Kraus rank of the obtained quantum channel typically comprises less than a few percent of its maximal possible value. This suggests that a relatively small Kraus rank quantum channel is typically sufficient to describe experimentally observed classical data. The theory was also applied to the problem of reconstructing projective operators from data. Finally, we discuss a classical computational model based on quantum channel transformation, performed and calculated on a classical computer, possibly hardware-optimized.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [140] [New Trends in the Stability of Sinkhorn Semigroups](https://arxiv.org/abs/2601.12633)
*Pierre Del Moral,Ajay Jasra*

Main category: math.PR

TL;DR: Review article presenting novel semigroup analysis methods for studying stability of Sinkhorn bridges, unifying and simplifying existing proofs while providing new contraction estimates.


<details>
  <summary>Details</summary>
Motivation: Entropic optimal transport problems are increasingly important in machine learning and generative modeling, but traditional convergence proofs for Sinkhorn bridges use complex methods. There's a need for more unified and simplified approaches to analyze stability properties.

Method: Introduces Sinkhorn/Gibbs-type semigroup analysis based on contraction coefficients and Lyapunov-type operator-theoretic techniques. Uses transportation cost inequalities (log-Sobolev, Talagrand), φ-divergences, Kantorovich-type criteria, and Dobrushin contraction coefficients on weighted Banach spaces and Wasserstein distances.

Result: The semigroup analysis unifies and simplifies many arguments in Sinkhorn algorithm stability. It yields new contraction estimates with respect to generalized φ-entropies, weighted total variation norms, Kantorovich criteria, and Wasserstein distances.

Conclusion: The novel semigroup approach provides powerful, off-the-shelf methods for analyzing Sinkhorn bridge stability, offering a more unified framework that improves upon traditional nonlinear Perron-Frobenius, Hilbert projective metric, and other complex techniques.

Abstract: Entropic optimal transport problems play an increasingly important role in machine learning and generative modelling. In contrast with optimal transport maps which often have limited applicability in high dimensions, Schrodinger bridges can be solved using the celebrated Sinkhorn's algorithm, a.k.a. the iterative proportional fitting procedure. The stability properties of Sinkhorn bridges when the number of iterations tends to infinity is a very active research area in applied probability and machine learning. Traditional proofs of convergence are mainly based on nonlinear versions of Perron-Frobenius theory and related Hilbert projective metric techniques, gradient descent, Bregman divergence techniques and Hamilton-Jacobi-Bellman equations, including propagation of convexity profiles based on coupling diffusions by reflection methods. The objective of this review article is to present, in a self-contained manner, recently developed Sinkhorn/Gibbs-type semigroup analysis based upon contraction coefficients and Lyapunov-type operator-theoretic techniques. These powerful, off-the-shelf semigroup methods are based upon transportation cost inequalities (e.g. log-Sobolev, Talagrand quadratic inequality, curvature estimates), $φ$-divergences, Kantorovich-type criteria and Dobrushin contraction-type coefficients on weighted Banach spaces as well as Wasserstein distances. This novel semigroup analysis allows one to unify and simplify many arguments in the stability of Sinkhorn algorithm. It also yields new contraction estimates w.r.t. generalized $φ$-entropies, as well as weighted total variation norms, Kantorovich criteria and Wasserstein distances.

</details>


### [141] [The global well-posedness for master equations of mean field games of controls](https://arxiv.org/abs/2601.11588)
*Shuhui Liu,Xintian Liu,Chenchen Mou,Defeng Sun*

Main category: math.PR

TL;DR: Global well-posedness established for master equations of mean field games of controls with interactions through joint state-control laws, under Lasry-Lions and displacement λ-monotonicity conditions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for mean field games of controls where interactions occur through the joint distribution of both states and controls, addressing the need for global well-posedness results under different monotonicity conditions.

Method: Analysis of both differential and integral versions of Lasry-Lions monotonicity and displacement λ-monotonicity conditions for nonseparable Hamiltonians, with proof relying on propagation of integral monotonicity forms and a priori uniform Lipschitz continuity with respect to measure variables.

Result: Successfully proved global well-posedness for master equations of mean field games of controls under both Lasry-Lions monotonicity and displacement λ-monotonicity conditions in their integral forms.

Conclusion: The paper provides comprehensive well-posedness results for mean field games of controls with joint state-control interactions, establishing mathematical foundations under different monotonicity frameworks and clarifying relationships between differential and integral monotonicity conditions.

Abstract: In this manuscript, we establish the global well-posedness for master equations of mean field games of controls, where the interaction is through the joint law of the state and control. Our results are proved under two different conditions: the Lasry-Lions monotonicity and the displacement $λ$-monotonicity, both considered in their integral forms. We provide a detailed analysis of both the differential and integral versions of these monotonicity conditions for the corresponding nonseparable Hamiltonian and examine their relation. The proof of global well-posedness relies on the propagation of these monotonicity conditions in their integral forms and a priori uniform Lipschitz continuity of the solution with respect to the measure variable.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [142] [An efficient numerical method for simulating two-dimensional non-periodic metasurfaces](https://arxiv.org/abs/2601.12674)
*Fuhao Liu,Ya Yan Lu*

Main category: physics.optics

TL;DR: Efficient numerical method for simulating large 2D non-periodic metasurfaces with many subwavelength elements using Neumann-to-Dirichlet operators and FEM, achieving high accuracy with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Full-wave numerical simulation of metasurfaces is computationally challenging due to the huge number of subwavelength elements, especially for large 2D non-periodic metasurfaces, creating a need for more efficient simulation methods.

Method: Developed a numerical method based on Neumann-to-Dirichlet operators, finite element method, and local function expansions that reduces the total number of unknowns by leveraging the relatively small number of distinct element types in the metasurface.

Result: The method can simulate 2D metasurfaces with 10^5 subwavelength elements on a personal computer, maintaining high accuracy while significantly reducing computational time and memory usage compared to classical full-domain FEM.

Conclusion: The proposed method is particularly well-suited for analyzing large metasurfaces, offering an efficient alternative to traditional full-wave simulation approaches for metasurface design and optimization.

Abstract: Metasurfaces are extremely useful for controlling and manipulating electromagnetic waves. Full-wave numerical simulation is highly desired for their design and optimization, but it is notoriously difficult, even for two-dimensional metasurfaces, when they comprise a huge number of subwavelength elements. This paper focuses on two-dimensional non-periodic metasurfaces that contain only a relatively small number of distinct subwavelength elements. We develop an efficient numerical method based on Neumann-to-Dirichlet operators, the finite element method and local function expansions. Our method drastically reduces the total number of unknowns and is capable of simulating two-dimensional metasurfaces with $10^{5}$ subwavelength elements on a personal computer. Numerical examples demonstrate that the method maintains high accuracy while offering significant advantages in both computational time and memory usage compared to the classical full-domain finite element method, making it particularly suited for the analysis of large metasurfaces.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [143] [Revisiting $^7$Be Weak and Radiative Transition Rates in Big Bang Nucleosynthesis: Implications for the Primordial Lithium Problem](https://arxiv.org/abs/2601.12438)
*Simone Taioli,Francesca Triggiani,Stefano Simonucci*

Main category: nucl-th

TL;DR: The paper investigates alternative depletion channels for 7Be during Big Bang Nucleosynthesis to address the lithium abundance discrepancy, finding electron capture with antineutrino channel significantly enhances depletion rates.


<details>
  <summary>Details</summary>
Motivation: To resolve the lithium abundance problem where standard BBN predicts 3-4 times more primordial 7Li than observed in old, metal-poor stars, by exploring additional 7Be depletion mechanisms beyond the standard electron capture.

Method: Used first-order perturbation theory with Fermi contact term for weak interactions, factorizing hadronic and leptonic currents. Computed thermally averaged rates by folding cross-sections with Maxwell-Boltzmann distributions, accounting for particle densities in 10-100 keV temperature range. Investigated electron capture (including antineutrino channel), positron decay, proton capture via 7Be(p,gamma)8B, stimulated emission from dense photon background, and three-body Auger-like processes.

Result: Electron capture rate decreases rapidly with temperature but is significantly enhanced by antineutrino channel. Stimulated emission and plasma screening increase radiative proton-capture by only 1-3% at ~87 keV. Auger-like channel contributes at ~0.001% level and becomes negligible at lower temperatures. Total rate revises previous estimates by nearly an order of magnitude.

Conclusion: Electron capture, proton capture, and positron decay provide corrections to the dominant 7Be(n,p)7Li depletion channel, with electron capture (especially with antineutrino channel) being the most significant enhancement, potentially helping resolve the lithium abundance discrepancy.

Abstract: The primordial 7Li abundance predicted by standard Big Bang Nucleosynthesis (BBN) exceeds that inferred from old, metal-poor stars by a factor of about 3-4. In standard BBN, most primordial 7Li is produced as 7Be in the early Universe and later converted by electron capture. Additional production or destruction channels of 7Be, such as proton capture or antineutrino capture during BBN, may therefore affect the final lithium yield. We quantify the depletion of 7Be due to in-situ electron capture, including the associated antineutrino channel, positron decay from nuclear excited states, and proton capture through the radiative 7Be(p,gamma)8B reaction. We also investigate stimulated emission induced by the dense photon background during the nuclear statistical equilibrium epoch, as well as a three-body Auger-like variant transferring the capture energy to a continuum electron. Decay rates are computed using first-order perturbation theory, modelling weak interactions with a Fermi contact term and factorising hadronic and leptonic currents. Thermally averaged rates are obtained by folding cross-sections with Maxwell-Boltzmann distributions and accounting for particle densities in the temperature range 10-100 keV. We find that the electron-capture rate decreases rapidly with temperature and is significantly enhanced by the inclusion of the antineutrino channel. Stimulated emission and plasma screening increase the radiative proton-capture rate by only 1-3 percent at temperatures around 87 keV. The Auger-like channel contributes at the level of a few thousandths of a percent and becomes negligible at lower temperatures. Overall, our total rate revises previous estimates by nearly an order of magnitude. Electron capture, proton capture, and positron decay provide corrections to the dominant depletion channel 7Be(n,p)7Li.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [144] [Qualitative analysis and numerical investigations of time-fractional Zika virus model arising in population dynamics](https://arxiv.org/abs/2601.11636)
*Gaurav Saini,Bappa Ghosh,Sunita Chand*

Main category: math.DS

TL;DR: This paper develops a time-fractional mathematical model for Zika virus transmission using Caputo fractional derivatives, analyzes its qualitative properties, and implements numerical simulations to study disease dynamics.


<details>
  <summary>Details</summary>
Motivation: Epidemic models are crucial for understanding disease transmission and aiding in prediction and control. The authors aim to develop a fractional-order model for Zika virus transmission that can provide deeper insights into disease dynamics compared to classical integer-order models.

Method: The authors use Caputo fractional derivatives (order α∈(0,1)) to create a time-fractional Zika virus transmission model. They conduct qualitative analysis using stability theory, prove existence and uniqueness of solutions, perform Hyers-Ulam stability analysis, and develop an efficient numerical scheme using the standard L1 technique with Newton-Raphson method for solving the resulting nonlinear algebraic system.

Result: The paper establishes the existence, uniqueness, and stability of solutions for the fractional Zika model. Numerical simulations demonstrate that the fractional model provides deeper insights into disease dynamics, and graphical results validate the theoretical findings, showing the model's utility for understanding virus spread.

Conclusion: The fractional-order Zika virus transmission model offers superior insights into disease dynamics compared to classical models. The developed framework aids in controlling the virus through contact precautions and therapies while helping predict future spread, demonstrating the value of fractional calculus in epidemiological modeling.

Abstract: Epidemic models play a crucial role in population dynamics, offering valuable insights into disease transmission while aiding in epidemic prediction and control. In this paper, we analyze the mathematical model of the time-fractional Zika virus transmission for human and mosquito populations. The fractional derivative is considered in the Caputo sense of order $α\in(0,1).$ We begin by conducting a qualitative analysis using the stability theory of differential equations. The existence and uniqueness of the solution are established, and the model's stability is examined through Hyers-Ulam stability analysis. Furthermore, an efficient difference scheme utilizing the standard L1 technique is developed to simulate the model and analyze the solution's behavior under key parameters. The resulting nonlinear algebraic system is solved using the Newton-Raphson method. Finally, illustrative examples are presented to validate the theoretical findings. Graphical results indicate that the fractional model provides deeper insights and a better understanding of disease dynamics. These findings aid in controlling the virus through contact precautions and recommended therapies while also helping to predict its future spread.

</details>


### [145] [Gromov-Hausdorff stability of global attractors of damped wave equations under perturbations of the domain](https://arxiv.org/abs/2601.13650)
*Ngoctu Bui,Jihoon Lee*

Main category: math.DS

TL;DR: The paper establishes continuous dependence and Gromov-Hausdorff stability of global attractors for damped wave equations under domain perturbations.


<details>
  <summary>Details</summary>
Motivation: To understand how global attractors for damped wave equations behave under perturbations of the domain, ensuring mathematical stability and continuity properties.

Method: Uses Gromov-Hausdorff distance between compact metric spaces to analyze the stability of global attractors for damped wave equations when the domain is perturbed.

Result: Demonstrates continuous dependence and Gromov-Hausdorff stability of global attractors under domain perturbations for damped wave equations.

Conclusion: The Gromov-Hausdorff framework provides a rigorous way to establish stability properties of global attractors for damped wave equations when the domain undergoes perturbations.

Abstract: In this paper, we will make use of the Gromov-Hausdorff distance between compact metric spaces to establish the continuous dependence and the Gromov-Hausdorff stability of global attractors for damped wave equations under perturbations of the domain.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [146] [Optimal bounds for the boundary control cost of one-dimensional fractional Schrödinger and heat equations](https://arxiv.org/abs/2601.12810)
*Hoai-Minh Nguyen*

Main category: math.OC

TL;DR: Sharp bounds derived for boundary control cost of 1D fractional Schrödinger and heat equations using complex analysis and moment methods.


<details>
  <summary>Details</summary>
Motivation: To establish precise mathematical bounds for the control cost of fractional PDEs, which is important for understanding controllability and practical control design for these systems.

Method: Lower bound analysis uses complex analysis tools to study related singular boundary control problem. Upper bound analysis uses moment method with Fourier transform estimates of compactly supported functions.

Result: Derived sharp bounds for boundary control cost of one-dimensional fractional Schrödinger and heat equations.

Conclusion: Successfully established precise mathematical bounds for control costs of fractional PDEs using complementary analytical approaches.

Abstract: We derive sharp bounds for the boundary control cost of the one-dimensional fractional Schrödinger and heat equations. The analysis of the lower bound is based on the study of the control cost of a related singular boundary control problem in finite time, using tools from complex analysis. The analysis of the upper bound relies on the moment method, involving estimates of the Fourier transform of a class of compactly supported functions.

</details>


### [147] [Long-time behavior of solutions to fluid dynamic shape optimization problems via phase-field method](https://arxiv.org/abs/2601.13293)
*Michael Hinze,Christian Kahle,John Sebastian H. Simon*

Main category: math.OC

TL;DR: Time-dependent shape/topology optimization for Navier-Stokes flows converges to stationary problem solutions as time horizon extends to infinity, with proven convergence rates.


<details>
  <summary>Details</summary>
Motivation: Extend earlier work on stationary shape/topology optimization to time-dependent Navier-Stokes problems, investigating long-term behavior and convergence to stationary solutions.

Method: Use phase-field representation for topology, porous media approximation for fluid equations, analyze convergence as time horizon → ∞, derive convergence rates analytically, validate numerically.

Result: Proved convergence of time-dependent problem minima to stationary problem minima with explicit convergence rates; numerical validation confirms analytical results.

Conclusion: Time-dependent shape optimization solutions approach stationary solutions as time extends to infinity, providing theoretical foundation for long-term optimization behavior.

Abstract: We investigate the long time behavior of solutions to a shape and topology optimization problem with respect to the time-dependent Navier--Stokes equations. The sought topology is represented by a stationary phase-field that represents a smooth indicator function. The fluid equations are approximated by a porous media approach and are time-dependent. In the latter aspect, the considered problem formulation extends earlier work.
  We prove that if the time horizon tends to infinity, minima of the time-dependent problem converge towards minima of the corresponding stationary problem. To do so, a convergence rate with respect to the time horizon, of the values of the objective functional, is analytically derived. This allowed us to prove that the solution to the time-dependent problem converges to a phase-field, as the time horizon goes to infinity, which is proven to be a minimizer for the stationary problem. We validate our results by numerical investigation.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [148] [The CP-PAW code package for first-principles calculations from a user's perspective](https://arxiv.org/abs/2601.12004)
*Peter E. Blöchl,Robert Schade,Lukas Allen-Rump,Sangeeta Rajpurohit,Amrith Rathnakaran,Konstantin Tamoev,Mani Lokamani,Thomas D. Kühne*

Main category: cond-mat.mtrl-sci

TL;DR: CP-PAW is a combined electronic structure and ab-initio molecular dynamics code for atomistic condensed phase systems that unifies the all-electron projector augmented-wave method with Car-Parrinello approach.


<details>
  <summary>Details</summary>
Motivation: To provide a unified computational tool for performing mixed quantum and classical simulations of atomistic condensed phase systems (solids, liquids, molecular systems) that can determine both electronic/nuclear ground states and study their properties and dynamics.

Method: Combines the all-electron projector augmented-wave (PAW) method with the Car-Parrinello approach for ab-initio molecular dynamics, enabling mixed quantum and classical simulations.

Result: Development of the CP-PAW code that unifies these two powerful methods, with focus on unique aspects of the implementation and practical guidance for users, including installation via a new build system.

Conclusion: CP-PAW provides a comprehensive computational framework for studying condensed matter systems by integrating PAW and Car-Parrinello methods, with practical implementation details and user guidance being key contributions.

Abstract: CP-PAW is a combined electronic structure and ab-initio molecular dynamics code to perform mixed quantum and classical simulations of atomistic condensed phase systems, such as solids, liquids, and molecular systems. As the name suggests, the CP-PAW code unifies the all-electron projector augmented-wave method with the Car-Parrinello approach to determine not only the electronic and nuclear ground state of condensed matter, but also to study their properties and dynamics. In addition to briefly outlining the underlying theory, the focus will be on unique aspects of CP-PAW and how to correctly employ them as a user. How to install CP-PAW using the new build system will also be briefly mentioned.

</details>


### [149] [Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectorie](https://arxiv.org/abs/2601.12554)
*Iman Peivaste,Salim Belouettar,Francesco Mercuri,Nicholas Fantuzzi,Hamidreza Dehghani,Razieh Izadi,Halliru Ibrahim,Jakub Lengiewicz,Maël Belouettar-Mathis,Kouider Bendine,Ahmed Makradi,Martin Hörsch,Peter Klein,Mohamed El Hachemi,Heinz A. Preisig,Yacine Rezgui,Natalia Konchakova,Ali Daouadji*

Main category: cond-mat.mtrl-sci

TL;DR: This review paper provides a comprehensive overview of AI applications in materials science, covering machine learning approaches, data representation strategies, and current challenges in the field.


<details>
  <summary>Details</summary>
Motivation: AI is rapidly transforming materials science by offering tools to navigate complexity, accelerate discovery, and optimize material design. The accelerating pace of algorithmic advancements and increasing data availability make AI an essential competency for materials researchers.

Method: The paper conducts a structured review synthesizing recent advancements and methodologies. It surveys machine learning approaches from traditional algorithms to advanced deep learning architectures (CNNs, GNNs, Transformers), generative AI, and probabilistic models like Gaussian Processes for uncertainty quantification.

Result: The review examines the pivotal role of data in materials AI, emphasizing how effective representation and featurization strategies (compositional, structural, image-based, language-inspired approaches) combined with appropriate preprocessing fundamentally underpin ML model performance in materials research.

Conclusion: The paper addresses persistent challenges related to data quality, quantity, and standardization that critically impact model development and application in materials science and engineering, providing a comprehensive overview of the current AI landscape in materials research.

Abstract: Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. Driven by the accelerating pace of algorithmic advancements and increasing data availability, AI is becoming an essential competency for materials researchers. This review provides a comprehensive and structured overview of the current landscape, synthesizing recent advancements and methodologies for materials scientists seeking to effectively leverage these data-driven techniques. We survey the spectrum of machine learning approaches, from traditional algorithms to advanced deep learning architectures, including CNNs, GNNs, and Transformers, alongside emerging generative AI and probabilistic models such as Gaussian Processes for uncertainty quantification. The review also examines the pivotal role of data in this field, emphasizing how effective representation and featurization strategies, spanning compositional, structural, image-based, and language-inspired approaches, combined with appropriate preprocessing, fundamentally underpin the performance of machine learning models in materials research. Persistent challenges related to data quality, quantity, and standardization, which critically impact model development and application in materials science and engineering, are also addressed.

</details>


### [150] [Disentangling the Discrepancy Between Theoretical and Experimental Curie Temperatures in Ferroelectric PbTiO$_3$](https://arxiv.org/abs/2601.13125)
*Denan Li,Chris Ahart,Shi Liu*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles predictions of ferroelectric Curie temperature (Tc) in PbTiO3 are systematically underestimated due to exchange-correlation functional limitations, not MLFF fitting errors. Short-range MLFFs show fortuitous agreement with experiments from error cancellation, while explicit long-range interactions improve accuracy but yield lower Tc values.


<details>
  <summary>Details</summary>
Motivation: There's a persistent discrepancy where theoretical predictions of ferroelectric Curie temperatures (Tc) consistently fall below experimental values. The paper aims to investigate the origin of these underestimations in the prototypical ferroelectric PbTiO3.

Method: Used extensive constant-pressure ab initio molecular dynamics (AIMD) simulations benchmarked against classical molecular dynamics with machine learning force fields (MLFFs) derived from first-principles data. Investigated finite-size effects, range of interatomic interactions, and exchange-correlation functional limitations.

Result: The underestimation of Tc primarily stems from limitations of the exchange-correlation functional rather than MLFF fitting inaccuracies. Short-range MLFFs show better agreement with experimental Tc due to fortuitous error cancellation. Explicit long-range interactions improve accuracy for larger supercells but lead to lower predicted Tc values.

Conclusion: Accurate finite-temperature predictions require: high-quality training data, sufficiently large simulation cells, explicit treatment of long-range interactions, and improved exchange-correlation functionals. Current underestimations are fundamentally limited by DFT functional accuracy.

Abstract: Accurately predicting the Curie temperature ($T_c$) of ferroelectrics from first principles remains a major challenge, as theoretical estimates often fall significantly below experimental values. In this work, we investigate the origin of these discrepancies in the prototypical ferroelectric PbTiO$_3$ by performing extensive constant-pressure ab initio molecular dynamics (AIMD) simulations and benchmarking them against classical molecular dynamics (MD) using machine learning force fields (MLFFs) derived from first-principles data. Our results show that the underestimation of $T_c$ primarily stems from the limitations of the exchange-correlation functional, rather than inaccuracies in the MLFF fitting. We uncover a critical interplay between finite-size effects and the range of interatomic interactions: although short-range MLFFs appear to yield better agreement with experimental $T_c$, this improvement results from a fortuitous cancellation of errors. Incorporating explicit long-range interactions improves accuracy for larger supercells but ultimately leads to lower predicted $T_c$ values. These findings highlight that accurate finite-temperature predictions require not only high-quality training data and sufficiently large simulation cells, but also the explicit treatment of long-range interactions and improved exchange-correlation functionals.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [151] [Moving Least Squares without Quasi-Uniformity: A Stochastic Approach](https://arxiv.org/abs/2601.13782)
*Shir Tapiro-Moshe,Yariv Aizenbud,Barak Sober*

Main category: math.ST

TL;DR: This paper provides the first unified stochastic analysis of Moving Least Squares (MLS), establishing that classical convergence and smoothness properties persist under random sampling despite deterministic assumptions failing.


<details>
  <summary>Details</summary>
Motivation: Local Polynomial Regression (LPR) and Moving Least Squares (MLS) are closely related methods developed independently in statistics and approximation theory. While statistical LPR focuses on sampling noise under probabilistic assumptions, deterministic MLS theory studies smoothness and convergence with respect to fill-distance. However, deterministic MLS assumptions fail under random sampling, creating a gap in understanding MLS behavior in practical random sampling scenarios.

Method: The authors first quantify probabilistic behavior of fill-distance and separation for i.i.d. random samples under mild regularity conditions. They then prove that for MLS of degree k-1, approximation error for differential operators decays as h_n^(k-|m|) up to logarithmic factors. They also show MLS approximants are smooth with high probability. Finally, they apply stochastic MLS theory to manifold estimation, showing Hausdorff distance decays as h_n^k for k-times smooth manifolds.

Result: Key results: 1) For i.i.d. random samples, h_n ∝ n^(-1/d)log^(1/d)(n) and δ_n ∝ n^(-1/d). 2) MLS approximation error decays as h_n^(k-|m|) up to logarithmic factors. 3) MLS approximants are smooth with high probability. 4) For manifold estimation, Hausdorff distance decays as h_n^k, extending deterministic guarantees to random samples.

Conclusion: This work provides the first unified stochastic analysis of MLS, demonstrating that despite the failure of deterministic sampling assumptions, classical convergence and smoothness properties persist under natural probabilistic models. The results bridge the gap between statistical LPR and deterministic MLS theories, establishing stochastic analogues of classical MLS estimates.

Abstract: Local Polynomial Regression (LPR) and Moving Least Squares (MLS) are closely related nonparametric estimation methods, developed independently in statistics and approximation theory. While statistical LPR analysis focuses on overcoming sampling noise under probabilistic assumptions, the deterministic MLS theory studies smoothness properties and convergence rates with respect to the \textit{fill-distance} (a resolution parameter). Despite this similarity, the deterministic assumptions underlying MLS fail to hold under random sampling. We begin by quantifying the probabilistic behavior of the fill-distance $h_n$ and \textit{separation} $δ_n$ of an i.i.d. random sample. That is, for a distribution satisfying a mild regularity condition, $h_n\propto n^{-1/d}\log^{1/d} (n)$ and $δ_n \propto n^{-1/d}$. We then prove that, for MLS of degree $k\!-\!1$, the approximation error associated with a differential operator $Q$ of order $|m|\le k-1$ decays as $h_n^{\,k-|m|}$ up to logarithmic factors, establishing stochastic analogues of the classical MLS estimates. Additionally, We show that the MLS approximant is smooth with high probability. Finally, we apply the stochastic MLS theory to manifold estimation. Assuming that the sampled Manifold is $k$-times smooth, we show that the Hausdorff distance between the true manifold and its MLS reconstruction decays as $h_n^k$, extending the deterministic Manifold-MLS guarantees to random samples. This work provides the first unified stochastic analysis of MLS, demonstrating that -- despite the failure of deterministic sampling assumptions -- the classical convergence and smoothness properties persist under natural probabilistic models

</details>


### [152] [Inverting the Fisher information operator in non-linear models](https://arxiv.org/abs/2601.13254)
*Dimitri Konen*

Main category: math.ST

TL;DR: The paper establishes theoretical foundations for efficient inference in non-linear regression models with generic noise, showing that injective score functions guarantee invertible Fisher information operators, enabling construction of optimal Gaussian lower bounds achieved by Bayesian methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop rigorous theoretical foundations for statistical inference in non-linear regression models corrupted by generic noise, particularly relevant for non-linear PDE inverse problems and data assimilation where regression functions form non-linear subspaces.

Method: The method involves analyzing non-linear regression models with generic noise, proving that when the score function is injective, the Fisher information operator becomes automatically invertible between well-identified Hilbert spaces. The authors provide operational characterization of these spaces and construct efficient Gaussian distributions for establishing information lower bounds.

Result: The main result shows that injective score functions guarantee invertibility of Fisher information operators, enabling construction of efficient Gaussian distributions for minimax and convolution theorems. This establishes information lower bounds that are typically achieved by Bayesian algorithms, demonstrating optimality of these methods.

Conclusion: The paper provides a general theoretical framework for optimal inference in non-linear regression models with generic noise, showing that Bayesian methods achieve optimality when score functions are injective. The results are illustrated on time-evolution PDE models including reaction-diffusion and Navier-Stokes equations.

Abstract: We consider non-linear regression models corrupted by generic noise when the regression functions form a non-linear subspace of L^2, relevant in non-linear PDE inverse problems and data assimilation. We show that when the score of the model is injective, the Fisher information operator is automatically invertible between well-identified Hilbert spaces, and we provide an operational characterization of these spaces. This allows us to construct in broad generality the efficient Gaussian involved in the classical minimax and convolution theorems to establish information lower bounds, that are typically achieved by Bayesian algorithms thus showing optimality of these methods. We illustrate our results on time-evolution PDE models for reaction-diffusion and Navier-Stokes equations.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [153] [sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation](https://arxiv.org/abs/2601.12029)
*Sandy H. S. Herho,Faruq Khadami,Iwan P. Anwar,Dasapta E. Irawan*

Main category: nlin.PS

TL;DR: sangkuriang is an open-source Python library for solving the KdV equation using Fourier pseudo-spectral methods with adaptive time integration, validated through soliton propagation and collision tests while maintaining computational efficiency and accessibility.


<details>
  <summary>Details</summary>
Motivation: To create an accessible, efficient, and accurate computational tool for solving the foundational KdV equation, suitable for both educational demonstrations and research exploration of nonlinear wave phenomena and soliton dynamics.

Method: Uses Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration, implemented in Python with JIT compilation for computational efficiency. Validated through progressively complex scenarios: isolated soliton propagation, symmetric two-wave configurations, overtaking collisions, and three-body interactions.

Result: The library successfully solves KdV equation with high accuracy - conservation of classical invariants remains small across all test cases, soliton velocities conform closely to theoretical predictions, and solutions preserve the regular phase-space structure expected for integrable systems.

Conclusion: sangkuriang provides an accurate, efficient, and accessible platform for solving the KdV equation on modest computational resources, suitable for both classroom demonstrations and exploratory research into nonlinear wave phenomena and soliton dynamics.

Abstract: The Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration. The implementation leverages just-in-time (JIT) compilation for computational efficiency while maintaining accessibility for instructional purposes. Validation encompasses progressively complex scenarios including isolated soliton propagation, symmetric two-wave configurations, overtaking collisions between waves of differing amplitudes, and three-body interactions. Conservation of the classical invariants is monitored throughout, with deviations remaining small across all test cases. Measured soliton velocities conform closely to theoretical predictions based on the amplitude-velocity relationship characteristic of integrable systems. Complementary diagnostics drawn from information theory and recurrence analysis confirm that computed solutions preserve the regular phase-space structure expected for completely integrable dynamics. The solver outputs data in standard scientific formats compatible with common analysis tools and generates visualizations of spatiotemporal wave evolution. By combining numerical accuracy with practical accessibility on modest computational resources, sangkuriang offers a platform suitable for both classroom demonstrations of nonlinear wave phenomena and exploratory research into soliton dynamics.

</details>
