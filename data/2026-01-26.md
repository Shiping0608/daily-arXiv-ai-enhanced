<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 13]
- [math.AP](#math.AP) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [nlin.CD](#nlin.CD) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [math.SP](#math.SP) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Numerical efficiency of explicit time integrators for phase-field models](https://arxiv.org/abs/2601.16522)
*Marco Seiz,Tomohiro Takaki*

Main category: math.NA

TL;DR: Comparison of explicit time integrators for phase-field models shows speedups of 4-114x over forward Euler with minimal accuracy loss, using reproducible benchmarks with exact sharp interface solutions.


<details>
  <summary>Details</summary>
Motivation: Phase-field simulations are computationally expensive, and there's a need to improve efficiency while maintaining accuracy for coupled phase-field and concentration models.

Method: Adapted explicit time integrators to phase-field constraints and storage schemes, defined reproducible benchmarks with exact sharp interface solutions to identify dominant error terms.

Result: Achieved speedups of 4 to 114 times over classic forward Euler integrator while maintaining comparable accuracy with fully explicit schemes.

Conclusion: Significant computational efficiency improvements are possible for phase-field simulations through optimized explicit time integrators, demonstrated in applications like final stage sintering.

Abstract: Phase-field simulations are a practical but also expensive tool to calculate microstructural evolution. This work aims to compare explicit time integrators for a broad class of phase-field models involving coupling between the phase-field and concentration. Particular integrators are adapted to constraints on the phase-field as well as storage scheme implications. Reproducible benchmarks are defined with a focus on having exact sharp interface solutions, allowing for identification of dominant error terms. Speedups of 4 to 114 over the classic forward Euler integrator are achievable while still using a fully explicit scheme without appreciable accuracy loss. Application examples include final stage sintering with pores slowing down grain growth as they move and merge over time.

</details>


### [2] [The inverse of the star discrepancy of a union of randomly shifted Korobov rank-1 lattice point sets depends polynomially on the dimension](https://arxiv.org/abs/2601.16571)
*Jiarui Du,Josef Dick*

Main category: math.NA

TL;DR: The paper analyzes star discrepancy bounds for multiset unions of Korobov rank-1 lattice point sets modulo a prime, showing O(s log(N)/√N) discrepancy with high probability across four construction scenarios.


<details>
  <summary>Details</summary>
Motivation: While the inverse star discrepancy N(ε,s) depends linearly on dimension s, explicit constructions achieving this optimal linear dependence are elusive. Recent work by Dick and Pillichshammer (2025) showed near-optimal dimension dependence using digitally shifted Korobov polynomial lattice points. This paper investigates whether similar results can be achieved using classical integer arithmetic instead of digital constructions.

Method: The paper analyzes point sets constructed as multiset unions of Korobov rank-1 lattice point sets modulo a prime N. Four distinct construction scenarios are examined: combinations of random/fixed integer generators with continuous torus shifts or discrete grid shifts. Fourier analysis is used to analyze these constructions in the classical integer arithmetic setting.

Result: For all four construction scenarios, the star discrepancy is bounded by O(s log(N_tot)/√N_tot) with high probability, where N_tot is the total number of points. This implies the inverse star discrepancy for these structured sets depends quadratically on dimension s (N(ε,s) = O(s²/ε²)).

Conclusion: While the proofs are probabilistic, the results significantly reduce the search space for optimal point sets from a continuum to a finite set of candidates parameterized by integer generators and random shifts. The quadratic dimension dependence represents progress toward finding explicit constructions with optimal linear dimension dependence.

Abstract: The inverse of the star discrepancy, $N(ε, s)$, defined as the minimum number of points required to achieve a star discrepancy of at most $ε$ in dimension $s$, is known to depend linearly on $s$. However, explicit constructions achieving this optimal linear dependence remain elusive. Recently, Dick and Pillichshammer (2025) made significant progress by showing that a multiset union of randomly digitally shifted Korobov polynomial lattice point sets almost achieve the optimal dimension dependence with high probability.
  In this paper, we investigate the analog of this result in the setting of classical integer arithmetic using Fourier analysis. We analyze point sets constructed as multiset unions of Korobov rank-1 lattice point sets modulo a prime $N$. We provide a comprehensive analysis covering four distinct construction scenarios, combining either random or fixed integer generators with either continuous torus shifts or discrete grid shifts. We prove that in all four cases, the star discrepancy is bounded by a term of order $O(s \log(N_{tot}) / \sqrt{N_{tot}})$ with high probability, where $N_{tot}$ is the total number of points. This implies that the inverse of the star discrepancy for these structured sets depends quadratically on the dimension $s$. While the proofs are probabilistic, our results significantly reduce the search space for optimal point sets from a continuum to a finite set of candidates parameterized by integer generators and random shifts.

</details>


### [3] [A High-resolution Spatiotemporal Coupling Ghost Fluid Method for Two-Dimensional Compressible Multimedium Flows with Source Terms](https://arxiv.org/abs/2601.16590)
*Zhixin Huo*

Main category: math.NA

TL;DR: Novel ghost fluid method integrates nonlinear geometrical optics for entropy evolution and Lax-Wendroff/Cauchy-Kowalevski approach for tangential fluxes, overcoming limitations of traditional Riemann solvers.


<details>
  <summary>Details</summary>
Motivation: Traditional Riemann solvers and ghost fluid methods have two key limitations: 1) They fail to represent continuous entropy transport processes, causing thermodynamic incompatibility for compressible flows. 2) They only consider normal components at interfaces while neglecting tangential flux and source term effects, making them unsuitable for multidimensional problems and cases with source terms.

Method: Developed a novel spatiotemporal coupling high-resolution ghost fluid method with two key advancements: 1) Integration of nonlinear geometrical optics to properly account for thermodynamic entropy evolution. 2) Implementation of the Lax-Wendroff/Cauchy-Kowalevski approach to incorporate tangential fluxes and source term effects. These enhancements were systematically applied to Riemann problem-based ghost fluid methods.

Result: Comprehensive numerical experiments demonstrate significant improvements in simulation accuracy and robustness compared to conventional approaches.

Conclusion: The proposed method successfully addresses the fundamental limitations of traditional Riemann solvers and ghost fluid methods by properly handling entropy evolution and incorporating tangential flux/source term effects, leading to more accurate and robust simulations for compressible flows and multidimensional problems.

Abstract: While exact and approximate Riemann solvers are widely used, they exhibit two fundamental limitations: 1) Fail to represent continuous entropy transport processes, resulting in thermodynamic incompatibility that limits their applicability to compressible flows. 2) Consider only the effects of normal components at interfaces while neglecting the effects of tangential flux and source term, making them unsuitable for multidimensional problems and cases involving source terms. These limitations persist in Riemann problem-based ghost fluid methods. To address these challenges, we developed a novel spatiotemporal coupling high-resolution ghost fluid method featuring two key advancements: 1) Integration of nonlinear geometrical optics to properly account for thermodynamic entropy evolution. 2) Implementation of the Lax-Wendroff/Cauchy-Kowalevski approach to incorporate tangential fluxes and source term effects. These enhancements have been systematically applied to Riemann problem-based ghost fluid methods. Comprehensive numerical experiments demonstrate significant improvements in simulation accuracy and robustness compared to conventional approaches.

</details>


### [4] [A robust and stable hybrid neural network/finite element method for 2D flows that generalizes to different geometries](https://arxiv.org/abs/2601.16598)
*Robert Jendersie,Nils Margenberg,Christian Lessig,Thomas Richter*

Main category: math.NA

TL;DR: DNN-MG combines coarse-grid FEM with DNN correction on finer grids, improving efficiency for Navier-Stokes equations. Enhanced with replay buffers, retraining, and Transformer architectures for better accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency for solving instationary Navier-Stokes equations by combining traditional numerical methods with deep learning, addressing the need for more efficient and accurate simulations.

Method: DNN-MG method with coarse-grid finite element simulation + deep neural network correction on finer grids. Uses replay buffers for robustness, retraining on hybrid simulation data without differentiable solver. Compares RNNs and Transformers for temporal/spatial receptive field utilization.

Result: Significant improvements in accuracy and generalizability. Transformers enable use of information from cells outside predicted patches on unstructured meshes while maintaining locality, further improving accuracy without major performance impact.

Conclusion: DNN-MG with proper design choices (replay buffers, retraining, Transformer architectures) provides efficient, accurate, and generalizable solution for Navier-Stokes equations, successfully combining traditional numerical methods with deep learning.

Abstract: The deep neural network multigrid solver (DNN-MG) combines a coarse-grid finite element simulation with a deep neural network that corrects the solution on finer grid levels, thereby improving the computational efficiency. In this work, we discuss various design choices for the DNN-MG method and demonstrate significant improvements in accuracy and generalizability when applied to the solution of the instationary Navier-Stokes equations. We investigate the stability of the hybrid simulation and show how the neural networks can be made more robust with the help of replay buffers. By retraining on data derived from the hybrid simulation, the error caused by the neural network over multiple time-steps can be minimized without the need for a differentiable numerical solver. Furthermore, we compare multiple neural network architectures, including recurrent neural networks and Transformers, and study their ability to utilize more information from an increased temporal and spatial receptive field. Transformers allow us to make use of information from cells outside the predicted patch even with unstructured meshes while maintaining the locality of our approach. This can further improve the accuracy of DNN-MG without a significant impact on performance.

</details>


### [5] [Convergent adaptive iterative schemes for solving multi-physics problems](https://arxiv.org/abs/2601.16640)
*Jakob S. Stokke,Kundan Kumar,Florin A. Radu*

Main category: math.NA

TL;DR: A framework for creating adaptive iterative algorithms for multi-physics problems with a posteriori estimators to predict method success/failure, enabling adaptive switching, time-stepping, and parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Multi-physics problems often require iterative methods (linearization or splitting), but these methods can fail or be inefficient. There's a need for practical, general frameworks that can adaptively control iterative algorithms based on real-time performance indicators.

Method: Derive a posteriori estimators to predict success or failure of iterative methods. Use these estimators to create adaptive algorithms including: 1) adaptively switching between different methods, 2) adaptive time-stepping methods, and 3) adaptive tuning of stabilization parameters.

Result: The framework is applied to three multi-physics problems: two-phase flow in porous media, surfactant transport in porous media, and quasi-static poroelasticity, demonstrating its practical utility across different physical domains.

Conclusion: The paper presents a general, practical framework for adaptive control of iterative algorithms in multi-physics simulations, enabling more robust and efficient computations through real-time adaptation based on a posteriori error estimators.

Abstract: In this paper, we derive a practical, general framework for creating adaptive iterative (linearization or splitting) algorithms to solve multi-physics problems. This means that, given an iterative method, we derive \textit{a posteriori} estimators to predict the success or failure of the method. Based on these estimators, we propose adaptive algorithms, including adaptively switching between methods, adaptive time-stepping methods, and the adaptive tuning of stabilization parameters. We apply this framework to two-phase flow in porous media, surfactant transport in porous media, and quasi-static poroelasticity.

</details>


### [6] [A Predictor Corrector Convex Splitting Method for Stefan Problems Based on Extreme Learning Machines](https://arxiv.org/abs/2601.16655)
*Siyuan Lang,Zhiyue Zhang*

Main category: math.NA

TL;DR: Proposes an Operator Splitting Method using Extreme Learning Machines to solve Stefan free boundary problems by decoupling interface evolution from field reconstruction via alternating convex subproblems.


<details>
  <summary>Details</summary>
Motivation: Neural network solutions for Stefan problems face challenges from nonlinear coupling between solutions and free boundaries, resulting in non-convex optimization that's difficult to solve.

Method: Uses Operator Splitting Method with Extreme Learning Machines in a predictor-corrector framework. Splits the coupled system into two alternating linear/convex subproblems: 1) solving diffusion equation on fixed domains, 2) updating interface geometry based on Stefan condition. Both steps formulated as linear least-squares problems.

Result: Method transforms computational strategy from non-convex gradient-based optimization to stable fixed-point iteration with alternating convex solvers. Theoretical analysis shows relaxed iterative operator is locally contractive. Benchmarks across 1D to 3D domains demonstrate stability and high accuracy.

Conclusion: The proposed framework provides a highly accurate and efficient numerical solution for free boundary problems by decoupling geometric interface evolution from physical field reconstruction through operator splitting.

Abstract: Solving Stefan problems via neural networks is inherently challenged by the nonlinear coupling between the solutions and the free boundary, which results in a non-convex optimization problem. To address this, this work proposes an Operator Splitting Method (OSM) based on Extreme Learning Machines (ELM) to decouple the geometric interface evolution from the physical field reconstruction. Within a predictor-corrector framework, the method splits the coupled system into an alternating sequence of two linear and convex subproblems: solving the diffusion equation on fixed subdomains and updating the interface geometry based on the Stefan condition. A key contribution is the formulation of both steps as linear least-squares problems; this transforms the computational strategy from a non-convex gradient-based optimization into a stable fixed-point iteration composed of alternating convex solvers. From a theoretical perspective, the relaxed iterative operator is shown to be locally contractive, and its fixed points are consistent with stationary points of the coupled residual functional. Benchmarks across 1D to 3D domains demonstrate the stability and high accuracy of the method, confirming that the proposed framework provides a highly accurate and efficient numerical solution for free boundary problems.

</details>


### [7] [Distance to nearest skew-symmetric matrix polynomials of bounded rank](https://arxiv.org/abs/2601.16676)
*Andrii Dmytryshyn,Froilán M. Dopico,Rakel Hellberg*

Main category: math.NA

TL;DR: Algorithm approximates matrix polynomials with skew-symmetric ones of specified even rank and bounded degree using eigenstructure theory.


<details>
  <summary>Details</summary>
Motivation: Need methods to approximate general matrix polynomials with skew-symmetric matrix polynomials of controlled rank and degree, building on recent theoretical advances in skew-symmetric matrix polynomial eigenstructures.

Method: Algorithm leverages generic eigenstructure and factorization theory for skew-symmetric matrix polynomials with bounded rank and degree. Adapts specialized version for matrix pencils to improve performance.

Result: Algorithm produces skew-symmetric matrix polynomials of exact prescribed even rank (≥2) and degree ≤d. Numerical experiments demonstrate performance and compare to existing methods.

Conclusion: Effective algorithm developed for approximating matrix polynomials with skew-symmetric ones of controlled rank and degree, with improved performance for matrix pencil cases.

Abstract: We propose an algorithm that approximates a given matrix polynomial of degree $d$ by another skew-symmetric matrix polynomial of a specified rank and degree at most $d$. The algorithm is built on recent advances in the theory of generic eigenstructures and factorizations for skew-symmetric matrix polynomials of bounded rank and degree. Taking into account that the rank of a skew-symmetric matrix polynomial is even, the algorithm works for any prescribed even rank greater than or equal to $2$ and produces a skew-symmetric matrix polynomial of that exact rank. We also adapt the algorithm for matrix pencils to achieve a better performance. Lastly, we present numerical experiments for testing our algorithms and for comparison to the previously known ones.

</details>


### [8] [Barotropic-Baroclinic Splitting for Multilayer Shallow Water Models with Exchanges](https://arxiv.org/abs/2601.16709)
*Nina Aguillon,Sophie Hörnschemeyer,Jacques Sainte-Marie*

Main category: math.NA

TL;DR: A barotropic-baroclinic splitting method for multilayer ocean models with exact operator splitting that preserves energy conservation, maximum principle, and entropy inequality while reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for multilayer ocean modeling that can handle complex vertical exchanges while maintaining important physical properties and reducing computational cost, especially for low Froude number simulations.

Method: Exact operator splitting with barotropic step handling free surface evolution and depth-averaged velocity via well-balanced one-layer model, and baroclinic step managing vertical exchanges between layers and velocity adjustments to mean values.

Result: The splitting preserves total energy conservation, satisfies discrete maximum principle and entropy inequality, shows computational cost gains (especially in low Froude simulations) without accuracy loss, and inherits well-balancing benefits for geostrophic equilibrium.

Conclusion: The barotropic-baroclinic splitting provides an efficient, accurate, and physically consistent framework for multilayer ocean modeling with terrain-following coordinates, maintaining key conservation properties while reducing computational expense.

Abstract: This work presents the numerical analysis of a barotropic-baroclinic splitting in a nonlinear multilayer framework with exchanges between the layers in terrain-following coordinates. The splitting is formulated as an exact operator splitting. The barotropic step handles free surface evolution and depth-averaged velocity via a well-balanced one-layer model, while the baroclinic step manages vertical exchanges between layers and adjusts velocities to their mean values. We show that the barotropic-baroclinic splitting preserves total energy conservation and meets both a discrete maximum principle and a discrete entropy inequality. Several numerical experiments are presented showing the gain in computational cost, particularly in low Froude simulations, with no loss of accuracy. The benefits of using a well-balancing strategy in the barotropic step to preserve the geostrophic equilibrium are inherited in the overall scheme.

</details>


### [9] [A locking-free nodal-based polytopal method for linear elasticity](https://arxiv.org/abs/2601.16728)
*Jerome Droniou,Raman Kumar*

Main category: math.NA

TL;DR: A locking-free Discrete de Rham method for linear elasticity on polyhedral meshes that prevents volumetric locking in quasi-incompressible regimes through face-bubble enrichment.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical scheme for linear elasticity that avoids volumetric locking when materials approach incompressibility (λ→∞), particularly on general polyhedral meshes where traditional methods fail.

Method: Uses Discrete de Rham (DDR) framework with lowest-order gradient space enriched with scalar face bubble degrees of freedom to capture normal flux across element faces, ensuring sufficient divergence approximation flexibility.

Result: Establishes λ-independent H¹-error estimates, robust across compressible to nearly incompressible regimes. Method adapts to frictionless contact mechanics while maintaining locking-free estimates. Numerical experiments confirm accuracy and stability on polytopal discretizations.

Conclusion: The face-bubble enriched DDR method provides a practical, locking-free alternative to mixed formulations for engineering applications with nearly incompressible elastic materials on general polyhedral meshes.

Abstract: This work presents a Discrete de Rham (DDR) numerical scheme for solving linear elasticity problems on general polyhedral meshes, with a focus on preventing volumetric locking in the quasi-incompressible regime. The method is formulated as a nodal-based approach using the lowest-order gradient space of the DDR complex, enriched with scalar face bubble degrees of freedom that effectively capture the normal flux across element faces. This face-bubble enrichment is crucial for ensuring sufficient approximation flexibility of the divergence field, thereby eliminating the {volumetric locking} phenomenon that typically occurs as the Lamé parameter $λ$ approaches infinity. We establish $H^1$-error estimates that are independent of $λ\ge 0$, and depend only on the lower bound of $μ$, guaranteeing robustness across the entire range from compressible to nearly incompressible regimes. We also show how to adapt our scheme to the frictionless contact mechanics model, maintaining a locking-free estimate for the primal variable (displacement). Numerical experiments confirm that the proposed {locking-free} method delivers accurate and stable approximations on general polytopal discretizations, even when the material behaves as an incompressible medium. The flexibility and robustness of this approach make it a practical alternative to mixed formulations for engineering applications involving nearly incompressible elastic materials.

</details>


### [10] [On the analysis of spectral deferred corrections for differential-algebraic equations of index one](https://arxiv.org/abs/2601.16744)
*Matthias Bolten,Lisa Wimmer*

Main category: math.NA

TL;DR: A new parallelizable SDC scheme for semi-explicit DAEs that only integrates differential equations, maintains one order per iteration like ODE SDC, and efficiently enforces algebraic constraints.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient parallelizable method for solving semi-explicit differential-algebraic equations (DAEs) that can achieve high accuracy while only numerically integrating the differential equations, unlike traditional approaches.

Method: Derived from spectral deferred corrections (SDC) and ε-embedding approach; enforces algebraic constraints in each iteration without numerical integration; parallelizable version only integrates differential equations.

Result: The scheme maintains one order per iteration (like SDC for ODEs); competitive with Runge-Kutta methods for DAEs in accuracy; parallelized versions are very efficient compared to other SDC methods.

Conclusion: The proposed SDC scheme provides an efficient, parallelizable approach for solving semi-explicit DAEs with high accuracy, combining the benefits of SDC order convergence with efficient constraint enforcement.

Abstract: In this paper, we present a new SDC scheme for solving semi-explicit DAEs with the ability to be parallelized in which only the differential equations are numerically integrated is presented. In Shu et al. (2007) it was shown that SDC for ODEs achieves one order per iteration. We show that this carries over to the new SDC scheme. The method is derived from the approach of spectral deferred corrections and the idea of enforcing the algebraic constraints without numerical integration as in the approach of $\varepsilon$-embedding in Hairer and Wanner (1996). It enforces the algebraic constraints to be satisfied in each iteration and allows an efficient solve of semi-explicit DAEs with high-accuracy. The proposed scheme is compared with other DAE methods. We demonstrate that the proposed SDC scheme is competitive with Runge-Kutta methods for DAEs in terms of accuracy and its parallelized versions are very efficient in comparison to other SDC methods.

</details>


### [11] [Adaptive integration of 5-convex and 5-concave functions](https://arxiv.org/abs/2601.16796)
*Szymon Wąsowicz*

Main category: math.NA

TL;DR: An adaptive numerical integration method combining 3-point Gauss and 4-point Lobatto quadrature is developed and analyzed for 5-convex functions.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient adaptive numerical integration method specifically designed for 5-convex functions, leveraging the complementary properties of Gauss and Lobatto quadrature rules to achieve better accuracy and computational efficiency.

Method: The method combines 3-point Gauss quadrature (which uses interior points only) with 4-point Lobatto quadrature (which includes endpoints) in an adaptive framework. The adaptation likely involves error estimation and refinement strategies tailored to 5-convex functions.

Result: The method is investigated and analyzed, presumably showing improved performance for 5-convex functions compared to standard quadrature methods, with better convergence rates and error control.

Conclusion: The adaptive hybrid method provides an effective numerical integration approach for 5-convex functions, offering advantages over traditional quadrature methods through strategic combination of Gauss and Lobatto rules with adaptive refinement.

Abstract: An adaptive method connected with 3-point Gauss quadrature and 4-point Lobatto quadrature is introduced and investigated for 5-convex functions.

</details>


### [12] [Discrete FEM-BEM coupling with the Generalized Optimized Schwarz Method](https://arxiv.org/abs/2601.16817)
*Antonin Boisneault,Marcella Bonazzoli,Xavier Claeys,Pierre Marchand*

Main category: math.NA

TL;DR: Develops GOSM, a non-overlapping domain decomposition method for Helmholtz acoustic wave problems, extending analysis to discrete settings and various boundary conditions including FEM-BEM couplings.


<details>
  <summary>Details</summary>
Motivation: To develop a robust domain decomposition approach for acoustic wave propagation problems that can handle both bounded and unbounded domains, various boundary conditions, and classical FEM-BEM couplings while addressing issues like spurious resonances.

Method: Generalized Optimized Schwarz Method (GOSM) - a substructuring domain decomposition approach where unknowns are associated with subdomain interfaces. Analysis extends to fully discrete settings and covers Dirichlet, Neumann, Robin conditions and three classical FEM-BEM couplings (Costabel, Johnson-Nédélec, Bielak-MacCamy).

Result: Establishes well-posed substructured formulations for FEM-BEM couplings at non-resonant wavenumbers. Proves Costabel FEM-BEM coupling formulation remains well-posed even at spurious resonances. Develops geometrically convergent iterative method for Costabel coupling with convergence speed estimates.

Conclusion: GOSM provides a robust framework for acoustic wave propagation problems with comprehensive analysis of well-posedness and convergence properties, particularly addressing challenges with FEM-BEM couplings and spurious resonances.

Abstract: The present contribution aims at developing a non-overlapping Domain Decomposition (DD) approach to the solution of acoustic wave propagation boundary value problems based on the Helmholtz equation, on both bounded and unbounded domains. This DD solver, called Generalized Optimized Schwarz Method (GOSM), is a substructuring method, that is, the unknowns of an iteration are associated with the subdomains interfaces. We extend the analysis presented in a previous paper of one of the author to a fully discrete setting. We do not consider only a specific set of boundary conditions, but a whole class including, e.g., Dirichlet, Neumann, and Robin conditions. Our analysis will also cover interface conditions corresponding to a Finite Element Method - Boundary Element Method (FEM-BEM) coupling. In particular, we shall focus on three classical FEM-BEM couplings, namely the Costabel, Johnson-Nédélec and Bielak-MacCamy couplings. As a remarkable outcome, the present contribution yields well-posed substructured formulations of these classical FEM-BEM couplings for wavenumbers different from classical spurious resonances. We also establish an explicit relation between the dimensions of the kernels of the initial variational formulation, the local problems and the substructured formulation. That relation especially holds for any wavenumber for the substructured formulation of Costabel FEM-BEM coupling, which allows us to prove that the latter formulation is well-posed even at spurious resonances. Besides, we introduce a systematically geometrically convergent iterative method for the Costabel FEM-BEM coupling, with estimates on the convergence speed.

</details>


### [13] [Cell-vertex WENO schemes with shock-capturing quadrature for high-order finite element discretizations of hyperbolic problems](https://arxiv.org/abs/2601.16911)
*Joshua Vedral,Dmitri Kuzmin*

Main category: math.NA

TL;DR: The paper proposes a new localized shock-capturing method for CG/DG discretizations using cell-vertex HWENO reconstruction and quadrature-driven viscosity distribution.


<details>
  <summary>Details</summary>
Motivation: To improve shock capturing for high-order CG/DG methods by addressing mesh imprinting issues and better localizing artificial viscosity near discontinuities while minimizing dissipation in smooth regions.

Method: Two main innovations: 1) Cell-vertex HWENO reconstruction that uses vertex-averaged data instead of cell-cell neighbor data to mitigate mesh imprinting, 2) Quadrature-driven distribution of artificial viscosity using nonlinear WENO-type weights to concentrate dissipation near discontinuities within troubled cells.

Result: Numerical experiments in 1D and 2D show substantial improvements in accuracy and robustness for high-order elements compared to the original cell-cell reconstruction approach.

Conclusion: The proposed cell-vertex HWENO reconstruction and quadrature-driven viscosity distribution provide an effective localized shock-capturing framework that improves both accuracy and robustness for high-order CG/DG discretizations of hyperbolic conservation laws.

Abstract: We propose a new kind of localized shock capturing for continuous (CG) and discontinuous Galerkin (DG) discretizations of hyperbolic conservation laws. The underlying framework of dissipation-based weighted essentially nonoscillatory (WENO) stabilization for high-order CG and DG approximations was introduced in our previous work. In this general framework, Hermite WENO (HWENO) reconstructions are used to calculate local smoothness sensors that determine the appropriate amount of artificial viscosity for each cell. In the original version, candidate polynomials for WENO averaging are constructed using the derivative data from von Neumann neighbors. We upgrade this standard `cell-cell' reconstruction procedure by using WENO polynomials associated with mesh vertices as candidate polynomials for cell-based WENO averaging. The Hermite data of individual cells is sent to vertices of those cells, after which vertex-averaged HWENO data is sent back to cells containing the vertices. The new `cell-vertex' averaging procedure includes the data of vertex neighbors without explicitly adding them to the reconstruction stencils. It mitigates mesh imprinting and can also be used in classical HWENO limiters for DG methods. The second main novelty of the proposed approach is a quadrature-driven distribution of artificial viscosity within high-order finite elements. Replacing the linear quadrature weights by their nonlinear WENO-type counterparts, we concentrate shock-capturing dissipation near discontinuities while minimizing it in smooth portions of troubled cells. This redistribution of WENO stabilization preserves the total dissipation rate within each cell and improves local shock resolution without relying on subcell decomposition techniques. Numerical experiments in one and two dimensions demonstrate substantial improvements in accuracy and robustness for high-order elements.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [14] [Anisotropic uncertainty principles for metaplectic operators](https://arxiv.org/abs/2601.16279)
*Elena Cordero,Gianluca Giacchi,Edoardo Pucci*

Main category: math.AP

TL;DR: Anisotropic uncertainty principles for metaplectic operators with degenerate B-blocks, showing directional uncertainty phenomena confined to effective phase-space dimension rank(B).


<details>
  <summary>Details</summary>
Motivation: To establish uncertainty principles for general metaplectic operators, including degenerate cases where the B-block has nontrivial kernel, revealing the geometric and anisotropic nature of uncertainty in symplectic degeneracies.

Method: Prove sharp Heisenberg-Pauli-Weyl type inequalities for directions in ker(B)⊥, characterize extremizers as partially Gaussian functions, extend Beurling-Hörmander theorem to metaplectic setting, and prove Morgan-type uncertainty principle with invariant thresholds.

Result: Uncertainty phenomena are directional and confined to effective dimension rank(B); sharp lower bounds with geometric quantities; complete characterization of extremizers as partially Gaussian functions; extended Beurling-Hörmander theorem; invariant threshold for Morgan-type principle.

Conclusion: Results recover classical Fourier and free metaplectic cases, revealing geometric and anisotropic nature of uncertainty principles in symplectic degeneracies, with uncertainty confined to directions where B acts nontrivially.

Abstract: We establish anisotropic uncertainty principles (UPs) for general metaplectic operators acting on
  $L^2(\mathbb{R}^d)$, including degenerate cases associated with symplectic matrices whose
  $B$-block has nontrivial kernel. In this setting, uncertainty phenomena are shown to be intrinsically
  directional and confined to an effective phase-space dimension given by $\mathrm{rank}(B)$.
  First, we prove sharp Heisenberg-Pauli-Weyl type inequalities involving only the directions
  corresponding to $\ker(B)^\perp$, with explicit lower bounds expressed in terms of geometric
  quantities associated with the underlying symplectic transformation. We also provide a complete
  characterization of all extremizers, which turn out to be partially Gaussian functions with free
  behavior along the null directions of $B$.
  Building on this framework, we extend the Beurling-Hörmander theorem to the metaplectic
  setting, obtaining a precise polynomial-Gaussian structure for functions satisfying suitable
  exponential integrability conditions involving both $f$ and its metaplectic transform. Finally,
  we prove a Morgan-type (or Gel'fand--Shilov type) uncertainty principle for metaplectic operators,
  identifying a sharp threshold separating triviality from density of admissible functions and
  showing that this threshold is invariant under metaplectic transformations.
  Our results recover the classical Fourier case and free metaplectic transformations as special
  instances, and reveal the geometric and anisotropic nature of uncertainty principles in the
  presence of symplectic degeneracies.

</details>


### [15] [Chemotactic Feedback Controls Patterning in Hybrid Tumor--Stroma Model](https://arxiv.org/abs/2601.16337)
*Jiguang Yu,Louis Shuo Wang,Zonghao Liu,Jingfeng Liu*

Main category: math.AP

TL;DR: Developed a PDE-ODE framework for tumor dynamics with competition, stromal switching, and drug diffusion, showing open-loop therapy yields only transient effects while closed-loop feedback enables resistance niche formation.


<details>
  <summary>Details</summary>
Motivation: Clinical management of solid tumors is limited by spatial heterogeneity and therapy-induced resistance niches that cannot be predicted from well-mixed models. Collaboration with clinical oncologists and pathologists motivated this work.

Method: Hybrid PDE-ODE framework capturing: (i) competition between susceptible and resistant phenotypes, (ii) stromal state switching, and (iii) clinically realistic open-loop, single-dose therapeutic agent with diffusion and clearance. Mathematical analysis includes forward invariance, global well-posedness, and analysis of decoupled drug equation.

Result: Proved long-time reduction during drug washout and showed damped base dynamics admit no diffusion-driven (Turing-type) instability. Formulated directionality-damping principle: unidirectional sensing yields only transient focusing, while bidirectional feedback reshapes effective mobility and produces explicit thresholds for stable homogeneity, finite-band patterning (resistance niche formation), and aggregation.

Conclusion: Closed-loop feedback mechanisms are essential for generating persistent resistance niches in tumors, while open-loop therapies produce only transient effects. The mathematical framework provides insights into when flux regularization is required for physical realism in tumor modeling.

Abstract: Motivated by an ongoing collaboration with clinical oncologists and pathologists, we develop a hybrid partial differential equation--ordinary differential equation (PDE--ODE) framework that captures (i) competition between susceptible and resistant phenotypes, (ii) stromal state switching, and (iii) a clinically realistic open-loop, single-dose therapeutic agent $I$ with diffusion and clearance.
  Clinical management of solid tumors is increasingly limited by spatial heterogeneity and therapy-induced resistance niches that are difficult to predict from well-mixed models. We establish a rigorous mathematical backbone with forward invariance of the nonnegative cone and global-in-time well-posedness. Exploiting the decoupled drug equation $\partial_t I=d_IΔI-γ_I I$, we prove a long-time reduction during washout and show that the damped base dynamics admit no diffusion-driven (Turing-type) instability. We then formulate a directionality--damping principle: unidirectional (open-loop) sensing yields at most transient focusing, whereas bidirectional (closed-loop) feedback reshapes the effective mobility and produces explicit thresholds separating stable homogeneity, finite-band patterning (resistance niche formation), and aggregation when strong parabolicity is violated. Reproducible simulations corroborate this classification and highlight when flux regularization is required for physical realism.

</details>


### [16] [Well-posedness of the Langmuir film problem](https://arxiv.org/abs/2601.16482)
*Yoichiro Mori,Shinya Okabe,Koya Sakakibara*

Main category: math.AP

TL;DR: The paper analyzes the inviscid Langmuir layer-Stokesian subfluid model for two-phase Langmuir monolayers, reformulates it using Dirichlet-to-Neumann operators, establishes connections to fractional Laplacians, proves well-posedness, and develops numerical methods.


<details>
  <summary>Details</summary>
Motivation: To understand and mathematically analyze the dynamics of two-phase Langmuir monolayers coupled to underlying Stokes flow, which is important for studying interfacial phenomena in fluid dynamics and materials science.

Method: Reformulate the 3D coupled system using Dirichlet-to-Neumann operators, identify connections to fractional Laplacians, derive boundary integral equations, prove well-posedness via maximal L²-regularity and DeTurck reparametrization, and develop finite-element numerical schemes.

Result: Established that the DtN operator coincides with fractional Laplacian, derived explicit Fourier representation and fundamental solution, proved curve-shortening identity, established local well-posedness, and developed numerical scheme capturing experimental dynamics.

Conclusion: The ILLSS model can be effectively analyzed using fractional calculus and boundary integral methods, with rigorous mathematical foundations established and numerical methods developed that match experimental observations.

Abstract: We analyze the inviscid Langmuir layer--Stokesian subfluid (ILLSS) model for two-phase Langmuir monolayers coupled to a Stokes flow in the underlying subfluid. Eliminating the bulk variables, we reformulate the coupled three-dimensional system as an evolution on the film involving the Dirichlet-to-Neumann (DtN) operator. We identify the Fourier symbol of the DtN operator and show it coincides with that of the fractional Laplacian, which yields an explicit Fourier-multiplier representation and allows construction of the corresponding fundamental solution. Using this representation we express the surface velocity as a convolution of the fundamental solution with the interfacial curvature forcing and analyze its normal limit to derive a boundary integral equation for the moving curve. Independently, exploiting the DtN representation we establish a curve-shortening identity: the interfacial perimeter decreases monotonically and its time derivative is controlled by $\dot{H}^{1/2}(\mathbb{R}^2)$-norm of the surface velocity. Building on the boundary integral equation, we prove local well-posedness via maximal $L^2$-regularity for quasilinear parabolic systems, employing a DeTurck-type reparametrization, and show equivalence with the original ILLSS system. Finally, we introduce a linearly implicit parametric finite-element scheme which captures experimentally observed relaxation dynamics.

</details>


### [17] [Stationary phase with Cauchy singularity. A critical point of signature $(+,-)$](https://arxiv.org/abs/2601.16542)
*Christian Klein,Johannes Sjöstrand,Maher Zerzeri*

Main category: math.AP

TL;DR: Asymptotic analysis of a solid Cauchy transform integral with oscillatory phase using polarization approach when stationary points are near singularity.


<details>
  <summary>Details</summary>
Motivation: Standard steepest descent methods fail when stationary points of the oscillatory phase are close to the integrand's singularity (within O(√h)), requiring new analytical techniques.

Method: Polarization approach treating ω and its conjugate as independent variables in ℂ², using steepest descent contours and Stokes' theorem to decompose the integral into three terms.

Result: Derived asymptotic expressions for the integral in terms of special functions for the case where |ζ-ω_k| < O(√h), providing analytical solutions where standard methods fail.

Conclusion: The polarization approach successfully handles the challenging case of stationary points near singularities, extending asymptotic analysis capabilities for oscillatory integrals in d-bar problems.

Abstract: Asymptotic expressions for an integral appearing in the solution of a d-bar problem are presented. The integral is a solid Cauchy transform of a function with a rapidly oscillating phase with a small parameter $h$, $0<h\ll 1$. Whereas standard steepest descent approaches can be applied to the case where the stationary points of the phase $ω_{k}$, $k=1,\ldots, N$ are far from the singularity $ζ$ of the integrand, a polarization approach is proposed for the case that $|ζ-ω_{k}|<\mathcal{O}(\sqrt{h})$ for some $k$. In this case the problem is studied in $\mathbb{C}^{2}$ ($\tildeω:=\barω$ is treated as an independent variable) on steepest descent contours. An application of Stokes' theorem allows for a decomposition of the integral into three terms for which asymptotics expressions in terms of special functions are given.

</details>


### [18] [Boundary regularity for parabolic systems with nonstandard $(p,q)$-growth conditions in smooth convex domains](https://arxiv.org/abs/2601.16546)
*Michael Strunk*

Main category: math.AP

TL;DR: The paper establishes local Lipschitz regularity up to the lateral boundary for weak solutions of nonlinear parabolic systems with Uhlenbeck-type coefficients and nonstandard (p,q)-growth, under convex domain and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To extend boundary regularity results for nonlinear parabolic systems beyond the standard p-growth case, particularly for systems with nonstandard (p,q)-growth conditions that arise in various applications.

Method: Studies weak solutions to Uhlenbeck-type parabolic systems with (p,q)-growth where 2 ≤ p ≤ q < p + 4/(n+2). Uses convex C²-domain assumptions and requires solutions to vanish on the lateral boundary. The inhomogeneity f has enhanced integrability L^{n+2+σ}.

Result: Proves local Lipschitz estimate up to the lateral boundary for any local weak solution that vanishes on the lateral boundary, extending interior regularity results to boundary regularity.

Conclusion: The paper successfully establishes boundary regularity for parabolic systems with nonstandard growth, showing that under appropriate domain convexity and boundary conditions, Lipschitz regularity holds up to the boundary despite the challenging (p,q)-growth structure.

Abstract: We study the boundary regularity of local weak solutions to nonlinear parabolic systems of the form \begin{equation*}
  \partial_t u^i - \mathrm{div} \big( a(|Du|) Du^i \big)= f^i, \qquad i=1,\dots,N, \end{equation*} in a space-time cylinder $Ω_T = Ω\times (0,T)$, where $Ω\subset \mathbb{R}^n$ ($n \geq 2$) is a bounded, convex $C^2$-domain and $T>0$. The inhomogeneity $f=(f^1,\dots,f^N)$ belongs to $L^{n+2+σ}(Ω_T,\mathbb{R}^N)$ for some $σ>0$. The coefficients $a\colon \mathbb{R}_{>0} \to \mathbb{R}_{>0}$ are of Uhlenbeck-type and satisfy a nonstandard $(p,q)$-growth condition with \[ 2 \leq p \leq q < p + \frac{4}{n+2}. \] Our main result establishes a local Lipschitz estimate up to the lateral boundary for any local weak solution that vanishes on the lateral boundary of the cylinder.

</details>


### [19] [Convergence speed for the average density of eigenfunctions for singular Riemannian manifolds](https://arxiv.org/abs/2601.16574)
*Charlotte Dietze*

Main category: math.AP

TL;DR: Quantitative estimate of convergence speed for eigenfunction density to boundary measure in Wasserstein metric


<details>
  <summary>Details</summary>
Motivation: Study eigenfunctions of Laplace-Beltrami operator on manifolds with singular Riemannian metrics, where eigenfunction density converges to boundary measure as eigenvalues grow

Method: Analyze singular Riemannian metrics on compact manifolds with boundary, examine eigenfunctions of Laplace-Beltrami operator, use Wasserstein metric to measure convergence speed

Result: Obtain quantitative estimate on convergence speed of eigenfunction density to uniform normalized boundary measure in transverse coordinate direction

Conclusion: Provides rigorous quantitative bound on how quickly eigenfunction distributions approach boundary measure in Wasserstein sense for singular metric settings

Abstract: We consider a class of singular Riemannian metrics on a compact Riemannian manifold with boundary and the eigenfunctions of the corresponding Laplace-Beltrami operator. In our setting, the average density of eigenfunctions with eigenvalue less than $λ$ converges weakly to the uniform normalised measure on the boundary as $λ\to\infty$. In this work, we show a quantitative estimate on the speed of this convergence in the Wasserstein-sense in the transverse coordinate to the boundary.

</details>


### [20] [Ground state of indefinite coupled nonlinear Schrödinger systems](https://arxiv.org/abs/2601.16601)
*Ruijin Xu,Jiabao Su,Rushun Tian*

Main category: math.AP

TL;DR: Existence of ground state solutions for indefinite coupled nonlinear Schrödinger system with Dirichlet boundary conditions in bounded domains.


<details>
  <summary>Details</summary>
Motivation: Study ground state solutions for coupled nonlinear Schrödinger systems in the indefinite case where τ₁, τ₂ are ≥ principal eigenvalue of -Δ, which is mathematically challenging due to lack of coercivity.

Method: Delicate variational arguments applied to the coupled system with cubic nonlinearities and coupling parameter β > 0 in bounded domains Ω ⊂ ℝᴺ (N ≤ 3).

Result: Obtained existence of ground state solution to the system and provided information on critical energy levels for coupling parameter β in certain ranges.

Conclusion: Successfully established existence results for indefinite coupled nonlinear Schrödinger systems using variational methods, with analysis of critical energy levels for different coupling strengths.

Abstract: In this paper, we study the ground state solutions of the following coupled nonlinear Schrödinger system (P) $-Δu_1-τ_1 u_1 =μ_1u_1^3+βu_1u_2^2$, $ -Δu_2-τ_2 u_2 =μ_2u_2^3+βu_1^2u_2$ in $Ω$, $u_1=u_2=0$ on $\partialΩ$, where $μ_1, μ_2>0$, $β>0$ and $Ω\subset \mathbb{R}^N (N\le3)$ is a bounded domain with smooth boundary. We are concerned with the indefinite case, i.e., $τ_1, τ_2$ are greater than or equal to the principal eigenvalue of $-Δ$ with the Dirichlet boundary datum. By delicate variational arguments, we obtain the existence of ground state solution to $(P)$, and also provide information on critical energy levels for coupling parameter $β$ in some ranges.

</details>


### [21] [Generalized Logarithmic Sobolev Inequality by the JKO Scheme](https://arxiv.org/abs/2601.16620)
*Thibault Caillet,Fanch Coudreuse*

Main category: math.AP

TL;DR: New generalized logarithmic Sobolev inequalities for log-concave measures using discrete Bakry-Émery method via JKO scheme, interpolating between Bakry-Émery and optimal transport approaches.


<details>
  <summary>Details</summary>
Motivation: To establish new generalized logarithmic Sobolev inequalities for log-concave measures under strict convexity assumptions, bridging the gap between traditional Bakry-Émery methods and optimal transport techniques.

Method: Discrete Bakry-Émery method based on JKO scheme, analyzing dissipation of entropy and Fisher information along discrete flow. The approach interpolates between Bakry-Émery method and optimal transport techniques based on geodesic convexity.

Result: Establishes new generalized logarithmic Sobolev inequalities for log-concave measures of the form e^{-V} under strict convexity assumptions on V. The method recovers some well-known inequalities.

Conclusion: The discrete approach successfully connects Bakry-Émery methodology with optimal transport techniques, providing a unified framework for establishing logarithmic Sobolev inequalities for convex measures.

Abstract: Using a discrete Bakry-{É}mery method based on the JKO scheme, relying on the dissipation of entropy and Fisher information along a discrete flow, we establish new generalized logarithmic Sobolev inequality for log-concave measures of the form $e^{-V} under strict convexity assumptions on $V$ . We then show how this method recovers some well-known inequalities. This approach can be viewed as interpolating between the Bakry-{É}mery method and optimal transport techniques based on geodesic convexity.

</details>


### [22] [Uniform $L^{\infty}$-boundedness for solutions of anisotropic quasilinear systems](https://arxiv.org/abs/2601.16673)
*Natalino Borgia,Silvia Cingolani,Giuseppina Vannella*

Main category: math.AP

TL;DR: The paper establishes uniform local L∞-estimates for solutions to non-autonomous quasilinear systems with divergence-form operators and critical-growth nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To develop regularity theory for quasilinear systems with critical nonlinearities, which are important in PDE analysis but present challenges due to their borderline growth behavior.

Method: The authors use analytical techniques for non-autonomous quasilinear systems involving divergence-form operators, likely employing methods from elliptic regularity theory and functional analysis.

Result: Obtains uniform local L∞-estimates for solutions, meaning solutions are bounded locally in space, even with critical-growth nonlinearities.

Conclusion: The paper successfully establishes regularity results for a broad class of quasilinear systems with critical nonlinearities, extending existing theory.

Abstract: In this paper we obtain uniformly locally $L^{\infty}$-estimate of solutions to non-autonomous quasilinear system involving operators in divergence form and a family of nonlinearities that are allowed to grow also critically.

</details>


### [23] [Global $W^{2,1+ε}$ regularity for potentials of optimal transport of non-convex planar domains](https://arxiv.org/abs/2601.16707)
*Shengnan Hu,Yuanyuan Li*

Main category: math.AP

TL;DR: The paper proves a global W^{2,1+ε} regularity estimate for optimal transport potentials when the source domain is a non-convex polygon in ℝ², with methods extending to more general domains.


<details>
  <summary>Details</summary>
Motivation: Optimal transport theory typically assumes convex or smooth source domains, but many real-world applications involve non-convex domains like polygonal regions. Understanding regularity properties in such non-convex settings is important for both theory and applications.

Method: The authors develop analytical techniques to handle the geometric challenges of non-convex polygonal domains, likely using tools from partial differential equations, geometric measure theory, and optimal transport theory to establish regularity estimates.

Result: Main result: global W^{2,1+ε} estimate for optimal transport potentials when the source is a non-convex polygonal domain in ℝ². The method works for a broader class of domains beyond just polygons.

Conclusion: The paper extends regularity theory for optimal transport to non-convex polygonal domains, providing important mathematical tools for applications involving irregular geometries and opening avenues for further research on optimal transport in non-smooth settings.

Abstract: In this paper, we investigate the optimal transport problem when the source is a non-convex polygonal domain in $\mathbb{R}^2$. We show a global $W^{2,1+ε}$ estimate for potentials of optimal transport. Our method applies to a more general class of domains.

</details>


### [24] [Multiplicity and concentration of dual solutions for a Helmholtz system](https://arxiv.org/abs/2601.16754)
*Ruowen Qiu,Fei Yuan,Fukun Zhao*

Main category: math.AP

TL;DR: Study of nonlinear Helmholtz system with Hamiltonian structure, establishing existence of ground state solutions via dual variational method, analyzing concentration behavior as k→∞, and relating solution multiplicity to topology of coefficient functions' maxima.


<details>
  <summary>Details</summary>
Motivation: To investigate existence, concentration behavior, and multiplicity of solutions for nonlinear Helmholtz systems with Hamiltonian structure, which arise in various physical contexts including nonlinear optics and quantum mechanics.

Method: Dual variational method for existence proof, rescaling technique and generalized Birman-Schwinger operator for concentration analysis, topological methods relating solution count to maxima of coefficient functions P and Q.

Result: Proved existence of ground state solutions, established concentration behavior as k→∞, and demonstrated relationship between number of solutions and topology of global maxima sets of P and Q.

Conclusion: The paper successfully develops a comprehensive theory for nonlinear Helmholtz systems, providing existence results, asymptotic behavior analysis, and topological insights into solution multiplicity.

Abstract: In this paper, we are concerned with the nonlinear Helmholtz system of Hamiltonian type \begin{equation*} \left\{\begin{array}{l} -Δu-k^2 u=P(x)|v|^{p-2}v,\quad \text{in}\ \mathbb{R}^N, \\ -Δv-k^2v=Q(x)|u|^{q-2}u,\quad \text{in}\ \mathbb{R}^N, \end{array} \right. \end{equation*} where $N\geq3$, $P,Q: \mathbb{R}^N\rightarrow \mathbb{R}$ are two positive continuous functions, the exponents $p,q>2$ satisfy $\frac{1}{p}+\frac{1}{q}>\frac{N-2}{N}$. First, we obtained the existence of a ground state solution via a dual variational method. Moreover, the concentration behavior of such dual ground state solutions is established
  as $k\rightarrow\infty$, where a rescaling technique and the generalized Birman-Schwinger operator are involved. In addition, we also investigated the relation between the number of solutions and the topology of the set of the global maxima of the functions $P$ and $Q$.

</details>


### [25] [Existence of spot and lane stationary solutions for an ant active matter PDE model](https://arxiv.org/abs/2601.16820)
*Matthias Rakotomalala,Oscar de Wit*

Main category: math.AP

TL;DR: The paper proves existence of multiple stationary solutions (spot and lane patterns) in an ant behavior PDE model, showing they emerge through bifurcation as interaction strength increases, with spot solutions being stable and lane solutions unstable for small anticipation.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by collective ant behavior and previous work suggesting the existence of two types of non-trivial stationary solutions (spot and lane patterns) in a PDE model. The authors aim to rigorously establish the existence of these solution families and analyze their stability properties.

Method: The paper uses bifurcation analysis to establish the existence of families of stationary solutions along a bifurcation sequence as the interaction strength parameter grows. The analysis shows progressively increasing numbers of clusters (for spot solutions) and parallel lanes (for lane solutions) emerge through this bifurcation process.

Result: The main results are: (1) Existence of families of spot and lane stationary solutions that bifurcate as interaction strength increases, with progressively more complex patterns; (2) For small values of the anticipation parameter, the first bifurcating spot solutions are locally dynamically stable, while the lane solutions are unstable.

Conclusion: The paper successfully establishes the existence of multiple non-trivial stationary solutions in the ant behavior PDE model, showing how complex patterns emerge through bifurcation. The stability analysis reveals that spot patterns are dynamically relevant (stable) while lane patterns are unstable for small anticipation parameters, providing insight into which patterns might be observed in actual ant behavior.

Abstract: This paper studies the existence of multiple non-trivial stationary solutions of a partial differential equation (PDE) model introduced in [3], motivated by collective ant behavior. Previous work suggested the presence of two types of non-trivial stationary solutions for this PDE system: spot and lane solutions. In this paper, we establish the existence of these families of solutions along a bifurcation sequence as the interaction strength grows, with progressively increasing numbers of clusters and parallel lanes, respectively. Finally, we show that, for small values of the anticipation parameter, the first bifurcating spot solutions are locally dynamically stable, while the lane solutions are unstable.

</details>


### [26] [On the de Thélin eigenvalue problem and Landesman-Lazer conditions for quasilinear systems](https://arxiv.org/abs/2601.16846)
*David Arcoya,Natalino Borgia,Silvia Cingolani*

Main category: math.AP

TL;DR: The paper proves that the smallest eigenvalue λ₁ of a quasilinear elliptic system is both simple and isolated, characterizes a sequence of eigenvalues variationally, and establishes existence of weak solutions under Landesman-Lazer type conditions near resonance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the understanding of eigenvalue problems for quasilinear elliptic systems, particularly addressing the properties of the smallest eigenvalue and establishing existence results for resonant problems, building on previous work by de Thélin and Arcoya & Orsina.

Method: The authors use variational methods, including a deformation lemma for C¹ submanifolds from previous work, to characterize eigenvalues variationally. They employ techniques to prove simplicity and isolation of λ₁, and apply Landesman-Lazer type conditions to establish existence of weak solutions in resonance cases.

Result: Main results: (1) λ₁ is both simple and isolated; (2) variational characterization of eigenvalue sequence {λₖ}; (3) existence of weak solutions for quasilinear elliptic systems in resonance around λ₁ under new Landesman-Lazer type conditions.

Conclusion: The paper successfully extends previous results on quasilinear elliptic systems, providing stronger properties for the smallest eigenvalue and establishing new existence criteria for resonant problems through variational methods and Landesman-Lazer type conditions.

Abstract: In this paper we prove that the smallest eigenvalue $λ_1$ of the eigenvalue problem for a quasilinear elliptic systems introduced by de Thélin in \cite{DT}, is not only simple (in a suitable sense), but also isolated.
  Moreover, we characterize variationally a sequence $\{λ_k\}_k$ of eigenvalues, taking into account a suitable deformation lemma for $C^1$ submanifolds proved in \cite{BON}. Furthermore we prove the existence of a weak solution for a quasilinear elliptic systems in resonance around $λ_1$, under new sufficient Landesman-Lazer type conditions, extending the results by Arcoya and Orsina \cite{AO}.

</details>


### [27] [On the stability of solutions to non-Newtonian Navier--Stokes--Fourier-like systems in the supercritical case](https://arxiv.org/abs/2601.16868)
*Anna Abbatiello,Miroslav Bulíček,Petr Kaplický*

Main category: math.AP

TL;DR: Existence and long-time stability of solutions for 3D non-Newtonian fluids with general constitutive relations and boundary temperature conditions.


<details>
  <summary>Details</summary>
Motivation: To address the mathematical challenges in 3D non-Newtonian fluid dynamics where standard results fail - specifically, for power-law and Ladyzhenskaya models in regimes where regularity, uniqueness, and energy equality are not guaranteed.

Method: Introduces a novel concept of solution suitable for this setting, establishing existence of global-in-time solutions for arbitrary physically relevant initial data without external body forces.

Result: Proves existence of global solutions and demonstrates nonlinear stability: every solution converges to the steady state as time tends to infinity.

Conclusion: Provides the first result combining existence with long-time stability in this physically relevant yet mathematically challenging regime for non-Newtonian fluids.

Abstract: We consider a three-dimensional domain occupied by a homogeneous, incompressible, non-Newtonian, heat-conducting fluid with prescribed nonuniform temperature on the boundary and no-slip boundary conditions for the velocity. No external body forces are assumed. The constitutive relation for the Cauchy stress tensor is assumed in a general form that includes, in particular, the power-law and Ladyzhenskaya models with the power-law exponent in the range where neither regularity, uniqueness, nor the validity of the energy equality is known to hold.
  Nevertheless, we introduce a novel concept of solution suitable for this setting, which enables us to establish the existence of global-in-time solutions for arbitrary physically relevant initial data. A remarkable feature of this formulation is that the steady-state solution is nonlinearly stable: every such solution converges, in a suitable sense, to the steady state as time tends to infinity. This provides the first result that combines existence with long-time stability in this physically relevant yet mathematically challenging regime.

</details>


### [28] [Stability inequalities for one-phase cones](https://arxiv.org/abs/2601.16966)
*Benjy Firester,Raphael Tsiamis,Yipeng Wang*

Main category: math.AP

TL;DR: The paper proves strict stability inequalities for homogeneous solutions of the one-phase Bernoulli problem, showing cohomogeneity one solutions with bi-orthogonal symmetry are strictly stable in dimensions 7+, with applications to eigenvalue bounds and generic regularity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish stability properties for solutions to the one-phase Bernoulli problem, which is important for understanding the regularity and behavior of free boundary problems in geometric analysis and PDE theory.

Method: The authors use geometric analysis techniques to prove strict stability inequalities for homogeneous solutions. They specifically analyze cohomogeneity one solutions with bi-orthogonal symmetry in higher dimensions (7 and above).

Result: Main result: In dimensions 7 and above, cohomogeneity one solutions with bi-orthogonal symmetry are strictly stable. This leads to bounds on the first eigenvalue and decay rates of Jacobi fields.

Conclusion: The strict stability results have applications to the generic regularity of the one-phase problem, providing important insights into the behavior of solutions in higher dimensions and their stability properties.

Abstract: We obtain strict stability inequalities for homogeneous solutions of the one-phase Bernoulli problem. We prove that in dimension $7$ and above, cohomogeneity one solutions with bi-orthogonal symmetry are strictly stable. As a consequence, we obtain a bound on the first eigenvalue and the decay rates of Jacobi fields, with applications to the generic regularity of the one-phase problem.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Collective Rabi-driven vibrational activation in molecular polaritons](https://arxiv.org/abs/2601.16299)
*Carlos M. Bustamante,Franco P. Bonafé,Richard Richardson,Michael Ruggenthaler,Wenxiang Ying,Abraham Nitzan,Maxim Sukharev,Angel Rubio*

Main category: physics.comp-ph

TL;DR: Vibrational activation emerges under collective electronic strong coupling in driven cavities, where collective electronic Rabi oscillations coherently drive nuclear motion.


<details>
  <summary>Details</summary>
Motivation: While molecular polaritons have been widely studied, the influence of electron-nuclear dynamics in driven cavities remains largely unknown, creating a gap in understanding cavity-matter interactions.

Method: Used semiclassical simulations combining Maxwell's equations with quantum molecular dynamics, including vibrational wave-packet dynamics in a two-level model and atomistic simulations with time-dependent density-functional tight-binding and Ehrenfest dynamics.

Result: Vibrational activation depends non-monotonically on Rabi frequency and is maximized when collective polaritonic splitting resonates with a molecular vibrational mode, exhibiting features consistent with stimulated Raman-like relaxation.

Conclusion: Establishes a self-consistent framework for realistic cavity-electron-nuclear dynamics and reveals a previously unrecognized mechanism of vibrational activation under collective electronic strong coupling.

Abstract: Hybrid light-matter states, known as molecular polaritons, arise from electronic or vibrational strong coupling (ESC and VSC) with confined electromagnetic fields. While these have been widely studied, the influence of electron-nuclear dynamics in driven cavities remains largely unknown. Here, we report a previously unrecognized mechanism of vibrational activation that emerges under collective ESC in driven optical cavities. Using semiclassical simulations that self-consistently combine Maxwell's equations with quantum molecular dynamics, we show that collective electronic Rabi oscillations coherently drive nuclear motion. This effect is captured using both vibrational wave-packet dynamics in a minimal two-level model and atomistic simulations based on time-dependent density-functional tight-binding with Ehrenfest dynamics. Vibrational activation depends non-monotonically on the Rabi frequency and is maximized when the collective polaritonic splitting resonates with a molecular vibrational mode. The mechanism exhibits features consistent with a stimulated Raman-like relaxation mechanism. Our results establish a self-consistent framework for realistic cavity-electron-nuclear dynamics.

</details>


### [30] [Physics Informed Differentiable Solvers for Learning Parametric Solution Manifolds in Heterogeneous Physical Systems](https://arxiv.org/abs/2601.16350)
*Milad Panahi,Giovanni Michele Porta,Monica Riva,Alberto Guadagnini*

Main category: physics.comp-ph

TL;DR: A single-training PINN framework learns the full solution manifold for parameterized PDEs (Darcy flow) with heterogeneous conductivity fields, using differentiable solvers and autoencoder-based latent representations.


<details>
  <summary>Details</summary>
Motivation: Learning the full family of solutions to parameterized PDEs is crucial for modeling heterogeneous systems like hydrogeology, where spatial heterogeneity is significant and uncertain. Current approaches require costly re-training for each parameter instance.

Method: Reformulate PINN as a differentiable solver that learns continuous solution manifold for steady-state Darcy flow. Two representations: direct analytical form and novel data-driven formulation using autoencoder for low-dimensional latent encoding. Integrate differentiable decoder into physics-informed loss for on-the-fly reconstruction via automatic differentiation.

Result: The framework yields accurate, mass-conserving flow solutions with only a single training run, circumventing costly re-training. Supports efficient uncertainty quantification and provides general methodology for physics-constrained data-driven modeling of heterogeneous systems.

Conclusion: The approach provides a versatile framework for learning solution manifolds of parameterized PDEs with heterogeneous properties, combining physics-informed learning with data-driven representations for efficient modeling and uncertainty quantification.

Abstract: Learning the full family of solutions to parameterized partial differential equations (PDEs) is a central challenge to our ability to model the behavior of heterogeneous systems, with a variety of fundamental and application-oriented implications in fields such as hydrogeology where system properties exhibit significant (and often uncertain) spatial heterogeneity. We address this by reformulating a Physics-Informed Neural Network (PINN) as a differentiable solver that learns the continuous solution manifold for steady-state Darcy flow. Our framework requires only a single training run, circumventing the need for costly re-training for each new parameter instance. Its versatility is demonstrated through two representations of spatially heterogeneous hydraulic conductivity fields: a direct analytical form and a novel data-driven formulation resting on an autoencoder to create a low-dimensional latent encoding. A key innovation is the integration of the differentiable decoder into the physics-informed loss function, enabling on-the-fly reconstruction of complex conductivity fields via automatic differentiation. The approach yields accurate, mass-conserving flow solutions and supports efficient uncertainty quantification, providing a general methodology for physics-constrained data-driven modeling of heterogeneous systems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [Improved Kelbg Potentials for $Z>1$ and Application to Carbon Plasmas](https://arxiv.org/abs/2601.16794)
*Heather D. Whitley,Michael S. Murillo,John I. Castor,Liam G. Stanton,Lorin X. Benedict,Philip A. Sterne,James N. Glosli,Frank R. Graziani*

Main category: physics.plasm-ph

TL;DR: The paper presents a general electron-ion diffractive potential derived from quantum pair density matrix, fitted to improved Kelbg potential for Z≤54, and tests it for carbon plasmas using classical MD with various Pauli potentials.


<details>
  <summary>Details</summary>
Motivation: To develop and validate improved electron-ion potentials (Kelbg potential) for warm dense matter and high energy density plasmas, extending previous hydrogen studies to heavier elements like carbon.

Method: Derived general electron-ion diffractive potential from quantum pair density matrix; fitted to improved Kelbg potential for Z≤54; applied classical molecular dynamics with improved Kelbg potential for carbon using various Pauli potentials; compared results to equation of state model based on path integral Monte Carlo and density functional theory simulations.

Result: The improved Kelbg potential reproduces internal energy and pressure of carbon plasmas reasonably well; regions of validity for carbon agree with previous hydrogen studies once pressure ionization effects are incorporated.

Conclusion: The improved Kelbg potential has general applicability for equation of state studies in warm dense matter and high energy density plasmas, with limitations that need consideration based on element-specific pressure ionization effects.

Abstract: In this work, we present a general form for the electron-ion diffractive potential derived from the quantum pair density matrix and fit to the improved Kelbg potential for atomic numbers up to $Z = 54$. We apply classical molecular dynamics using the improved Kelbg potential for carbon with various forms of the Pauli potential to compute internal energies and pressures for hot, dense plasma conditions. Our results are compared to an equation of state model based on path integral Monte Carlo and density functional theory simulations to examine the extent to which the improved Kelbg potential reproduces the internal energy and pressure of carbon plasmas. The regions of validity for carbon agree generally with those derived previously for hydrogen once pressure ionization effects are incorporated. Based on our carbon results and previously published hydrogen studies, we discuss the general applicability and limitations of these potentials for equation of state studies in warm dense matter and high energy density plasmas.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [32] [Mercury-Ar$χ$es: a high-performance n-body code for planet formation studies](https://arxiv.org/abs/2601.16791)
*Diego Turrini,Sergio Fonte,Romolo Politi,Danae Polychroni,Scigé J. Liu,Paolo Matteo Simonetti,Simona Pirani*

Main category: astro-ph.EP

TL;DR: Mercury-Arχes is a parallel n-body planet formation code that extends the Mercury code to model planetary growth, migration, gas-disk interactions, and planetesimal impacts using OpenMP for high-performance computing.


<details>
  <summary>Details</summary>
Motivation: Planet formation simulations are computationally demanding due to the need to model numerous gravitationally interacting bodies embedded in protoplanetary disks, requiring high-performance methods for accurate modeling of planetary system formation.

Method: Developed Mercury-Arχes as a parallel n-body code building on the Mercury code, implementing OpenMP directive-based parallelism for shared memory environments to leverage multi-threading and vectorization features of modern processors.

Result: The code has been successfully used in multiple exoplanetary and Solar System studies, and this work provides an up-to-date overview of its physical modeling capabilities and detailed description of its high-performance implementation.

Conclusion: Mercury-Arχes addresses the computational challenges of realistic planet formation simulations by providing a high-performance parallel code capable of modeling the complex interactions between planetary bodies and disk gas throughout the formation process.

Abstract: Forming planetary systems are populated by large numbers of gravitationally interacting planetary bodies, spanning from massive giant planets to small planetesimals akin to present-day asteroids and comets. All these planetary bodies are embedded in the gaseous embrace of their native protoplanetary disks, and their interactions with the disk gas play a central role in shaping their dynamical evolution and the outcomes of planet formation. These factors make realistic planet formation simulations extremely computationally demanding, which in turn means that accurately modeling the formation of planetary systems requires the use of high-performance methods. The planet formation code Mercury-Ar$χ$es was developed to address these challenges and, since its first implementation, has been used in multiple exoplanetary and Solar System studies. Mercury-Ar$χ$es is a parallel n-body code that builds on the widely used Mercury code and is capable of modeling the growth and migration of forming planets, the interactions between planetary bodies and the disk gas, as well as the evolving impact flux of planetesimals on forming planets across the different stages of their formation process. In this work we provide the up-to-date overview of its physical modeling capabilities and the first detailed description of its high-performance implementation based on the OpenMP directive-based parallelism for shared memory environments, to harness the multi-thread and vectorization features of modern processor architectures.

</details>


### [33] [The Origins of Planets for ArieL (OPAL) Key Science Project: the end-to-end planet formation campaign for the ESA space mission Ariel](https://arxiv.org/abs/2601.16841)
*Danae Polychroni,Diego Turrini,Romolo Politi,Sergio Fonte,Eugenio Schisano,Elenia Pacetti,Paolo Matteo Simonetti,Michele Zusi,Sergio Molinari,Stavro Ivanovski*

Main category: astro-ph.EP

TL;DR: The OPAL project creates a synthetic atmosphere library for the Ariel exoplanet mission to understand planetary compositional diversity by modeling genetic links between planets, protoplanetary disks, and host stars.


<details>
  <summary>Details</summary>
Motivation: To prepare for the ESA Ariel mission that will characterize hundreds of exoplanetary atmospheres, there's a need for realistic synthetic atmosphere models to test and validate analysis codes and pipelines before launch.

Method: The OPAL project develops an interdisciplinary pipeline tracing genetic links between planets, their native protoplanetary disks, and host stars, using high-performance computing to model complex parameter spaces.

Result: Early results reveal great diversity in planetary outcomes emerging from large degeneracy in initial conditions of planet formation, highlighting the complexity of high-dimensionality problems.

Conclusion: Interdisciplinary modeling supported by high-performance computing is essential for investigating the high-dimensionality problems of planetary formation and atmospheric composition diversity.

Abstract: The growing body of atmospheric observations of exoplanets from space and ground-based facilities showcases how the great diversity of the planetary population is not limited to their physical properties but extends to their compositions. The ESA space mission Ariel will observe and characterise hundreds of exoplanetary atmospheres to explore and understand the roots of this compositional diversity. To lay the foundations for the Ariel mission, the OPAL Key Science Project is tasked with creating an unprecedented library of realistic synthetic atmospheres spanning tens of elements and hundreds of molecules on which the Ariel consortium will test and validate its codes and pipelines ahead of launch. In this work we describe the aims and the pipeline of codes of the OPAL project, as well as the process through which we trace the genetic link connecting planets to their native protoplanetary disks and host stars. We present the early results of this complex and unprecedented endeavour and discuss how they highlight the great diversity of outcomes that emerge from the large degeneracy in the parameter space of possible initial conditions to the planet formation process. This, in turn, illustrates the growing importance of interdisciplinary modelling studies supported by high-performance computing methods and infrastructures to properly investigate this class of high-dimensionality problems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [34] [Electronic structure, phase stability, and transport properties of the AlTiVCr lightweight high-entropy alloy: A computational study](https://arxiv.org/abs/2601.16528)
*Christopher D. Woodgate,Hubert J. Naguszewski,Nicolas F. Piwek,David Redka*

Main category: cond-mat.mtrl-sci

TL;DR: AlTiVCr high-entropy alloy shows B2 ordering at high temperatures, increasing resistivity due to reduced electronic states at Fermi level, with full ordering and zero resistivity predicted at low temperatures.


<details>
  <summary>Details</summary>
Motivation: To understand the thermodynamics, phase stability, and electronic transport properties of the AlTiVCr lightweight high-entropy alloy, which is technologically relevant but not fully understood at atomic scale.

Method: Combined approach using ab initio electronic structure calculations, concentration wave analysis, and atomistic Monte Carlo simulations for thermodynamics, plus Kubo-Greenwood linear response framework for electronic transport properties.

Result: Predicted B2 chemical ordering at high temperatures driven by Al/Ti separation, increasing residual resistivity counter-intuitively due to reduced density of states at Fermi level. Low-temperature simulations reveal full ordering with vanishing resistivity.

Conclusion: The study provides new atomic-scale insights into AlTiVCr's structure-property relationships, explaining how chemical ordering affects electronic transport and predicting a fully-ordered ground state with zero residual resistivity.

Abstract: We investigate the thermodynamics and phase stability of the AlTiVCr lightweight high-entropy alloy using a combination of ab initio electronic structure calculations, a concentration wave analysis, and atomistic Monte Carlo simulations. In alignment both with experimental data and with results obtained using other computational approaches, we predict a $\textrm{B2}$ (CsCl) chemical ordering emerging in this alloy at comparatively high temperatures, which is driven by Al and Ti moving to separate sublattices, while V and Cr express weaker site preferences. The impact of this $\textrm{B2}$ chemical ordering on the electronic transport properties of the alloy is investigated within a Kubo-Greenwood linear response framework and it is found that, counter-intuitively, the alloy's residual resistivity increases as the material transitions from the $\textrm{A2}$ (disordered bcc) phase to our predicted $\textrm{B2}$ (partially) ordered structure. This is understood to result primarily from a reduction in the density of electronic states at the Fermi level induced by the chemical ordering. At low temperatures, our atomistic Monte Carlo simulations then reveal subsequent sublattice orderings, with the ground-state configuration predicted to be a fully-ordered, single-phase structure with vanishing associated residual resistivity. These results give fresh, insight into the atomic-scale structure and consequent physical properties of this well-studied, technologically relevant material.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [35] [Simulations of multi-phase gas in and around galaxies](https://arxiv.org/abs/2601.16566)
*Max Gronke,Evan Schneider*

Main category: astro-ph.GA

TL;DR: A comprehensive review of numerical simulations for multiphase gas in astrophysical environments, covering physical challenges, idealized setups, and large-scale simulations from galactic to cosmological scales.


<details>
  <summary>Details</summary>
Motivation: Multiphase gas (from cold molecular clouds to hot plasma) is fundamental across astrophysical environments but challenging to simulate due to multi-scale, multi-physics nature. Accurate simulation is critical for understanding galaxy formation and evolution.

Method: Review paper analyzing numerical simulation approaches: starts with physical/computational challenges, discusses idealized setups (turbulent mixing layers, cloud-wind interactions, thermal instability), then progresses to less idealized simulations (supernovae bubbles, tall boxes, isolated galaxies, cosmological zoom-ins).

Result: Provides systematic overview of simulation methodologies across scales, emphasizing scale connections, robust diagnostics, and observational comparisons. Identifies key approaches for modeling multiphase gas dynamics in different astrophysical contexts.

Conclusion: Multiphase gas simulation requires bridging scales and physics. The review outlines persistent challenges and promising future directions for simulating the multiphase Universe, highlighting the importance of connecting simulations to observations.

Abstract: Multiphase gas -- ranging from cold molecular clouds ($\lesssim 100\,$K) to hot, diffuse plasma ($\gtrsim 10^6\,$K) is a defining feature of the interstellar, circumgalactic, intracluster, and intergalactic media. Accurately simulating its dynamics is critical to improving our understanding of galaxy formation and evolution, however, due to their multi-scale and multi-physics nature, multiphase systems are highly challenging to model. In this review, we provide a comprehensive overview of numerical simulations of multiphase gas in and around galaxies. We begin by outlining the environments where multiphase gas arises and the physical and computational challenges associated with its modeling. Key quantities that characterize multiphase gas dynamics are discussed, followed by an in-depth look at idealized setups such as turbulent mixing layers, cloud-wind interactions, thermal instability, and turbulent boxes. The review then transitions to less idealized and/or larger-scale simulations, covering radiative supernovae bubbles, tall box simulations, isolated galaxy models including dwarf and Milky Way-mass systems, and cosmological zoom-in simulations, with a particular focus on simulations that enhance resolution in the halo. Throughout, we emphasize the importance of connecting scales, extracting robust diagnostics, and comparing simulations to observations. We conclude by outlining persistent challenges and promising directions for future work in simulating the multiphase Universe.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [36] [A Constructive Cayley Representation of Orthogonal Matrices and Applications to Optimization](https://arxiv.org/abs/2601.16271)
*Iwo Biborski*

Main category: math.OC

TL;DR: Constructive algorithm to find diagonal signature matrix D such that Cayley transform of DU is well-defined for any real orthogonal matrix U, enabling representation U = D(I-S)(I+S)^{-1} with skew-symmetric S.


<details>
  <summary>Details</summary>
Motivation: The Cayley transform is useful for representing orthogonal matrices via skew-symmetric generators, but it's not always well-defined for arbitrary orthogonal matrices. The motivation is to develop a constructive method to ensure the Cayley transform can be applied to any orthogonal matrix by pre-multiplying with a suitable diagonal signature matrix.

Method: The paper provides a constructive algorithm that, given a real orthogonal matrix U, computes a diagonal matrix D with entries in {±1} such that the Cayley transform of DU is well-defined. The algorithm requires O(n³) arithmetic operations and produces explicit quantitative bounds on the associated skew-symmetric generator S.

Result: The main result is an efficient O(n³) algorithm that guarantees the existence of a diagonal signature matrix D for any orthogonal matrix U, enabling the representation U = D(I-S)(I+S)^{-1} where S is skew-symmetric. The algorithm also provides explicit bounds on the skew-symmetric generator S.

Conclusion: The algorithm enables reliable application of Cayley transforms to any orthogonal matrix, which has practical applications in optimization methods on the orthogonal group by controlling singularities that arise in Cayley-transform-based approaches.

Abstract: It is known that every real orthogonal matrix can be brought into the domain of the Cayley transform by multiplication with a suitable diagonal signature matrix. In this paper we provide a constructive and numerically efficient algorithm that, given a real orthogonal matrix $U$, computes a diagonal matrix $D$ with entries in $\{\pm1\}$ such that the Cayley transform of $DU$ is well defined. This yields a representation of $U$ in the form \[ U = D(I-S)(I+S)^{-1}, \] where $S$ is a skew-symmetric matrix. The proposed algorithm requires $O(n^{3})$ arithmetic operations and produces an explicit quantitative bound on the associated skew-symmetric generator. As an application, we show how this construction can be used to control singularities in Cayley-transform-based optimization methods on the orthogonal group.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [37] [SeeMPS: A Python-based Matrix Product State and Tensor Train Library](https://arxiv.org/abs/2601.16734)
*Paula García-Molina,Juan José Rodríguez-Aldavero,Jorge Gidi,Juan José García-Ripoll*

Main category: quant-ph

TL;DR: SeeMPS is a Python library for tensor network algorithms using Matrix Product States (MPS) and Quantized Tensor Train (QTT) formalisms to handle exponentially large vector spaces efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a comprehensive tool for implementing tensor network algorithms that can handle exponentially large vector spaces efficiently, bridging applications in quantum many-body physics and quantum-inspired numerical analysis.

Method: The library implements MPS and QTT formalisms as a finite precision linear algebra package, enabling both low-level operations (vector addition, linear transformations, Hadamard products) and high-level algorithms (solving linear equations, eigenvalue computations, Fourier transforms).

Result: SeeMPS provides a complete Python library that can compress exponentially large vector spaces using tensor network techniques, enabling efficient computation for both quantum physics and numerical analysis applications.

Conclusion: SeeMPS is a versatile tensor network library that enables efficient handling of exponentially large computational problems across multiple domains, from quantum physics to classical numerical analysis.

Abstract: We introduce SeeMPS, a Python library dedicated to implementing tensor network algorithms based on the well-known Matrix Product States (MPS) and Quantized Tensor Train (QTT) formalisms. SeeMPS is implemented as a complete finite precision linear algebra package where exponentially large vector spaces are compressed using the MPS/TT formalism. It enables both low-level operations, such as vector addition, linear transformations, and Hadamard products, as well as high-level algorithms, including the approximation of linear equations, eigenvalue computations, and exponentially efficient Fourier transforms. This library can be used for traditional quantum many-body physics applications and also for quantum-inspired numerical analysis problems, such as solving PDEs, interpolating and integrating multidimensional functions, sampling multivariate probability distributions, etc.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [38] [Stochastic Modeling and Resource Dimensioning of Multi-Cellular Edge Intelligent Systems](https://arxiv.org/abs/2601.16848)
*Jaume Anguera Peris,Joakim Jaldén*

Main category: cs.NI

TL;DR: Edge intelligence system dimensioning framework that jointly optimizes radio and computational resources under uncertainty to minimize deployment cost while meeting QoS constraints.


<details>
  <summary>Details</summary>
Motivation: Edge intelligence requires tight integration of wireless access and on-site computing for low-latency applications, but existing approaches either focus on run-time allocation or use simplified models that miss end-to-end correlations in large-scale deployments.

Method: Unified stochastic framework combining Poisson point processes for network topology modeling (random user/BS locations, interference, power control) with queueing theory and empirical AI inference workload profiling to derive tractable end-to-end delay expressions.

Result: Proves problem decomposes into convex subproblems yielding global optimality; identifies cost-efficient design regions showing trade-offs: smaller cells reduce transmission delay but increase computing cost, larger cells show opposite trend; densification only reduces computational costs when frequency reuse scales with BS density.

Conclusion: Joint dimensioning of radio and computational resources is crucial for edge intelligence systems; the proposed framework enables optimal deployment planning under uncertainty, revealing important design trade-offs between cell size, interference management, and resource utilization.

Abstract: Edge intelligence enables AI inference at the network edge, co-located with or near the radio access network, rather than in centralized clouds or on mobile devices. It targets low-latency, resource-constrained applications with large data volumes, requiring tight integration of wireless access and on-site computing. Yet system performance and cost-efficiency hinge on joint pre-deployment dimensioning of radio and computational resources, especially under spatial and temporal uncertainty. Prior work largely emphasizes run-time allocation or relies on simplified models that decouple radio and computing, missing end-to-end correlations in large-scale deployments. This paper introduces a unified stochastic framework to dimension multi-cell edge-intelligent systems. We model network topology with Poisson point processes, capturing random user and base-station locations, inter-cell interference, distance-based fractional power control, and peak-power constraints. By combining this with queueing theory and empirical AI inference workload profiling, we derive tractable expressions for end-to-end offloading delay. These enable a non-convex joint optimization that minimizes deployment cost under statistical QoS guarantees, expressed through strict tail-latency and inference-accuracy constraints. We prove the problem decomposes into convex subproblems, yielding global optimality. Numerical results in noise- and interference-limited regimes identify cost-efficient design regions and configurations that cause under-utilization or user unfairness. Smaller cells reduce transmission delay but raise per-request computing cost due to weaker server multiplexing, whereas larger cells show the opposite trend. Densification reduces computational costs only when frequency reuse scales with base-station density; otherwise, sparser deployments improve fairness and efficiency in interference-limited settings.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [39] [A Study of Improved Limiter Formulations for Second-Order Finite Volume Schemes Applied to Unstructured Grids](https://arxiv.org/abs/2601.16291)
*Frederico Bolsoni Oliveira,João Luiz F. Azevedo*

Main category: physics.flu-dyn

TL;DR: This paper compares three different limiter formulations (Venkatakrishnan, Wang's modification, and Nishikawa's R3) in second-order finite volume methods for turbulent flow simulations, finding they yield similar results with different dissipation characteristics when properly tuned.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the drawbacks of limiter functions in second-order finite volume methods. While limiters prevent spurious oscillations in MUSCL-like reconstructions, they can introduce undesirable effects to the numerical scheme. The paper aims to compare different limiter formulations to understand their behavior in practical turbulent flow simulations.

Method: The study uses a second-order finite volume scheme for steady, turbulent flows on unstructured meshes. Three limiter formulations are tested: original Venkatakrishnan limiter, Wang's modification to Venkatakrishnan limiter, and Nishikawa's R3 limiter. The analysis focuses on three configurations of a fully-developed, two-dimensional, transonic NACA 0012 airfoil with different angles of attack. The Reynolds-averaged Navier-Stokes (RANS) equations model gas dynamics, with the negative Spalart-Allmaras turbulence model for closure.

Result: All three limiters produce similar results for all tested configurations, but with different dissipative characteristics. The similarity in results occurs when the control constants are used within appropriate intervals. The numerical results show good agreement with experimental data from literature.

Conclusion: The study demonstrates that different limiter formulations can yield comparable results in practical turbulent flow simulations when properly tuned, though they exhibit different dissipation properties. This suggests that while limiters are necessary to prevent oscillations, careful selection and tuning of limiter parameters is crucial for optimal performance in finite volume methods.

Abstract: A general, compact way of achieving second-order in finite-volume numerical methods is to perform a MUSCL-like, piecewise linear reconstruction of flow properties at each cell interface. To avoid the surge of spurious oscillations in the discrete solution, a limiter function is commonly employed. This strategy, however, can add a series of drawbacks to the overall numerical scheme. The present paper investigates this behavior by considering three different limiter formulations in the context of a second-order, finite volume scheme for the simulation of steady, turbulent flows on unstructured meshes. Three limiter formulations are considered: the original Venkatakrishnan limiter, Wang's modification to the Venkatakrishnan limiter and Nishikawa's recently introduced R3 limiter. Three different configurations of the fully-developed, two-dimensional, transonic NACA 0012 airfoil are analyzed, configured with different angles of attack and similar freestream properties. The gas dynamics are modeled using the Reynolds-averaged Navier-Stokes (RANS) equations, where the negative Spalart-Allmaras turbulence model is used to solve the closure problem. All limiters are shown to yield similar results for all configurations of this case, although with different dissipative characteristics, provided their control constants are used within appropriate intervals. The presented numerical results are in good agreement with experimental data available in the literature.

</details>


### [40] [Libby-Fox perturbations and the analytic adjoint solution for laminar viscous flow along a flat plate](https://arxiv.org/abs/2601.16718)
*Carlos Lozano,Jorge Ponsin*

Main category: physics.flu-dyn

TL;DR: Analysis of adjoint 2D boundary layer equations on flat plate using Libby-Fox theory, examining algebraic perturbations to Blasius boundary layer and extending to Falkner-Skan cases.


<details>
  <summary>Details</summary>
Motivation: To investigate properties of adjoint boundary layer equations and understand algebraic perturbations to the classical Blasius boundary layer solution, with extension to cases with pressure gradients.

Method: Using Libby-Fox theory to analyze adjoint 2D boundary layer equations, obtaining adjoint solution from Green's function as sum over infinite perturbation modes of Blasius solution, and analyzing eigenvalue/eigenfunction constraints.

Result: Obtained constraints on eigenvalues and eigenfunctions through analysis of adjoint solution, with extension to Falkner-Skan solutions for non-zero pressure gradient cases.

Conclusion: The adjoint boundary layer analysis provides insights into perturbation behavior and eigenvalue constraints for both Blasius and Falkner-Skan boundary layers, extending theoretical understanding of boundary layer stability.

Abstract: The properties of the solution to the adjoint two-dimensional boundary layer equations on a flat plate are investigated from the viewpoint of Libby-Fox theory that describes the algebraic perturbations to the Blasius boundary layer. The adjoint solution is obtained from the Green's function of the perturbation equation as a sum over the infinite perturbation modes of the Blasius solution. The analysis of the solution allows us to obtain constraints on the eigenvalues and eigenfunctions. The extension of the analysis to the case with non-zero pressure gradient, corresponding to the Falkner-Skan solution, is also briefly discussed.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [41] [Fractals in rate-induced tipping](https://arxiv.org/abs/2601.16373)
*Jason Qianchuan Wang,Yi Zheng,Eduardo G. Altmann*

Main category: nlin.CD

TL;DR: Rate-induced tipping can create fractal patterns in parameter space when non-attracting fractal sets exist in autonomous systems, changing the simple tipping point picture.


<details>
  <summary>Details</summary>
Motivation: To understand how rate-induced tipping (critical transitions from fast parameter changes) behaves differently when non-attracting fractal sets exist in the autonomous system, moving beyond the simple tipping point model.

Method: Develop general theory explaining how fractals in phase space induce fractals in parameter space, relating their fractal dimensions. Illustrate with three paradigmatic systems: piecewise linear 1D map, 2D Hénon map, and forced pendulum.

Result: Fractals in phase space create fractals in parameter space that control which rates and parameter changes cause tipping, changing the simple critical rate picture to a fractal structure.

Conclusion: The presence of non-attracting fractal sets fundamentally alters rate-induced tipping, creating fractal boundaries in parameter space rather than simple tipping points, with implications for understanding transitions in nonlinear systems.

Abstract: When parameters of a dynamical system change sufficiently fast, critical transitions can take place even in the absence of bifurcations. This phenomenon is known as rate-induced tipping and has been reported in a variety of systems, from simple ordinary differential equations and maps to mathematical models in climate sciences and ecology. In most examples, the transition happens at a critical rate of parameter change, a rate-induced tipping point, and is associated with a simple unstable orbit (edge state). In this work, we show how this simple picture changes when non-attracting fractal sets exist in the autonomous system, a ubiquitous situation in non-linear dynamics. We show that these fractals in phase space induce fractals in parameter space, which control the rates and parameter changes that result in tipping. We explain how such rate-induced fractals appear and how the fractal dimensions of the different sets are related to each other. We illustrate our general theory in three paradigmatic systems: a piecewise linear one-dimensional map, the two-dimensional Hénon map, and a forced pendulum.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [42] [The Pauli-Villars-regularized Dirac vacuum in electromagnetic fields at positive temperature](https://arxiv.org/abs/2601.16743)
*William Borrelli,Umberto Morellini*

Main category: math-ph

TL;DR: The paper develops a model of the Dirac vacuum in classical electromagnetic fields at positive temperature using Pauli-Villars regularization to define the free energy, extending previous work on purely magnetic cases.


<details>
  <summary>Details</summary>
Motivation: To understand polarization effects in the vacuum at positive temperature in the presence of both electrostatic and magnetic potentials, building on previous work limited to purely magnetic cases.

Method: Adopts the Pauli-Villars regularization technique to properly define the free energy of the Dirac vacuum in classical electromagnetic fields at positive temperature.

Result: Extends previous work on purely magnetic cases to include both electrostatic and magnetic potentials, establishing a framework for studying vacuum polarization at positive temperature.

Conclusion: This work serves as a foundational step toward understanding polarization effects in quantum vacuum at finite temperature with combined electromagnetic potentials.

Abstract: In this paper we consider a model of the Dirac vacuum in classical electromagnetic fields at positive temperature. We adopt the Pauli-Villars regularisation technique in order to properly define the free energy of the vacuum, extending the previous work by the second named author on the purely magnetic case. This work is intended as a first step in understanding polarisation effects in the vacuum at positive temperature, in presence of both electrostatic and magnetic potentials.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [43] [Distributional Computational Graphs: Error Bounds](https://arxiv.org/abs/2601.16250)
*Olof Hallqvist Elias,Michael Selby,Phillip Stanley-Marbell*

Main category: stat.ML

TL;DR: Analysis of discretization error in distributional computational graphs using Wasserstein-1 distance bounds


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the error that arises when continuous probability distributions in computational graphs are approximated using discrete representations or empirical distributions from samples

Method: Study distributional computational graphs where inputs are probability distributions, analyze discretization error using Wasserstein-1 distance, establish non-asymptotic error bounds without structural assumptions on the computational graph

Result: Established non-asymptotic error bounds in terms of Wasserstein-1 distance for discretization error in distributional computational graphs

Conclusion: Provides theoretical foundation for error analysis in distributional computational graphs, enabling better understanding of approximation quality when using discrete representations of continuous distributions

Abstract: We study a general framework of distributional computational graphs: computational graphs whose inputs are probability distributions rather than point values. We analyze the discretization error that arises when these graphs are evaluated using finite approximations of continuous probability distributions. Such an approximation might be the result of representing a continuous real-valued distribution using a discrete representation or from constructing an empirical distribution from samples (or might be the output of another distributional computational graph). We establish non-asymptotic error bounds in terms of the Wasserstein-1 distance, without imposing structural assumptions on the computational graph.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Multigrade Neural Network Approximation](https://arxiv.org/abs/2601.16884)
*Shijun Zhang,Zuowei Shen,Yuesheng Xu*

Main category: cs.LG

TL;DR: Multigrade deep learning (MGDL) trains deep networks grade-by-grade, freezing previous layers and training new residual blocks to reduce approximation error, providing theoretical guarantees for vanishing error.


<details>
  <summary>Details</summary>
Motivation: Training very deep neural networks is challenging due to non-convex, ill-conditioned optimization landscapes, while shallow networks (especially one-hidden-layer ReLU models) admit convex reformulations with global guarantees. This motivates a structured approach that maintains stability while scaling to depth.

Method: MGDL trains deep networks hierarchically: previously learned grades (layers/blocks) are frozen, and each new residual block is trained solely to reduce the remaining approximation error. This creates an interpretable, stable refinement process based on operator-theoretic foundations.

Result: Theoretical proof that for any continuous target function, there exists a fixed-width multigrade ReLU scheme whose residuals decrease strictly across grades and converge uniformly to zero. This provides the first rigorous guarantee that grade-wise training yields provable vanishing approximation error in deep networks.

Conclusion: MGDL offers a principled framework for structured error refinement in deep networks with theoretical guarantees of convergence, addressing optimization challenges in deep learning while maintaining interpretability and stability through hierarchical training.

Abstract: We study multigrade deep learning (MGDL) as a principled framework for structured error refinement in deep neural networks. While the approximation power of neural networks is now relatively well understood, training very deep architectures remains challenging due to highly non-convex and often ill-conditioned optimization landscapes. In contrast, for relatively shallow networks, most notably one-hidden-layer $\texttt{ReLU}$ models, training admits convex reformulations with global guarantees, motivating learning paradigms that improve stability while scaling to depth. MGDL builds upon this insight by training deep networks grade by grade: previously learned grades are frozen, and each new residual block is trained solely to reduce the remaining approximation error, yielding an interpretable and stable hierarchical refinement process. We develop an operator-theoretic foundation for MGDL and prove that, for any continuous target function, there exists a fixed-width multigrade $\texttt{ReLU}$ scheme whose residuals decrease strictly across grades and converge uniformly to zero. To the best of our knowledge, this work provides the first rigorous theoretical guarantee that grade-wise training yields provable vanishing approximation error in deep networks. Numerical experiments further illustrate the theoretical results.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [45] [Ultrafast Dipolar Electrostatic Modeling of Plasmonic Nanoparticles with Arbitrary Geometry](https://arxiv.org/abs/2601.16797)
*Paulo S. S. dos Santos,João P. Mendes,José M. M M. de Almeida,Luís C. C. Coelho*

Main category: physics.optics

TL;DR: Ultrafast method for LSPR calculations using dipolar approximation and geometric formulation, avoiding large eigenproblems for rapid parametric studies.


<details>
  <summary>Details</summary>
Motivation: Full-wave numerical methods like BEM and DDA are computationally expensive for rapid parametric studies of localized surface plasmon resonances in metallic nanoparticles, which are essential for sensing, nano-optics, and energy harvesting applications.

Method: Retains only dipolar component of induced surface charge density through expansion into Cartesian dipole basis, yielding compact 3×3 geometric formulation. Uses projection of Neumann-Poincaré surface operator onto dipole subspace with Rayleigh quotient to obtain spectral response without solving N×N eigenproblems. Incorporates retardation effects via modified long-wavelength approximation (MLWA).

Result: Geometry-dependent quantities computed once per nanoparticle, with material dispersion and environmental changes handled through simple algebraic expressions for polarizability. Enables rapid evaluation across wavelengths with significantly lower computational cost than BEM, DDA, and other standard tools.

Conclusion: The framework provides a valuable tool for fast modeling and optimization of plasmonic nanoparticles, extending accuracy into weakly retarded regime while maintaining computational efficiency for parametric studies.

Abstract: Accurate and fast calculations of localized surface plasmon resonances (LSPR) in metallic nanoparticles is essential for applications in sensing, nano-optics, and energy harvesting. Although full-wave numerical techniques such as the boundary element method (BEM) or the discrete dipole approximation (DDA) provide high accuracy, their computational cost often hinders rapid parametric studies. Here it is presented an ultrafast method that avoids solving large eigenproblems. Instead, only the dipolar component of the induced surface charge density \((σ_{dipolar})\) is retained through a expansion into Cartesion dipole basis, yielding a compact $3\times3$ geometric formulation that avoids full boundary-integral solves. The spectral response is obtained in a similar way, by projecting the Neumann--Poincaré surface operator onto the dipole subspace and evaluating a Rayleigh quotient, giving geometry-only eigenvalues again without an $N\times N$ eigenproblem. A major advantage of this method is that all geometry-dependent quantities are computed once per nanoparticle, while material dispersion and environmental changes enter only through simple algebraic expressions for the polarizability, enabling rapid evaluation across wavelengths. Retardation effects are incorporated through the modified long-wavelength approximation (MLWA), extending accuracy into the weakly retarded regime. The resulting framework provides a valuable tool for fast modelling and optimization of plasmonic nanoparticles at a significant lesser computational cost than BEM, DDA, and other standard tools.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [46] [Minimal Graph Transformations and their Classification](https://arxiv.org/abs/2601.16783)
*Sam K Mathew*

Main category: math.DG

TL;DR: Complete classification of minimal graph surfaces that can be transformed into other minimal surfaces via height function mappings, solving the Non-Trivial Minimal Graph Transformation Problem through PDE reduction to ODE and elliptic function analysis.


<details>
  <summary>Details</summary>
Motivation: To understand and classify non-trivial transformations between minimal graph surfaces beyond obvious transformations like translations and reflections, addressing the fundamental question of when one minimal surface can be transformed into another via graphical mappings.

Method: Formulated the Non-Trivial Minimal Graph Transformation Problem as a coupled PDE system, reduced it to a modified harmonic function problem using complex variable approach, then further reduced to solving a fundamental ODE parameterized by constant k, integrated using elliptic integrals and elliptic function identities.

Result: Full classification of all admissible minimal surfaces and their transformations by solving the ODE for three cases (k=0, k>0, k<0), yielding several new families of minimal surfaces previously unknown in the literature.

Conclusion: Successfully solved the Non-Trivial Minimal Graph Transformation Problem, providing complete classification of minimal graph surfaces that admit graphical transformations, discovering new families of minimal surfaces through rigorous mathematical analysis of elliptic function solutions.

Abstract: This paper presents a complete classification of minimal graph surfaces that admit graphical transformations into other minimal surfaces. These transformations are functions that map the height function of a minimal graph surface to another height function, which also describes a minimal graph surface. While trivial maps such as translations and reflections exist, we formulate and solve the Non-Trivial Minimal Graph Transformation Problem, governed by a coupled system of partial differential equations. A central result establishes the rigorous equivalence of this original system to a modified problem for a harmonic function. Through a complex variable approach and a weakening technique, the analysis is reduced to solving a fundamental ordinary differential equation parameterized by a real constant k. The explicit integration of this ordinary differential equation involves various elliptic integrals and identities of elliptic functions. Solving the ordinary differential equation for the three cases: when the constant k equals zero, when k is greater than zero, and when k is less than zero yields the full classification of all admissible surfaces and their associated transformations. This process yields several classes of minimal surfaces that, to the best of the author's knowledge, constitute new families of minimal surfaces.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [47] [Monotonicity of the first Dirichlet eigenvalue of regular polygons](https://arxiv.org/abs/2601.16285)
*Joel Dahne,Javier Gómez-Serrano,Joana Pech-Alberich*

Main category: math.SP

TL;DR: The paper proves that the first Dirichlet eigenvalue of regular polygons with fixed area decreases as the number of sides increases, confirming a 2006 conjecture.


<details>
  <summary>Details</summary>
Motivation: The motivation is to settle a long-standing conjecture by Antunes and Freitas from 2006 about the monotonic behavior of Dirichlet eigenvalues for regular polygons with fixed area as the number of sides increases.

Method: The authors use mathematical analysis and proof techniques to establish monotonicity properties of the first Dirichlet eigenvalue λ₁ᴺ for N-sided regular polygons with fixed area.

Result: The main result proves that λ₁ᴺ is monotonically decreasing in N for all N ≥ 3, and also establishes monotonicity of the eigenvalue quotients λ₁ᴺ/λ₁ᴺ⁺¹.

Conclusion: The paper successfully settles the Antunes-Freitas conjecture from 2006, providing rigorous mathematical proof that as regular polygons with fixed area become more circular (increasing sides), their first Dirichlet eigenvalues decrease monotonically.

Abstract: In this paper we prove that the first Dirichlet eigenvalue $λ_1^N$ of an $N$-sided regular polygon of fixed area is a monotonically decreasing function of $N$ for all $N \geq 3$, as well as the monotonicity of the quotients $\displaystyle \frac{λ_1^{N}}{λ_1^{N+1}}$. This settles a conjecture of Antunes-Freitas from 2006 [P. Antunes, P. Freitas, Experiment. Math., 15(3):333-342, 2006].

</details>
