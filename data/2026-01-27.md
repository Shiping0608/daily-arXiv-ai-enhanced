<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 27]
- [math.AP](#math.AP) [Total: 28]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [stat.ML](#stat.ML) [Total: 1]
- [math.SP](#math.SP) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 6]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [physics.optics](#physics.optics) [Total: 4]
- [math.DG](#math.DG) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 3]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Boundary Integral Formulation of an Acoustic Boundary Layer Model in 2D](https://arxiv.org/abs/2601.17283)
*Jacob Linden,Travis Askham,Jeremy Hoskins*

Main category: math.NA

TL;DR: Boundary integral formulation for 2D Helmholtz equation with visco-thermal boundary conditions enabling accurate simulation of viscous/thermal losses near boundaries in acoustic devices with narrow features.


<details>
  <summary>Details</summary>
Motivation: Accurate simulation of viscous and thermal losses near boundaries is crucial for acoustic devices with narrow features, where boundary layer effects significantly impact performance.

Method: Developed boundary integral formulation using cancellations between hyper-singular operators, method of images variant, and analytic pre-conditioners to derive Fredholm second-kind integral equations.

Result: Derived integral equations that are Fredholm second-kind (up to boundedly invertible operator application), enabling fast and accurate solution of acoustics problems with boundary layers.

Conclusion: The formulation provides an effective approach for solving acoustics problems with boundary layers, particularly suitable for devices with narrow features where visco-thermal losses are significant.

Abstract: We present a boundary integral formulation of the Helmholtz equation with visco-thermal boundary conditions, in two dimensions. Such boundary conditions allow for the accurate simulation of viscous and thermal losses in the vicinity of the boundary, which are particularly relevant in acoustic devices with narrow features. Using cancellations between hyper-singular operators, a variant of the method of images technique, and analytic pre-conditioners, we derive integral equations that are Fredholm second-kind, up to the application of a boundedly invertible operator. This approach allows for the fast and accurate solution of acoustics problems with boundary layers.

</details>


### [2] [A New Look at the Ensemble Kalman Filter for Inverse Problems: Duality, Non-Asymptotic Analysis and Convergence Acceleration](https://arxiv.org/abs/2601.17305)
*C G Krishnanunni,Jonathan Wittmer,Tan Bui-Thanh,Quoc P. Nguyen*

Main category: math.NA

TL;DR: The paper presents new duality perspective for Ensemble Kalman Filter (EnKF) for inverse problems, derives non-asymptotic convergence results, and proposes two multiplicative covariance correction strategies (EnKI-MC I & II) to accelerate convergence of Ensemble Kalman Inversion.


<details>
  <summary>Details</summary>
Motivation: To develop better theoretical understanding of EnKF for inverse problems through Lagrangian dual perspective, and to improve convergence of Ensemble Kalman Inversion algorithms with more efficient covariance correction strategies.

Method: 1) Derive EnKF from sample average approximation of Lagrangian dual function; 2) Propose two multiplicative covariance correction strategies: EnKI-MC (I) with adaptive correction to sample covariance matrix, and EnKI-MC (II) with ensemble-specific corrections; 3) Analyze convergence through fixed-point iteration perspective.

Result: 1) Novel non-asymptotic convergence result for EnKF; 2) Theoretical guarantee for convergence of EnKI-MC (I); 3) Numerical verification on four inverse problems showing proposed strategies lead to faster convergence and better quality solutions compared to existing techniques.

Conclusion: The Lagrangian dual perspective provides new theoretical insights into EnKF, and the proposed multiplicative covariance correction strategies significantly improve convergence and solution quality for Ensemble Kalman Inversion algorithms.

Abstract: This work presents new results and understanding of the Ensemble Kalman filter (EnKF) for inverse problems. In particular, using a Lagrangian dual perspective we show that EnKF can be derived from the sample average approximation (SAA) of the Lagrangian dual function. The beauty of this new duality perspective is that it facilitates us to prove and numerically verify a novel non-asymptotic convergence result for the EnKF. Motivated by the new perspective, we also present a new convergence improvement strategy for the Ensemble Kalman Inversion Algorithm (EnKI), which is an iterative version of the EnKF for inverse problems. In particular, we propose an adaptive multiplicative correction to the sample covariance matrix at each iteration and we call this new algorithm as EnKI-MC (I). Based on the new duality perspective, we derive an expression for the optimal correction factor at each iteration of the EnKI algorithm to accelerate the convergence. In addition, we also consider an ensemble specific multiplicative covariance correction strategy (EnKI-MC (II)) where a different correction is employed for each ensemble. By viewing EnKI through the lens of fixed-point iteration, we also provide theoretical results that guarantees the convergence of EnKI-MC (I) algorithm. Numerical investigations for the deconvolution problem, initial condition inversion in advection-convection problem, initial condition inversion in a Lorenz 96 model, and inverse problem constrained by elliptic partial differential equation are conducted to verify the non-asymptotic results for EnKF and to assess the performance of convergence improvement strategies for EnKI. The numerical results suggest that the proposed strategies for EnKI not only led to faster convergence in comparison to the currently employed techniques but also better quality solutions at termination of the algorithm.

</details>


### [3] [The direct-line method for forward and inverse linear elasticity problems of composite materials in general domains with multiple singularities](https://arxiv.org/abs/2601.17318)
*Qinghua Wei,Xiaopeng Zhu,Zhongyi Huang*

Main category: math.NA

TL;DR: A domain decomposition and direct-line method for solving forward/inverse elasticity problems in composite materials with multiple singularities, featuring optimal error estimates and TV regularization for inverse problems.


<details>
  <summary>Details</summary>
Motivation: To solve linear elasticity problems in composite materials with general domains containing multiple singularities, which are difficult for most existing methods to handle effectively.

Method: Combines domain decomposition (splitting general domains into star-shaped subdomains) with direct-line method (for rapid convergence of eigenvalues and singularity capture). For inverse problems, uses energy functional minimization with total variational regularization, employing the forward method as solver.

Result: Establishes optimal error estimates, demonstrates ability to handle multiple singular points in general regions, and can simultaneously reconstruct heterogeneous Lamé coefficients (μ and λ) between different materials through numerical experiments.

Conclusion: The proposed method provides accurate and reliable solution for both forward and inverse elasticity problems in general domains with multiple singularities, validated through systematic numerical experiments on three test problems.

Abstract: In this work, a combined strategy of domain decomposition and the direct-line method is implemented to solve the forward and inverse linear elasticity problems of composite materials in general domains with multiple singularities. Domain decomposition technology treats the general domain as the union of some star-shaped subdomains, which can be handled using the direct-line method. The direct-line method demonstrates rapid convergence of the semi-discrete eigenvalues towards the exact eigenvalues of the elliptic operator, thereby naturally capturing the singularities. We also establish optimal error estimates for the proposed method. Especially, our method can handle multiple singular point problems in general regions, which are difficult to deal with by most methods. On the other hand, the inverse elasticity problem is constructed as a energy functional minimization problem with total variational regularization, we use the aforementioned method as a forward solver to reconstruct the lamé coefficient of multiple singular points in general regions. Our method can simultaneously deduce heterogeneous $μ$ and $λ$ between different materials. Through numerical experiments on three forward and inverse problems, we systematically verified the accuracy and reliability of this method to solve forward and inverse elastic problems in general domains with multiple singularities.

</details>


### [4] [Operator splitting based diffusion samplers and improved convergence analysis](https://arxiv.org/abs/2601.17375)
*Peiyi Liu,Zhaoqiang Liu,Yiqi Gu*

Main category: math.NA

TL;DR: A new class of diffusion model samplers using operator-splitting technique achieves sharper error bounds with quadratic dependence on step size.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient and accurate samplers for diffusion models with improved theoretical guarantees compared to existing methods.

Method: Operator-splitting technique that alternates between linear drift term and nonlinear score-driven drift of probability flow ODE, with detailed analysis of second-order samplers.

Result: Established non-asymptotic total variation distance error bound of O(d/T² + √dε_score + dε_Jac), which is sharper than existing works and shows quadratic dependence on step size.

Conclusion: The proposed operator-splitting samplers provide improved theoretical guarantees with quadratic convergence in step size, validated by numerical experiments.

Abstract: In this paper, we develop a class of samplers for the diffusion model using the operator-splitting technique. The linear drift term and the nonlinear score-driven drift of the probability flow ordinary differential equation are split and applied by flow maps alternatively. Moreover, we conduct detailed analyses for the second-order sampler, establishing a non-asymptotic total variation distance error bound of order $O(d/T^2+\sqrt{d}\varepsilon_{\mathrm{score}}+d\varepsilon_{\mathrm{Jac}})$, where $d$ is the data dimension; $T$ is the number of sampling steps; $\varepsilon_{\mathrm{score}}$ and $\varepsilon_{\mathrm{Jac}}$ measure the discrepancy between the actual score function and learned score function. Our bound is sharper than existing works, yielding bounds of $O(d^p/T^2)$ with some $p>1$ for specific second-order samplers. Numerical experiments on a two-dimensional synthetic dataset corroborate the established quadratic dependence on the step size $1/T$ in the error bound.

</details>


### [5] [An algorithmic approach to direct spline products: procedures and computational aspects](https://arxiv.org/abs/2601.17432)
*Francesco Patrizi,Alessandra Sestini*

Main category: math.NA

TL;DR: Efficient algorithm for computing spline products in B-spline basis using direct formula approach, overcoming ill-conditioning issues of implicit methods.


<details>
  <summary>Details</summary>
Motivation: Implicit methods like collocation can fail due to severe ill-conditioning of system matrices, while direct formula approach remains robust but computationally expensive.

Method: Recast direct formula into algorithmic framework based on Oslo Algorithm, then enhance through factorization of terms to dramatically improve computational efficiency.

Result: Proposed method achieves substantial reduction in computational cost while maintaining robustness against ill-conditioning issues.

Conclusion: The enhanced direct formula algorithm provides efficient, numerically stable solution for spline product computation, overcoming limitations of traditional implicit methods.

Abstract: We introduce an efficient algorithmic procedure for implementing the direct formula that represents the product of splines in the B-spline basis. We first demonstrate the relevance of this direct approach through numerical evidences showing that implicit methods, such as collocation, may fail in some instances due to severe ill-conditioning of the associated system matrices, whereas the direct formula remains robust. We then recast the direct formula into an algorithmic framework based on the Oslo Algorithm and subsequently enhance it, through a factorization of the terms to be computed, to dramatically improve computational efficiency. Extensive numerical experiments illustrate the substantial reduction in computational cost achieved by the proposed method. Implementation aspects are also discussed to ensure numerical stability and applicability.

</details>


### [6] [Numerical Study of Dissipative Weak Solutions for the Euler Equations of Gas Dynamics](https://arxiv.org/abs/2601.17452)
*Shaoshuai Chu,Michael Herty,Alexander Kurganov,Maria Lukacova-Medviova,Changsheng Yu*

Main category: math.NA

TL;DR: Numerical study of dissipative weak solutions to Euler equations using various high-order central-upwind and finite volume methods, showing scheme-dependent convergence to generalized solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate how different high-order numerical schemes approximate dissipative weak solutions of Euler equations, and understand scheme-dependent convergence behavior.

Method: Use first- to ninth-order local characteristic decomposition-based central-upwind (LCDCU), low-dissipation central-upwind (LDCU), and viscous finite volume (VFV) methods with A-WENO framework for higher-order extensions. Apply to 2D Riemann problems and Kelvin-Helmholtz instability test.

Result: Methods converging only weakly in space and time produce generalized dissipative weak solutions approximated via K-convergence, dependent on numerical scheme. Young measures computed and solutions compared using entropy production and energy defect criteria.

Conclusion: Numerical approximations of Euler equations yield scheme-dependent dissipative weak solutions, with convergence behavior characterized by K-convergence and measurable through entropy production and energy defects.

Abstract: We study dissipative weak (DW) solutions of the Euler equations of gas dynamics using the first-, second-, third-, fifth-, seventh-, and ninth-order local characteristic decomposition-based central-upwind (LCDCU), low-dissipation central-upwind (LDCU), and viscous finite volume (VFV) methods, whose higher-order extensions are obtained via the framework of the alternative weighted essentially non-oscillatory (A-WENO) schemes. These methods are applied to several benchmark problems, including several two-dimensional Riemann problems and a Kelvin-Helmholtz instability test. The numerical results demonstrate that for methods converging only weakly in space and time, the limiting solutions are generalized DW solutions, approximated in the sense of ${\cal K}$-convergence and dependent on the numerical scheme. For all of the studied methods, we compute the associated Young measures and compare the DW solutions using entropy production and energy defect criteria.

</details>


### [7] [Sparse RBF Networks for PDEs and nonlocal equations: function space theory, operator calculus, and training algorithms](https://arxiv.org/abs/2601.17562)
*Zihan Shao,Konstantin Pieper,Xiaochuan Tian*

Main category: math.NA

TL;DR: Systematic analysis and extension of SparseRBFnet for solving nonlinear PDEs, providing theoretical characterization of solution spaces, quasi-analytical operator evaluation, and computational analysis of adaptive training strategies.


<details>
  <summary>Details</summary>
Motivation: To consolidate and generalize the theoretical and computational framework of SparseRBFnet for solving nonlinear PDEs, providing a unified understanding of solution spaces, efficient operator evaluation, and guidance for algorithmic choices.

Method: Systematic analysis of SparseRBFnet's adaptive-width shallow kernel network formulation, including function-space characterization (Besov spaces), quasi-analytical evaluation of differential and nonlocal operators, and computational study of three-phase training strategy with comparisons to variants.

Result: The solution space admits characterization as a Besov space independent of kernel choice; explicit kernel structure enables quasi-analytical operator evaluation; empirical insensitivity to kernel choice demonstrated; trade-offs between accuracy, sparsity, and computational cost analyzed.

Conclusion: The work consolidates and generalizes SparseRBFnet framework, supporting accurate sparse representations with efficient operator evaluation and providing theory-grounded guidance for algorithmic and modeling choices in solving nonlinear PDEs.

Abstract: This work presents a systematic analysis and extension of the sparse radial basis function network (SparseRBFnet) previously introduced for solving nonlinear partial differential equations (PDEs). Based on its adaptive-width shallow kernel network formulation, we further investigate its function-space characterization, operator evaluation, and computational algorithm. We provide a unified description of the solution space for a broad class of radial basis functions (RBFs). Under mild assumptions, this space admits a characterization as a Besov space, independent of the specific kernel choice. We further demonstrate how the explicit kernel-based structure enables quasi-analytical evaluation of both differential and nonlocal operators, including fractional Laplacians. On the computational end, we study the adaptive-width network and related three-phase training strategy through a comparison with variants concerning the modeling and algorithmic details. In particular, we assess the roles of second-order optimization, inner-weight training, network adaptivity, and anisotropic kernel parameterizations. Numerical experiments on high-order, fractional, and anisotropic PDE benchmarks illustrate the empirical insensitivity to kernel choice, as well as the resulting trade-offs between accuracy, sparsity, and computational cost. Collectively, these results consolidate and generalize the theoretical and computational framework of SparseRBFnet, supporting accurate sparse representations with efficient operator evaluation and offering theory-grounded guidance for algorithmic and modeling choices.

</details>


### [8] [Sinh regularized Lagrangian nonuniform sampling series](https://arxiv.org/abs/2601.17685)
*Haixin Jiang,Xinyu Chen,Liang Chen*

Main category: math.NA

TL;DR: Sinh-type window function accelerates Lagrangian nonuniform sampling series convergence, outperforming Gaussian regularization.


<details>
  <summary>Details</summary>
Motivation: Recent window functions in nonuniform fast Fourier transforms and regularized Shannon sampling inspired development of faster-converging nonuniform sampling methods.

Method: Utilized sinh-type function to regularize Lagrangian nonuniform sampling series, creating sinh regularized nonuniform sampling series.

Result: Sinh regularization achieves superior convergence rate compared to fastest existing Gaussian regularized nonuniform sampling series, validated by theoretical error estimates and numerical experiments.

Conclusion: Sinh-type regularization provides effective acceleration for nonuniform sampling series, offering better performance than current state-of-the-art Gaussian regularization methods.

Abstract: Recently, some window functions have been introduced into the nonuniform fast Fourier transform and the regularized Shannon sampling. Inspired by these works, we utilize a sinh-type function to accelerate the convergence of the Lagrangian nonuniform sampling series. Our theoretical error estimates and numerical experiments demonstrate that the sinh regularized nonuniform sampling series achieves a superior convergence rate compared to the fastest existing Gaussian regularized nonuniform sampling series.

</details>


### [9] [Interpreting Moment Matrix Blocks Spectra using Mutual Shadow Area](https://arxiv.org/abs/2601.17965)
*Yaniv Brick,Francesco P. Andriulli,Mats Gustafsson*

Main category: math.NA

TL;DR: Mutual shadow area predicts singular value curve knee location, partitioning wave interactions into aperture and diffraction subspaces with different scaling behaviors.


<details>
  <summary>Details</summary>
Motivation: To understand why fast solvers perform well even for arbitrary scatterers without special geometric characteristics, by analyzing the spectral components and rank scaling of wave interactions between surface regions.

Method: Using mutual shadow area of pairs of surface regions to guide study of spectral components and rank of wave interactions, analyzing the corresponding moment matrix blocks and their singular value decomposition.

Result: Mutual shadow area accurately predicts singular value curve knee location, partitioning interactions into aperture subspace (scales with area/length) and diffraction subspace (scales slower with electrical length). For 3D interactions with small aspect angles, diffraction subspace dominates rank until large electrical lengths, explaining delayed asymptotic scaling.

Conclusion: The analysis explains impressive fast solver performance for arbitrary scatterers and reveals that in extreme "endfire" cases where shadow area vanishes, diffraction governs asymptotic rank, leading to superior solver performance.

Abstract: The mutual shadow area of pairs of surface regions is used for guiding the study of the spectral components and rank of their wave interaction, as captured by the corresponding moment matrix blocks. It is demonstrated that the mutual shadow area provides an asymptotically accurate predictor of the location of the singular value curve knee. This predicted knee index is shown to partition the interacting parts of the range and domain of blocks into two subspaces that can be associated with different wave phenomena: an "aperture" subspace of dimension that scales with the subdomains area (or length in 2-D) and a remainder "diffraction" subspace of dimension that scales much slower with the electrical length, depending on the geometric configuration. For interactions between open surface domains typical for the common hierarchical partitioning in most fast solvers, the latter can be attributed to the domain edges visible by its interacting counterpart. For interactions in 3-D with a small aspect angles between the source and observers, the diffraction subspace dimension is dominant in determining the rank until fairly large electrical lengths are reached. This explains the delayed asymptotic scaling of ranks and impressive fast solver performance observed in recent literature for seemingly arbitrary scatterers with no special geometric characteristics. In the extreme cases of "endfire" reduced dimensionality interactions, where the shadow area vanishes, the diffraction governs also the asymptotic rank, which translates to superior asymptotic solver performance.

</details>


### [10] [High-Order Mesh r-Adaptivity with Tangential Relaxation and Guaranteed Mesh Validity](https://arxiv.org/abs/2601.17708)
*Ketan Mittal,Veselin Dobrev,Tzanio Kolev,Vladimir Tomov*

Main category: math.NA

TL;DR: Extends TMOP framework for high-order mesh r-adaptivity with two key improvements: tangential relaxation on curved surfaces using only discrete mesh data, and guaranteed positive Jacobian determinant throughout domain.


<details>
  <summary>Details</summary>
Motivation: High-order meshes are essential for optimal convergence in curvilinear domains, symmetry preservation, and alignment with flow features in moving mesh simulations, but quality control is challenging. Existing TMOP methods need improvements for practical r-adaptivity.

Method: Extends Target-Matrix Optimization Paradigm (TMOP) framework with two novel techniques: 1) tangential relaxation on curved surfaces using only discrete mesh representation (no CAD model needed), 2) ensures continuously positive Jacobian determinant throughout domain for compatibility with arbitrary quadrature schemes.

Result: The proposed approach demonstrates robustness through various numerical experiments, enabling practical high-order mesh r-adaptivity with guaranteed mesh quality and compatibility with simulation requirements.

Conclusion: The extended TMOP framework successfully addresses two critical gaps in high-order mesh r-adaptivity literature, providing practical solutions for tangential relaxation without geometry access and ensuring positive Jacobian determinant for simulation compatibility.

Abstract: High-order meshes are crucial for achieving optimal convergence rates in curvilinear domains, preserving symmetry, and aligning with key flow features in moving mesh simulations, but their quality is challenging to control. In prior work, we have developed techniques based on Target-Matrix Optimization Paradigm (TMOP) to adapt a given high-order mesh to the geometry and solution of the partial differential equation (PDE). Here, we extend this framework to address two key gaps in the literature for high-order mesh r-adaptivity. First, we introduce tangential relaxation on curved surfaces using solely the discrete mesh representation, eliminating the need for access to underlying geometry (e.g., CAD model). Second, we ensure a continuously positive Jacobian determinant throughout the domain. This determinant positivity is essential for using the high-order mesh resulting from r-adaptivity with arbitrary quadrature schemes in simulations. The proposed approach is demonstrated to be robust using a variety of numerical experiments.

</details>


### [11] [Novel Product Manifold Modeling and Orthogonality-Constrained Neural Network Solver for Parameterized Generalized Inverse Eigenvalue Problems](https://arxiv.org/abs/2601.17798)
*Shuai Zhang,Xuelian Jiang,Yingxiang Xu*

Main category: math.NA

TL;DR: A neural network with orthogonality constraints solves parameterized generalized inverse eigenvalue problems on product manifolds.


<details>
  <summary>Details</summary>
Motivation: To address parameterized generalized inverse eigenvalue problems (PGIEPs) which existing models cannot handle using Stiefel manifold optimization algorithms.

Method: Proposes a parameterized orthogonality-constrained neural network with a novel PGIEP model on product manifolds (Stiefel × Euclidean) and introduces Parameterized Stiefel Multilayer Perceptron (P-SMLP) with hard orthogonality constraints for end-to-end training.

Result: Numerical experiments demonstrate the effectiveness of the proposed method for solving generic PGIEPs.

Conclusion: The proposed approach provides a new perspective and robust computational framework for PGIEPs by enabling Stiefel manifold optimization and end-to-end training without alternating between manifolds.

Abstract: A parameterized orthogonality-constrained neural network is proposed for the first time to solve the parameterized generalized inverse eigenvalue problem (PGIEP) on product manifolds, offering a new perspective to address PGIEP. The key contributions are twofold. First, we construct a novel model for the PGIEP, where the optimization variables are located on the product of a Stiefel manifold and a Euclidean manifold. This model enables the application of optimization algorithms on the Stiefel manifold, a capability that is not achievable with existing models. Additionally, the gradient Lipschitz continuity of the objective function is proved. Second, a parameterized Stiefel multilayer perceptron (P-SMLP) that incorporates orthogonality constraints is proposed. Through hard constraints, P-SMLP enables end-to-end training without the need of alternating training between the two manifolds, providing a robust computational framework for generic PGIEPs. Numerical experiments demonstrate the effectiveness of the proposed method.

</details>


### [12] [Stability and Convergence of Mixed Finite Elements for Linear Regularized 13-Moment Equations](https://arxiv.org/abs/2601.17904)
*Shuang Hu,Huiteng Li,Zhenning Cai*

Main category: math.NA

TL;DR: A stable mixed finite element method for linear regularized 13-moment equations using bubble functions for inherent stability without penalty terms.


<details>
  <summary>Details</summary>
Motivation: Existing methods for the linear regularized 13-moment equations in rarefied gas dynamics require stabilization via penalty terms, which can be problematic. The authors aim to develop a method with inherent stability.

Method: Mixed finite element method enriched with bubble functions to achieve inherent stability without penalty terms. The method is designed for the linear regularized 13-moment equations.

Result: The scheme achieves second-order convergence rates in the L² norm under mild regularity assumptions. It demonstrates practical advantages over standard MFEM schemes, yielding robust numerical results even with geometric singularities.

Conclusion: The proposed bubble-enriched mixed finite element method provides a stable and convergent approach for the linear R13 equations with inherent stability, eliminating the need for penalty terms while maintaining robustness in challenging scenarios.

Abstract: We present a stable and convergent mixed finite element method (MFEM) for the linear regularized 13-moment (R13) equations in rarefied gas dynamics. Unlike existing methods that require stabilization via penalty terms, our scheme achieves inherent stability by enriching the finite element basis with bubble functions. We provide a rigorous theoretical analysis, establishing second-order convergence rates in the $L^2$ norm under mild regularity assumptions. Beyond theoretical properties, our scheme demonstrates practical advantages over standard MFEM schemes, yielding robust numerical results even in the presence of geometric singularities.

</details>


### [13] [A theoretical and computational framework for three dimensional inverse medium scattering using the linearized low-rank structure](https://arxiv.org/abs/2601.18016)
*Yuyuan Zhou,Lorenzo Audibert,Shixu Meng,Bo Zhang*

Main category: math.NA

TL;DR: A computational framework for 3D inverse medium scattering using data-driven 3D prolate spheroidal wave functions for low-rank approximation, with Tikhonov regularization and localized imaging.


<details>
  <summary>Details</summary>
Motivation: To solve the challenging three-dimensional inverse medium scattering problem by developing an efficient computational framework that can handle complex scattering scenarios and provide accurate reconstructions.

Method: Uses 3D prolate spheroidal wave functions (3D PSWFs) as data-driven basis functions for low-rank approximation of inverse solutions, combined with theoretical analysis in customized Sobolev spaces, Tikhonov regularization with customized penalty norms, and localized imaging techniques.

Result: Establishes theoretical fundamentals including regularity analysis and a priori estimates, develops computational framework for computing 3D PSWFs and low-rank approximations, and demonstrates effectiveness through various numerical examples.

Conclusion: The proposed framework successfully addresses 3D inverse scattering problems using data-driven basis functions with regularization and localized imaging, showing promising results for practical applications.

Abstract: In this work we propose a theoretical and computational framework for solving the three dimensional inverse medium scattering problem, based on a set of data-driven basis arising from the linearized problem. This set of data-driven basis consists of generalizations of prolate spheroidal wave functions to three dimensions (3D PSWFs), the main ingredients to explore a low-rank approximation of the inverse solution. We first establish the fundamentals of the inverse scattering analysis, including regularity in a customized Sobolev space and new a priori estimate. This is followed by a computational framework showcasing computing the 3D PSWFs and the low-rank approximation of the inverse solution. These results rely heavily on the fact that the 3D PSWFs are eigenfunctions of both a restricted Fourier integral operator and a Sturm-Liouville differential operator. Furthermore we propose a Tikhonov regularization method with a customized penalty norm and a localized imaging technique to image a targeting object despite the possible presence of its surroundings. Finally various numerical examples are provided to demonstrate the potential of the proposed method.

</details>


### [14] [A Generalized Weak Galerkin Method for Linear Elasticity with Nonpolynomial Approximations](https://arxiv.org/abs/2601.18120)
*Junping Wang,Yue Wang*

Main category: math.NA

TL;DR: A generalized weak Galerkin (gWG) finite element method for linear elasticity on polygonal/polyhedral meshes that allows nonpolynomial approximating functions and reduces computational cost.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible and efficient finite element method for linear elasticity problems that works on general polygonal and polyhedral meshes, allowing for nonpolynomial approximating functions and reducing computational costs compared to standard weak Galerkin formulations.

Method: Generalized weak Galerkin (gWG) finite element method with element-level correction of classical differential operators to account for boundary discontinuities. The framework accommodates arbitrary finite-dimensional approximation spaces, including nonpolynomial activation-based spaces with randomly selected parameters.

Result: The method is locking-free, robust with respect to mesh geometry, and effective on general polygonal and polyhedral partitions. Activation-based interior approximation spaces show convergence behavior comparable to classical polynomial spaces.

Conclusion: The gWG framework provides a flexible, efficient, and robust approach for linear elasticity problems on complex meshes, supporting both polynomial and nonpolynomial approximation spaces with comparable performance.

Abstract: This paper presents a generalized weak Galerkin (gWG) finite element method for linear elasticity problems on general polygonal and polyhedral meshes. The proposed framework is flexible and efficient, allowing for the use of nonpolynomial approximating functions. The generalized weak differential operators are defined as an element-level correction of the classical differential operators accounting for boundary discontinuities. This construction reduces computational cost and provides greater flexibility than standard weak Galerkin formulations. The gWG framework naturally accommodates arbitrary finite-dimensional approximation spaces, including nonpolynomial activation-based spaces with randomly selected parameters. Error equations and error estimates are established for the proposed method. Numerical experiments demonstrate that the method is locking-free, robust with respect to mesh geometry, and effective on general polygonal and polyhedral partitions. In particular, activation-based interior approximation spaces exhibit convergence behavior comparable to that of classical polynomial spaces.

</details>


### [15] [Trajectory-Based RBF Collocation Method for Surface Advection-Diffusion Equations](https://arxiv.org/abs/2601.18186)
*Xiaobin Li,Leevan Ling,Yizhong Sun*

Main category: math.NA

TL;DR: TBRBF method solves surface advection-diffusion equations on manifolds using operator splitting with characteristic ODE and RBF collocation for diffusion, proving equivalence to original PDE without splitting error.


<details>
  <summary>Details</summary>
Motivation: Need efficient methods for solving surface advection-diffusion equations on smooth compact manifolds, which arise in various applications like fluid dynamics on surfaces, biological pattern formation, and surface transport phenomena.

Method: Trajectory-Based RBF Collocation method decouples advection and diffusion using operator splitting: characteristic ODE for advection and Kansa-type RBF collocation for diffusion PDE. Uses narrow band embedding and push-forward operators in atlas charts to establish equivalence with original PDE.

Result: Rigorous proof of equivalence between OSC system and original surface PDE, ensuring no operator splitting error. Numerical experiments demonstrate robust stability and accuracy of the method.

Conclusion: TBRBF provides an accurate, stable method for solving surface advection-diffusion equations on manifolds with theoretical guarantees of equivalence to original PDE and no splitting error.

Abstract: We introduce a Trajectory-Based RBF Collocation (TBRBF) method for solving surface advection-diffusion equations on smooth, compact manifolds. TBRBF decouples advection and diffusion by applying a characteristic treatment with a Kansa-type RBF collocation method for diffusion PDE, which yields an operator-split characteristic (OSC) system comprising a characteristic ODE and a diffusion PDE. We rigorously prove the equivalence between the OSC system and the original surface PDE on manifolds by embedding the latter into a narrow band domain. Using an intrinsic approach, we construct a time-continuous embedded PDE with push-forward operators in each chart of the atlas and establish its equivalence with the OSC system in the narrow band. Restricting the solution back to the manifold recovers the OSC system on manifolds, ensuring that the method introduces no operator splitting error. Extensive numerical experiments confirm the robust stability and accuracy of the proposed method.

</details>


### [16] [On the Generalized Conditional Gradient Method for Mean Field Games with Local Coupling Terms](https://arxiv.org/abs/2601.18224)
*Haruka Nakamura,Norikazu Saito*

Main category: math.NA

TL;DR: GCG method convergence rates established for time-dependent MFGs with local coupling terms, overcoming previous limitations that only covered global interactions.


<details>
  <summary>Details</summary>
Motivation: Previous convergence analysis for GCG method only worked for globally coupled MFG interactions, failing to cover important local interactions like congestion effects that are common in applications.

Method: Introduce refined analytical framework adapted to local couplings, use Cole-Hopf transformation to establish uniform bounds on HJB solutions for quadratic Hamiltonian with convection effect.

Result: Derive explicit convergence estimates in terms of exploitability and optimality gap, prove existence and uniqueness of smooth solutions for locally coupled MFG systems, confirm theoretical rates with numerical experiments.

Conclusion: Successfully extend GCG convergence analysis to local coupling MFGs, providing theoretical foundation and numerical validation for important class of problems including congestion effects.

Abstract: We study the generalized conditional gradient (GCG) method for time-dependent second-order mean field games (MFG) with local coupling terms. While explicit convergence rates of the GCG method were previously established only for globally coupled interactions, the assumptions used there fail to cover typical local interactions such as congestion effects. To overcome this limitation, we introduce a refined analytical framework adapted to local couplings and derive explicit convergence estimates in terms of the exploitability and optimality gap. The key difficulty lies in establishing uniform bounds on the Hamilton--Jacobi--Bellman solutions; this is solved via the Cole--Hopf transformation under a standard quadratic Hamiltonian with a convection effect. We further provide numerical experiments demonstrating convergence behavior and confirming the theoretical rates. Additionally, the existence and uniqueness of smooth solutions to the MFG system with locally coupled interactions are established.

</details>


### [17] [Symplecticity-Preserving Prediction of Hamiltonian Dynamics by Generalized Kernel Interpolation](https://arxiv.org/abs/2601.18364)
*Robin Herkert,Tobias Ehring,Bernard Haasdonk*

Main category: math.NA

TL;DR: A kernel-based surrogate for Hamiltonian dynamics that is symplectic by construction, learns a scalar potential, and achieves high accuracy for long prediction horizons.


<details>
  <summary>Details</summary>
Motivation: To develop a structure-preserving surrogate model for Hamiltonian systems that can handle large prediction horizons while maintaining symplecticity, which is crucial for long-time stability and accuracy in dynamical systems.

Method: Learns a scalar potential whose gradient enters a symplectic-Euler update, formulated as a gradient Hermite-Birkhoff interpolation problem in a reproducing kernel Hilbert space. Combines symplectic kernel predictor with structure-preserving model order reduction for high-dimensional PDEs.

Result: Numerical tests show nearly algebraic greedy convergence and long-time trajectory errors reduced by 2-3 orders of magnitude compared to implicit midpoint baseline at same macro time step, demonstrated on pendulum, nonlinear spring-mass chain, and semi-discrete wave equation.

Conclusion: The proposed kernel-based surrogate provides a systematic framework for symplectic integration with guaranteed structure preservation, efficient error control, and superior long-time accuracy for Hamiltonian systems, particularly beneficial for high-dimensional discretized PDEs.

Abstract: In this work, a kernel-based surrogate for integrating Hamiltonian dynamics that is symplectic by construction and tailored to large prediction horizons is proposed. The method learns a scalar potential whose gradient enters a symplectic-Euler update, yielding a discrete flow map that exactly preserves the canonical symplectic structure. Training is formulated as a gradient Hermite--Birkhoff interpolation problem in a reproducing kernel Hilbert space, providing a systematic framework for existence, uniqueness, and error control. Algorithmically, the symplectic kernel predictor is combined with structure-preserving model order reduction, enabling efficient treatment of high-dimensional discretized PDEs. Numerical tests for a pendulum, a nonlinear spring--mass chain, and a semi-discrete wave equation show nearly algebraic greedy convergence and long-time trajectory errors reduce by two to three orders of magnitude compared to an implicit midpoint baseline at the same macro time step.

</details>


### [18] [A Jacobian-free Newton-Krylov method for high-order cell-centred finite volume solid mechanics](https://arxiv.org/abs/2601.18417)
*Ivan Batistic,Pablo Castrillo,Philip Cardiff*

Main category: math.NA

TL;DR: Higher-order (3rd/4th order) cell-centered finite-volume methods for solid mechanics with JFNK solvers, achieving better accuracy than 2nd-order schemes while maintaining efficiency through preconditioning and stabilization.


<details>
  <summary>Details</summary>
Motivation: Conventional finite-volume methods for solid mechanics are typically limited to second-order accuracy, which may be insufficient for accurately resolving complex stress and deformation fields in linear and nonlinear solids. There's a need for higher-order formulations that retain the flexibility of finite-volume methods while improving accuracy.

Method: Develop third- and fourth-order cell-centered finite-volume formulations using local least-squares reconstructions for gradient evaluation and Gaussian quadrature at cell faces. Implement a Jacobian-free Newton-Krylov (JFNK) solution strategy that eliminates complex Jacobian matrix assembly, using a compact-stencil approximate Jacobian as a preconditioner. Incorporate alpha-stabilization for robustness on irregular meshes.

Result: The higher-order formulations deliver substantial accuracy improvements over second-order schemes. The JFNK approach achieves strong performance with minimal modifications to existing segregated frameworks, providing efficiency gains similar to second-order frameworks while handling higher-order accuracy.

Conclusion: Combining higher-order finite-volume methods with JFNK solvers advances computational solid mechanics, offering improved accuracy while maintaining computational efficiency. The methodology is implemented in the open-source solids4foam toolbox for OpenFOAM, supporting community adoption and further exploration.

Abstract: This work extends the application of Jacobian-free Newton-Krylov (JFNK) methods to higher-order cell-centred finite-volume formulations for solid mechanics. While conventional schemes are typically limited to second-order accuracy, we present third- and fourth-order formulations employing local least-squares reconstructions for gradient evaluation and Gaussian quadrature at cell faces. These schemes enable accurate resolution of complex stress and deformation fields in linear and nonlinear solids while retaining the flexibility of finite-volume methods. A key contribution is a JFNK solution strategy for these higher-order schemes, eliminating the need to assemble complex Jacobian matrices. A compact-stencil approximate Jacobian is used as a preconditioner, providing efficiency gains similar to second-order frameworks. To enhance robustness on irregular meshes, an alpha-stabilisation scheme is incorporated, damping high-frequency error modes without compromising formal accuracy. The proposed methodology is benchmarked across a suite of two- and three-dimensional test problems involving elastic and nonlinear materials, where key performance metrics, including accuracy, computational cost, memory usage, and robustness, are systematically evaluated. Results confirm that the higher-order formulations deliver substantial accuracy improvements over second-order schemes, while the JFNK approach achieves strong performance with only minimal modifications to existing segregated frameworks. These findings underscore the potential of combining higher-order finite-volume methods with JFNK solvers to advance the state of the art in computational solid mechanics. The implementations are openly released in the solids4foam toolbox for OpenFOAM, supporting further exploration and adoption by the community.

</details>


### [19] [Analyzing the Error of Generative Diffusion Models: From Euler-Maruyama to Higher-Order Schemes](https://arxiv.org/abs/2601.18425)
*Emanuel Pfarr,Radu Timofte,Frank Werner*

Main category: math.NA

TL;DR: Theoretical analysis shows higher-order SDE discretization methods can outperform Euler-Maruyama for sampling in generative diffusion models, with comprehensive error bounds and experimental validation.


<details>
  <summary>Details</summary>
Motivation: Despite widespread use of generative diffusion models (GDMs), theoretical understanding is limited, particularly regarding how different SDE discretization schemes affect sampling performance. Existing analysis focuses only on Euler-Maruyama methods, leaving higher-order schemes unexamined despite their potential for better accuracy.

Method: Established asymptotic 2-Wasserstein convergence results for SDE-based discretization methods in GDM sampling. Provided all-at-once error bound analysis for Euler-Maruyama method accounting for all error sources. Developed first error bound results for arbitrary higher-order SDE-discretization methods with known strong L₂ convergence, analyzing dependence on discretization grid and score-matching error.

Result: Convergence proved under all prevalent score-matching error assumptions for strongly log-concave data distributions. Numerical experiments demonstrate that higher-order discretization methods can indeed retain their theoretical advantage over Euler-Maruyama for sampling GDMs, contrary to common belief.

Conclusion: Higher-order SDE discretization methods provide theoretical and practical advantages over Euler-Maruyama for sampling in generative diffusion models, with comprehensive error analysis showing improved performance potential.

Abstract: Although generative diffusion models (GDMs) are widely used in practice, their theoretical foundations remain limited, especially concerning the impact of different discretization schemes applied to the underlying stochastic differential equation (SDE). Existing convergence analysis largely focuses on Euler-Maruyama (EM)-like methods and does not extend to higher-order schemes, which are naturally expected to provide improved discretization accuracy. In this paper, we establish asymptotic 2-Wasserstein convergence results for SDE-based discretization methods employed in sampling for GDMs. We provide an all-at-once error bound analysis of the EM method that accounts for all error sources and establish convergence under all prevalent score-matching error assumptions in the literature, assuming a strongly log-concave data distribution. Moreover, we present the first error bound result for arbitrary higher-order SDE-discretization methods with known strong L_2 convergence in dependence on the discretization grid and the score-matching error. Finally, we complement our theoretical findings with an extensive numerical study, providing comprehensive experimental evidence and showing that, contrary to popular believe, higher order discretization methods can in fact retain their theoretical advantage over EM for sampling GDMs.

</details>


### [20] [A stabilized finite element method for a flow problem arising from 4D flow magnetic resonance imaging](https://arxiv.org/abs/2601.18454)
*Gabriel Barrenechea,Cristian Cárcamo,Abner Poza*

Main category: math.NA

TL;DR: A stabilized finite element method for 4D Flow MRI quality assessment, enabling equal-order velocity-pressure interpolation with proven stability and error estimates.


<details>
  <summary>Details</summary>
Motivation: To assess 4D Flow MRI quality and provide non-invasive pressure reconstruction by analyzing velocity field errors, avoiding invasive procedures.

Method: Derived modified Navier-Stokes equation by splitting velocity into MRI-observed data plus error term, then designed stabilized finite element method allowing equal-order velocity-pressure interpolation.

Result: Proved stability and optimal order error estimates for linearized model; validated with numerical experiments for both linearized and nonlinear cases.

Conclusion: The method successfully enables 4D Flow MRI quality assessment and non-invasive pressure reconstruction using equal-order finite element approximations.

Abstract: In this work we propose, {analyze}, and validate a stabilized finite element method for a flow problem arising from the assessment of {4D Flow Magnetic Resonance Imaging quality}. Starting from the Navier-Stokes equation and splitting its velocity as the MRI-observed one (considered a datum) plus an ``observation error'', a modified Navier-Stokes problem is derived. This procedure allows us to estimate the quality of the measured velocity fields, while also providing an alternative approach to pressure reconstruction, thereby avoiding invasive procedures. Since equal-order approximations have become a popular choice for problems linked to pressure recovery from MRI images, we design a stabilized finite element method allowing equal-order interpolations for velocity and pressure. In the linearized version of the resulting model, we prove stability and (optimal order) error estimates and test the method with a variety of numerical experiments testing both the linearized case and the more realistic nonlinear one.

</details>


### [21] [Justification of a Relaxation Approximation for the Navier-Stokes-Cahn-Hilliard System](https://arxiv.org/abs/2601.18463)
*Jan Giesselmann,Jens Keim,Fabio Leotta,Christian Rohde*

Main category: math.NA

TL;DR: The paper analyzes a relaxation approximation of the Navier-Stokes-Cahn-Hilliard system for two-phase flows, proves convergence to the original system via relative entropy methods, and provides numerical validation using a novel conservative finite-difference scheme.


<details>
  <summary>Details</summary>
Motivation: To develop a relaxation approximation for the NSCH system that simplifies numerical computation while maintaining physical accuracy, and to rigorously prove convergence of this approximation to the original system.

Method: 1) Theoretical: Construct a relaxation approximation using first-order hyperbolic balance laws and second-order elliptic operators, prove convergence using relative entropy framework and energy dissipation. 2) Numerical: Implement a novel marker-and-cell conservative finite-difference approach for both approximation and limit systems.

Result: Proved that solutions of the relaxation approximation recover the limiting NSCH system as relaxation parameters vanish. Provided numerical evidence supporting theoretical results, even in flow regimes beyond theoretical assumptions, including Ostwald ripening and high-velocity flows.

Conclusion: The relaxation approximation is mathematically justified and computationally effective for simulating interfacial flow problems, bridging theoretical analysis with practical numerical implementation for complex two-phase flow phenomena.

Abstract: The Navier-Stokes-Cahn-Hilliard (NSCH) system governs the diffuse-interface dynamics of two incompressible and immiscible fluids. We consider a relaxation approximation of the NSCH system that is composed by a system of first-order hyperbolic balance laws and second-order elliptic operators. We prove first that the solutions of an initial boundary value problem for the approximation recover the limiting NSCH system for vanishing relaxation parameters. To cope with the singular limit we exploit the fact that the approximate solutions dissipate an almost quadratic energy, and employ the relative entropy-framework. In the second part of the work we provide numerical evidence for the analytical results, even in flow regimes not covered by the assumptions needed for the theoretical results. Using a novel marker-and-cell conservative finite-difference approach for both the approximation and the limit system, we are able to compute physically relevant interfacial flow problems including Ostwald ripening and high-velocity flow.

</details>


### [22] [Pointwise-in-time convergence analysis of an Alikhanov scheme for a 2D nonlinear subdiffusion equation](https://arxiv.org/abs/2601.18505)
*Chang Hou,Hu Chen,Jian Wang*

Main category: math.NA

TL;DR: A linearized fully discrete scheme for 2D nonlinear time fractional subdiffusion equations using Alikhanov discretization and Newton linearization, achieving optimal convergence rates for weakly singular solutions.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving two-dimensional nonlinear time fractional subdiffusion equations with weakly singular solutions, which are challenging due to the singular nature of fractional derivatives and nonlinear terms.

Method: Discretize Caputo time derivative using Alikhanov scheme on quasi-graded temporal mesh, apply Newton linearization for nonlinear term, develop new stability analysis using comparison principle for pointwise convergence analysis.

Result: Global L²-norm convergence order is min{αr, 2}, local L²-norm convergence order is min{r, 2} under appropriate conditions, validated by numerical experiments.

Conclusion: The proposed linearized fully discrete scheme effectively handles weakly singular solutions in nonlinear time fractional subdiffusion equations with theoretically proven and numerically validated optimal convergence rates.

Abstract: In this paper, we discretize the Caputo time derivative of order α\in (0,1) using the Alikhanov scheme on a quasi-graded temporal mesh, and employ the Newton linearization method to approximate the nonlinear term. This yields a linearized fully discrete scheme for the two-dimensional nonlinear time fractional subdiffusion equation with weakly singular solutions. For the purpose of conducting a pointwise convergence analysis using the comparison principle, we develop a new stability result. The global L^2-norm convergence order is min{αr, 2}, and the local L^2-norm convergence order is min{r, 2} under appropriate conditions and assumptions. Ultimately, the rates of convergence demonstrated by the numerical experiments serve to validate the analytical outcomes.

</details>


### [23] [A BFBt preconditioner for Double Saddle-Point Systems](https://arxiv.org/abs/2601.18520)
*Chen Greif*

Main category: math.NA

TL;DR: Analysis of block preconditioners for double saddle-point systems, focusing on approximating nested Schur complements and their effect on eigenvalue distributions.


<details>
  <summary>Details</summary>
Motivation: To develop effective preconditioners for challenging double saddle-point systems that arise in applications like Stokes-Darcy equations, and to understand how approximations of nested Schur complements affect preconditioner performance.

Method: Developed a variant of Elman's BFBt method adapted for double saddle-point systems, with investigation of approximating the nested Schur complement associated with the trailing diagonal block. Analysis focuses on eigenvalue distribution of the preconditioned matrix.

Result: Theoretical analysis of eigenvalue distribution effects when approximating nested Schur complements, with numerical illustration on Marker-and-Cell discretization of Stokes-Darcy equations.

Conclusion: The adapted BFBt method provides effective preconditioning for double saddle-point systems, with approximations of nested Schur complements having predictable effects on eigenvalue distributions that can be leveraged for efficient solution strategies.

Abstract: We consider block preconditioners for double saddle-point systems, and investigate the effect of approximating the nested Schur complement associated with the trailing diagonal block on the eigenvalue distribution of the preconditioned matrix. We develop a variant of Elman's BFBt method and adapt it to this family of linear systems. Our findings are illustrated on a Marker-and-Cell discretization of the Stokes-Darcy equations.

</details>


### [24] [Moving sample method for solving time-dependent partial differential equations](https://arxiv.org/abs/2601.18575)
*Beining Xu,Haijun Yu,Jiayu Zhai,Kejun Tang,Xiaoliang Wan*

Main category: math.NA

TL;DR: Adaptive sampling framework for PINNs that uses residual-driven point allocation to efficiently solve time-dependent PDEs with sharp gradients and local singularities.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs waste computational resources on well-resolved regions when solving PDEs with sharp gradients or local singularities, leading to inefficient point allocation and high computational costs.

Method: Residual-driven adaptive sampling strategy that iteratively updates spatial-temporal training point distribution based on error fields from previous iterations, concentrating computational effort on regions with significant residuals.

Result: The method achieves higher accuracy with fewer sampling points compared to uniform sampling, as demonstrated through numerical experiments on representative PDE benchmarks.

Conclusion: The adaptive sampling framework improves solution quality for time-dependent PDEs with pronounced local singularities by efficiently allocating computational resources to problematic regions.

Abstract: Solving time-dependent partial differential equations (PDEs) that exhibit sharp gradients or local singularities is computationally demanding, as traditional physics-informed neural networks (PINNs) often suffer from inefficient point allocation that wastes resources on regions already well-resolved. This paper presents an adaptive sampling framework for PINNs aimed at efficiently solving time-dependent partial differential equations with pronounced local singularities. The method employs a residual-driven strategy, where the spatial-temporal distribution of training points is iteratively updated according to the error field from the previous iteration. This targeted allocation enables the network to concentrate computational effort on regions with significant residuals, achieving higher accuracy with fewer sampling points compared to uniform sampling. Numerical experiments on representative PDE benchmarks demonstrate that the proposed approach improves solution quality.

</details>


### [25] [Efficient SN-like and PN-like Dynamic Low Rank methods for Thermal Radiative Transfer](https://arxiv.org/abs/2601.18705)
*Terry Haut,John Loffeld,Lukas Einkemmer,Pierson Guthrey,Stefan Brunner,William Schill*

Main category: math.NA

TL;DR: New Dynamic Low Rank (DLR) methods for thermal radiative transfer reduce computational cost while maintaining accuracy, with SN-like DLR using optimized transport sweeps and PN-like DLR using positive-definite systems.


<details>
  <summary>Details</summary>
Motivation: Thermal radiative transfer equations are computationally expensive high-dimensional PDEs that bottleneck multi-physics simulations. Existing DLR methods have limitations making them impractical for realistic scenarios and uncompetitive with current production codes.

Method: Developed two new DLR approaches: 1) SN-like DLR uses time-evolving angular basis functions to select time-evolving angles, enabling use of highly optimized SN transport sweeps; 2) PN-like DLR uses even-parity formulation resulting in positive-definite linear systems per time step.

Result: The methods demonstrate significant reduction in angular artifacts ("ray effects") on challenging heterogeneous 2D spatial problems (4D total) with the same computational cost as gold-standard SN methods.

Conclusion: These new DLR methods provide practical ways to leverage low-rank techniques in production TRT codes, offering computational efficiency improvements while maintaining or improving accuracy compared to existing approaches.

Abstract: Dynamic Low Rank (DLR) methods are a promising way to reduce the computational cost and memory footprint of the high-dimensional thermal radiative transfer (TRT) equations. The TRT equations are a system of nonlinear PDEs that model the energy exhchange between the material temperature and the radiation energy density; due to their high dimensionality, solving the TRT equations is often bottleneck in multi-physics simulations. DLR methods represent the solution in terms of time-evolving SVD-like factors of angle and space. Although previous work has explored DLR methods for TRT, most of the methods have limitations that make them impractical for realistic scenarios and uncompetitive with current non-DLR production codes.
  Here we develop new PN-like and SN-like Dynamic Low Rank (DLR) methods for TRT. In the SN-like DLR method, we use the time-evolving angular basis functions to select time-evolving angles; this DLR formulation enables us to use the highly optimized SN transport sweep as our main computational kernel, and results in a practical way of leveraging low-rank methods in production TRT codes. In contrast, our PN-like DLR method uses an even-parity formulation and results in positive-definite linear systems to solve for each time step.
  We demonstrate the methods on several challenging, highly heterogenous problems in two spatial dimensions $(4$D) that these DLR schemes can give significant reduction in angular artifacts (``ray effects'') with the same cost as gold-standard SN methods.

</details>


### [26] [A mixed interpolation-regression method for numerical integration on the unit circle using zeros of para-orthogonal polynomials](https://arxiv.org/abs/2601.18721)
*Ruymán Cruz-Barroso,Lidia Fernández,Francisco Marcellán*

Main category: math.NA

TL;DR: A new numerical method for estimating integrals on the unit circle using Hermite interpolation and complex regression, as an alternative to Szegő quadrature.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the practical limitation of Szegő quadrature formulas when integrand values are only known at a finite number of points on the unit circle, which is common in real-world applications.

Method: The method uses Hermite interpolation to create a Laurent polynomial approximation at points mimicking para-orthogonal polynomial zeros, then improves accuracy through simultaneous complex regression using remaining data points.

Result: The paper presents numerical examples demonstrating the effectiveness of the proposed technique, though specific numerical results are not detailed in the abstract.

Conclusion: The proposed method provides a viable alternative to Szegő quadrature for numerical integration on the unit circle when only discrete data points are available, with potential applications in practical computational settings.

Abstract: A new alternative numerical procedure to the Szegő quadrature formulas for the estimation of integrals with respect to a positive Borel measure $μ$ supported on the unit circle is presented. As in many practical situations, we assume that the values of the integrand $F$ are only known at a finite number of points, which we will assume to be uniformly distributed on the unit circle (although this does not actually constitute a restriction). Our technique consists of obtaining an approximating Laurent polynomial $L$ to $F$ by interpolation in the Hermite sense in a collection of these points that mimic the zeros of a para-orthogonal polynomial with respect to $μ$, and to use the values of $F$ at the remaining nodes to improve the accuracy of the approximation by a process of simultaneous complex regression. Some numerical examples are carried out.

</details>


### [27] [Divergence-free and mass-conservative virtual element methods for the Navier-Stokes-Cahn-Hilliard system](https://arxiv.org/abs/2601.18758)
*Alberth Silgado,Giuseppe Vacca*

Main category: math.NA

TL;DR: Semi/fully-discrete virtual element methods for time-dependent Navier-Stokes-Cahn-Hilliard equations modeling two-phase incompressible fluid flows with diffuse interfaces.


<details>
  <summary>Details</summary>
Motivation: To develop accurate and stable numerical schemes for modeling two-phase incompressible fluid flows with diffuse interfaces, which require proper handling of mass conservation and energy stability.

Method: New variational formulation using only velocity, pressure, and phase field; spatial discretization with divergence-free and C¹-conforming high-order elements; time discretization with backward Euler scheme; novel skew-symmetric trilinear form for convective term in Cahn-Hilliard equation.

Result: Proposed discrete schemes satisfy mass conservation and energy bounds; optimal error estimates provided for both formulations; numerical experiments demonstrate good performance across different polynomial degrees and polygonal meshes.

Conclusion: The developed virtual element approximations provide theoretically sound and practically effective numerical methods for two-phase incompressible fluid flow problems with guaranteed mass conservation and energy stability.

Abstract: In this work, we design and analyze semi/fully-discrete virtual element approximations for the time-dependent Navier--Stokes-Cahn--Hilliard equations, modeling the dynamics of two-phase incompressible fluid flows with diffuse interfaces. A new variational formulation is derived involving solely the velocity, pressure, and phase field, together with corresponding a priori energy estimates. The spatial discretization is based on the coupling divergence-free and $C^1$-conforming elements of high-order, while the time discretization employs a classical backward Euler scheme. By introducing a novel skew-symmetric trilinear form to discretize the convective term in the Cahn--Hilliard equation, we propose discrete schemes that satisfy mass conservation and energy bounds. Moreover, optimal error estimates are provided for both formulations. Finally, two numerical experiments are presented to support our theoretical findings and to illustrate the good performance of the proposed schemes for different polynomial degrees and polygonal meshes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [28] [Nodal Deficiency of Neumann Eigenfunctions on a Symmetric Dumbbell Domain](https://arxiv.org/abs/2601.17140)
*Thomas Beck,Andrew Lyons*

Main category: math.AP

TL;DR: Analysis of nodal deficiency in symmetric dumbbell domains as neck width shrinks, showing nodal deficiencies of dumbbell eigenfunctions are no smaller than those of limiting eigenfunctions.


<details>
  <summary>Details</summary>
Motivation: To understand how nodal deficiency (difference between eigenvalue index and nodal domain count) behaves for Neumann eigenfunctions on symmetric dumbbell domains as the connecting neck becomes narrow, and to establish relationships between dumbbell eigenfunctions and their limiting counterparts.

Method: Study symmetric dumbbell domains with shrinking neck width, analyze convergence of Neumann eigenfunctions to limiting eigenfunctions on ends plus 1D Sturm-Liouville solution in neck, examine eigenvalue degeneracy, and compare nodal deficiencies.

Result: For small neck widths, nodal deficiencies of dumbbell eigenfunctions are no smaller than nodal deficiencies of limiting eigenfunctions in the ends; conditions for equality are provided; criterion established for identifying eigenfunctions of zero nodal deficiency.

Conclusion: The paper provides a framework for understanding nodal deficiency in dumbbell domains, showing how geometric constraints affect nodal properties and establishing conditions under which nodal deficiency is preserved in the limiting process.

Abstract: We study the nodal deficiency of pairs of Neumann eigenfunctions defined over symmetric dumbbell domains. As the width of the connecting neck shrinks, these eigenfunctions converge to Neumann eigenfunctions defined over the ends of the dumbbell, together with a one-dimensional Sturm-Liouville solution in the neck. In this limit, the corresponding eigenvalues become degenerate, with multiplicity two. The nodal deficiency, defined as the difference between the eigenvalue index and the nodal domain count, is known by the Courant nodal domain theorem to be nonnegative. We show that, for small neck widths, the nodal deficiencies of the dumbbell eigenfunctions are no smaller than the nodal deficiencies of the limiting eigenfunctions in the ends, and we provide conditions under which equality is achieved. As a consequence, we establish a criterion for identifying eigenfunctions of zero nodal deficiency for the dumbbell domain.

</details>


### [29] [Calderón's inverse problem via Vekua theory](https://arxiv.org/abs/2601.17313)
*Briceyda B. Delgado*

Main category: math.AP

TL;DR: Uniqueness result for Calderón's inverse problem using Clifford analysis and Vekua equation integral representations


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness in Calderón's inverse conductivity problem, which is fundamental in inverse problems and medical imaging (like electrical impedance tomography)

Method: Uses integral representation formulas for solutions of the Vekua equation within Clifford analysis framework

Result: Proves a uniqueness result for Calderón's inverse problem

Conclusion: Clifford analysis and Vekua equation techniques provide new approach to establish uniqueness in inverse conductivity problems

Abstract: In this work, we will prove a uniqueness result for Calderón's inverse problem via some integral representation formulas for solutions of the Vekua equation in the framework of Clifford analysis.

</details>


### [30] [A Spectral Fractional Hirota Bilinear Operator: Analysis and Application to a Time-Fractional KdV Equation](https://arxiv.org/abs/2601.17347)
*S. Ray*

Main category: math.AP

TL;DR: Develops a fractional version of Hirota's bilinear calculus using spectral fractional derivatives, establishes key properties, and applies it to derive a fractional KdV equation with soliton solutions.


<details>
  <summary>Details</summary>
Motivation: To extend Hirota's bilinear method to fractional calculus, enabling the study of fractional integrable systems while preserving algebraic structure and soliton properties.

Method: Defines fractional Hirota derivative via spectral fractional derivative on ℝ, proves Marchaud-type integral representation, establishes algebraic identities and Sobolev estimates, applies to fractional KdV equation.

Result: Successfully constructs fractional bilinear calculus with proper algebraic properties, proves Sobolev mapping estimates, obtains explicit one- and two-soliton τ-functions for fractional KdV, shows classical limit as α→1.

Conclusion: Fractional Hirota calculus provides a rigorous framework for studying fractional integrable systems, preserving key algebraic structures while modifying dispersion relations.

Abstract: We develop a fractional version of Hirota's bilinear calculus that is built directly from the spectral (Fourier-multiplier) fractional derivative on $\mathbb{R}$. For $0<α\le 1$ we define \[ D_ξ^αf\cdot g := (D_ξ^αf)\,g - f\,(D_ξ^αg), \] equivalently through the two-variable extension $D_{ξ_1}^α-D_{ξ_2}^α$. In Fourier variables this is a bilinear multiplier with symbol $(ik_1)^α-(ik_2)^α$. For $0<α<1$ we prove a Marchaud-type singular integral representation, and we use it to establish basic algebraic identities (bilinearity, skew-symmetry and $D_ξ^αf\cdot f=0$), a Sobolev estimate $H^{s}\times H^{s}\to H^{s-α}$ for $s>\tfrac12$, and convergence to the classical Hirota derivative as $α\to 1^-$. As an application we derive a Hirota bilinear form for a spectral time-fractional KdV equation and construct explicit one- and two-soliton $τ$-functions. The fractional order changes the dispersion relation to $ω^α=-k^{3}$, while the two-soliton interaction coefficient agrees with the classical KdV value.

</details>


### [31] [Well-posedness and numerical approximation of nonlinear conservation laws with hysteresis](https://arxiv.org/abs/2601.17403)
*Paola Goatin,Stefan Moreti*

Main category: math.AP

TL;DR: The paper studies a scalar conservation law with hysteresis (Play operator) that introduces rate-independent memory effects, develops entropy weak solutions, analyzes Riemann problems, creates a Godunov-type numerical scheme, proves convergence for BV data, and establishes well-posedness.


<details>
  <summary>Details</summary>
Motivation: To understand and solve conservation laws with hysteresis effects, specifically the Play hysteresis operator, which introduces rate-independent memory into PDEs - a challenging non-local feature that requires new analytical and numerical approaches.

Method: 1) Define entropy weak solutions for the hysteresis conservation law; 2) Detailed analysis of Riemann problem; 3) Develop Godunov-type finite volume numerical scheme; 4) Prove convergence for BV initial data; 5) Establish stability estimates.

Result: 1) Existence of entropy weak solutions via scheme convergence for BV data; 2) Uniqueness and well-posedness through stability estimates; 3) Successful development of numerical scheme for hysteresis conservation laws; 4) Complete analysis of Riemann problem.

Conclusion: The paper provides a comprehensive framework for analyzing and solving scalar conservation laws with Play hysteresis, establishing well-posedness, developing effective numerical methods, and laying foundation for further study of PDEs with memory effects.

Abstract: This article studies the Cauchy problem for the scalar conservation law \[ \partial_t u + \partial_t w + \partial_x f(u) = 0, \] where $w(x,t) = [\mathcal{F}(u)(x,t)]$ is the output of a specific hysteresis operator, namely the Play hysteresis operator, and $f$ is a $\mathbf{C}^2$ convex flux function. The hysteresis operator models a rate-independent memory effect, introducing a specific non-local feature into the partial differential equation. We define a suitable notion of entropy weak solution and analyse in detail the Riemann problem. Furthermore, a Godunov-type finite volume numerical scheme is developed to compute approximate solutions. The convergence of the scheme for $\mathrm{BV}$ initial data provides the existence of an entropy weak solution. Finally, a stability estimate is established, implying the uniqueness and overall well-posedness of the entropy weak solution.

</details>


### [32] [Explicit inversion of spherical Radon transforms in odd dimensions with partial radial data](https://arxiv.org/abs/2601.17411)
*Pradipta Chatterjee,Venkateswaran P. Krishnan,Abhilash Tushir*

Main category: math.AP

TL;DR: Explicit inversion algorithm for spherical Radon transform in odd dimensions using partial radial data, reducing reconstruction to solving ODEs instead of Volterra integral equations.


<details>
  <summary>Details</summary>
Motivation: To address Rubin's question about inversion formulae for spherical mean in odd dimensions, providing a more explicit approach than existing methods that require solving Volterra integral equations.

Method: Derived explicit inversion algorithm that reduces reconstruction to solving ordinary differential equations, with analytical solutions for special cases and numerical simulations for validation.

Result: Successfully developed explicit inversion method for spherical Radon transform in odd dimensions with partial radial data, providing analytical solutions in special cases and validating with numerical simulations.

Conclusion: The work answers Rubin's question by providing a more explicit ODE-based approach for spherical Radon transform inversion in odd dimensions, offering computational advantages over previous Volterra integral equation methods.

Abstract: We derive an explicit inversion algorithm for the spherical Radon transform in odd dimensions with partial radial data. We prove that the reconstruction of the unknown function can be reduced to solving ordinary differential equations, thereby providing a more explicit approach in odd dimensions than solving Volterra integral equation of the first kind established in prior works. We also provide analytical solutions in some special cases. Finally, we present numerical simulations validating our theoretical results. Our work answers a question posed by Rubin in ``Inversion formulae for the spherical mean in odd dimensions and the Euler-Poisson-Darboux equation,'' Inverse Problems 24 (2008), no. 2, 025021, 10 pp.

</details>


### [33] [Resolvent, spectrum and resonances for the acoustic operator with piecewise constant coefficients](https://arxiv.org/abs/2601.17522)
*Andrea Mantile,Andrea Posilicano*

Main category: math.AP

TL;DR: Analysis of acoustic operator with piecewise constant material parameters, establishing spectral properties, limiting absorption principle, and resonance expansions for small inclusions.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties and resonances of acoustic wave propagation in heterogeneous media with piecewise constant material parameters and transmission conditions at interfaces.

Method: Use resolvent difference formulas to analyze operator A_{v,ρ}, establish limiting absorption principle, characterize spectrum and resonances. For small inclusions, employ analytic ε-expansions of resonances based on convergence rates of material parameters.

Result: Spectrum is purely absolutely continuous, limiting absorption principle established, resonance set characterized for smooth domains. For small inclusions, analytic expansions of resonances provided depending on convergence rates of material parameters to zero.

Conclusion: The paper provides complete spectral analysis of acoustic operator with transmission conditions and develops perturbation theory for resonances in small inclusions with vanishing material parameters.

Abstract: We study the acoustic operator $A_{v,ρ}:=v^{2}ρ\nabla\!\cdotρ^{-1}\nabla$ with transmission conditions at the boundary of $Ω=Ω_{1}\cup\dots\cupΩ_{n}$, where the $Ω_{\ell}$'s are connected disjoint open bounded Lipschitz domains, the positive functions $v$ and $ρ$ are constant on each connected component of $Ω$ and $v=ρ=1$ on ${\mathbb R}^{3}\backslash\overlineΩ$. Through a formula for the resolvents difference $(-A_{v,ρ}+z)^{-1}-(-Δ+z)^{-1}$, we provide a Limiting Absorption Principle, determine the spectrum, which turns out to be purely absolutely continuous, and, in the case the connected components of $Ω$ are of class ${\mathcal C}^{1,α}$, characterize the resonance set. The second part of the paper is devoted to the case where $Ω=Ω(\varepsilon)$ is connected with a small size $\varepsilon$ and the $\varepsilon$-analytic functions $v=v(\varepsilon)$ and/or $ρ=ρ(\varepsilon)$ converge to $0_{+}$ inside $Ω(\varepsilon)$ as $\varepsilon\downarrow 0$; there, we provide the analytic $\varepsilon$-expansions of the resonances of $A_{v,ρ}$ according to different choices of the rate of convergence towards zero of the material parameters.

</details>


### [34] [Characterisation of homogenisation for nonlocal diffusion by local topologies](https://arxiv.org/abs/2601.17579)
*Andreas Buchinger,Krešimir Burazin,Ivana Crnjac,Marko Erceg,Maja Jolić,Marcus Waurick*

Main category: math.AP

TL;DR: Analysis of fractional divergence form problems with oscillatory coefficients using H-convergence and weak-* convergence, with applications to symmetric coefficients and homogenization of fractional heat equations.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior of fractional divergence form problems with highly oscillatory local coefficients, which requires handling both local behavior and nonlocal effects due to fractional differential operators.

Method: Characterizes convergence using classical H-convergence for local behavior and weak-* convergence on the complement for nonlocal effects, analyzed through nonlocal H-convergence and Schur topologies.

Result: Develops a framework to characterize convergence of oscillatory coefficients in fractional problems, with specific applications to symmetric coefficients and homogenization of fractional heat equations.

Conclusion: Provides a comprehensive convergence analysis for fractional divergence form problems with oscillatory coefficients, bridging local and nonlocal convergence concepts with practical applications.

Abstract: We consider fractional variants of divergence form problems with highly oscillatory local coefficients. We characterise the convergence of these coefficients by means of classical $H$-convergence covering the local behaviour of the fractional divergence form problem and weak-$\ast$ convergence on the complement caused by the nonlocality of the differential operators. The results are further described in the light of nonlocal $H$-convergence as introduced in [Waurick, Calc Var PDEs, 57, 2018] and certain Schur topologies. Applications to symmetric coefficients and a homogenisation problem for a fractional heat type equation are provided.

</details>


### [35] [Long time behavior of Fokker-Planck equations for bosons and fermions](https://arxiv.org/abs/2601.17594)
*Anton Arnold,Marlies Pirner,Gayrat Toshpulatov*

Main category: math.AP

TL;DR: Proves exponential decay to global equilibrium for space inhomogeneous quantum Fokker-Planck equations without close-to-equilibrium assumptions, using L²-hypocoercivity methods.


<details>
  <summary>Details</summary>
Motivation: To analyze the long-time behavior of quantum Fokker-Planck equations that incorporate quantum statistical effects (Bose-Einstein and Fermi-Dirac statistics) through nonlinear factors f(1±f), accounting for boson inclusion and fermion exclusion principles.

Method: Uses L²-hypocoercivity approach with a main Lyapunov functional constructed from logarithmic relative entropy and the nonlinear projection of solutions to the manifold of local-in-x equilibria. Assumes existence of global solutions.

Result: Proves exponential decay of solutions to global equilibrium in weighted L²-space without requiring close-to-equilibrium assumptions.

Conclusion: The L²-hypocoercivity method with carefully constructed Lyapunov functional provides rigorous convergence to equilibrium for quantum Fokker-Planck equations with quantum statistical effects, even far from equilibrium.

Abstract: This paper is concerned with space inhomogeneous quantum Fokker-Planck equations posed on a classical kinetic phase space. The nonlinear factor $f(1\pm f)$ appears both in the transport term and in the collison part of the Fokker-Planck operator, accounting for the inclusion principle of bosons and the exclusion principle of fermions. Assuming that global solutions exist, we prove exponential decay of the solutions to the global equilibrium in a weighted $L^2$-space without a close-to-equilibrium assumption. Our analysis is in the spirit of an $L^2$-hypocoercivity method. Our main Lyapunov functional is constructed from a logarithmic relative entropy and the (nonlinear) projection of the solution to the manifold of local-in-$x$ equilibria.

</details>


### [36] [Uniqueness and stability in bottom detection through surface measurements of water waves](https://arxiv.org/abs/2601.17639)
*Noureddine Lamsahel,Lionel Rosier*

Main category: math.AP

TL;DR: This paper studies the inverse problem of recovering underwater bathymetry (bottom shape) from surface water wave measurements, establishing uniqueness and logarithmic stability estimates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the geometric inverse problem of determining underwater topography from surface wave observations, which has practical applications in oceanography, coastal engineering, and environmental monitoring where direct bottom measurements are difficult or expensive.

Method: The authors use the general water-waves system on a bounded subdomain and address identifiability and stability issues. They establish uniqueness without additional assumptions and derive stability estimates using the size estimates method, requiring only a local fatness condition on the region between bottom profiles.

Result: The main results are: 1) Uniqueness in determining bathymetry from surface measurements (free surface, its time derivative, and velocity potential at a given time) plus boundary bottom knowledge; 2) Logarithmic stability estimates for bathymetry recovery in 1D and 2D domains.

Conclusion: The paper successfully solves the inverse bathymetry problem, demonstrating that bottom shape can be uniquely determined from surface wave data with reasonable stability properties, providing a theoretical foundation for practical applications in underwater topography reconstruction.

Abstract: This paper investigates the geometric inverse problem of recovering the bottom shape from surface measurements of water waves. Using the general water-waves system on a bounded subdomain of the fluid domain, we address this inverse problem, focusing on the identifiability and the stability issues. We establish uniqueness and derive logarithmic stability estimates in the determination of the bathymetry on any fixed smooth, bounded, open domain
  ${\mathcal O}\subset {\mathbb R} ^d$, $d=1,2$, from the knowledge of the free surface, its first time derivative, the velocity potential at a given instant $t_0$ within $\mathcal O $, and the knowledge of the bottom along $\partial \mathcal O$. No further assumptions are required for uniqueness. For stability, we impose only a \textit{local fatness} condition on the region between the bottom profiles, allowing us to adapt the size estimates method.

</details>


### [37] [Global well-posedness of 3D two-fluid type model with vacuum: smallness on scaling invariant quantity](https://arxiv.org/abs/2601.17709)
*Huanyao Wen,Chanxin Xie*

Main category: math.AP

TL;DR: Global well-posedness of strong solutions for 3D two-fluid type model with vacuum, under smallness conditions on scaling-invariant initial quantities.


<details>
  <summary>Details</summary>
Motivation: Study the Cauchy problem for three-dimensional two-fluid type models where vacuum regions are allowed, which is physically relevant for fluid flows with varying densities.

Method: Introduce scaling-invariant initial quantities and establish global well-posedness under assumptions of appropriate regularity, compatibility constraints, and smallness of these quantities.

Result: Proves global existence of strong solutions when the scaling-invariant quantities $\bar P^{\frac{3}{γ}} (\|\sqrt{ρ_0}u_0\|_{L^2}^2+\|P_0\|_{L^1}) (\|\nabla u_0\|_{L^2}^2+\|P_0\|_{L^2}^2)$ and $\bar P^{\frac{6}{γ}+1} (\|\sqrt{ρ_0}u_0\|_{L^2}^2+\|P_0\|_{L^1})^3 (\|\nabla u_0\|_{L^2}^2+\|P_0\|_{L^2}^2)$ are sufficiently small.

Conclusion: The paper establishes global well-posedness for the 3D two-fluid type model with vacuum under specific smallness conditions on scaling-invariant initial data quantities.

Abstract: This paper focuses on Cauchy problem for the three-dimensional two-fluid type model, in which the presence of vacuum is permitted. Under some assumptions that the initial data satisfy appropriate regularity conditions and a compatibility constraint, and that the newly introduced scaling-invariant initial quantities $\bar P^{\frac{ 3}γ} \left(\|\sqrt{ρ_0}u_0\|_{L^2}^2+\|P_0\|_{L^1}\right) \left(\|\nabla u_0\|_{L^2}^2+\|P_0\|_{L^2}^2\right)$ and $\bar P^{\frac{6}γ+1} \left(\|\sqrt{ρ_0}u_0\|_{L^2}^2+\|P_0\|_{L^1}\right)^3 \left(\|\nabla u_0\|_{L^2}^2+\|P_0\|_{L^2}^2\right)$ are sufficiently small, the global well-posedness of strong solutions to the two-fluid type model is derived.

</details>


### [38] [The existence of solutions of Schrödinger equations with essence resonance](https://arxiv.org/abs/2601.17776)
*Chong Li*

Main category: math.AP

TL;DR: The paper develops a bootstrap iteration approach to solve asymptotically linear Schrödinger equations where Palais-Smale condition fails, obtaining nontrivial solutions without mountain pass geometry and characterizing solution nondegeneracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of compactness in asymptotically linear Schrödinger equations where the Palais-Smale condition fails, particularly due to interactions between nonlinear terms and continuum spectrum under hypothesis (V2).

Method: Introduces a self-contained bootstrap iteration approach for elliptic equations on ℝᴺ, which generalizes the Agmon-Douglis-Nirenberg theorem. The method provides precise estimates for asymptotically linear functions and establishes a comparison theorem for Schrödinger operator spectra.

Result: Obtains nontrivial solutions without requiring mountain pass geometry and provides explicit descriptions of solution nondegeneracy under monotonicity hypotheses.

Conclusion: The bootstrap iteration approach successfully handles the compactness issues in asymptotically linear Schrödinger equations, yielding existence results and detailed characterization of solution properties without traditional variational structures.

Abstract: The current paper investigates a class of asymptotically linear Schrodinger equations. The Palais-Smale condition fails to hold in this case. Especially under the hypothesis (V2), the lack of compactness occurs at the interaction between nonlinear term and continuum spectrum. For this reason, we introduce a bootstrap iteration approach for elliptic equation on RN. The iteration is self-contained and can be regarded as a generalization of Agmon-Douglis-Nirenberg theorem. The proof characterizes iteration steps independent of the choice of the parameter, which are indeed manipulated by intrinsic natures of potentials and nonlinear terms, and furthermore presents precise estimates for asymptotically linear functions or continuous nonlinear terms restricted on a bounded domain in RN. Additionally, a comparison theorem for the spectrum of Schrodinger operator is also established in this paper. With above preparations, we can get a nontrivial solution without mountain pass geometry, and more importantly make an explicit description of nondegeneracy of solutions with monotonicity hypothesis.

</details>


### [39] [Asymptotic stability of smooth solitons and multi-solitons for the Camassa--Holm equation](https://arxiv.org/abs/2601.17793)
*Robin Ming Chen,Yang Lan,Yue Liu,Zhong Wang*

Main category: math.AP

TL;DR: Asymptotic stability of Camassa-Holm solitons and multi-solitons in H¹ space, with weak convergence to solitons via Liouville-type rigidity and spectral analysis using integrable structure.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous asymptotic stability results for solitons and multi-solitons of the Camassa-Holm equation in the energy space H¹(ℝ), extending classical stability theory from KdV/gKdV to the nonlocal, variable-coefficient setting of CH.

Method: Uses Liouville-type rigidity theorem characterizing localized solutions; develops complete spectral resolution of linearized CH operator around solitons via bi-Hamiltonian/integrable structure, recursion operators, and squared eigenfunction completeness; obtains sharp decay estimates in exponentially weighted spaces.

Result: Proves asymptotic stability of single solitons and extends to well-ordered trains of solitons and explicit multi-soliton solutions; solutions initially close to solitons converge weakly in H¹(ℝ) to (possibly different) solitons; methodology also applies to KdV and mKdV via squared-eigenfunction expansions.

Conclusion: The integrable structure of CH provides substitute spectral framework unavailable in classical approaches, enabling rigorous asymptotic stability analysis; method unifies treatment of CH with other integrable dispersive equations through squared-eigenfunction perspective.

Abstract: We establish the asymptotic stability of smooth solitons and multi-solitons for the Camassa-Holm (CH) equation in the energy space $H^1(\R)$. We show that solutions initially close to a soliton converge, up to translation, weakly in $H^1(\R)$ as time tends to infinity to a (possibly different) soliton. The analysis is based on a Liouville-type rigidity theorem characterizing solutions that remain localized near a soliton trajectory.
  A central feature of the proof is a complete spectral resolution of the linearized CH operator around a soliton. This linear theory is obtained via the bi-Hamiltonian and integrable structure of the CH equation, through the recursion operator and the completeness of the associated squared eigenfunctions. It provides a substitute for the classical spectral framework used in KdV and gKdV equations, which is unavailable in the nonlocal and variable-coefficient setting of CH.
  The spectral resolution yields sharp decay estimates for the linearized flow in exponentially weighted spaces, which in turn lead to the nonlinear rigidity result and the asymptotic stability of a single soliton. Combined with known orbital stability results, this approach extends to well-ordered trains of solitons and to the explicit multi-soliton solutions generated by the inverse scattering method. As an additional application, we revisit the linearized problems associated with other integrable dispersive equations, including the KdV and mKdV equations, from the perspective of squared-eigenfunction expansions.

</details>


### [40] [The effect of boundary geometry in nonlocal critical problems with Hardy-Littlewood-Sobolev exponent](https://arxiv.org/abs/2601.17872)
*Hichem Chtioui,Tuhina Mukherjee,Lovelesh Sharma*

Main category: math.AP

TL;DR: Mixed boundary value problem with Choquard nonlinearity at critical exponent; examines how Neumann boundary geometry affects ground state existence.


<details>
  <summary>Details</summary>
Motivation: To understand how the geometry of the Neumann boundary region influences the existence of ground state solutions for mixed Dirichlet-Neumann problems with critical Choquard nonlinearity.

Method: Analysis of mixed Dirichlet-Neumann boundary value problem with Choquard nonlinearity at upper critical exponent (Hardy-Littlewood-Sobolev inequality). Investigates geometric effects of Neumann boundary region.

Result: The paper likely establishes existence/non-existence results for ground state solutions depending on the geometry of the Neumann boundary region.

Conclusion: Geometry of Neumann boundary region significantly affects existence of ground state solutions for critical Choquard problems with mixed boundary conditions.

Abstract: In this paper we consider a mixed Dirichlet-Neumann boundary value problem. lem involving Choquard nonlinearity with upper critical exponent in the sense of Hardy- Littlewood Sobolev inequality. We investigate the effect of the geometry of the boundary part where the Neumann condition is prescribed on the existence problem of ground state solutions.

</details>


### [41] [Spectrum properties of mixed operators under the mixed boundary conditions](https://arxiv.org/abs/2601.17878)
*Lovelesh Sharma*

Main category: math.AP

TL;DR: The paper studies spectral properties of mixed operators combining classical and fractional Laplacians with mixed boundary conditions (Dirichlet and Neumann).


<details>
  <summary>Details</summary>
Motivation: To analyze the spectral behavior of hybrid operators that combine local (classical Laplacian) and non-local (fractional Laplacian) differential operators under mixed boundary conditions, which arise in various physical and mathematical contexts.

Method: The authors study the eigenvalue problem for the mixed operator $\mathcal{L} = -\Delta + (-\Delta)^s$ with $s \in (0,1)$ on a bounded domain $\Omega$ with smooth boundary. They consider mixed boundary conditions: Dirichlet conditions on part of the boundary and Neumann conditions on another part, with specific conditions on the complementary sets $\mathcal{D}$ and $\mathcal{N}$.

Result: The paper describes the spectral properties of this mixed operator, including existence and properties of eigenvalues $\lambda > 0$ and corresponding eigenfunctions $u$ satisfying the mixed boundary value problem $(P_\lambda)$.

Conclusion: The mixed operator combining classical and fractional Laplacians with mixed boundary conditions exhibits interesting spectral properties that warrant further investigation, with potential applications to problems involving both local and non-local diffusion processes.

Abstract: In this paper, we describe the spectrum properties of mixed operators, precisely the superposition of the classical Laplace operator and the fractional Laplace operator in the presence of mixed boundary conditions, that is
  \begin{equation} \label{1}
  \left\{\begin{split} \mathcal{L}u\: &= λu,~~\text{in} ~Ω,
  u&=0~~~~~\text{in} ~~{U^c},
  \mathcal{N}_s(u)&=0 ~~~~~\text{in} ~~{\mathcal{N}},
  \frac{\partial u}{\partial ν}&=0 ~~~~~\text{in}~~ \partial Ω\cap \overline{\mathcal{N}},
  \end{split} \right.\tag{$P_λ$} \end{equation} where $U= (Ω\cup {\mathcal{N}} \cup (\partialΩ\cap\overline{\mathcal{N}}))$, $Ω\subseteq \mathbb{R}^n$ is a non empty bounded open set with sufficiently smooth boundary $\partialΩ$, say of class $C^1$, and $\mathcal{D}$, $\mathcal{N}$ are open subsets of $\mathbb{R}^n\setminus{\bar{Ω}}$ such that $\overline{\mathcal{D} \cup {\mathcal{N}}}= \mathbb{R}^n\setminusΩ$, $\mathcal{D} \cap {\mathcal{N}}= \emptyset $ and $Ω\cup \mathcal{N}$ is a bounded set with sufficiently smooth boundary, $λ>0$ is a real parameter and
  $\mathcal{L}= -Δ+(-Δ)^{s},~ \text{for}~s \in (0, 1).$

</details>


### [42] [Cloaking laminate design based on GPT-vanishing structures](https://arxiv.org/abs/2601.17932)
*Eleanor Gemida,Mikyoung Lim*

Main category: math.AP

TL;DR: A near-cloaking design using laminated isotropic materials that approximates GPT-vanishing structures for improved constructibility.


<details>
  <summary>Details</summary>
Motivation: To create more practical cloaking laminates by reducing microscale requirements and material contrast, making them easier to construct compared to non-coated designs.

Method: Use finite-layer lamination of isotropic materials that approximates a cloaking material derived from pushing forward a multi-coated structure, which cancels generalized polarization tensors (GPTs) up to several leading orders.

Result: Enhanced cloaking effect with coarser microscale requirements and reduced contrast in constituent materials, improving constructibility of cloaking laminates.

Conclusion: The proposed GPT-vanishing structure approach enables more practical and constructible near-cloaking designs compared to traditional non-coated structures.

Abstract: We propose a near-cloaking design which is a lamination of a finite number of layers of isotropic materials. The proposed design is an approximation of a cloaking material obtained by pushing forward a multi-coated structure for which the coating cancels the generalized polarization tensors (GPTs) up to several leading orders. The enhanced cloaking effect achieved by the GPT-vanishing structure permits a coarser microscale requirement and reduces the contrast in the constituent isotropic materials, thereby improving constructibility of cloaking laminates compared with designs based on a non-coated structure.

</details>


### [43] [Time decay estimates for localized perturbations around a helical state for the Landau-Lifshitz-Gilbert equation](https://arxiv.org/abs/2601.17941)
*Ikkei Shimizu*

Main category: math.AP

TL;DR: The paper studies stability of helical states in Landau-Lifshitz-Gilbert equation with Dzyaloshinskii-Moriya interaction, proving global existence and decay estimates for small perturbations.


<details>
  <summary>Details</summary>
Motivation: To investigate dynamical stability of exact stationary solutions (helical states) in the Landau-Lifshitz-Gilbert equation with Dzyaloshinskii-Moriya interaction, which is important for understanding magnetization dynamics in magnetic materials with chiral interactions.

Method: Using Bloch-Fourier-wave decomposition to analyze the linear operator, characterizing eigenvalue problems via Mathieu equations, and proving global existence and time decay estimates for solutions under small initial perturbations in Lebesgue and Sobolev spaces.

Result: Demonstrates that helical states are stable under small perturbations, with global existence of solutions and time decay estimates established through careful analysis of the linear operator's spectral properties.

Conclusion: The helical states in the Landau-Lifshitz-Gilbert equation with Dzyaloshinskii-Moriya interaction are dynamically stable, with perturbations decaying over time, providing important insights into magnetization dynamics in chiral magnetic systems.

Abstract: We study the dynamics of the Landau--Lifshitz--Gilbert equation with the Dzyaloshinskii--Moriya interaction. The equation admits a family of exact stationary solutions, referred to as helical states, which are periodic in one spatial variable and constant in the others. We investigate the dynamical stability of a helical state with respect to perturbations belonging to suitable Lebesgue and Sobolev spaces. Under a smallness assumption on the initial perturbation, we prove global existence and time decay estimates for solutions, demonstrating that the above helical state is stable. The analysis of the relevant linear operator is carried out via the Bloch--Fourier-wave decomposition, where the eigenvalue problem for the reduced operator is characterized by certain Mathieu equations.

</details>


### [44] [Global Well-Posedness and Numerical Approximation of a Coupled Darcy-Convection-Diffusion System with Exponential Nonlinearity](https://arxiv.org/abs/2601.17968)
*Sahil Kundu,Amiya K. Pani,Manoranjan Mishra*

Main category: math.AP

TL;DR: This paper analyzes density-driven flow instability in porous media with viscosity/density contrasts and adsorption, proving existence/uniqueness of weak solutions and showing concentration decays exponentially to zero over time.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how viscosity contrast, density contrast, and linear adsorption affect density-driven instabilities (fingering) in porous media, which has applications in groundwater contamination, CO₂ sequestration, and oil recovery.

Method: Mathematical modeling using Darcy's law coupled with convection-diffusion-reaction equations, theoretical analysis via Galerkin approximation and truncation techniques for existence/uniqueness proofs, maximum principle for non-negativity, and numerical simulations based on pressure formulation to track kinetic energy and mixing measures.

Result: Proved existence of unique weak solutions in 2D/3D, concentration non-negativity, exponential decay of concentration to zero in L^p-norm. Numerical simulations show density contrast increases kinetic energy but with diminishing returns, and adsorption suppresses mixing but also with saturating efficiency.

Conclusion: Theoretical and numerical analyses reveal nonlinear sensitivities: density contrast enhances instability but with diminishing marginal impact, while adsorption suppresses mixing but with saturating effectiveness. These findings provide insights for controlling fingering phenomena in practical applications.

Abstract: This paper investigates density driven flow in porous media, focusing on the roles of viscosity contrast, density contrast, and linear adsorption. In this setup, the fluid on top is heavier and more viscous than the fluid below. Under the effect of gravity, this system becomes unstable, and finger-like structures appear. The phenomenon is described mathematically by coupling Darcy's law with a convection-diffusion reaction equation. The nonlinearity in this model arises mainly from the concentration dependence of viscosity and the convective transport term. The existence of a unique pair of weak solutions is shown in both two and three dimensions using the Galerkin approximation method and truncation technique. Moreover, an application of the maximum principle shows non-negativity of the concentration. Additionally, we analyze the long-time behavior of the solution and prove that the concentration converges exponentially to zero in the $L^p$-norm for all $1 \le p \le \infty$ as $t \to \infty.$ To complement the theoretical analysis, we perform numerical simulations based on a pressure formulation. By tracking total kinetic energy and mixing measures over time, we discuss the instability and the mixing efficiency, respectively. The present study reveals that although increasing the density contrast amplifies the total kinetic energy, the marginal impact diminishes with successive increments of density contrast. Similarly, while adsorption acts to suppress mixing, its efficiency in doing so tends to saturate with further increases. These non-linear sensitivities are predicted by our theoretical estimates and confirmed by the numerical simulations.

</details>


### [45] [Global Existence and Finite-Time Blow-Up for a Coupled Darcy-Forchheimer-Brinkman System with Quadratic Reaction Dynamics](https://arxiv.org/abs/2601.17984)
*Sahil Kundu,Manmohan Vashisth,Manoranjan Mishra*

Main category: math.AP

TL;DR: Study of reactive transport in porous media with Darcy-Forchheimer-Brinkman equations coupled to convection-diffusion-reaction. Shows local weak solutions exist, global existence/uniqueness when 0≤c₀≤1, exponential decay for regular data, finite-time blow-up when c₀>1, with numerical validation.


<details>
  <summary>Details</summary>
Motivation: Model reactive transport through porous media involving complex nonlinear couplings: Darcy-Forchheimer-Brinkman flow with nonlinear viscosity, inertial drag, convective transport, and quadratic reaction terms. Need to understand solution behavior including existence, uniqueness, stability, and potential blow-up phenomena.

Method: Mathematical analysis of coupled PDE system: Darcy-Forchheimer-Brinkman equations with convection-diffusion-reaction. Uses maximum principle techniques, energy estimates, and functional analysis. Numerical simulations with finite element method to validate theoretical predictions.

Result: 1) Local weak solutions exist for general initial data. 2) When 0≤c₀≤1, maximum principle holds → global existence/uniqueness in 2D/3D. 3) For regular data: strong solutions exist uniquely with continuous dependence and exponential decay in L^p-norm. 4) When c₀>1: finite-time blow-up occurs with explicit upper bound for blow-up time. 5) Numerical simulations confirm decay and blow-up behaviors.

Conclusion: The concentration initial condition threshold c₀=1 separates fundamentally different behaviors: subcritical (0≤c₀≤1) yields global well-posedness and decay, while supercritical (c₀>1) leads to finite-time blow-up. The model captures rich dynamics of reactive porous media transport with practical implications for engineering applications.

Abstract: We study a nonlinear system coupling the Darcy-Forchheimer-Brinkman equations with a convection-diffusion-reaction equation, arising in reactive transport through porous media. The model features a nonlinear viscosity coupling, Forchheimer inertial drag, convective transport, and a quadratic reaction term. We establish the existence of local-in-time weak solutions for general initial data. Under the physically relevant condition on initial data $0 \leq c_0 \leq 1$, a maximum principle for the concentration is proved, yielding global existence and uniqueness of weak solutions in two and three space dimensions. For higher regular initial data, we obtain the existence, uniqueness, and continuous dependence of strong solutions. In this regime, the concentration decays exponentially to zero in $L^p$-norm for all $1 \leq p \leq \infty$ with a uniform decay rate. In contrast, if $c_0 > 1$, we demonstrate the occurrence of finite-time blow-up of solutions and derive an explicit upper bound for the blow-up time. Finally, numerical simulations based on the finite element method are presented to illustrate both the decay behavior and finite-time blow-up predicted by the theory.

</details>


### [46] [Resolvent Approach to Atangana--Baleanu Evolution Equations: Laplace Symbols, Mild Solutions, and Regularity](https://arxiv.org/abs/2601.17992)
*Mohamed Wakrim*

Main category: math.AP

TL;DR: A unified resolvent approach for Atangana-Baleanu (AB) fractional evolution equations in Banach spaces, establishing analytic framework, solution representation, and stability estimates.


<details>
  <summary>Details</summary>
Motivation: AB fractional derivatives with non-singular Mittag-Leffler kernels are widely used for modeling anomalous diffusion and hereditary dynamics, but lack proper analytic treatment since AB kernels don't fit classical Volterra or Bernstein-function frameworks.

Method: Develops a unified resolvent approach using Laplace-domain formulation inspired by Hille-Phillips theory. Introduces fractional resolvent associated with AB kernel, establishes optimal bounds on sectorial contours, constructs AB-Mittag-Leffler resolvent family under condition β<1+α.

Result: Complete representation of mild solutions to AB Cauchy problem, sharp stability and regularity estimates of Mittag-Leffler type including fractional-domain bounds. Numerical illustrations confirm predicted decay.

Conclusion: Places AB-type equations within functional-analytic framework comparable to classical theory for Caputo and Volterra models, with connections to non-autonomous operators, maximal L^p-regularity, and weighted AB kernels.

Abstract: Fractional evolution equations with memory terms are widely used to model anomalous diffusion, viscoelastic response, and hereditary dynamics in physics, biology, and engineering. Among the recently introduced operators, the Atangana--Baleanu (AB) derivatives have attracted considerable attention due to their non-singular Mittag--Leffler kernels. However, their analytic treatment remains limited, as the AB kernel does not fall within the classical Volterra or Bernstein-function frameworks.
  This paper develops a unified resolvent approach for AB-type evolution equations in Banach spaces. Using a Laplace-domain formulation inspired by Hille--Phillips theory, we introduce a fractional resolvent associated with the AB kernel and establish optimal bounds on sectorial contours. Under the natural condition $β<1+α$, we construct an AB--Mittag--Leffler resolvent family and obtain a complete representation of mild solutions to the AB Cauchy problem. Sharp stability and regularity estimates of Mittag--Leffler type are derived, including fractional-domain bounds.
  Numerical illustrations confirm the predicted decay, and connections with non-autonomous operators, maximal $L^p$-regularity, and weighted AB kernels are outlined. The results place AB-type equations within a functional-analytic framework comparable to the classical theory for Caputo and Volterra models.

</details>


### [47] [A unified view of nonlinear, nonlocal operators and qualitative properties of associated elliptic and parabolic problems](https://arxiv.org/abs/2601.18028)
*Ralph Chill,Mahamadi Warma*

Main category: math.AP

TL;DR: A general framework for elliptic/parabolic equations with nonlinear nonlocal operators using semigroup theory and convex analysis.


<details>
  <summary>Details</summary>
Motivation: To develop a unified mathematical framework that can handle both classical elliptic operators and various nonlinear nonlocal/fractional operators that haven't been systematically studied.

Method: Uses abstract theory of nonlinear semigroups generated by subgradients of convex functionals on Hilbert spaces, introducing general classes of nonlinear nonlocal elliptic operators and rigorously defining subgradients/j-subgradients.

Result: Establishes well-posedness of Cauchy problems, proves comparison/maximum principles, submarkovian properties, domination, ultracontractivity, and Hölder estimates for the generated semigroups.

Conclusion: Provides a comprehensive framework applicable to various nonlocal operators across Euclidean spaces, graphs, metric random walk spaces, fractional Brownian motions, and Lévy flights.

Abstract: We put together a general framework to deal with elliptic and parabolic equations associated with (nonlinear) nonlocal (fractional order) operators. Many well-known nonlocal operators enter into our framework, and in addition one may introduce many other, new nonlocal operators that have not yet been considered in the literature. We use the abstract theory of (nonlinear) semigroups generated by subgradients of proper, lower semicontinuous and convex functionals on Hilbert spaces to build a rigorous and applicable framework that works for many classical elliptic operators but also nonlocal or sometimes fractional operators. After recalling the notion of a nonlinear semigroup generated by subgradients and $j$-subgradients of the associated energy functions, we introduce a general class of (nonlinear) nonlocal elliptic type operators and define rigorously subgradients and $j$-subgradients of such functionals that generate (nonlinear) submarkovian semigroups and hence, the abstract Cauchy problem associated with these subgradients and/or $j$-subgradients is wellposed. The existence and the qualitative properties of solutions to these Cauchy problems and the corresponding semigroups are investigated. More precisely, we show some comparison and maximum principles, submarkovian, domination, ultracontractivity properties, and some Hölder type estimates for these semigroups of operators. These results are usually useful in several branches of pure and applied partial differential equations. We finish the paper by giving several examples of nonlocal operators in Euclidean spaces, graphs, metric random walk spaces, fractional Brownian motions, and Lévy flights, that fit in our general framework.

</details>


### [48] [Heat flow of harmonic maps into CAT($0$)-spaces](https://arxiv.org/abs/2601.18046)
*Fang-Hua Lin,Antonio Segatti,Yannick Sire,Changyou Wang*

Main category: math.AP

TL;DR: New method proves global existence, uniqueness, and spatial Lipschitz continuity for harmonic map heat flow into CAT(0) spaces, solving long-standing open problem using elliptic regularization and parabolic frequency functions.


<details>
  <summary>Details</summary>
Motivation: To establish global existence and uniqueness of weak solutions for harmonic map heat flow into CAT(0) metric spaces, and to prove spatial Lipschitz continuity for such solutions, addressing a long-standing open problem in geometric analysis.

Method: Elliptic regularization of gradient flow of Dirichlet energy, combined with a Dynamical Variational Principle and parabolic frequency function of Almgren-Poon type, exploiting variational structure at regularized level.

Result: Proves global existence and uniqueness of suitable weak solutions, establishes spatial Lipschitz continuity for solutions into wide class of CAT(0) spaces, provides new proof of Eells-Sampson theorem, and introduces first use of monotonicity methods for parabolic deformations into singular targets.

Conclusion: The paper presents a novel approach that solves fundamental problems in harmonic map theory, extends classical results to singular CAT(0) targets, and introduces new techniques (elliptic regularization, parabolic frequency functions) that provide fresh perspectives even for smooth Riemannian targets.

Abstract: We introduce a new approach to prove the global existence and uniqueness of suitable weak solutions of the heat flow of harmonic mappings into CAT(0) metric spaces. Our method allows also to prove Lipschitz continuity in spatial variables for such solutions into a wide class of CAT$(0)$ spaces, answering a long-standing open problem in the field. Our approach is based on an elliptic regularization of the gradient flow of the Dirichlet energy and even in the case of smooth Riemannian targets provides a novel viewpoint, together with a new Dynamical Variational Principle and a new proof of the celebrated Eells-Sampson theorem. The spatial Lipschitz regularity for such weak solutions is achieved by fully exploiting the variational structure of the problem at the regularized level and introducing a parabolic frequency function of Almgren-Poon type. Our contribution is the first instance of the use of monotonicity methods for parabolic deformations of maps into singular targets.

</details>


### [49] [On fractional semilinear wave equations in non-cylindrical domains](https://arxiv.org/abs/2601.18355)
*Bonafini Mauro,Le Van Phu Cuong,Molinarolo Riccardo*

Main category: math.AP

TL;DR: Existence of weak solutions for semilinear wave equations in time-dependent domains using two methods: time-discretization and penalty approach.


<details>
  <summary>Details</summary>
Motivation: To study semilinear wave equations in non-cylindrical, time-dependent domains with exterior homogeneous Dirichlet conditions, which arise in problems involving moving boundaries and evolving spatial domains.

Method: Two different approaches: 1) A constructive time-discretization scheme, and 2) A penalty approach. Both methods work under mild regularity and monotonicity assumptions on the evolving domains.

Result: Established existence of weak solutions for semilinear wave equations in time-dependent domains. The analysis applies to nonlocal fractional Laplacians, potentials with Lipschitz continuous gradient, and vector-valued maps.

Conclusion: The paper successfully demonstrates two methods for proving existence of weak solutions for semilinear wave equations in evolving domains, extending the analysis to include fractional Laplacians and vector-valued cases.

Abstract: In this paper, we investigate a class of semilinear wave equations in non-cylindrical time-dependent domains, subject to exterior homogeneous Dirichlet conditions. Under mild regularity and monotonicity assumptions on the evolving spatial domains, we establish existence of weak solutions by two different methods: a constructive time-discretization scheme and a penalty approach. The analysis applies to nonlocal fractional Laplacians and potentials with Lipschitz continuous gradient, and to vector-valued maps.

</details>


### [50] [Stability of the free boundary Willmore problem](https://arxiv.org/abs/2601.18388)
*Anna Dall'Acqua,Fabian Rupp,Reiner Schätzle,Manuel Schlierf*

Main category: math.AP

TL;DR: New Łojasiewicz-Simon gradient inequality enables analysis of Willmore problem with free boundary, proving global existence and convergence of flow near minimizers, and stability/rigidity results near minimal surfaces.


<details>
  <summary>Details</summary>
Motivation: To develop new analytical tools for studying the Willmore problem with free boundary conditions, overcoming limitations of previous approaches that required gradient-like representations of Fréchet derivatives.

Method: Introduces a new Łojasiewicz-Simon gradient inequality for functionals on infinite dimensional manifolds that only requires an inequality rather than a gradient-like representation of the Fréchet derivative. Applies this to analyze free boundary Willmore flow and static Willmore immersions.

Result: For free boundary Willmore flow: solutions starting sufficiently close to a local minimizer exist for all times and converge. For static setting: proves quantitative stability of free boundary Willmore immersions and local rigidity in neighborhoods of free boundary minimal surfaces.

Conclusion: The new Łojasiewicz-Simon gradient inequality provides a powerful tool for analyzing free boundary Willmore problems, enabling both dynamic flow analysis and static stability/rigidity results without requiring gradient-like derivative representations.

Abstract: We study the Willmore problem with free boundary by means of a new Łojasiewicz-Simon gradient inequality for functionals on infinite dimensional manifolds. In contrast to previous works, we do not rely on a gradient-like representation of the Fréchet derivative, but merely on an inequality. For the free boundary Willmore flow, we prove that solutions starting sufficiently close to a local minimizer exist for all times and converge. In the static setting, we prove quantitative stability of free boundary Willmore immersions and a local rigidity result in a neighborhood of free boundary minimal surfaces.

</details>


### [51] [A brush problem. Homogenization involving thin domains and PDEs in graphs](https://arxiv.org/abs/2601.18430)
*José M. Arrieta,Joaquín Domínguez-de-Tena*

Main category: math.AP

TL;DR: Analysis of homogenization for elliptic PDEs in comb/brush domains with thin teeth using non-periodic unfolding method, leading to graph limit problems.


<details>
  <summary>Details</summary>
Motivation: To study homogenization of elliptic equations in complex domains with thin structures (combs/brushes) where traditional periodic assumptions don't apply, enabling analysis of more realistic geometries.

Method: Adaptation of unfolding operator method to non-periodic framework, assuming asymptotic limit density θ for tooth distribution rather than periodicity, with teeth as rescalings of arbitrary model tooth.

Result: Shows convergence to homogenized limit problem, which under certain geometric conditions on teeth can be interpreted as a differential equation on a graph structure.

Conclusion: The non-periodic unfolding method successfully handles homogenization in comb domains with arbitrary tooth shapes and distributions, providing mathematical framework for analyzing PDEs on complex thin structures.

Abstract: This work analyses the homogenization of a linear elliptic equation with Neumann boundary conditions in a comb/brush domain, composed of a fixed base and a family of thin teeth. The teeth are defined as rescalings of order less than or equal to $\varepsilon$ of a model tooth of arbitrary shape. Periodicity in their distribution is not assumed; instead, the existence of an asymptotic limit density $θ$, which may vanish in certain regions, is assumed. The convergence analysis is performed using an adaptation of the unfolding operator method to a non-periodic framework. Finally, it is shown that, under certain conditions on the geometry of the teeth, the resulting limit problem can be interpreted as a differential equation on a graph.

</details>


### [52] [Unstable Interface Dynamics for Gravity Stokes Flow](https://arxiv.org/abs/2601.18460)
*Francisco Gancedo,Rafael Granero-Belinchón,Zhongtian Hu,Elena Salguero,Yao Yao*

Main category: math.AP

TL;DR: Study of interface instability in two-fluid Stokes flow with gravity: proves infinite growth of interface length/curvature in unstable configurations, shows stable configurations can become unstable in finite time.


<details>
  <summary>Details</summary>
Motivation: To understand the unstable behavior of fluid interfaces in Stokes flow with gravity, particularly when denser fluid lies above lighter fluid (Rayleigh-Taylor type instability), and to analyze how stable configurations can transition to unstable regimes.

Method: Analytical proofs of infinite-in-time growth of interface length or curvature in unstable scenarios, supported by numerical simulations. Also analytical demonstration that certain initially stable configurations evolve into unstable regime in finite time.

Result: Proves infinite growth of interface length/curvature in unstable configurations (denser fluid above lighter). Shows stable configurations can become unstable in finite time. Numerical simulations confirm predicted growth phenomena.

Conclusion: Fluid interfaces in Stokes flow with gravity exhibit complex instability behavior: unstable configurations lead to unbounded growth, while seemingly stable configurations can transition to instability, highlighting the delicate nature of fluid interface stability.

Abstract: We investigate some unstable behavior of the interface given by two incompressible fluids of different densities evolving by the regular Stokes law with gravity force. In the unstable scenario, where the denser fluid lies above the lighter fluid, we prove infinite-in-time growth of the length or the curvature of the interface. We support these analytical results with numerical simulations that confirm the predicted growth phenomena. In the stable configuration, where the denser fluid lies below the lighter fluid, we show that certain initial configurations evolve into the unstable regime in finite time.

</details>


### [53] [On decay of solutions to the anisotropic Boussinesq equations near the hydrostatic balance in half space $\mathbb{R}_+^3$](https://arxiv.org/abs/2601.18481)
*Wangrong Yang,Aibin Zang*

Main category: math.AP

TL;DR: Global stability and decay rates for 3D anisotropic Boussinesq system with horizontal dissipation near hydrostatic balance in H³ space.


<details>
  <summary>Details</summary>
Motivation: The Boussinesq equations are crucial for geophysical fluid modeling. This paper aims to analyze stability and long-term behavior of perturbations near hydrostatic balance in the 3D anisotropic system with horizontal dissipation.

Method: Uses energy methods and bootstrapping arguments in Sobolev space H³. Applies Fourier transform in horizontal directions and Fourier cosine/sine transforms in vertical direction to analyze decay rates.

Result: Proves global stability property in H³ space and obtains decay rates for both the global solution and its derivatives.

Conclusion: The 3D anisotropic Boussinesq system with horizontal dissipation exhibits global stability near hydrostatic balance with quantifiable decay rates for solutions and their derivatives.

Abstract: The system of the Boussinesq equations is one of the most important models for geophysical fluids. This paper focuses on the initial-boundary problem of the 3D incompressible anisotropic Boussinesq system with horizontal dissipation. The goal here is to assess the stability property and large-time behavior of perturbations near the hydrostatic balance. By utilizing the structure of the system, the energy methods and the means of bootstrapping argument, we prove the global stability property in the Sobolev space $H^3(\mathbb{R}^3_+)$. After taking a Fourier transform in $x_h = (x_1, x_2)$ and Fourier cosine and sine transforms in $x_3$ for the system, we obtain the decay rates for the global solution itself as well as its derivatives.

</details>


### [54] [Semilinear Diffusion Equations on Infinite Graphs: The Dissipative and Lipschitz Cases](https://arxiv.org/abs/2601.18549)
*Elvise Berchio,Davide Bianchi,Alberto G. Setti,Maria Vallarino*

Main category: math.AP

TL;DR: Existence, uniqueness, and regularity results for semilinear diffusion equations on infinite weighted graphs with monotone decreasing or Lipschitz nonlinearities, using time discretization and exhaustion techniques.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous solution theory for semilinear diffusion equations on infinite graphs under minimal structural assumptions, extending PDE analysis to graph settings.

Method: Time discretization via implicit Euler scheme combined with exhaustion technique using Dirichlet subgraphs to handle infinite graph structure.

Result: Proved existence, uniqueness, and regularity of mild solutions in ℓ^p spaces (1≤p<∞), plus existence/uniqueness for related time-independent equation, finite-time extinction, and positivity results.

Conclusion: Developed comprehensive solution theory for semilinear diffusion on infinite graphs with minimal assumptions, providing analytical tools for nonlinear graph evolution equations.

Abstract: We study a class of semilinear diffusion equations on infinite, connected, weighted graphs, focusing on two types of nonlinearities: monotone decreasing and Lipschitz continuous. Under minimal structural assumptions on the graph, we establish existence, uniqueness, and regularity of mild solutions for initial data in $\ell^p$ spaces, with $1\leq p<\infty$. Our approach relies on time discretization via an implicit Euler scheme and an exhaustion technique using Dirichlet subgraphs. As a by-product, we obtain existence and uniqueness results for a related time-independent equation. Finite-time extinction and positivity for solutions under a specific forcing term are also proved.

</details>


### [55] [Liquid crystals and topological vorticity: smoothness of mild solutions](https://arxiv.org/abs/2601.18726)
*Fanghua Lin,Yannick Sire,Yantao Wu,Yifu Zhou*

Main category: math.AP

TL;DR: New PDE models incorporating topological vorticity effects for liquid crystal flows, featuring SQG-type anomalous diffusion coupled with crystal orientation gradient flow.


<details>
  <summary>Details</summary>
Motivation: To better understand vorticity formulation of Liquid Crystal Flow, address regularity issues in conserved geometric motions, and capture Navier-Stokes/Euler features through scalar unknowns while maintaining advection-diffusion structure.

Method: Introduce several new models with topological vorticity effects, where macroscopic unknown is driven by dissipative anomalous diffusion (SQG-type) coupled with crystal orientation moving by gradient flow of energy maps.

Result: Obtain regularity for mild solutions under natural assumptions for initial data (near-optimal conditions), and establish connections with existing (anti-)ferromagnet models.

Conclusion: The proposed PDE models successfully capture Navier-Stokes-like features through scalar unknowns while addressing regularity issues in liquid crystal flows, with connections to magnetic systems.

Abstract: We introduce several new models whose common feature is to take into account effects from topological vorticity. The macroscopic unknown is driven by a dissipative anomalous diffusion (of SQG-type) and is coupled with the orientation of the crystal, moving by the gradient flow of the energy of maps. The main idea of such models is to have a better insight on the vorticity formulation of the Liquid Crystal Flow and to tackle some regularity issues in the associated conserved geometric motions. One of the advantage of the present PDEs is to capture features of the Navier-Stokes equations (or Euler) through a {\sl scalar} unknown, keeping the advection-diffusion structure of the orientation field. We obtain regularity for mild solutions under natural assumptions for the initial data, which are actually near-optimal. Along the way, we also draw some links with natural models of (anti-)ferromagnets previously investigated.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [56] [Physics-guided curriculum learning for the identification of reaction-diffusion dynamics from partial observations](https://arxiv.org/abs/2601.17382)
*Hanyu Zhou,Yuansheng Cao,Yaomin Zhao*

Main category: physics.comp-ph

TL;DR: CLIP method uses physics-informed neural networks with curriculum learning for accurate parameter estimation and hidden state reconstruction in reaction-diffusion systems, outperforming baselines and successfully applied to bacterial Min system.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation in reaction-diffusion systems is challenging with sparse, noisy observations of only subset state variables, requiring methods for joint parameter inference and hidden state reconstruction.

Method: Physics-guided Curriculum Learning Identification via PINNs (CLIP) method using physics-informed neural networks with curriculum learning that progresses from reaction-dominated regimes to full spatiotemporal dynamics, employing anchored widening transfer strategy.

Result: CLIP achieves more accurate and robust identification than baseline methods across three canonical reaction-diffusion benchmarks, successfully infers dynamics of bacterial Min system with membrane-bound observations only, and ablation studies show curriculum stages enhance trainability.

Conclusion: CLIP framework effectively addresses parameter estimation challenges in reaction-diffusion systems through physics-guided curriculum learning, demonstrating practical applicability to complex biological systems like bacterial Min dynamics.

Abstract: Reaction-diffusion (RD) systems provide fundamental models for understanding self-organized spatiotemporal patterns across diverse natural and engineered settings, but reliable parameter estimation remains challenging, particularly when observations are sparse, noisy, and restricted to a subset of state variables. Based on physics-informed neural networks (PINNs), a physics-guided Curriculum Learning Identification via PINNs (CLIP) method is introduced in this work, for joint parameter inference and hidden state reconstruction. Leveraging the physical separability of RD systems, the CLIP training progresses from reaction-dominated regimes to full spatiotemporal dynamics using curriculum learning and an anchored widening transfer strategy. Across three canonical reaction-diffusion benchmarks, CLIP achieves more accurate and robust identification than baseline methods. Furthermore, the CLIP framework is successfully applied to infer the dynamics of Min system in bacteria, where only membrane bound species are observed and key kinetic rates span multiple orders of magnitude. Moreover, ablation experiments and loss landscape analyses provide mechanistic evidence that the curriculum stages and anchored transfer enhance trainability and convergence.

</details>


### [57] [Conformal Quantile Regression for Probabilistic Constitutive Modeling of Anisotropic Soft Materials](https://arxiv.org/abs/2601.17437)
*Bahador Bahmani*

Main category: physics.comp-ph

TL;DR: A probabilistic data-driven framework for anisotropic soft tissue modeling using conformalized quantile regression to quantify uncertainty while ensuring thermodynamic consistency.


<details>
  <summary>Details</summary>
Motivation: Biological soft tissues have high inter-subject variability and intrinsic stochasticity, but most existing data-driven constitutive models are deterministic and lack uncertainty quantification, limiting reliability in patient-specific analysis.

Method: Probabilistic framework using conformalized quantile regression applied to tensor-valued fields, built on strain-invariant polyconvex formulation for thermodynamic consistency. Plug-and-play approach that can convert existing deterministic models to probabilistic ones without distribution assumptions.

Result: Method validated on benchmark datasets from literature, showing robust predictive performance including in extrapolative regimes. Framework is simple to train, scalable to large parameter models, computationally efficient without Monte Carlo sampling.

Conclusion: Proposed framework provides reliable uncertainty quantification for anisotropic soft tissue modeling, enabling more trustworthy patient-specific mechanical analyses while maintaining computational efficiency and thermodynamic consistency.

Abstract: Biological soft tissues exhibit substantial inter-subject variability, making the automation of constitutive material modeling essential for patient-specific analysis and design. Such materials are not only highly nonlinear but also display intrinsic stochasticity arising from their complex and heterogeneous microstructure. Despite recent advances in data-driven constitutive modeling, most existing approaches remain deterministic and fail to quantify predictive uncertainty, thereby limiting their reliability in downstream mechanical analyses. In this work, we propose a probabilistic, data-driven constitutive modeling framework for anisotropic soft materials that explicitly accounts for uncertainty through conformalized quantile regression applied to tensor-valued fields. The proposed framework is built upon a strain-invariant, polyconvex formulation that ensures thermodynamic consistency and promotes robust predictive performance, including in extrapolative regimes. A key advantage of the proposed approach is its simplicity: it can be applied in a plug-and-play manner to endow existing deterministic models with probabilistic predictions, while remaining distribution-free and requiring no assumptions on the underlying data distribution. Moreover, the method is straightforward to train, scalable to models with a large number of parameters, and avoids Monte Carlo sampling at inference, making it computationally efficient and well suited for uncertainty propagation in large-scale mechanical simulations. The proposed method is validated using several benchmark datasets synthesized and collected from the literature.

</details>


### [58] [Frequency-domain general synthetic iterative scheme for efficient simulation of oscillatory rarefied gas flows](https://arxiv.org/abs/2601.17484)
*Pengshuo Li,Lei Wu*

Main category: physics.comp-ph

TL;DR: GSIS method accelerates oscillatory rarefied gas flow simulations by combining kinetic and macroscopic equations in frequency domain, achieving 1000x speedup in near-continuum regimes.


<details>
  <summary>Details</summary>
Motivation: Efficient simulation of oscillatory rarefied gas flows in MEMS is challenging due to time-dependent nature and high dimensionality of Boltzmann equation.

Method: Focus on periodic steady state, solve using frequency domain GSIS that simultaneously solves mesoscopic kinetic equation and macroscopic synthetic equation with mutual acceleration.

Result: Super convergence achieved with asymptotic preserving property, allowing coarse spatial grids; 3 orders of magnitude faster than conventional kinetic schemes in near continuum regimes.

Conclusion: GSIS provides efficient numerical framework for oscillatory rarefied gas flows with significant computational advantages over traditional methods.

Abstract: Oscillatory rarefied gas flows are frequently encountered in MEMS, and their efficient numerical simulation remains a major challenge due to the time dependent nature of the problem and the high dimensionality of the Boltzmann kinetic equation. Here, we address this challenge by focusing on the periodic steady state and solving the resulting problem using the frequency domain general synthetic iterative scheme (GSIS). The key idea of GSIS is to simultaneously solve the mesoscopic kinetic equation and the macroscopic synthetic equation. The kinetic equation provides high-order constitutive relations, beyond those given by the Newton law of viscosity and the Fourier law of heat conduction, to the synthetic equation. In turn, the synthetic equation, which converges to the periodic steady state much faster than the kinetic equation, boosts the evolution of the kinetic equation toward the periodic steady state. As a result, super convergence is achieved, together with an asymptotic preserving property that allows the use of coarse spatial grids. The analytical Fourier stability analysis and the Chapman-Enskog expansion, together with challenging numerical simulations, are employed to demonstrate the fast convergence and asymptotic-preserving properties of GSIS, revealing that it can be three orders of magnitude faster than conventional kinetic schemes in near continuum flow regimes.

</details>


### [59] [Smooth Polar B-Splines with High-Order Regularity at the Origin](https://arxiv.org/abs/2601.17841)
*Peiyou Jiang,Roman Hatzky,Zhixin Lu,Eric Sonnendrücker,Matthias Borchardt,Ralf Kleiber,Martin Campos Pinto,Ronald Remmerswaal*

Main category: physics.comp-ph

TL;DR: Smooth polar B-splines correct coordinate singularity at origin in polar coordinates using harmonic polar functions projected onto tensor-product B-splines, enabling C∞-regularity while preserving local support and sparsity.


<details>
  <summary>Details</summary>
Motivation: Standard tensor-product B-spline formulations in polar coordinates suffer from loss of regularity at the origin due to the coordinate singularity, which causes numerical issues in physics simulations.

Method: Constructs "smooth polar splines" via Galerkin projection of harmonic polar functions (r^l sin(mθ) and r^l cos(mθ)) onto central tensor-product B-spline basis in innermost radial region, reproducing r^l exactly for 0 ≤ l ≤ p.

Result: The method achieves near-origin regularity, improves matrix conditioning, conserves charge, reduces statistical errors in particle-in-cell simulations, eliminates spurious eigenvalues, and provides robust high-order adaptation while maintaining local support and sparse matrices.

Conclusion: Smooth polar splines provide an effective solution to the coordinate singularity problem in polar coordinates, enabling high-order accurate physics simulations with improved numerical properties while preserving computational efficiency.

Abstract: We introduce a smooth B-spline discretization in polar coordinates on the unit disc that corrects the loss of regularity present at the origin caused by the coordinate singularity in standard tensor-product B-spline formulations. The method constructs "smooth polar splines" via a Galerkin projection of harmonic polar functions $S_l^{-m}(r,θ) := r^l \sin(mθ)$ and $S_l^{m}(r,θ) := r^l \cos(mθ)$, derived from the polar representation of Cartesian monomials, onto the central tensor-product B-spline basis in the innermost radial region. The radial component reproduces $r^l$ exactly for $0 \le l \leq p$, where $p$ is the B-spline degree, satisfying the near-origin regularity condition. However, exact compatibility with $C^\infty$-regularity at the origin is recovered only in the limit $Δθ\to 0$, when the angular component resolves all angular harmonics accurately. The smooth polar splines are linear combinations of standard tensor-product B-splines and lie in the same function space, enabling mapping between the $C^\infty$-regular subspace and the original discretization space via an exact prolongation operator and a corresponding restriction operator acting on the discrete variables. They match standard tensor-product B-splines away from the origin, preserve orthogonality among the newly constructed origin-centered basis functions, and maintain local support and sparse matrices. This smoothness and locality improve the conditioning of mass and stiffness matrices, conserve charge, and reduce statistical errors in particle-in-cell simulations near the origin, while eliminating spurious eigenvalues in eigenvalue problems. The approach provides a robust, high-order, and efficient adaptation of tensor-product B-splines for polar coordinates in physics simulations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [60] [Hybrid collisional-radiative modeling for high-fidelity atomic kinetics](https://arxiv.org/abs/2601.17308)
*Prashant Sharma,Christopher J. Fontes,Mark Zammit,James Colgan,Nathan Garland,Xian-Zhu Tang*

Main category: physics.plasm-ph

TL;DR: Hybrid collisional-radiative models combine fine-structure resolution with superconfiguration averaging for efficient yet accurate plasma simulations.


<details>
  <summary>Details</summary>
Motivation: To develop practical CR models that balance accuracy and computational efficiency for fusion plasma simulations, addressing the need for predictive fidelity without excessive computational cost.

Method: Developed two hybrid CR models for helium, lithium, and beryllium that retain detailed fine-structure states up to selected principal quantum numbers, while statistically averaging higher-lying states into superconfigurations.

Result: Models successfully computed radiative power loss, average and effective charge states across wide temperature/density ranges, with accuracy validated against fully fine-structure-resolved benchmarks.

Conclusion: Hybrid CR schemes offer a versatile and practical compromise between accuracy and computational efficiency, suitable for detailed plasma simulations requiring balanced predictive fidelity.

Abstract: The fidelity of collisional-radiative (CR) models is critical for advancing our understanding of radiative properties and ionization balance in fusion plasmas. In this work, we present and evaluate hybrid CR schemes that combine fine-structure resolution with superconfiguration averaging, offering a practical compromise between accuracy and computational efficiency. Two hybrid CR models are developed for helium, lithium, and beryllium, retaining detailed fine-structure states up to selected principal quantum numbers, while higher-lying states are statistically averaged to form superconfigurations. These models are applied to compute radiative power loss, as well as average and effective charge states, across a wide range of electron temperatures and densities. The results are benchmarked against a fully fine-structure-resolved CR model to assess the accuracy of the hybrid approach. The findings demonstrate the versatility of hybrid CR schemes and their suitability for detailed plasma simulations where predictive fidelity must be balanced with computational cost.

</details>


### [61] [Plasma Decay of Nanosecond Pulsed Laser-Produced Ar and Ar-H2O Sparks at Atmospheric Pressure](https://arxiv.org/abs/2601.17547)
*Ji Yung Ahn,Jianan Wang,Tasnim Akbar Faruquee,Marien Simeni Simeni*

Main category: physics.plasm-ph

TL;DR: Researchers used time-resolved diagnostics to study free-electron properties in nanosecond laser-produced atmospheric discharges in Ar and Ar-H2O mixtures, finding peak electron densities of ~2×10¹⁷ cm⁻³ and temperatures of ~7 eV, with excellent agreement between Thomson scattering and Stark broadening techniques.


<details>
  <summary>Details</summary>
Motivation: To investigate free-electron properties in nanosecond laser-produced atmospheric discharges and provide benchmark data for modeling such discharges, while demonstrating the reliability of combining Thomson scattering with Stark broadening diagnostics.

Method: Combined broadband plasma imaging, laser Thomson scattering, and optical emission spectroscopy (with emphasis on Stark broadening of Hα and Hβ lines) to study discharges generated by 23 ns, 1064 nm laser pulses in Ar and Ar-H2O mixtures at atmospheric pressure.

Result: Measured peak electron density of ~2×10¹⁷ cm⁻³ and electron temperature of ~7 eV; observed bright emission persisting for 30-40 μs followed by weak glow up to 19 ms; found excellent agreement between Thomson scattering and Stark broadening for electron densities; inferred electron decay consistent with ambipolar expansion and electron-ion recombination processes.

Conclusion: The study provides valuable benchmark data for modeling nanosecond laser discharges and successfully demonstrates the reliability of combining Thomson scattering with Stark broadening diagnostics for atmospheric laser spark characterization.

Abstract: Time-resolved diagnostics were applied to investigate free-electron properties in nanosecond laser-produced discharges sustained at atmospheric pressure in Ar and in Ar with 3% H2O. The discharges were generated using 23 ns, 1064 nm laser pulses. Broadband plasma imaging and laser Thomson scattering were combined with optical emission spectroscopy, with particular emphasis on Stark broadening of the Halpha and Hbeta lines. The plasma exhibited a bright emission that persisted for up to 30--40 us after breakdown, followed by a very weak glow lasting up to 19 ms. Peak electron number density of about 2 x 10^17 cm-3 and electron temperature of about 7 eV were measured. Excellent agreement between both techniques was obtained for absolute electron number densities. The inferred temporal decay of free electrons is consistent with processes dominated by ambipolar expansion and two- and three-body electron-ion recombination. These results provide benchmark data for modeling nanosecond laser discharges and demonstrate the reliability of combining Thomson scattering with Stark broadening in atmospheric laser sparks.

</details>


### [62] [Sub-ion scale current sheets in kinetic Alfvén wave turbulence](https://arxiv.org/abs/2601.18131)
*Johan Sharma,Ch Akshath Kumar,Kirit D. Makwana,Tulasi N Parashar,Sruti Satyasmita*

Main category: physics.plasm-ph

TL;DR: 3D PIC simulations study kinetic Alfvén wave turbulence at sub-ion scales, finding electron-scale current sheets whose thickness scales with electron skin depth, showing enhanced intermittency consistent with space observations.


<details>
  <summary>Details</summary>
Motivation: To understand turbulent fluctuations and intermittent structures at sub-ion and electron scales in plasma turbulence, particularly the role of electron scales in current sheet formation and their connection to dissipation in kinetic Alfvén wave turbulence.

Method: 3D kinetic particle-in-cell simulations initialized with KAW eigenvectors from two-fluid model; varied ion-to-electron mass ratios; used BFS and DBSCAN algorithms to analyze current sheet geometry (thickness, length, width); analyzed scale-dependent kurtosis for intermittency.

Result: Current sheet thickness scales inversely with square root of mass ratio (~electron skin depth); widths/lengths show weaker scaling; enhanced intermittency at electron scales matches magnetosheath observations; current sheet properties align with electron skin depth and solar wind turbulence ranges.

Conclusion: Electron-scale current sheets form in KAW turbulence with thickness scaling with electron skin depth, contributing to enhanced intermittency and dissipation at sub-ion scales, consistent with observational data from space plasmas.

Abstract: 3D kinetic particle-in-cell (PIC) simulations are performed using the kinetic Alfvén wave (KAW) eigenvector relations from a two-fluid model as initial conditions, in order to study turbulent fluctuations and intermittent structures at sub-ion and electron scales. Simulations with different ion-to-electron mass ratios are set up to investigate the role of electron scales in the formation of intermittent structures. We analyze the current sheet structures that develop in these simulations. Two algorithms, namely Breadth-First Search (BFS) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN), are employed to determine the thickness, length, and width of the current sheets, and both methods are found to yield consistent results. The average current sheet thickness scales inversely with the square root of the ion-to-electron mass ratio, with values close to the electron skin depth ($d_e$), indicating the presence of electron-scale current sheets in the simulations. The widths and lengths of the current sheets show a weaker scaling with the mass ratio. The scale-dependent kurtosis reveals enhanced intermittency at electron scales, consistent with magnetosheath observations. Distributions of scale-dependent properties of the current sheets also align with the electron skin depth of the different simulations and they lie within ranges observed in kinetic scale solar wind turbulence. This study reveals the nature of sub-ion-scale current sheets in KAW turbulence and their role in dissipation.

</details>


### [63] [Evidence of Langmuir/$\mathcal{Z}$-mode Wave Decay into $\mathcal{Z}$-mode Electromagnetic Radiation in the Solar Wind](https://arxiv.org/abs/2601.18495)
*F. J. Polanco-Rodríguez,C. Krafft,P. Savoini*

Main category: physics.plasm-ph

TL;DR: First observation of nonlinear decay of Langmuir/Z-mode waves into electromagnetic Z-mode radiation at plasma frequency in solar wind, enabled by Solar Orbiter's high-resolution measurements.


<details>
  <summary>Details</summary>
Motivation: To identify and characterize nonlinear wave decay processes in space plasma using unprecedented high-resolution measurements from Solar Orbiter's RPW instrument during a Type III radio burst event.

Method: Used Solar Orbiter's RPW instrument for high-resolution electric/magnetic field measurements during electron beam event, applied multiple evidence criteria (resonance conditions, phase coherence, temporal coincidence), excluded competing mechanisms, and performed particle-in-cell simulations under beam-plasma conditions.

Result: First definitive observation of Langmuir/Z-mode wave decay into electromagnetic Z-mode radiation in solar wind; simulations reproduced key features and suggested wave packet may be trapped in extended density well preventing scattering effects.

Conclusion: Nonlinear wave decay processes can be directly observed in space plasma with sufficient measurement resolution; wave trapping in density wells may enable these processes to occur despite typical scattering effects in turbulent solar wind.

Abstract: The nonlinear decay of Langmuir/$\mathcal{Z}$-mode waves into electromagnetic $\mathcal{Z}$-mode wave radiation at the plasma frequency is observed for the first time in the solar wind. This finding was enabled by the unprecedented high-resolution electric and magnetic field measurements provided by the Radio Plasma Waves (RPW) instrument aboard the Solar Orbiter spacecraft, which encountered an electron beam associated with a Type III radio burst. The decay process is definitively identified through multiple lines of evidence: satisfaction of frequency and wavevector resonance conditions, strong phase coherence and temporal coincidence between the interacting waves, exclusion of competing mechanisms, and full agreement with theoretical predictions. Particle-in-cell simulations, conducted under close beam-plasma conditions, successfully reproduce the key features of the observations. Notably, they suggest that the wave packet observed by Solar Orbiter may be trapped within an extended, nearly flat-bottomed density well, where the decay process is not overcome by wave scattering on random density fluctuations and subsequent mode conversion effects.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [64] [Error Analysis of Bayesian Inverse Problems with Generative Priors](https://arxiv.org/abs/2601.17374)
*Bamdad Hosseini,Ziqi Huang*

Main category: stat.ML

TL;DR: The paper analyzes error bounds for inverse problems solved using generative models as priors, showing posterior errors inherit prior error rates under Wasserstein distances.


<details>
  <summary>Details</summary>
Motivation: Data-driven methods for inverse problems using generative models as bespoke priors have become popular, but there's a need for theoretical analysis of error bounds for such approaches.

Method: Theoretical analysis presenting quantitative error bounds for minimum Wasserstein-2 generative models used as priors, showing posterior error rates inherit prior error rates under Wasserstein-1 distance.

Result: Under certain assumptions, the error in the posterior due to the generative prior inherits the same rate as the prior error with respect to the Wasserstein-1 distance.

Conclusion: The theoretical error analysis is validated through numerical experiments including an elliptic PDE inverse problem where a generative prior models a non-stationary field.

Abstract: Data-driven methods for the solution of inverse problems have become widely popular in recent years thanks to the rise of machine learning techniques. A popular approach concerns the training of a generative model on additional data to learn a bespoke prior for the problem at hand. In this article we present an analysis for such problems by presenting quantitative error bounds for minimum Wasserstein-2 generative models for the prior. We show that under some assumptions, the error in the posterior due to the generative prior will inherit the same rate as the prior with respect to the Wasserstein-1 distance. We further present numerical experiments that verify that aspects of our error analysis manifests in some benchmarks followed by an elliptic PDE inverse problem where a generative prior is used to model a non-stationary field.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [65] [Harmonic Approximation and Resolvent Estimates for Non-Self-Adjoint Operators](https://arxiv.org/abs/2601.17643)
*Stepan Malkov*

Main category: math.SP

TL;DR: Semiclassical resolvent estimate for non-self-adjoint, non-elliptic pseudodifferential operators in low-lying spectral regime, with application to Schrödinger operators with complex potentials.


<details>
  <summary>Details</summary>
Motivation: To study spectral properties of non-self-adjoint, non-elliptic operators in semiclassical analysis, particularly for operators that lack standard ellipticity conditions needed for resolvent estimates.

Method: Impose dynamical condition on average of real part of symbol along Hamiltonian flow generated by imaginary part to obtain improved ellipticity properties, then prove resolvent estimate using these properties.

Result: Proved semiclassical resolvent estimate for broad class of non-self-adjoint, non-elliptic pseudodifferential operators, allowing localization of spectral problem to O(h)-sized neighborhood of origin for Schrödinger operators with complex potentials.

Conclusion: Dynamical conditions on symbol averages provide sufficient ellipticity for resolvent estimates in non-elliptic regimes, enabling spectral localization for important classes of non-self-adjoint operators.

Abstract: We prove a semiclassical resolvent estimate for a broad class of non-self-adjoint, non-elliptic pseudodifferential operators in the low-lying spectral regime. The proof relies on improved ellipticity properties for the symbol of the operator, which we obtain by imposing a dynamical condition on the average of the real part of the symbol along the Hamiltonian flow generated by its imaginary part. An application of the resolvent estimate to a family of semiclassical Schrödinger operators with complex potentials allows us to localize the spectral problem to an $O(h)$-sized neighborhood of the origin.

</details>


### [66] [Eigenvalue optimization in higher dimensions and $p$-harmonic maps](https://arxiv.org/abs/2601.17896)
*Denis Vinokurov*

Main category: math.SP

TL;DR: Existence results for k-th Laplace eigenvalue optimization on Riemannian manifolds (m≥3) with various normalizations, linking maximizers to p-harmonic maps into spheres.


<details>
  <summary>Details</summary>
Motivation: To establish existence results for eigenvalue optimization problems on higher-dimensional Riemannian manifolds (m≥3), extending previous work limited to dimension two, and to understand the geometric structure of maximizers.

Method: Uses techniques from topological tensor product theory to analyze eigenvalue optimization problems. Proves that absolutely continuous maximizers are induced by p-harmonic maps into spheres (p∈[2,m]), with smoothness for p close to m and no bubbling for p<m.

Result: Existence of maximizers for eigenvalue optimization within conformal classes in dimensions m≥3 (previously only known for m=2). Characterization of maximizers as p-harmonic maps into spheres with specific regularity properties.

Conclusion: Topological tensor product techniques are effective for eigenvalue optimization problems. Maximizers exhibit geometric structure as p-harmonic maps, with different regularity regimes depending on p relative to dimension m.

Abstract: We prove existence results for optimization problems for the $k$th Laplace eigenvalue on closed Riemannian manifolds of dimension $m \geq 3$, depending on the choice of normalization. One such normalization leads to eigenvalue optimization within a conformal class, for which existence of maximizers was previously known only in dimension two. We also prove that all absolutely continuous maximizers of the normalized eigenvalue functionals are always induced by $p$-harmonic maps into spheres, where $p \in [2,m]$. For $p$ sufficiently close to $m$, the maximizers are smooth, whereas for $p<m$ no bubbling occurs. A key tool in our analysis is the application of techniques from the theory of topological tensor products, which appear to be well suited for studying eigenvalue-related optimization problems.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [67] [Weighted Birkhoff averages: Deterministic and probabilistic perspectives](https://arxiv.org/abs/2505.03210)
*Zhicheng Tong,Yong Li*

Main category: math.DS

TL;DR: Survey of weighted quasi-Monte Carlo methods for physical applications with universal rapid convergence results, contrasting classical ergodic theory's slow convergence.


<details>
  <summary>Details</summary>
Motivation: To address the typically slow convergence in classical ergodic theory by developing weighted quasi-Monte Carlo methods that achieve universal rapid convergence for physically related applications.

Method: Introduce weighting with compact support to Birkhoff ergodic averages for quasi-periodic, almost periodic, and periodic systems; analyze from deterministic and probabilistic perspectives; establish quantitative convergence results under various regularity assumptions.

Result: Achieved universal rapid convergence (polynomial and exponential types) for weighted computations; established first universal exponential convergence results for Fourier coefficients in finite and infinite dimensions; provided quantitative improvements to existing results.

Conclusion: Weighted quasi-Monte Carlo methods enable rapid convergence for physical applications, with explicit regularity settings facilitating practical implementation and bridging deterministic and probabilistic perspectives.

Abstract: In this paper, we survey physically related applications of a class of weighted quasi-Monte Carlo methods from a theoretical, deterministic perspective, and establish quantitative universal rapid convergence results via various regularity assumptions. Specifically, we introduce weighting with compact support to the Birkhoff ergodic averages of quasi-periodic, almost periodic, and periodic systems, thereby achieving universal rapid convergence, including both arbitrary polynomial and exponential types. This is in stark contrast to the typically slow convergence in classical ergodic theory. As new contributions, we not only discuss more general weighting functions but also provide quantitative improvements to existing results; the explicit regularity settings facilitate the application of these methods to specific problems. We also revisit the physically related problems and, for the first time, establish universal exponential convergence results for the weighted computation of Fourier coefficients, in both finite-dimensional and infinite-dimensional cases. In addition to the above, we explore results from a probabilistic perspective, including the weighted strong law of large numbers and the weighted central limit theorem, by building upon the historical results.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [68] [Qhronology: A Python package for studying quantum models of closed timelike curves](https://arxiv.org/abs/2601.17459)
*Lachlan G. Bishop*

Main category: quant-ph

TL;DR: Qhronology is a Python package for simulating quantum closed timelike curves (CTCs) and general quantum information processing, offering both numerical and symbolic quantum circuit simulation capabilities.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive computational framework for studying quantum models of closed timelike curves and temporal paradoxes, while also serving as a general quantum circuit simulator for quantum algorithms and protocols.

Method: Developed as a Python package with a modular architecture, featuring functionality for analyzing quantum theories of antichronological time travel, calculating quantum resolutions to temporal paradoxes, and simulating quantum circuits in both numerical and symbolic modes.

Result: The paper formally introduces Qhronology, covering its design philosophy, architecture, basic usage, and demonstrates capabilities through various examples. Performance benchmarking of the circuit simulation component is also presented.

Conclusion: Qhronology provides a versatile tool for quantum information research, particularly for studying closed timelike curves and temporal paradoxes, while also serving as a general-purpose quantum circuit simulator with both numerical and symbolic capabilities.

Abstract: Qhronology is a novel scientific-computing package for studying quantum models of closed timelike curves (CTCs) and simulating general quantum information processing and computation. Written in Python, the program provides a comprehensive framework for analyzing quantum theories of antichronological time travel, including functionality to calculate quantum resolutions to temporal paradoxes. It also operates as a complete quantum circuit simulator, enabling the examination of quantum algorithms and protocols in both numerical and symbolic capacities. In this paper, we formally introduce Qhronology, beginning with discussion on aspects of its design philosophy and architecture. An overview of its basic usage is then presented, along with a collection of examples demonstrating its various capabilities within a variety of distinct contexts. Lastly, the performance of the package's circuit simulation component is characterized by way of some simple empirical benchmarking.

</details>


### [69] [Feedback-Based Quantum Control for Safe and Synergistic Drug Combination Design](https://arxiv.org/abs/2601.18082)
*Mai Nguyen Phuong Nhi,Lan Nguyen Tran,Le Bin Ho*

Main category: quant-ph

TL;DR: Quantum-control framework for optimizing drug combinations using DDI data encoded into Ising Hamiltonians, solved with FALQON algorithm.


<details>
  <summary>Details</summary>
Motivation: Drug-drug interactions significantly impact combination therapy safety and efficacy. While DDI databases exist, finding optimal multi-drug combinations that balance safety, benefit, and regimen size remains a challenging combinatorial optimization problem.

Method: A quantum-control-based framework encodes known harmful and synergistic DDIs into Ising Hamiltonians as penalties and rewards. Optimization is performed using FALQON (feedback-based quantum algorithm), a gradient-free variational approach.

Result: Numerical simulations using data from Drugs.com and SYNERGxDB show efficient convergence and high-quality solutions for clinically relevant drug sets, including COVID-19 case studies. The framework successfully addresses two tasks: Maximum Safe Subset and Synergy-Constrained Optimization problems.

Conclusion: The quantum-control framework provides an effective approach for DDI-aware drug combination optimization, demonstrating practical applicability to real-world clinical scenarios including COVID-19 treatment optimization.

Abstract: Drug-drug interactions (DDIs) strongly affect the safety and efficacy of combination therapies. Despite the availability of large DDI databases, selecting optimal multi-drug combinations that balance safety, therapeutic benefit, and regimen size remains a challenging combinatorial optimization problem. Here, we present a quantum-control-based framework for DDI-aware drug combination optimization, in which known harmful and synergistic interactions are encoded into Ising Hamiltonians as penalties and rewards, respectively. The optimization is performed using the feedback-based quantum algorithm FALQON, a gradient-free variational approach. We study two clinically motivated tasks: the Maximum Safe Subset problem and the Synergy-Constrained Optimization problem. Numerical simulations using interaction data from Drugs.com and SYNERGxDB demonstrate efficient convergence and high-quality solutions for clinically relevant drug sets, including COVID-19 case studies.

</details>


### [70] [Efficient Trotter-Suzuki Schemes for Long-time Quantum Dynamics](https://arxiv.org/abs/2601.18756)
*Marko Maležič,Johann Ostmeyer*

Main category: quant-ph

TL;DR: A framework for constructing high-order Trotter-Suzuki schemes with optimized parameters that outperform traditional methods for long-time quantum dynamics simulations.


<details>
  <summary>Details</summary>
Motivation: Accurately simulating long-time dynamics of many-body systems is challenging due to Trotter error accumulation. Low-order Trotter-Suzuki decompositions have rapidly growing errors that limit access to long-time observables.

Method: A framework for constructing efficient high-order Trotter-Suzuki schemes by identifying their structure and directly optimizing parameters over a high-dimensional space. This enables discovery of new schemes with improved efficiency compared to traditional constructions.

Result: Two novel highly efficient schemes at 4th and 6th order are recommended based on theoretical efficiency and practical performance. These decompositions perform better across computational cost for fixed final time, even surpassing established low-order schemes like Leapfrog when using large time steps. Schemes with more uniform coefficients show improved error accumulation over long times.

Conclusion: The proposed optimization framework enables discovery of high-order Trotter-Suzuki schemes with significantly improved efficiency for long-time quantum dynamics simulations, with recommended 4th and 6th order schemes demonstrating superior performance over traditional methods.

Abstract: Accurately simulating long-time dynamics of many-body systems is a challenge in both classical and quantum computing due to the accumulation of Trotter errors. While low-order Trotter-Suzuki decompositions are straightforward to implement, their rapidly growing error limits access to long-time observables. We present a framework for constructing efficient high-order Trotter-Suzuki schemes by identifying their structure and directly optimizing their parameters over a high-dimensional space. This method enables the discovery of new schemes with significantly improved efficiency compared to traditional constructions, such as those by Suzuki and Yoshida. Based on the theoretical efficiency and practical performance, we recommend two novel highly efficient schemes at $4^{\textrm{th}}$ and $6^{\textrm{th}}$ order. We also demonstrate the effectiveness of these decompositions on the Heisenberg model and the quantum harmonic oscillator, and find that for a fixed final time they perform better across the computational cost. Even when using large time steps, they surpass established low-order schemes like the Leapfrog. Finally, we investigate the in-practice performance of different Trotter schemes and find the decompositions with more uniform coefficients tend to feature improved error accumulation over long times. We have included this observation into our choice of recommended schemes.

</details>


### [71] [Perturbation Theory and the Quantum Rabi-model](https://arxiv.org/abs/2601.17924)
*Marcello Malagutti,Alberto Parmeggiani*

Main category: quant-ph

TL;DR: The paper studies the Rabi system in two parts: first analyzing finite eigenvalue families using perturbative methods and proving the Braak conjecture for them, then examining spectral asymptotics for N-level atom generalizations of the Quantum Rabi Model.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical analysis of the Rabi system in quantum optics, addressing eigenvalue expansions and spectral properties of generalized quantum models.

Method: Part 1: Uses perturbative approach and Rellich's theory to study analytic expansions of finite eigenvalue families. Part 2: Analyzes asymptotics of Weyl spectral counting function for N-level atom generalizations with multiple cavity modes.

Result: Proves that for finite families of eigenvalues, the Braak conjecture holds. Provides asymptotic analysis of spectral counting functions for generalized N-level atom systems with N-1 cavity modes.

Conclusion: The paper establishes mathematical foundations for understanding eigenvalue structure in Rabi systems and extends analysis to more complex quantum optical models with multiple energy levels and cavity modes.

Abstract: In the first part of the paper we study a perturbative model of the Rabi system of Quantum Optics. We therefore are able to describe, through Rellich's theory, an analytic expansion of finite families, but of any fixed length, of eigenvalues. In particular, we prove that for finite families of eigenvalues the Braak conjecture holds. In the second part we study the asymptotics of the Weyl spectral counting function of a class of systems that generalize the Quantum Rabi Model to an $N$-level atom ($N\geq3$) with $N-1$ cavity modes of the electromagnetic field.

</details>


### [72] [A Rigorous and Self--Contained Proof of the Grover--Rudolph State Preparation Algorithm](https://arxiv.org/abs/2601.17930)
*Antonio Falco,Daniela Falco-Pomares,Hermann G. Matthies*

Main category: quant-ph

TL;DR: A rigorous formalization and circuit-theoretic analysis of the Grover-Rudolph procedure for preparing quantum states that encode classical probability distributions.


<details>
  <summary>Details</summary>
Motivation: The Grover-Rudurch procedure is fundamental for quantum algorithms using amplitude encoding, but existing presentations often lack formal correctness proofs and clear conventions about the underlying dyadic tree structure.

Method: Formalizes the dyadic probability tree, defines angle maps via conditional masses, derives trigonometric factorizations, and proves correctness by induction. Also provides explicit circuit transpilation using Gray-code ladders and Walsh-Hadamard angle transform.

Result: Provides rigorous proof that the circuit prepares exactly the desired measurement law in computational basis, and shows each stage is a uniformly controlled RY rotation with explicit ancilla-free transpilation.

Conclusion: Offers a complete, self-contained formal analysis of Grover-Rudolph construction, addressing previous informalities and providing practical circuit implementations for quantum state preparation.

Abstract: Preparing quantum states whose amplitudes encode classical probability distributions is a fundamental primitive in quantum algorithms based on amplitude encoding and amplitude estimation. Given a probability distribution $\{p_k\}_{k=0}^{2^n-1}$, the Grover--Rudolph procedure constructs an $n$-qubit state $\ketψ=\sum_{k=0}^{2^n-1}\sqrt{p_k}\ket{k}$ by recursively applying families of controlled one-qubit rotations determined by a dyadic refinement of the target distribution. Despite its widespread use, the algorithm is often presented with informal correctness arguments and implicit conventions on the underlying dyadic tree. In this work we give a rigorous and self-contained analysis of the Grover--Rudolph construction: we formalize the dyadic probability tree, define the associated angle map via conditional masses, derive the resulting trigonometric factorizations, and prove by induction that the circuit prepares exactly the desired measurement law in the computational basis. As a complementary circuit-theoretic contribution, we show that each Grover--Rudolph stage is a uniformly controlled $\RY$ rotation on an active register and provide an explicit ancilla-free transpilation into the gate dictionary $\{\RY(\cdot),X,\CNOT(\cdot\to\cdot)\}$ using Gray-code ladders and a Walsh--Hadamard angle transform.

</details>


### [73] [Elementary Quantum Gates from Lie Group Embeddings in $U(2^n)$: Geometry, Universality, and Discretization](https://arxiv.org/abs/2601.17936)
*Antonio Falco,Daniela Falco-Pomares,Hermann G. Matthies*

Main category: quant-ph

TL;DR: The paper introduces an intrinsic approach to quantum circuit design by defining primitive gates as motions within embedded SU(2) subgroups of U(N), proving phase-free universality and establishing connections to geometric structures.


<details>
  <summary>Details</summary>
Motivation: Standard quantum circuit models rely on extrinsic tensor factorizations for gate definitions. The authors aim to develop an intrinsic descriptor layer within U(N) that doesn't depend on chosen tensor decompositions, providing a more fundamental geometric perspective on quantum computation.

Method: Define primitive gates as motions within faithful embedded copies of SU(2) in U(N), creating phase-free dictionary G^{SU}_{elem}(n). Analyze the embedding space Emb(SU(2),U(N)) using homogeneous strata indexed by isotypic multiplicities. Use two-level QR/Givens factorization and diagonal torus generation to prove universality.

Result: Prove phase-free universality: ⟨G^{SU}_{2lvl}(n)⟩ = SU(N) and ⟨G^{SU}_{elem}(n)⟩ = SU(N). Show full U(N) universality by adding diagonal/global U(1) factors. Demonstrate that embedded subgroups are totally geodesic in Hilbert-Schmidt metric. Provide modular finite-alphabet interface via Solovay-Kitaev lifting.

Conclusion: The intrinsic approach to quantum circuit design using embedded SU(2) subgroups provides a geometrically natural foundation for quantum computation, achieving universality without reliance on tensor factorizations and connecting to geometric structures in unitary groups.

Abstract: In the standard circuit model, elementary gates are specified relative to a chosen tensor factorization and are therefore extrinsic to the ambient group $U(2^n)$. Writing $N=2^n$, we introduce an intrinsic descriptor layer in $U(N)$ by declaring as primitive the motions inside faithful embedded copies of $SU(2)$, leading to the phase-free dictionary $\mathcal{G}^{SU}_{\mathrm{elem}}(n)=\bigcup_{φ\in\Emb(SU(2),U(N))}φ(SU(2))$, and we also discuss the phase-inclusive $U(2)$ variant. We show that $\Emb(SU(2),U(N))$ decomposes into finitely many $U(N)$-homogeneous strata indexed by isotypic multiplicities, with stabilizers given by centralizers; the canonical two-level sector is organized by $\Gr_2(\C^N)$ up to a $PSU(2)$ gauge. Equipping $U(N)$ with the Hilbert--Schmidt bi-invariant metric, each embedded subgroup is totally geodesic. Using two-level QR/Givens factorization together with an explicit generation of diagonal tori by two-level phase rotations, we prove phase-free universality $\langle\mathcal{G}^{SU}_{\mathrm{2lvl}}(n)\rangle=SU(N)$ and hence $\langle\mathcal{G}^{SU}_{\mathrm{elem}}(n)\rangle=SU(N)$. Full universality in $U(N)$ follows by adjoining the abelian diagonal/global $U(1)$ factors (equivalently, by passing to the $U(2)$ two-level dictionary). Finally, we record a modular finite-alphabet interface by lifting Solovay--Kitaev approximation in $SU(2)$ through two-level embeddings.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [74] [Spectral Evolution and Current Sheet Analysis as Probes of Reconnection-Mediated Decay in Magnetically Dominated Turbulence](https://arxiv.org/abs/2601.17955)
*Chandranathan Anandavijayan,Pallavi Bhat*

Main category: astro-ph.HE

TL;DR: Magnetic reconnection, not traditional cascade phenomenology, governs inverse energy transfer and decay in magnetically dominated turbulence across 2D, 2.5D, and 3D systems with both helical and nonhelical conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the puzzling inverse energy transfer in decaying magnetically dominated turbulence that occurs even without net magnetic helicity, challenging traditional cascade-based models and requiring a comprehensive explanation.

Method: Test reconnection-mediated model across 2D, 2.5D, and 3D systems with helical/nonhelical initial conditions; analyze decay timescales vs. Lundquist number; develop broken power-law spectral models; use Minkowski functionals to examine reconnecting current sheets in real space; conduct high-resolution simulations.

Result: Magnetic energy decay follows Sweet-Parker-type reconnection scaling; broken power-law spectral models match simulations; anastrophy dominates over helicity fluctuations in nonhelical turbulence; controlling current sheets are smaller than global scales, explaining weak sensitivity to resolution; aspect ratios converge to Sweet-Parker predictions at high resolution.

Conclusion: Magnetic reconnection serves as the fundamental organizing principle for inverse transfer, spectral evolution, and decay in magnetically dominated turbulence, providing a unified framework applicable across different dimensionalities and helicity regimes with astrophysical relevance.

Abstract: The decay of magnetically dominated turbulence exhibits robust inverse transfer of magnetic energy even in the absence of net magnetic helicity, challenging traditional cascade-based phenomenology. While recent studies suggest that magnetic reconnection governs the evolution of such systems, a comprehensive understanding has been lacking. Here we test a reconnection-mediated model for decaying magnetic turbulence in two-dimensional (strict-2D), 2.5D, and three-dimensional (3D) systems with both helical and nonhelical initial conditions. We show that the magnetic-energy decay timescale scales with the Lundquist number in a manner consistent with Sweet-Parker-type reconnection rather than Alfvenic or purely resistive timescales. We develop a broken power-law model for the magnetic energy spectra and provide analytic predictions for the temporal evolution of energy across both sub-inertial and inertial ranges, which are confirmed by high-resolution simulations. In nonhelical turbulence, these results favor anastrophy as the dominant constraint over helicity fluctuations. Using Minkowski functionals to analyze reconnecting current sheets in real space, we find that the structures controlling the decay are substantially smaller than the global magnetic correlation scale, implying local Lundquist numbers well below the system-scale value. This explains the weak sensitivity of global decay laws to current-sheet resolution and that the current-sheet aspect ratios converge toward Sweet-Parker predictions only at sufficiently high resolution. Together, these results establish magnetic reconnection as the organizing principle underlying inverse transfer, spectral evolution, and decay in magnetically dominated turbulence, providing a unified picture applicable across dimensionality and helicity regimes with direct implications for astrophysical plasmas.

</details>


### [75] [Closure models for the feedback of energetic particles on plasma turbulence](https://arxiv.org/abs/2601.18649)
*J. Pratt*

Main category: astro-ph.HE

TL;DR: A new PDF-based model for energetic particle-plasma interactions that bridges the gap between computationally expensive kinetic models and oversimplified fluid models, offering better statistical quality than particle-sampling methods.


<details>
  <summary>Details</summary>
Motivation: Current models for energetic particle-plasma interactions are inadequate: kinetic models are computationally prohibitive for astrophysical scales, while multi-fluid and hybrid models suffer from physical modeling limitations. There's a need for a model that balances physical realism with computational efficiency.

Method: Develops a new probability distribution function (PDF) model that follows the PDF for all particles in 7-dimensional space, treating both background plasma and energetic particles self-consistently. The approach adapts sophisticated PDF methods previously developed for engineering applications like reactive flows and turbulent combustion.

Result: The proposed PDF model represents a step toward physical realism above multi-fluid MHD models while being more computationally efficient than full kinetic models. It offers better statistical quality for evaluating mean characteristics (including density) compared to particle-sampling procedures.

Conclusion: A PDF closure approach provides a promising middle ground for modeling energetic particle-plasma interactions, addressing the limitations of existing methods by combining physical realism with computational feasibility, particularly for evaluating feedback effects on background plasma.

Abstract: Energetic particles interact with the plasma surrounding them, resonating with certain plasma waves to stabilize them while destabilizing others, and changing the character of the background turbulence in ways that have not been fully quantified or understood. Interaction with the turbulent background plasma is key to the acceleration of many types of energetic particles including high-energy cosmic rays, solar energetic particles, and pick-up ions. This is a process that would ideally be described by a kinetic model, a type of model that follows a probability distribution function (PDF) for all particles in 7-dimensional space. Because of the high dimensionality of a kinetic model, such simulations use the largest computational resources available, and are yet unable to simulate a realistic number of particles, reach the large scales necessary for astrophysical problems, or use high-precision numerical methods. Two available alternatives to kinetic plasma models have been explored: a multi-fluid model, and a hybrid fluid/Fokker-Planck model. These methods are hampered by the physical modeling of the coupling. We develop a new model, which follows the PDF for all particles; this can be viewed as a step toward physical realism above a multi-fluid MHD model, while also being more computationally efficient than a kinetic model. The equations we develop model both the background plasma and the energetic particles self-consistently. Over the last decade, similar PDF methods have been developed to a high level of sophistication to model reactive flows and turbulent combustion for engineering applications. For treatment of the feedback of the energetic particles on a background plasma, a PDF closure approach should evaluate the mean characteristics, including the density, with better statistical quality than will particle-sampling procedures.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [76] [Minimal model for vortex nucleation and reversal in spherical magnetic nanoparticles](https://arxiv.org/abs/2601.17176)
*Michael P. Adams,Andreas Michels*

Main category: cond-mat.mtrl-sci

TL;DR: Semi-analytical framework for vortex states in spherical magnetic nanoparticles using parametrized Ansatz to study vortex-mediated hysteresis and nucleation.


<details>
  <summary>Details</summary>
Motivation: Magnetic nanoparticles beyond single-domain limit develop vortex states, but analytical descriptions of vortex-mediated hysteresis and nucleation are scarce despite routine micromagnetic simulations.

Method: Develop parametrized vortex magnetization Ansatz based on hyperbolic functions that interpolates between uniform and vortex states, enabling complexity reduction to minimal Hamiltonian for efficient computation.

Result: Framework enables efficient computation of magnetization curves and provides insight into vortex-mediated magnetization reversal; derives analytical estimates for critical vortex nucleation radius and field, recovering Brown's classic result form.

Conclusion: Semi-analytical minimal framework successfully describes vortex states in spherical nanoparticles, bridging gap between micromagnetic simulations and transparent analytical descriptions of vortex-mediated phenomena.

Abstract: Magnetic nanoparticles beyond the single-domain limit often develop vortex-like magnetization textures arising from the competition between exchange and magnetostatic energies. While such states are routinely studied using micromagnetic simulations, transparent analytical descriptions of vortex-mediated hysteresis and nucleation remain scarce. Here, we develop a semi-analytical minimal framework for vortex states in spherical magnetic nanoparticles. Guided by micromagnetic simulations, we introduce a parametrized vortex magnetization Ansatz based on hyperbolic functions that continuously interpolates between uniform and vortex states. In this way, we achieve a complexity reduction leading to a minimal Hamiltonian, which enables the efficient computation of magnetization curves and provides insight into vortex-mediated magnetization reversal. As an application, we derive analytical estimates for the critical vortex nucleation radius and field, recovering the functional form of Brown's classic result and extending it within a variational framework.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [77] [Fully Turbulent Wakes at Low Reynolds Numbers: the Case of the Thin Flat Plate](https://arxiv.org/abs/2601.17341)
*Isaac T. Rosin,Melanie S. Chapman,Bartosz Protas,Robert J. Martinuzzi*

Main category: physics.flu-dyn

TL;DR: Wake flow past a 2D flat plate becomes turbulent at surprisingly low Re=400, unlike canonical cylinder wakes, with turbulent characteristics matching those at much higher Re=12500-19700.


<details>
  <summary>Details</summary>
Motivation: To investigate transition to turbulence in wake flows behind thin flat plates and understand how it differs from canonical cylinder wakes, particularly at low Reynolds numbers.

Method: Direct Numerical Simulation (DNS) of wake flow at Re=400 and Re=150, compared with experimental measurements at much higher Re=12500 and 19700. Analysis includes mean velocity, Reynolds stresses, turbulent kinetic energy transport, energy spectra, and intermittency in strain/rotation rates.

Result: Wake flow at Re=400 shows fully turbulent characteristics matching high-Re experiments (Re=12500-19700), while Re=150 flow lacks these features. Key turbulent quantities (mean velocity, Reynolds stresses, energy transport) are essentially the same across the high-Re range despite the 30-50x difference in Reynolds number.

Conclusion: Transition to turbulence in flat plate wakes occurs at much lower Reynolds numbers (Re~400) than in canonical cylinder wakes, suggesting different transition pathways. The paper identifies possible physical mechanisms responsible for these differences.

Abstract: We consider the wake flow past a thin two-dimensional flat plate normal to the uniform stream and demonstrate that this flow is turbulent already at a relatively low Reynolds number of $Re = 400$. This is achieved by performing a careful comparison of the results of a DNS of this flow with experimental measurements of wake flows in the same geometric configuration at the Reynolds numbers of $Re=12500, 19700$. This comparison reveals that the distribution of several key quantities, including the mean velocity, Reynolds stresses and different effects contributing to the transport of the turbulent kinetic energy, are, up to measurement uncertainty, the same in these flows. Moreover, the wake flow at $Re = 400$ also features energy spectra characteristic of turbulent flows with intermittency detected in the distributions of the fluctuating strain and rotation rates. In contrast, these features are absent from the results of the DNS of the wake flow at $Re = 150$ where the distribution of the key quantities is also fundamentally different. These results show that the path to transition to turbulence in the wake past a thin flat plate is different from that in the wakes of canonical (i.e., circular or square) cylinders. We also identify possible physical mechanisms that may be responsible for these differences.

</details>


### [78] [Flash evaporation Riemann Problem: Formulation and its Exact Solution](https://arxiv.org/abs/2601.18404)
*Haotong Bai,Ping Yi,Yixin Yang,Guoyan Zhao,Wenjia Xie,Mingbo Sun*

Main category: physics.flu-dyn

TL;DR: The paper develops exact solution frameworks for flash evaporation Riemann problems, analyzes non-classical wave structures, and compares Wood's mechanical equilibrium model with complete equilibrium models, finding Wood's model underestimates key flow parameters.


<details>
  <summary>Details</summary>
Motivation: To understand flash evaporation in aerospace propulsion systems and provide theoretical benchmarks for CFD simulations by formalizing the flash evaporation Riemann problem.

Method: Establishes exact solution framework for equilibrium two-phase fluids with arbitrary EOS, analyzes non-classical wave structures using Landau fundamental derivative, proposes iterative solution strategy with Chapman-Jouguet constraint, and develops exact solution using Wood's mechanical equilibrium model.

Result: Wood's model alters two-phase mixture entropy definition, introduces "density lag" effect and non-physical entropy decrease. Under scramjet fuel injection conditions, Wood's model captures general trend but significantly underestimates intermediate pressure, velocity, and vaporization extent compared to complete equilibrium model.

Conclusion: The study provides analytical tools for flash evaporation flows, reveals limitations of Wood's mechanical equilibrium model, and demonstrates the importance of complete thermodynamic equilibrium modeling for accurate prediction of flash evaporation phenomena in aerospace applications.

Abstract: Flash evaporation, a liquid-to-gas phase transition phenomenon in real fluids, is prevalent in aerospace propulsion systems. To elucidate the physical mechanisms of such complex flows and provide theoretical benchmarks for Computational Fluid Dynamics simulations, this paper formalizes the Flash evaporation Riemann problem (FeRP) characterized by the expansion branch crossing the saturation line, within the framework of Homogeneous Equilibrium and Vapor-Liquid Equilibrium assumptions. An exact solution framework that analytically resolves all thermodynamic derivatives of equilibrium two-phase fluids is established for arbitrary two-parameter equations of state. By evaluating the Landau fundamental derivative, the non-classical wave structures arising in the FeRP are analyzed, for which a stable iterative solution strategy incorporating the Chapman-Jouguet condition as an outer constraint is proposed. Furthermore, an exact solution for the FeRP based on Wood's mechanical equilibrium speed of sound is developed, enabling a comprehensive evaluation of its thermodynamic implications. Results indicate that Wood's model alters the definition of the two-phase mixture entropy in the Euler equations, introducing an isentropic path characterized by a "density lag" effect and non-physical entropy decrease. Comparative analysis of the FeRP under typical scramjet fuel injection conditions reveals that, although Wood's model captures the general trend of the Riemann solution curve, it significantly underestimates intermediate pressure, velocity, and the extent of vaporization relative to the complete equilibrium model.

</details>


### [79] [The global attractor of the Toner-Tu-Swift-Hohenberg equations of active turbulence and its properties](https://arxiv.org/abs/2601.18233)
*Daniel W. Boutros,Kolluru Venkata Kiran,John D. Gibbon,Rahul Pandit*

Main category: physics.flu-dyn

TL;DR: The paper proves the existence of a finite-dimensional compact global attractor for the TTSH equations modeling bacterial turbulence, establishes explicit bounds for its Lyapunov dimension matching the Swift-Hohenberg length scale, and validates these bounds through 2D numerical simulations.


<details>
  <summary>Details</summary>
Motivation: The TTSH equations are fundamental for modeling turbulent behavior in active matter (bacterial swarming), but rigorous mathematical analysis of their dynamical properties was lacking. The paper aims to provide theoretical foundation for the observed predominance of the Swift-Hohenberg length scale in bacterial turbulence.

Method: The authors prove existence of a finite-dimensional compact global attractor for TTSH equations on periodic domains (2D and 3D) using mathematical analysis. They derive explicit estimates for the Lyapunov dimension. Additionally, they perform pseudospectral direct numerical simulations in 2D to compute Lyapunov spectra and validate analytical bounds.

Result: The paper demonstrates that TTSH equations possess a compact global attractor with finite Lyapunov dimension. The derived bounds agree with heuristic predictions based on Swift-Hohenberg length scale. Numerical simulations in 2D show consistency between computed Lyapunov spectra and analytical bounds, confirming the theoretical predictions.

Conclusion: The study provides rigorous mathematical foundation for the observed Swift-Hohenberg length scale dominance in bacterial turbulence. The combination of analytical proofs and numerical validation establishes that TTSH equations exhibit finite-dimensional dynamics consistent with experimental observations of vortex length scales in active matter systems.

Abstract: The Toner-Tu-Swift-Hohenberg (TTSH) equations are one of the basic equations that are used to model turbulent behaviour in active matter, specifically the swarming of bacteria in suspension. They combine features of the incompressible Navier-Stokes, the Toner-Tu and Swift-Hohenberg equations, together with the important properties that they are linearly driven, and that the Laplacian diffusion is taken to be negative in combination with hyper-dissipation. We prove that the TTSH equations possess a finite-dimensional compact global attractor on the periodic domain $\mathbb{T}^d$ ($d=2,3$) and we establish explicit estimates for its Lyapunov dimension which agree with the heuristic prediction based on the Swift-Hohenberg length scale. The predominance of this length scale (as a vortex length scale) has been observed in both numerical and experimental studies of bacterial turbulence, so our methods and results provide a rigorous theoretical foundation for this phenomenon. We also carry out pseudospectral direct numerical simulations of these PDEs in dimension $d=2$ through which we obtain Lyapunov spectra for representative parameter values. We show that our numerical results are consistent with the analytically derived rigorous bounds.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [80] [Near-Field Mechanical Fingerprints for THz Sensing of 'Hidden' Nanoparticles in Complex Media](https://arxiv.org/abs/2601.17889)
*Ricardo Martin Abraham-Ekeroth,Dani Torrent*

Main category: physics.app-ph

TL;DR: THz spectroscopy can detect hidden nanoparticles in complex media using mechanical signatures (forces/torques) from magneto-optical heterodimers, overcoming far-field diffraction limits.


<details>
  <summary>Details</summary>
Motivation: Characterizing individual nanoparticles in complex biological environments at THz frequencies is challenging due to the far-field diffraction limit. Near-field dipolar theory exists but hasn't been applied to nanoparticle identification in complex media at THz frequencies.

Method: Numerical simulations of magneto-optical heterodimers (n-InSb with SiO2 or GaSe particles) under counter-propagating, circularly polarized THz illumination. Analyzed mechanical variables (binding forces, spin/orbital torques) rather than just far-field observables.

Result: Mechanical signatures show superior sensitivity for detecting hidden neighboring components compared to far-field absorption. Material-specific spectral hotspots and zeros serve as robust calibration markers. Spin torque on non-MO particles is significantly modified by MO-neighbor proximity, controllable via static magnetic fields. High angular sensitivity in perpendicular configurations.

Conclusion: Optomechanical signatures provide a roadmap for high-resolution detectors for in-vivo diagnostics, signal transduction, and low-energy nanocircuit control, overcoming THz diffraction limits in complex biological environments.

Abstract: Terahertz (THz) spectroscopy holds transformative potential for non-invasive sensing, yet characterizing individual nanoparticles in complex biological environments remains challenging due to the far-field diffraction limit. While near-field dipolar theory is well established, its application to characterizing/identifying nanoparticles immersed in complex media at THz frequencies is largely unexplored.
  This work utilizes numerical simulations of magneto-optical (MO) heterodimers -- comprising n-doped Indium Antimonide (n-InSb) and isotropic or birefringent particles (e.g., SiO2, GaSe) -- under counter-propagating, circularly polarized THz illumination. We demonstrate that while far-field observables like absorption cross-sections are often dominated by the MO-active particle, mechanical variables-specifically induced binding forces and spin/orbital torques-exhibit superior sensitivity for detecting "hidden" neighboring components. Because these mechanical signatures depend directly on near-field interactions, they provide higher information density regarding interparticle coupling.
  Key findings reveal material-specific spectral "hotspots" and "zeros" that serve as robust calibration markers even within dispersive biological surrogates. We show that the spin torque on non-MO particles is significantly modified by MO-neighbor proximity, a phenomenon controllable via static magnetic fields. Furthermore, these variables exhibit high angular sensitivity in perpendicular configurations. Our results provide a roadmap for using optomechanical signatures as high-resolution detectors for in-vivo diagnostics, signal transduction, and low-energy nanocircuit control.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [81] [On the contraction rate of the posterior distribution for nonlinear PDE parameter identification](https://arxiv.org/abs/2601.17805)
*Yuxin Fan,Bangti Jin*

Main category: math.ST

TL;DR: Bayesian parameter estimation for PDEs using Gaussian process priors with variational approximation, establishing contraction rates for low-regularity parameters without requiring truth to be in RKHS.


<details>
  <summary>Details</summary>
Motivation: To develop Bayesian procedures for parameter estimation in PDEs that can handle low-regularity parameters, relaxing the restrictive assumption that the true parameter must lie in the reproducing kernel Hilbert space of the Gaussian process prior.

Method: Uses Gaussian process priors with variational approximations, employs delicate approximation arguments to balance error sources, and applies the theory to three nonlinear inverse problems for PDEs.

Result: Establishes contraction rates for both the posterior distribution and its variational approximation in the low-regularity parameter regime, successfully relaxing the RKHS membership requirement.

Conclusion: The study provides a more flexible Bayesian framework for PDE parameter estimation that accommodates low-regularity parameters, with theoretical guarantees for contraction rates even when the truth lies outside the prior's RKHS.

Abstract: In this work, we investigate the estimation of a parameter $f$ in PDEs using Bayesian procedures, and focus on posterior distributions constructed using Gaussian process priors, and its variational approximation. We establish contraction rates for the posterior distribution and the variational approximation in the regime of low-regularity parameters. The main novelty of the study lies in relaxing the condition that the ground truth parameter must lie in the reproducing kernel Hilbert space of the Gaussian process prior, which is commonly imposed in existing studies on posterior contraction rate analysis [14,40,44]. The analysis relies on a delicate approximation argument that suitably balances various error sources. We illustrate the general theory on three nonlinear inverse problems for PDEs.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [82] [The Compounded BSDE method: A fully-forward method for option pricing and optimal stopping problems in finance](https://arxiv.org/abs/2601.18634)
*Zhipeng Huang,Cornelis W. Oosterlee*

Main category: q-fin.CP

TL;DR: A forward deep learning method using compound BSDEs for financial math problems like optimal stopping and Bermudan options, with proven convergence and high-dimensional efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving complex financial mathematics problems, particularly optimal stopping problems like Bermudan option pricing, which are challenging in high dimensions and require backward-looking formulations.

Method: Compound BSDE method: Reformulates option pricing as a system of backward stochastic differential equations, extends the classical deep BSDE method to handle compound BSDEs, uses deep learning for forward computation, and includes an a posteriori error estimation framework.

Result: Establishes convergence properties of the algorithm, derives rigorous error estimates, and demonstrates through numerical experiments that the method is accurate and computationally efficient for high-dimensional option pricing and optimal stopping problems.

Conclusion: The Compound BSDE method provides an effective forward deep learning approach for solving complex financial mathematics problems, offering both theoretical guarantees and practical computational advantages for high-dimensional applications.

Abstract: We propose the Compound BSDE method, a fully forward, deep-learning-based approach for solving a broad class of problems in financial mathematics, including optimal stopping. The method is based on a reformulation of option pricing problems in terms of a system of backward stochastic differential equations (BSDEs), which offers a new perspective on the numerical treatment of compound options and optimal stopping problems such as Bermudan option pricing. Building on the classical deep BSDE method for a single BSDE, we develop an algorithm for compound BSDEs and establish its convergence properties. In particular, we derive an \emph{a posteriori} error estimate for the proposed method. Numerical experiments demonstrate the accuracy and computational efficiency of the approach, and illustrate its effectiveness for high-dimensional option pricing and optimal stopping problems.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [83] [ChemNavigator: Agentic AI Discovery of Design Rules for Organic Photocatalysts](https://arxiv.org/abs/2601.17084)
*Iman Peivaste,Ahmed Makradi,Salim Belouettar*

Main category: physics.chem-ph

TL;DR: ChemNavigator is an AI system that autonomously discovers structure-property relationships for organic photocatalysts through hypothesis-driven exploration, identifying six statistically significant design rules that align with established chemical principles.


<details>
  <summary>Details</summary>
Motivation: The discovery of high-performance organic photocatalysts is limited by the vast chemical space and reliance on human intuition, creating a need for autonomous AI systems that can systematically explore molecular design.

Method: ChemNavigator integrates large language model reasoning with density functional tight binding calculations in a multi-agent architecture that mimics the scientific method: formulating hypotheses, designing experiments, executing calculations, and validating findings through statistical analysis.

Result: The system autonomously identified six statistically significant design rules governing frontier orbital energies from 200 molecules, including effects of ether linkages, carbonyl groups, extended conjugation, cyano groups, halogen substituents, and amine groups. These rules correspond to established organic electronic structure principles (resonance donation, inductive withdrawal, π-delocalization).

Conclusion: Agentic AI systems can autonomously derive interpretable, chemically grounded design principles, establishing a framework for AI-assisted materials discovery that complements rather than replaces chemical intuition.

Abstract: The discovery of high-performance organic photocatalysts for hydrogen evolution remains limited by the vastness of chemical space and the reliance on human intuition for molecular design. Here we present ChemNavigator, an agentic AI system that autonomously derives structure-property relationships through hypothesis-driven exploration of organic photocatalyst candidates. The system integrates large language model reasoning with density functional tight binding calculations in a multi-agent architecture that mirrors the scientific method: formulating hypotheses, designing experiments, executing calculations, and validating findings through rigorous statistical analysis. Through iterative discovery cycles encompassing 200 molecules, ChemNavigator autonomously identified six statistically significant design rules governing frontier orbital energies, including the effects of ether linkages, carbonyl groups, extended conjugation, cyano groups, halogen substituents, and amine groups. Importantly, these rules correspond to established principles of organic electronic structure (resonance donation, inductive withdrawal, $π$-delocalization), demonstrating that the system can independently derive chemical knowledge without explicit programming. Notably, autonomous agentic reasoning extracted these six validated rules from a molecular library where previous ML approaches identified only carbonyl effects. Furthermore, the quantified effect sizes provide a prioritized ranking for synthetic chemists, while feature interaction analysis revealed diminishing returns when combining strategies, challenging additive assumptions in molecular design. This work demonstrates that agentic AI systems can autonomously derive interpretable, chemically grounded design principles, establishing a framework for AI-assisted materials discovery that complements rather than replaces chemical intuition.

</details>


### [84] [cuGUGA: Operator-Direct Graphical Unitary Group Approach Accelerated with CUDA](https://arxiv.org/abs/2601.17729)
*Zihan Pengmei*

Main category: physics.chem-ph

TL;DR: cuGUGA is a GPU-accelerated graphical unitary group approach CI solver that provides significant speedups over CPU implementations for quantum chemistry calculations.


<details>
  <summary>Details</summary>
Motivation: To accelerate configuration interaction calculations using spin-adapted configuration state functions by leveraging GPU hardware, overcoming limitations of traditional CPU-based implementations.

Method: Uses dynamic-programming walk counts for constant-time CSF ranking/unranking, pretabulated segment factors for constant-time coupling coefficient evaluation, intermediate-weight formulation for two-electron contributions, and custom CUDA kernels for irregular DRT traversal with CUDA libraries for contractions.

Result: Achieves 10^{-11} Eh accuracy, matches CPU/GPU sigma-vectors to 10^{-14}, provides up to ~10x speedup on RTX 4090 for smaller active spaces, and delivers >2x speedup over PySCF's determinant backend and >4x over PySCF CSF backend.

Conclusion: cuGUGA successfully accelerates GUGA-CI calculations on GPUs with high accuracy, though speedup diminishes for larger active spaces due to FP64 GEMM limitations on consumer GPUs.

Abstract: We present cuGUGA, an operator-direct graphical unitary group approach (GUGA) configuration interaction (CI) solver in a spin-adapted configuration state function (CSF) basis. Dynamic-programming walk counts provide constant-time CSF ranking/unranking, and pretabulated segment factors enable constant-time evaluation of coupling coefficients. Two-electron contributions are organized through an intermediate-weight formulation that separates sparse generator enumeration from integral contraction and supports both dense and density-fitted/Cholesky backends. We further map the same primitives to GPUs by implementing the irregular DRT traversal and accumulation in custom CUDA kernels while delegating contractions to CUDA libraries. The implementation reproduces reference energies at the 10^{-11} Eh level and matches CPU/GPU sigma-vectors to 10^{-14}. On an RTX 4090, the GPU backend provides up to ~10x speedup over the CPU backend for smaller active spaces and multifold speedups on representative CASCI kernels. Speedup decreases as the active space grows because the workload becomes increasingly dominated by FP64 GEMM, which is not strongly accelerated on consumer GPUs. In addition, the cuGUGA CPU backend generally delivers >2x speedup over PySCF's determinant backend and >4x speedup over PySCF CSF backend.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [85] [Spin-redirection Berry phase with planar rays](https://arxiv.org/abs/2601.18624)
*Aymeric Braud,Renaud Gueroult*

Main category: physics.optics

TL;DR: Spin-redirection Berry phase can occur along planar rays when spin evolves, challenging the traditional assumption that nonplanar trajectories are required.


<details>
  <summary>Details</summary>
Motivation: To challenge the common assumption that spin-redirection Berry phase requires nonplanar ray trajectories, and to explore new mechanisms for spin-orbit interactions in electromagnetic waves.

Method: Analyze spin-redirection phase in moving unmagnetized plasma as a singular example, and demonstrate how finite transverse spin enables this effect along planar rays.

Result: Identified a new spin-redirection mechanism where Berry phase can arise along planar rays when spin evolves, providing additional degrees of freedom for controlling light.

Conclusion: The work reveals previously overlooked spin-redirection mechanisms, expands understanding of spin-orbit interactions, and provides new tools for controlling light through supplemental degrees of freedom.

Abstract: Geometric or Berry phases are fundamental manifestations that appear in many areas of physics. They arise from the geometry of the space describing the properties of multi-component wave fields. An important example for electromagnetic waves is the spin-redirection Berry phase associated with the evolution of the spin direction. Because this effect has traditionally been studied in isotropic media where the spin is aligned with the ray trajectory, it has become commonly assumed that this spin-redirection Berry phase requires nonplanar rays. Here we show that a spin-redirection phase can in fact arise along a planar ray if the spin evolves along the ray. We expose this effect through the singular example of a moving unmagnetized plasma, and demonstrate how this behavior can more generally arise from a finite transverse spin. In identifying this new spin-redirection mechanism our work not only provides the tools to discover additional manifestations of SOIs in nature, but also uncovers supplemental degrees of freedom to harness SOIs to control light.

</details>


### [86] [Residual neural-field ptychography for dose-efficient electron, X-ray, and optical nanoscopy](https://arxiv.org/abs/2601.17694)
*Qianhao Zhao,Zhixuan Hong,Ruihai Wang,Tianbo Wang,Lingzhi Jiang,Qiong Ma,Peng-Han Lu,Rafal E. Dunin-Borkowski,Andrew Maiden,Guoan Zheng*

Main category: physics.optics

TL;DR: Self-correcting residual neural fields enable dose-efficient ptychography across electron, X-ray, and optical wavelengths by learning corrections to physical priors rather than complete wavefields.


<details>
  <summary>Details</summary>
Motivation: Ptychography suffers from convergence instability and excessive data redundancy across scales (sub-angstrom to meter), requiring a more efficient framework that can work across different wavelengths while being dose-efficient.

Method: Complex-valued neural network architecture with holomorphic phasor activation (e^iωz) preserves phase-amplitude coupling. Reformulates reconstruction as residual learning where network learns corrections to physical priors. Embeds physical model as differentiable layer for end-to-end automatic differentiation with joint correction of experimental parameters.

Result: Achieves record-breaking lensless resolution of 244-nm linewidth with visible light. Extends to electron wavelengths to reveal synaptic connectivity in brain sections with superior performance over conventional approaches.

Conclusion: Provides a solution for high-throughput, dose-efficient nanoscopy across the electromagnetic spectrum, validated across conventional, near-field, coded, and Fourier ptychography.

Abstract: Ptychography spans from sub-angstrom to meter scales yet suffers from convergence instability and excessive data redundancy. Here we introduce self-correcting residual neural fields as a dose-efficient framework for electron, X-ray, and optical ptychography. Unlike approaches that split complex fields, our complex-valued architecture employs holomorphic phasor activation e^iωz to preserve intrinsic phase-amplitude coupling. We reformulate reconstruction as residual learning, where the network learns only corrections to physical priors rather than complete wavefields. By embedding the physical model as a differentiable layer within the network, we enable end-to-end automatic differentiation where experimental parameters are jointly corrected alongside the neural fields. We validate our scheme across conventional, near-field, coded, and Fourier ptychography and achieve record-breaking lensless resolution of 244-nm linewidth with visible light. Extending to electron wavelengths, we reveal synaptic connectivity in brain sections with superior performance over conventional approaches. Our framework provides a solution for high-throughput, dose-efficient nanoscopy across the electromagnetic spectrum.

</details>


### [87] [Data-Efficient Electromagnetic Surrogate Solver Through Dissipative Relaxation Transfer Learning](https://arxiv.org/abs/2601.18235)
*Sunghyun Nam,Chan Y. Park,Min Seok Jang*

Main category: physics.optics

TL;DR: DIRTL uses transfer learning with loss-regularized optimization to improve neural network surrogate solvers for electromagnetic simulations by first pretraining on lossy data to smooth resonant peaks, then fine-tuning on lossless data for stable adaptation to high-amplitude resonances.


<details>
  <summary>Details</summary>
Motivation: High-amplitude resonances in electromagnetic simulations create outlier samples with strongly localized field patterns that deviate from non-resonant cases, causing instability and degraded predictive performance in neural-network surrogate solvers.

Method: Dissipative relaxation transfer learning (DIRTL) first pretrains models on data generated with small fictitious material loss to broaden sharp resonant modes and suppress extreme field amplitudes. This smoothed response landscape helps learn global modal features, after which the pretrained model is fine-tuned on target lossless datasets containing true high-amplitude resonances.

Result: Applied to Fourier Neural Operator (FNO) and UNet architectures, DIRTL yields substantial improvements in prediction accuracy, including up to two-fold error reduction for FNO variant. The method exhibits robustness across diverse training conditions and supports strong multi-task performance.

Conclusion: DIRTL establishes a physically grounded, architecture-agnostic curriculum for enhancing reliability of machine-learning-based electromagnetic surrogate solvers by addressing the challenge of modeling resonant phenomena through transfer learning with loss-regularized optimization.

Abstract: In neural-network surrogate solvers for electromagnetic simulations, accurately modeling resonant phenomena remains a central challenge. High-amplitude resonances generate strongly localized field patterns that appear as outlier samples, deviating significantly from the general distribution of non-resonant cases, leading to instability and degraded predictive performance. To address this, we introduce dissipative relaxation transfer learning (DIRTL), a data-efficient training framework that integrates transfer learning with loss-regularized optimization principles from high-Q photonics. DIRTL first pretrains the model on data generated with a small fictitious material loss, which broadens sharp resonant modes and suppresses extreme field amplitudes. This smoothing of the response landscape enables the model to learn global modal features more effectively. The pretrained model is subsequently fine-tuned on the target lossless dataset containing the true high-amplitude resonances, allowing stable adaptation based on the pretrained information. Applied to both the Fourier Neural Operator (FNO) and UNet architectures, DIRTL yields substantial improvements in prediction accuracy, including up to a two-fold error reduction for the FNO variant. Furthermore, DIRTL exhibits robustness across diverse training conditions and supports strong multi-task performance, underscoring the generalizability and flexibility of the pretrained core. Altogether, these results establish DIRTL as a physically grounded and architecture-agnostic curriculum for enhancing the reliability of machine-learning-based electromagnetic surrogate solvers.

</details>


### [88] [An exploration of lateral optical forces from a triangular periodic motif](https://arxiv.org/abs/2601.18550)
*Bo Gao,Henkjan Gersen,Simon Hanna*

Main category: physics.optics

TL;DR: Computational study reveals Fano-resonance behavior in optical forces of asymmetric dielectric nanostructures, with stable zones and switching bands controlled by structural geometry.


<details>
  <summary>Details</summary>
Motivation: To understand how structural geometry influences optical forces through resonant light-matter interactions, particularly in asymmetric dielectric nanostructures, and to provide guidance for designing optically-driven systems with controlled force responses.

Method: Computational investigation of isosceles triangular nanostructures under plane wave illumination, using parameter-space analysis to identify stable zones and switching bands, combined with eigenfrequency analysis to study resonant behavior.

Result: Two distinct types of optical force response observed: stable zones with consistent forces and switching bands where forces change abruptly. Force spectra show asymmetric lineshapes indicating Fano-resonance behavior, confirmed by eigenfrequency analysis showing interference between discrete eigenmodes and continuum states, with Q-factors correlating with transition sharpness.

Conclusion: Structural geometry significantly influences optical forces through resonant effects, with Fano-type interference playing a key role. These insights enable design of optically-driven systems with controlled force responses by engineering structural parameters to achieve desired stable or switching behaviors.

Abstract: This computational study investigates lateral optical forces in asymmetric dielectric nanostructures, focusing on their connection to resonant light-matter interactions. We examine isosceles triangular motifs that exhibit two distinct types of optical force response under plane wave illumination. Through parameter-space analysis, we identify stable zones where optical forces remain consistent and switching bands where forces change abruptly as parameters are altered. The observed force spectra show characteristic asymmetric lineshapes, suggesting Fano-resonance behavior. Eigenfrequency analysis confirms these effects arise from interference between discrete eigenmodes and continuum propagation states, with the eigenmode Q-factors correlating with transition sharpness. These findings provide insights into how structural geometry influences optical forces through resonant effects, offering guidance for designing optically-driven systems where controlled optical force responses are desired.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [89] [Recovering Riemannian Geometry from Diffusion](https://arxiv.org/abs/2601.17166)
*Amandip Sangha*

Main category: math.DG

TL;DR: Intrinsic reconstruction of Riemannian geometry from diffusion semigroups without assuming prior metric structure.


<details>
  <summary>Details</summary>
Motivation: To establish a foundation where geometric structure emerges intrinsically from diffusion processes rather than being assumed a priori, providing an information-theoretic perspective on differential geometry.

Method: Start from a symmetric, strongly local diffusion semigroup and its associated diffusion calculus. Use the carre du champ operator to recover Riemannian metric, iterated carre du champ to encode curvature, and symmetry to fix Levi-Civita connection and reference measure.

Result: The diffusion semigroup uniquely determines the full weighted Riemannian structure up to isometry. Carre du champ yields unique smooth Riemannian metric, iterated version encodes curvature, and symmetry fixes connection and measure.

Conclusion: Geometric structure can be reconstructed intrinsically from diffusion behavior without prior metric assumptions, establishing diffusion semigroups as fundamental for Riemannian geometry from an information-theoretic viewpoint.

Abstract: We present an intrinsic reconstruction of Riemannian geometry from a symmetric, strongly local diffusion semigroup. Starting from a diffusion operator and its associated first- and second-order diffusion calculus, we recover the full weighted Riemannian structure of the underlying manifold. In particular, we show that the carre du champ determines a unique smooth Riemannian metric, that the iterated carre du champ encodes curvature, and that the symmetry of the diffusion fixes the Levi-Civita connection and reference measure. As a consequence, the diffusion semigroup determines the global Riemannian manifold uniquely up to isometry. The results provide an information-theoretic perspective on differential geometry in which geometric structure emerges from the intrinsic behavior of diffusion, without assuming any prior metric or coordinate description.

</details>


### [90] [Logarithmic Sobolev inequality in manifolds with nonnegative curvature via the ABP method](https://arxiv.org/abs/2601.17748)
*Lingen Lu*

Main category: math.DG

TL;DR: Using Brendle's ABP method, the paper establishes optimal L^p-logarithmic-Sobolev inequalities on manifolds with nonnegative Ricci curvature and sharp L^2-logarithmic-Sobolev inequalities for submanifolds in manifolds with nonnegative sectional curvature.


<details>
  <summary>Details</summary>
Motivation: To extend logarithmic-Sobolev inequalities to Riemannian manifolds with curvature conditions, establishing sharp constants that depend on geometric properties like asymptotic volume ratio.

Method: Employ Brendle's ABP (Alexandrov-Bakelman-Pucci) method to prove optimal logarithmic-Sobolev inequalities on manifolds with curvature conditions.

Result: Proves optimal L^p-logarithmic-Sobolev inequality on manifolds with nonnegative Ricci curvature, and sharp L^2-logarithmic-Sobolev inequality for submanifolds in manifolds with nonnegative sectional curvature.

Conclusion: The ABP method successfully establishes sharp logarithmic-Sobolev inequalities on curved manifolds, with constants depending on the asymptotic volume ratio of the ambient manifold.

Abstract: In this paper, we employ the ABP method developed by Brendle to establish the optimal $L^p$-logarithmic-Sobolev inequality on manifolds with nonnegative Ricci curvature, as well as a sharp $L^2$-logarithmic-Sobolev inequality for submanifolds in manifolds with nonnegative sectional curvature. The sharp constants in both inequalities depend on the asymptotic volume ratio of the ambient manifold.

</details>


### [91] [Area-minimizing capillary cones](https://arxiv.org/abs/2601.18794)
*Benjy Firester,Raphael Tsiamis,Yipeng Wang*

Main category: math.DG

TL;DR: Non-flat minimal capillary cones with bi-orthogonal symmetry constructed for all dimensions and contact angles, interpolating between singular solutions and free-boundary cones.


<details>
  <summary>Details</summary>
Motivation: To understand singularities in capillary hypersurfaces and complete the regularity theory for contact angles near π/2, while connecting capillary hypersurfaces to the one-phase problem.

Method: Construct cones by solving nonlinear free boundary equations parametrized by contact angle, proving existence/uniqueness via monotonicity properties of solutions.

Result: Cones are minimizing in ambient dimension ≥8 for appropriate contact angles, showing singularities can occur in codimension 7, and producing new singular minimizing free boundaries for Alt-Caffarelli functional.

Conclusion: Completes capillary regularity theory near π/2 contact angle, demonstrates connection between capillary hypersurfaces and one-phase problem, and provides new examples of singular minimizing free boundaries.

Abstract: We construct non-flat minimal capillary cones with bi-orthogonal symmetry groups for any dimension and contact angle. These cones interpolate between rescalings of a singular solution to the one-phase problem and the free-boundary cone obtained by halving a Lawson cone along a hyperplane of symmetry. The existence and uniqueness of such cones is proved by solving a nonlinear free boundary equation parametrized by the contact angle and obtaining monotonicity properties for the solutions. The constructed cones are minimizing in ambient dimension $8$ or higher, for appropriate contact angles, demonstrating that the regularity theory for minimizing capillary hypersurfaces can have singularities in codimension $7$ and completing the capillary regularity theory for contact angles near $π/2$. We further develop the connection between capillary hypersurfaces and solutions of the one-phase problem, consequently producing new examples of singular minimizing free boundaries for the Alt-Caffarelli functional.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [92] [The dimensionality of the Hopfield model](https://arxiv.org/abs/2601.17427)
*Cristopher Erazo,Santiago Acevedo,Alessandro Ingrosso*

Main category: cond-mat.dis-nn

TL;DR: The paper uses Binary Intrinsic Dimension (BID) to analyze the Hopfield model, showing BID can characterize phases/transitions, is robust against finite-size effects, and reveals connections between state-space geometry and spin order parameters.


<details>
  <summary>Details</summary>
Motivation: To apply a geometrical measure (Binary Intrinsic Dimension) designed for binary data to analyze the Hopfield model, which is important in statistical mechanics, machine learning, and neuroscience. The goal is to overcome limitations of traditional order parameters like the spin-glass order parameter (q) that suffer from finite-size effects.

Method: Using Binary Intrinsic Dimension (BID), a geometrical measure specifically designed for binary data, to analyze the Hopfield model. The BID is applied to characterize different phases and transitions in the system, and its relationship with the overlap distribution is established.

Result: BID scales linearly with system size in retrieval and paramagnetic phases (where spin correlations are small), but exhibits sublinear scaling throughout the spin-glass phase (highlighting correlated structure). A direct relationship between BID and overlap distribution is established, revealing novel connections between state-space geometry and standard spin order parameters.

Conclusion: BID provides a robust geometrical approach for analyzing the Hopfield model that overcomes finite-size limitations of traditional order parameters, offers clear phase characterization, and establishes fundamental connections between state-space geometry and physical order parameters.

Abstract: We use the Binary Intrinsic Dimension (BID), a geometrical measure designed for binary data, to analyze the Hopfield model, a paradigmatic spin system from statistical mechanics, machine learning and neuroscience. The BID allows us to characterize the phases and transitions of this system, and moreover it is robust against finite-size effects that interfere with the correct numerical estimation of the spin-glass order parameter ($q$). We observe that the BID scales linearly with system size in the retrieval and paramagnetic phases, where the correlations between spins are small, and exhibits sublinear scaling in the whole spin-glass phase, highlighting its correlated structure. Furthermore, we establish a direct relationship between the BID and the overlap distribution, unveiling a novel connection between the geometry of the state-space and standard spin order parameters.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [93] [Low-Bit Quantization of Bandlimited Graph Signals via Iterative Methods](https://arxiv.org/abs/2601.18782)
*Felix Krahmer,He Lyu,Rayan Saab,Jinna Qian,Anna Veselovska,Rongrong Wang*

Main category: eess.SP

TL;DR: The paper proposes iterative noise-shaping algorithms for quantizing bandlimited graph signals using low-bit representations, with theoretical guarantees for random sampling methods.


<details>
  <summary>Details</summary>
Motivation: To develop efficient quantization methods for real-valued bandlimited signals on graphs that can work with low-bit representations while maintaining signal fidelity.

Method: Proposes iterative noise-shaping quantization algorithms that leverage graph Laplacian spectral properties and graph incoherence. Includes sampling approaches with and without vertex replacement.

Result: Provides theoretical guarantees for random sampling method. Extensive numerical experiments on synthetic and real-world graphs demonstrate efficiency and robustness of proposed schemes.

Conclusion: The proposed quantization algorithms effectively handle low-bit representations of bandlimited graph signals with theoretical support and practical validation through experiments.

Abstract: We study the quantization of real-valued bandlimited signals on graphs, focusing on low-bit representations. We propose iterative noise-shaping algorithms for quantization, including sampling approaches with and without vertex replacement. The methods leverage the spectral properties of the graph Laplacian and exploit graph incoherence to achieve high-fidelity approximations. Theoretical guarantees are provided for the random sampling method, and extensive numerical experiments on synthetic and real-world graphs illustrate the efficiency and robustness of the proposed schemes.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [94] [A note on continuous data assimilation for stochastic convective Brinkman-Forchheimer equations in 2D and 3D](https://arxiv.org/abs/2601.17650)
*Kush Kinra*

Main category: math.PR

TL;DR: The paper introduces and analyzes a continuous data assimilation algorithm for stochastic convective Brinkman-Forchheimer equations with Gaussian noise, showing convergence conditions and improved results compared to Navier-Stokes equations.


<details>
  <summary>Details</summary>
Motivation: Continuous data assimilation methods like nudging work well in deterministic settings, but need extension to stochastic fluid dynamics models like CBFEs which describe flows beyond Darcy's law validity with moderate porosity.

Method: Develops a nudging algorithm for CDA applied to 2D/3D stochastic CBFEs with additive or multiplicative Gaussian noise, deriving sufficient conditions on nudging parameter and observation spatial resolution.

Result: Proves mean-square convergence of assimilated solution to true stochastic flow, plus pathwise convergence for additive noise. Shows nonlinear damping enables CDA in 3D and yields improved convergence in 2D compared to Navier-Stokes.

Conclusion: The nonlinear damping in CBFEs not only makes CDA feasible in three dimensions but also provides better convergence results than classical Navier-Stokes equations, establishing effective data assimilation for stochastic porous media flows.

Abstract: Continuous data assimilation (CDA) methods, such as the nudging algorithm introduced by Azouani, Olson, and Titi (AOT), have proven to be highly effective in deterministic settings for asymptotically synchronizing approximate solutions with observed dynamics. In this note, we introduce and analyze an algorithm for CDA for the two- and three-dimensional stochastic convective Brinkman-Forchheimer equations (CBFEs) driven by either additive or multiplicative Gaussian noise. The model is believed to provide an accurate description when the flow velocity exceeds the regime of validity of Darcy's law and the porosity remains moderately large. We derive sufficient conditions on the nudging parameter and the spatial resolution of observations that ensure convergence of the assimilated solution to the true stochastic flow. We demonstrate convergence in the mean-square sense, and additionally establish pathwise convergence in the presence of additive noise. The CBFEs, also known as Navier-Stokes equations with damping, exhibit enhanced stability properties due to the presence of nonlinear damping term. In particular, we show that nonlinear damping not only enables the implementation of CDA in three dimensions but also yields improved convergence results in two dimensions when compared to the classical Navier-Stokes equations.

</details>


### [95] [Validity of the stochastic Ginzburg-Landau approximation in higher space dimensions -A Wiener algebra approach-](https://arxiv.org/abs/2601.18406)
*Anna Logioti,Guido Schneider*

Main category: math.PR

TL;DR: The paper analyzes a stochastic anisotropic Swift-Hohenberg model near its first instability, deriving a stochastic Ginzburg-Landau amplitude equation and proving its validity for pattern formation with additive noise.


<details>
  <summary>Details</summary>
Motivation: To understand pattern formation in stochastic anisotropic systems near bifurcation points, particularly how additive noise affects the dynamics and whether amplitude equations provide valid approximations.

Method: Uses a multiple scaling ansatz to derive a stochastic d-dimensional Ginzburg-Landau equation from the anisotropic Swift-Hohenberg model. Proves validity of the approximation on natural time scales for periodic domains of size O(1/ε) with noise conditions requiring Fourier coefficients to be in ℓ¹.

Result: Proves the validity of the stochastic Ginzburg-Landau amplitude equation approximation for the Swift-Hohenberg model with additive noise. Improves existing results by allowing larger stable parts of the noise.

Conclusion: The stochastic Ginzburg-Landau equation provides a valid amplitude approximation for the anisotropic Swift-Hohenberg model near instability with additive noise, with broader applicability than previous results.

Abstract: We consider an anisotropic $d$-dimensional Swift-Hohenberg model $ \mathcal{O}(\varepsilon^2) $-close to the first instability, where $ 0 < \varepsilon \ll 1 $ is a small perturbation parameter. This model for pattern formation is perturbed with additive noise in time and space. By a multiple scaling ansatz we derive a stochastic $ d $-dimensional Ginzburg-Landau equation for the approximate description of the bifurcating solutions. We prove the validity of the approximation by this amplitude equation on its natural time scale in case of $ d $-dimensional periodic domains of length $ \mathcal{O}(1/\varepsilon) $ for the Swift-Hohenberg model under suitable conditions on the additive noise. In detail, we prove the validity of this approximation for noise whose set of Fourier coefficients with respect to $ x $ is in $ \ell^1 $ for fixed $ t \geq 0 $. Moreover, we improve existing approximation results in the sense that the stable part of the noise can be larger.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [96] [Multi-Criteria Inverse Robustness in Radiotherapy Planning Using Semidefinite Programming](https://arxiv.org/abs/2601.17750)
*Jan Schröeder,Yair Censor,Philipp Süss,Karl-Heinz Küfer*

Main category: math.OC

TL;DR: This paper presents a multi-criteria optimization approach for radiotherapy planning that handles uncertainty through interval matrices and inverse robustness, solving the resulting QCQP via SDP relaxation.


<details>
  <summary>Details</summary>
Motivation: Radiotherapy planning involves multi-criteria optimization under uncertainty, requiring decision makers to balance treatment objectives against robustness to uncertainty. There's a need for quantitative approaches that combine theoretical models with practical implementation.

Method: Models uncertainty using interval matrices for dose-influence matrix, introduces inverse robustness as additional objective to maximize uncertainty set volume, uses multi-criteria optimization to handle uncertainty while maintaining other objectives, and solves QCQP by relaxing to convex SDP then reconstructing optimal QCQP solutions.

Result: The paper demonstrates a practical framework for radiotherapy planning that quantitatively balances treatment objectives with robustness to uncertainty, with computational solution via SDP relaxation.

Conclusion: The proposed approach provides a quantitative method for decision makers to balance treatment objectives and robustness in radiotherapy planning under uncertainty, with practical computational implementation.

Abstract: Radiotherapy planning naturally leads to a multi-criteria optimization problem which is subject to different sources of uncertainty. In order to find the desired treatment plan, a decision maker must balance these objectives as well as the level of robustness towards uncertainty against each other. This paper showcases a quantitative approach to do so, which combines the theoretical model with the ability to deal with practical challenges. To this end, the uncertainty, which can be expressed via the so-called dose-influence matrix, is modelled using interval matrices. We use inverse robustness to introduce an additional objective, which aims to maximize the volume of the uncertainty set. A multi-criteria approach allows to handle the uncertainty while keeping appropriate values of the other objective functions. We solve the resulting quadratically constrained quadratic optimization problem (QCQP) by first relaxing it to a convex semidefinite problem (SDP) and then reconstructing optimal solutions of the QCQP from solutions of the SDP.

</details>


### [97] [A Unique Inverse Decomposition of Positive Definite Matrices under Linear Constraints](https://arxiv.org/abs/2601.18662)
*Yan Dolinsky,Or Zuk*

Main category: math.OC

TL;DR: A unique nonlinear decomposition of positive definite matrices into inverse and symmetric components with subspace constraints, admitting variational characterization and efficient algorithms.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical framework for decomposing positive definite matrices that has computational advantages and arises naturally in applications like mathematical finance.

Method: Proposes a nonlinear decomposition where a positive definite matrix is split into: 1) the inverse of another positive definite matrix, and 2) a symmetric matrix in a prescribed linear subspace. Shows uniqueness under nondegeneracy conditions, provides variational characterization via strictly convex log-determinant optimization, and develops Newton-type algorithms with convergence guarantees.

Result: Proves existence and uniqueness of the decomposition under sharp nondegeneracy conditions, establishes stability properties, develops efficient Newton-type algorithms with provable convergence, and demonstrates natural occurrence in exponential utility maximization problems.

Conclusion: The proposed decomposition provides a mathematically rigorous framework with computational efficiency, stability guarantees, and practical relevance to financial applications, particularly in exponential utility maximization problems.

Abstract: We study a nonlinear decomposition of a positive definite matrix into two components: the inverse of another positive definite matrix and a symmetric matrix constrained to lie in a prescribed linear subspace. Equivalently, the inverse component is required to belong to the orthogonal complement of that subspace with respect to the trace inner product. Under a sharp nondegeneracy condition on the subspace, we show that every positive definite matrix admits a \emph{unique} decomposition of this form.
  This decomposition admits a variational characterization as the unique minimizer of a strictly convex log-determinant optimization problem, which in turn yields a natural dual formulation that can be efficiently exploited computationally. We derive several properties, including the stability of the decomposition.
  We further develop feasibility-preserving Newton-type algorithms with provable convergence guarantees and analyze their per-iteration complexity in terms of algebraic properties of the decomposed matrix and the underlying subspace. Finally, we show that the proposed decomposition arises naturally in exponential utility maximization, a central problem in mathematical finance.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [98] [A fast and accurate method for simulating Bragg atom interferometers](https://arxiv.org/abs/2601.17236)
*Jack Roth,Andrew Christensen,Madeline Bernstein,Yuno Iwasaki,Holger Mueller*

Main category: physics.atom-ph

TL;DR: The paper presents a computational method for simulating Bragg atom interferometers by separating the 1D time-dependent Schrödinger equation into systems of ordinary differential equations, enabling use of adaptive Runge-Kutta methods with lookup table optimizations.


<details>
  <summary>Details</summary>
Motivation: Bragg atom interferometers are important for precision measurements but Bragg diffraction introduces systematic phase effects that are difficult to characterize. Current computational methods for simulating these systems face challenges in efficiency and accuracy.

Method: The authors show that for Bragg diffraction, the 1D time-dependent Schrödinger equation can be separated into several systems of ordinary differential equations. This allows the use of adaptive step size Runge-Kutta methods. They also introduce a lookup table approach for computational speed-ups.

Result: The method's convergence is compared to split-step and Crank-Nicolson methods. The separation approach with adaptive Runge-Kutta and lookup table optimization provides computational advantages for simulating Bragg atom interferometers.

Conclusion: The proposed computational method offers an efficient way to simulate Bragg atom interferometers, addressing the challenge of characterizing systematic phase effects introduced by Bragg diffraction, which is crucial for improving interferometer sensitivity.

Abstract: Atom interferometers are used in a variety of applications, from measuring gravity and gravity gradients in the field to performing tests of fundamental physics in the lab. One method of increasing interferometer sensitivity is to produce a larger momentum difference between interferometer arms through the use of large momentum transfer methods, such as Bragg diffraction. However, Bragg diffraction introduces systematic effects in the accumulated interferometer phase that are challenging to characterize.
  A Bragg atom interferometer is described by the one-dimensional time-dependent Schrödinger equation (1D-TDSE). In this paper we show that for the case of Bragg diffraction the 1D-TDSE partial differential equation can be separated into several systems of ordinary differential equations, allowing for the use of adaptive step size Runge-Kutta methods. We compare the convergence of this method to the split-step and Crank-Nicolson methods, and present a method for further computational speed-ups using a lookup table.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [99] [From microscopic social force models to macroscopic continuum models for pedestrian flow](https://arxiv.org/abs/2601.17304)
*Liangze Yang,Hui Yu,Jie Du*

Main category: nlin.AO

TL;DR: This paper bridges microscopic and macroscopic pedestrian flow models by deriving continuum models from the social force model through kinetic equations.


<details>
  <summary>Details</summary>
Motivation: Pedestrian flow involves complex multi-agent interactions. While microscopic models capture individual forces and macroscopic models provide analytical insights and efficient simulations, the relationship between different scales remains unexplored.

Method: Starting from the microscopic social force model with reactive optimal route choice, the authors derive kinetic equations at the mesoscopic level, then derive several continuum models at the macroscopic level by varying interaction forces in different scenarios.

Result: Numerical examples evaluate behaviors of both the original social force model and the derived continuum models, demonstrating the connection between scales.

Conclusion: The study successfully bridges microscopic and macroscopic pedestrian flow modeling by deriving continuum models from the social force model, providing a multi-scale framework for pedestrian dynamics analysis.

Abstract: The pedestrian flow is one of the most complex systems, involving large populations of interacting agents. Models at microscopic and macroscopic scales offer different advantages for studying related problems. In general, microscopic models can describe interaction forces at the individual level. Macroscopic models, on the other hand, provide analytical insights into global interactions and long-term overall dynamics, along with efficient numerical simulations and predictions. However, the relationship between models at different scales has rarely been explored. In this study, based on the original microscopic social force model with a reactive optimal route choice strategy, we first derive kinetic equations at the mesoscopic level. By varying the interaction force in different scenarios, we then derive several continuum models at the macroscopic level. Finally, numerical examples are given to evaluate the behaviors of the social force model and our continuum models.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [100] [Chemotaxis-inspired PDE models of airborne infectious disease transmission: epidemiologically-motivated mathematical and numerical analyses](https://arxiv.org/abs/2601.18703)
*Alex Viguerie,Malú Grave,Alvaro L. G. A. Coutinho,Alessandro Veneziani,Thomas J. R. Hughes*

Main category: q-bio.PE

TL;DR: This paper extends PDE models for infectious diseases by adding chemotaxis terms to better capture human mobility patterns, deriving a spatially-aware reproduction number, and demonstrating improved performance over pure reaction-diffusion models using COVID-19 data from Lombardy and Georgia.


<details>
  <summary>Details</summary>
Motivation: Traditional reaction-diffusion PDE models for infectious diseases are limited in capturing complex human mobility patterns, particularly for airborne diseases. Recent work suggests adding chemotaxis terms to better model infection spread from low-to-high susceptible concentrations, but these models lack epidemiological interpretability for policymakers.

Method: The authors extend PDE models with chemotaxis terms and provide epidemiological interpretation. They derive a spatially-aware basic reproduction number that accounts for population density heterogeneity. They introduce numerical stabilization schemes for solving the model and conduct simulation studies using COVID-19 data from Lombardy, Italy and Georgia, USA.

Result: The extended model with chemotaxis terms better captures spatiotemporal dynamics observed in real-world COVID-19 data compared to pure reaction-diffusion models. The spatially-aware reproduction number provides policymakers with interpretable metrics that account for spatial heterogeneity in population density.

Conclusion: Incorporating chemotaxis terms into PDE models for infectious diseases improves their ability to capture complex human mobility patterns. The derived spatially-aware reproduction number and numerical stabilization schemes make these models more useful for policymakers, as demonstrated by better performance in capturing COVID-19 dynamics in Lombardy and Georgia.

Abstract: Partial differential equation (PDE) models for infectious diseases, while less common than their ordinary differential equation (ODE) counterparts, have found successful applications for many years. Such models are typically of reaction-diffusion type, and model spatial propagation as a diffusive process. However, given the complex nature of human mobility, such models are limited in their ability to describe airborne infectious diseases in human populations. Recent work has advocated for the inclusion of an additional chemotaxis-type term as an alternative; spatial propagation of infection fronts is assumed additionally to flow from low-to-high concentrations of susceptible populations. The present work extends the study of such models by providing an epidemiologically interpretable analysis, directly connecting model behavior to information readily available to policymakers. In particular, we derive a spatially-aware basic reproduction number, which accounts for spatial heterogeneity in population density. Furthermore, we discuss several important aspects concerning the numerical solution of the model, including the introduction of a stabilization scheme. Finally, we perform a series of simulation studies in the Italian region of Lombardy (severely affected by the COVID-19 outbreak in 2020) and in the US state of Georgia, in which we demonstrate the model's potential to better capture important spatiotemporal dynamics observed in real-world data compared to pure reaction-diffusion models.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [101] [Recursive Manifold Coherence: A Geometric Framework for Deadtime Recovery in Distributed Trigger Systems](https://arxiv.org/abs/2601.17043)
*Thammarat Yawisit,Pittaya Pannil*

Main category: physics.ins-det

TL;DR: RMC is a geometric framework that replaces binary coincidence triggers with continuous state estimation to handle detector deadtime and signal pile-up in neutrino observatories.


<details>
  <summary>Details</summary>
Motivation: Conventional coincidence-based triggers in large-scale neutrino observatories suffer from systematic inefficiencies due to detector deadtime and signal pile-up, causing loss of correlated signal information during non-live intervals.

Method: Recursive Manifold Coherence (RMC) reformulates distributed trigger logic as continuous state estimation in a low-dimensional information space using correlated charge and timing observables. It employs recursive update rules that propagate coherence states across sensor nodes instead of applying hard vetoes during deadtime.

Result: Simulation studies show RMC successfully recovers event-level coherence for high-multiplicity topologies even when direct coincidence chains are broken, achieving superior robustness against data fragmentation compared to standard binary logic.

Conclusion: RMC provides a flexible, detector-agnostic framework for deadtime-aware analysis and triggering strategies in future distributed detector systems, treating detector response as a smooth manifold rather than discrete hits.

Abstract: Large-scale neutrino observatories operate under unavoidable detector deadtime and signal pile-up, leading to systematic inefficiencies in conventional coincidence-based trigger systems. Such triggers typically rely on binary temporal windows and assume continuous sensor availability, causing partial or complete loss of correlated signal information during non-live intervals. We introduce Recursive Manifold Coherence (RMC), a geometric framework that reformulates distributed trigger logic as a continuous state estimation problem in a low-dimensional information space defined by correlated charge and timing observables. Instead of applying hard vetoes during deadtime, the proposed method employs a recursive update rule that propagates a coherence state across sensor nodes, allowing partially obscured signals to be retained and evaluated consistently. Using simulation studies representative of large optical detector arrays, we demonstrate that RMC successfully recovers event-level coherence for high-multiplicity topologies even when direct coincidence chains are broken. By treating the detector response as a smooth manifold rather than discrete hits, the framework achieves superior robustness against data fragmentation compared to standard binary logic. The framework is detector-agnostic and compatible with software-defined trigger pipelines, providing a flexible foundation for deadtime-aware analysis and triggering strategies in future distributed detector systems.

</details>


### [102] [Mitigating Deadtime in Distributed Optical Arrays: A Liveness-Aware Trigger Approach for High-Energy Neutrino Detection](https://arxiv.org/abs/2601.18114)
*Thammarat Yawisit,Pittaya Pannil*

Main category: physics.ins-det

TL;DR: A liveness-aware trigger architecture for neutrino observatories that maintains detection efficiency during sensor deadtime by using continuous observables instead of conventional coincidence logic.


<details>
  <summary>Details</summary>
Motivation: Large-scale neutrino detectors suffer from unavoidable deadtime due to photomultiplier saturation and readout constraints. Conventional coincidence-based triggers assume continuous sensor availability, leading to systematic efficiency loss when channels become temporarily unavailable.

Method: Design of a liveness-aware trigger architecture using recursive Infinite Impulse Response (IIR) update law implemented as FPGA pipeline. Creates continuity-preserving effective observable at each sensor node that decays smoothly during non-liveness intervals while retaining phase/amplitude information for network-level coherence estimation.

Result: The trigger sustains high event recovery efficiency under elevated deadtime probability where conventional logic degrades substantially. Provides up to two-order-of-magnitude improvement in effective signal-to-noise ratio, enabling robust detection under strong saturation and non-ideal conditions.

Conclusion: The method provides a robust foundation for next-generation firmware-level trigger strategies in large-scale, noise-dominated detector systems by enabling graceful degradation under partial channel non-liveness.

Abstract: Large-scale neutrino observatories operate under unavoidable detector deadtime arising from photomultiplier saturation, digitizer limits, and front-end readout constraints. Conventional coincidence-based trigger logic implicitly assumes continuous sensor availability and therefore suffers systematic efficiency loss when channels become temporarily non-live. This work presents the design of a liveness-aware trigger architecture targeting low-latency FPGA deployment in distributed optical arrays. We introduce a recursive Infinite Impulse Response (IIR) update law implemented as a fully synthesizable pipeline that constructs a continuity-preserving effective observable at each sensor node. Rather than collapsing during non-liveness intervals, the observable decays smoothly while retaining phase and amplitude information relevant for network-level coherence estimation. By explicitly separating continuous measurement construction from discrete trigger decision logic, the proposed architecture enables graceful degradation under partial channel non-liveness. Performance is evaluated using a hybrid validation framework that combines representative event topologies derived from IceCube Open Data with a hardware-accurate signal and noise model spanning a wide dynamic range. Simulation results demonstrate that the proposed trigger sustains high event recovery efficiency in regimes of elevated deadtime probability, where conventional coincidence logic degrades substantially. Furthermore, the continuity-preserving observable yields up to a two-order-of-magnitude improvement in effective signal-to-noise ratio, enabling robust detection under strong saturation and non-ideal operating conditions. This method provides a robust foundation for next-generation firmware-level trigger strategies in large-scale, noise-dominated detector systems.

</details>


### [103] [OptiGAN for Crystal Arrays: Physics-Informed Generative Modeling of Optical Photon Transport in PET Detector Arrays](https://arxiv.org/abs/2601.18780)
*Stephan Naunheim,Brandon Pardi,Guneet Mummaneni,Carlotta Trigila,Emilie Roncali*

Main category: physics.ins-det

TL;DR: OptiGAN: A physics-informed conditional GAN that replaces Monte Carlo simulations for optical photon transport in scintillator arrays, enabling efficient simulation of PET detector systems.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo simulations for optical photon transport are computationally prohibitive for large-scale optical systems like PET detector arrays, limiting practical use to single-crystal studies only.

Method: Enhanced conditional GAN (optiGAN) with Wasserstein-GAN framework, Fourier feature encoding, learnable latent mapping network, and physics-informed loss enforcing momentum conservation. Training data reduced 8-fold using symmetry exploitation.

Result: OptiGAN achieves sliced Wasserstein similarity within 3σ-agreement of Monte Carlo baseline across all test conditions. Successfully generalizes to full 3x3 BGO array, reproduces characteristic flood map patterns including photopeak clusters and inter-crystal scatter lines.

Conclusion: Physics-informed generative models can accurately simulate optical photon transport in segmented scintillator arrays. OptiGAN validates for PET detector development and establishes foundation for models generalizing across diverse array configurations.

Abstract: Monte Carlo simulations of optical photon transport are computationally prohibitive for large-scale optical systems including detector arrays and PET systems, restricting practical use to single-crystal studies. This work presents an enhanced conditional generative adversarial network (optiGAN) replacing optical simulations at the crystal array level, extending our single-crystal approach to a 3x3 BGO array. We enhance the Wasserstein-GAN framework with Fourier feature encoding, a learnable latent mapping network, and a physics-informed loss enforcing momentum conservation. Training data is reduced eight-fold by exploiting symmetry. Evaluation employs three studies: a full array evaluation testing generalization from the fundamental domain to the complete geometry, a high-resolution study probing out-of-distribution generalization to untrained positions, and a pencil beam $γ$-photon study assessing practical applicability for experimental detector characterization. Performance is benchmarked against GATE10/Geant4 ground truth, using intrinsic fluctuations between independent Monte Carlo runs as baseline. OptiGAN achieves sliced Wasserstein similarity within 3$σ$-agreement of the baseline across all conditions, demonstrating successful generalization to the full array. The model transitions from electron-emission training data to realistic $γ$-photon interactions, producing flood maps that reproduce characteristic patterns including photopeak clusters and inter-crystal scatter lines. This proof-of-concept demonstrates that physics-informed generative models can accurately simulate optical photon transport in segmented scintillator arrays. The reproduction of experimentally relevant flood map features validates optiGAN for PET detector development and establishes a foundation for models generalizing across diverse array configurations.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [104] [Structural and dynamic anomalous properties of TIP4P/2005 water at extreme pressures](https://arxiv.org/abs/2601.18318)
*José Martín-Roca,Alberto Zaragoza,Frédéric Caupin,Chantal Valeriani*

Main category: cond-mat.soft

TL;DR: Molecular dynamics simulations confirm experimental finding of minimum in water's structural relaxation time at high pressure, linking it to hydrogen bond network reorganization.


<details>
  <summary>Details</summary>
Motivation: Recent experiments showed water has a minimum in structural relaxation time τα with pressure at room temperature. The study aims to investigate this anomaly using molecular dynamics simulations to understand the microscopic origins.

Method: Used molecular dynamics simulations of TIP4P/2005 water model at extreme pressures (up to 2.7 GPa) and temperatures down to 220K. Computed dynamic properties (self-diffusion, shear/bulk viscosities, structural relaxation time) and structural properties (oxygen-oxygen radial distribution function, structure factor, translational order parameter).

Result: Found good agreement with experimental observations and confirmed existence of minimum in τα. Simulations revealed the anomaly is connected to sudden reorganization of hydrogen bond network induced by pressurization.

Conclusion: The study validates experimental findings through simulations and provides microscopic explanation: the minimum in structural relaxation time results from pressure-induced reorganization of water's hydrogen bond network.

Abstract: Water shows numerous thermodynamic, dynamic, and structural anomalies. Recent experiments [Eichler et al. Phys. Rev. Lett. 134, 134101 (2025)], based on measurements of shear and bulk viscosities of liquid water up to 1.6 GPa, have reported the existence of a minimum in the variation of the structural relaxation time τα with pressure at room temperature. Here we investigate this and related properties with molecular dynamics simulations of the TIP4P/2005 water model, performed at extreme pressures commensurate with the experiments. Specifically, we compute dynamic (self-diffusion, shear and bulk viscosities, and structural relaxation time) and structural (oxygen-oxygen radial distribution function and structure factor, translational order parameter) properties down to 220 K and up to 2.7 GPa. We find good agreement with the experimental observations, and confirm the existence of a minimum in τα . The microscopic information obtained from the simulations suggests that this anomaly is connected with the sudden reorganization of the hydrogen bond network induced by pressurization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [105] [Physics-Informed Uncertainty Enables Reliable AI-driven Design](https://arxiv.org/abs/2601.18638)
*Tingkai Xue,Chin Chun Ooi,Yang Jiang,Luu Trung Pham Duong,Pao-Hsiung Chiu,Weijiang Zhao,Nagarajan Raghavan,My Ha Dao*

Main category: cs.LG

TL;DR: Physics-informed uncertainty uses physical law violations as a cheap proxy for predictive uncertainty in surrogate models, improving inverse design success rates from <10% to >50% while reducing computational costs 10x for frequency-selective surfaces.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning surrogate models for inverse design lack uncertainty quantification, leading to poor optimization performance due to erroneous predictions in data-sparse regions. There's a need for effective uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems.

Method: Introduces Physics-Informed Uncertainty paradigm where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap proxy for predictive uncertainty. Integrates this into a multi-fidelity uncertainty-aware optimization workflow for designing complex frequency-selective surfaces in the 20-30 GHz range.

Result: Increased success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to using only a high-fidelity solver.

Conclusion: Physics-informed uncertainty is a viable alternative for quantifying uncertainty in surrogate models for physical systems, establishing a foundation for autonomous scientific discovery systems that can efficiently and robustly explore candidate designs.

Abstract: Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs.

</details>
