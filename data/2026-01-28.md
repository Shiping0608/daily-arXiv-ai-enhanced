<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 15]
- [math.AP](#math.AP) [Total: 24]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math-ph](#math-ph) [Total: 2]
- [math.PR](#math.PR) [Total: 4]
- [astro-ph.HE](#astro-ph.HE) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [math.CA](#math.CA) [Total: 2]
- [math.CV](#math.CV) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cs.LG](#cs.LG) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Hybrid Discretize-then-Project Reduced Order Model for Turbulent Flows on Collocated Grids with Data-Driven Closure](https://arxiv.org/abs/2601.18817)
*Nadim Rooholamin,Kabir Bakhshaei,Giovanni Stabile*

Main category: math.NA

TL;DR: Hybrid ROM framework for turbulent flows combines intrusive projection for velocity/pressure with data-driven neural networks for turbulent viscosity closure, achieving high accuracy on 3D lid-driven cavity case.


<details>
  <summary>Details</summary>
Motivation: Standard Galerkin projection fails to produce physically consistent turbulent viscosity fields in reduced-order modeling of turbulent incompressible flows, requiring a hybrid approach that maintains mathematical rigor while addressing turbulence closure challenges.

Method: Uses "discretize-then-project" consistent flux strategy for mass conservation and pressure-velocity coupling without stabilization. Velocity/pressure solved via intrusive projection, while turbulent viscosity reconstructed using non-intrusive data-driven closure with three neural network architectures (MLP, Transformers, LSTM) evaluated for temporal evolution modeling.

Result: LSTM-based closure shows superior performance in capturing transient dynamics, achieving 0.7% relative error for velocity and 4% for turbulent viscosity when validated against 3D Large Eddy Simulation of lid-driven cavity.

Conclusion: The hybrid framework successfully combines mathematical rigor of consistent flux formulation with deep learning adaptability for turbulence modeling, providing an effective solution for turbulent flow ROM that maintains physical consistency while leveraging data-driven approaches for challenging closure problems.

Abstract: This study presents a hybrid reduced-order modeling (ROM) framework for turbulent incompressible flows on collocated finite volume grids. The methodology employs the "discretize-then-project" consistent flux strategy, which ensures mass conservation and pressure-velocity coupling without requiring auxiliary stabilization like boundary control or pressure stabilization techniques. However, because standard Galerkin projection fails to yield physically consistent results for the turbulent viscosity field, a hybrid strategy is adopted: velocity and pressure are resolved via intrusive projection, while the turbulent viscosity is reconstructed using a non-intrusive data-driven closure. We evaluate three neural network architectures, Multilayer Perceptron (MLP), Transformers, and Long Short-Term Memory (LSTM), to model the temporal evolution of the viscosity coefficients. Validated against a 3D Large Eddy Simulation of a lid-driven cavity, the LSTM-based closure demonstrates superior performance in capturing transient dynamics, achieving relative errors of 0.7\% for velocity and 4\% for turbulent viscosity. The resulting framework effectively combines the mathematical rigor of the consistent flux formulation with the adaptability of deep learning for turbulence modeling.

</details>


### [2] [Kroneckerised Particle Mesh Ewald](https://arxiv.org/abs/2601.18838)
*Igor Chollet*

Main category: math.NA

TL;DR: New PME variant replaces FFT with Sum of Kronecker Products for better parallel performance on distributed-memory architectures.


<details>
  <summary>Details</summary>
Motivation: FFT dependence in classical Particle Mesh Ewald methods limits performance on parallel distributed-memory architectures due to communication overhead and scalability issues.

Method: Introduces a new variant of PME's reciprocal part using Sum of Kronecker Products (SKP) instead of FFT, with implementation designed for parallel distributed-memory systems.

Result: The SKP-based method has better parallel potential than classical PME (which is linearithmic), with numerical examples demonstrating its viability on distributed-memory architectures.

Conclusion: SKP-based PME variant offers promising alternative to FFT-based methods for parallel computing, potentially improving performance on distributed-memory systems for N-body problems with periodic structures.

Abstract: Particle Mesh Ewald (PME) methods accelerated through Fast Fourier Transforms (FFTs) for their reciprocal part are widely used to solve N -body problems over periodic structures with Laplace-like kernels. The FFT dependence of classical PME may mitigate its performance on parallel distributed-memory architectures. We here introduce a new variant of the reciprocal part based on Sum of Kronecker Products (SKP) instead of FFT. Moreover, our implementation of this new method is not linearithmic (as opposed to classical PME) but has an important parallel potential. We present the different approximation levels exploited in our new scheme and demonstrate to what extent it could be used on parallel distributed-memory architectures. Numerical examples supplement presented assertions.

</details>


### [3] [On the Strong Stability Preserving Property of Runge-Kutta Methods for Hyperbolic Problems](https://arxiv.org/abs/2601.18947)
*Mohammad R. Najafian,Brian C. Vermeire*

Main category: math.NA

TL;DR: The paper introduces a new mathematical framework to analyze nonlinear stability of Runge-Kutta methods beyond the SSP class, showing that many non-SSP schemes can maintain stability properties for various discretizations.


<details>
  <summary>Details</summary>
Motivation: Only a small subset of Runge-Kutta methods are Strong Stability Preserving (SSP), which limits the use of many efficient high-order time integration schemes that don't formally belong to this class.

Method: Developed a mathematical strategy to analyze nonlinear stability of RK schemes outside the SSP class, applying it to demonstrate stability properties for Lax-Friedrichs discretization, first-order upwind, and second-order MUSCL schemes.

Result: Showed that non-SSP RK methods (including classical 4th-order RK) can maintain entropy stability, positivity of density/pressure, and TVD stability for various discretizations, expanding the range of usable time integration schemes.

Conclusion: The analysis framework reveals that many RK methods previously excluded from SSP class can actually maintain stability for important numerical problems, broadening the practical options for high-order time integration.

Abstract: Strong Stability Preserving (SSP) time integration schemes maintain stability of the forward Euler method for any initial value problem. However, only a small subset of Runge-Kutta (RK) methods are SSP, and many efficient high-order time integration schemes do not formally belong to this class. In this work, we introduce a mathematical strategy to analyze the nonlinear stability of RK schemes that may not necessarily belong to the SSP class. With this approach, we mathematically demonstrate that there are time integration schemes outside the class of SSP schemes that can maintain entropy stability and positivity of density and pressure for the Lax-Friedrichs discretization, and Total Variation Diminishing stability for the first-order upwind and the second-order MUSCL schemes. As a result, for these problems, a broader range of RK methods, including the classical fourth-order, four-stage RK scheme, can be used while the numerical integration remains stable. Numerical experiments confirm these theoretical findings, and additional experiments demonstrate similar observations for a wider class of space discretizatins.

</details>


### [4] [AAA least squares solution of Helmholtz problems](https://arxiv.org/abs/2601.19020)
*Stefano Costa*

Main category: math.NA

TL;DR: Adaptive meshless framework for Helmholtz scattering problems using rational approximation and automated singularity placement via analytic continuation.


<details>
  <summary>Details</summary>
Motivation: To develop a robust, automated alternative to heuristic source placement methods for solving exterior sound-soft scattering problems governed by the Helmholtz equation, addressing challenges with complex, non star-shaped geometries.

Method: Interpret Method of Fundamental Solutions through rational approximation, use analytic continuation of boundary data for automated singularity placement, develop AAALS-Helmholtz algorithm based on continuum variant of AAA algorithm, and establish theoretical connection between Helmholtz and Laplace problems with "double poles" technique.

Result: An automated strategy for optimal source distribution that works even for complex geometries, providing a meshless alternative to heuristic approaches with theoretical justification for the double poles technique.

Conclusion: The proposed framework offers a robust, adaptive numerical method for Helmholtz scattering problems that eliminates the need for heuristic source placement and provides optimal performance for complex geometries through automated singularity identification.

Abstract: This paper presents an adaptive numerical framework for solving exterior "sound-soft" scattering problems governed by the Helmholtz equation. By interpreting the Method of Fundamental Solutions through the lens of rational approximation, we introduce an automated strategy for singularity placement based on the analytic continuation of boundary data. The proposed AAALS-Helmholtz algorithm leverages a "continuum" variant of the AAA algorithm to identify the singularities limiting analytic extension, and to ensure an optimal source distribution even for complex, non star-shaped geometries. Furthermore, we establish a formal connection between the Helmholtz and Laplace problems, providing a theoretical justification for the "double poles" technique. The approach offers a robust, meshless alternative to heuristic source placement.

</details>


### [5] [A sixth-order compact time-splitting Fourier pseudospectral method](https://arxiv.org/abs/2601.19172)
*Weiguo Gao,Zhansi He,Jia Yin*

Main category: math.NA

TL;DR: A novel sixth-order compact time-splitting scheme (S₆c) for solving the Dirac equation without external magnetic potentials, offering reduced computational complexity and extended to time-dependent potentials via time-ordering.


<details>
  <summary>Details</summary>
Motivation: Existing sixth-order splitting schemes for the Dirac equation have high computational complexity. There's a need for more efficient high-order methods that maintain accuracy while being easier to implement.

Method: Developed a sixth-order compact time-splitting scheme (S₆c) using time-splitting techniques. Extended the method to handle time-dependent potentials through time-ordering techniques. The scheme is designed to be easy to implement while reducing computational complexity.

Result: S₆c shows significant advantages in both precision and efficiency compared to existing time-splitting methods. It maintains the super-resolution property for the Dirac equation in the nonrelativistic regime without external magnetic potentials.

Conclusion: The proposed S₆c scheme provides an efficient, high-order accurate method for solving the Dirac equation, with reduced computational complexity and applicability to both time-independent and time-dependent potential problems.

Abstract: In this paper, we propose a novel sixth-order compact time-splitting scheme, denoted as $ S_{6\text{c}}$, for solving the Dirac equation in the absence of external magnetic potentials. This method is easy to implement, and it provides a substantial reduction in computational complexity compared to the existing sixth-order splitting schemes. By incorporating a time-ordering technique, we also extend $S_{6\text{c}}$ to address problems with time-dependent potentials. Comprehensive comparisons with various time-splitting methods show that $S_{6\text{c}}$ exhibits significant advantages in terms of both precision and efficiency. Moreover, numerical results indicate that $S_{6\text{c}}$ maintains the super-resolution property for the Dirac equation in the nonrelativistic regime in the absence of external magnetic potentials.

</details>


### [6] [On a Class of Multi-Dimensional Non-linear Time-Fractional Fokker-Planck Equations Capturing Brownian Motion](https://arxiv.org/abs/2601.19211)
*Neetu Garg,Varsha R*

Main category: math.NA

TL;DR: Developed a semi-analytical solution for multi-dimensional time-fractional Fokker-Planck equations using the Laplace residual power series method, validated through numerical examples and error analysis.


<details>
  <summary>Details</summary>
Motivation: The time-fractional Fokker-Planck equation is crucial for modeling anomalous diffusion and non-equilibrium systems in finance, physics, and biology, but solving it efficiently while capturing memory and nonlocal effects remains challenging.

Method: Laplace residual power series method that combines Laplace transform with traditional residual power series approach to handle multi-dimensional time-fractional Fokker-Planck equations.

Result: Successfully applied the method to several examples including non-linear multi-dimensional problems, showing smooth and stable error evolution, with comparisons to existing methods confirming its effectiveness.

Conclusion: The proposed Laplace residual power series method is a powerful and efficient tool for analyzing time-fractional Fokker-Planck equations, capable of handling memory and nonlocal effects in multi-dimensional settings.

Abstract: The time-fractional Fokker-Planck equation is a key model for characterizing anomalous diffusion, stochastic transport, and non-equilibrium statistical mechanics with applications in finance, chaotic dynamics, optical physics, and biological systems. In this work, we develop a semi-analytical solution for the multi-dimensional time-fractional Fokker-Planck equation employing the Laplace residual power series method. This method blends the Laplace transform and the traditional residual power series method, guaranteeing efficient solutions incorporating the memory and nonlocal effects. To validate the accuracy and effectiveness of the approach, we address several examples, including non-linear problems in multi-dimensions, and analyze the evolution of errors. The numerical simulations are compared with existing methods to confirm the adopted method's strength. The smooth and stable error evolution promises that the suggested method is a powerful tool for analyzing time-fractional Fokker-Planck equations.

</details>


### [7] [Precision-induced Adaptive Randomized Low-Rank Approximation for SVD and Matrix Inversion](https://arxiv.org/abs/2601.19250)
*Weiwei Xu,Weijie Shen,Zhengjian Bai,Chen Xu*

Main category: math.NA

TL;DR: New algorithms for SVD and matrix inversion that automatically determine optimal rank based on singular value distribution, eliminating need for manual rank tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional SVD and matrix inversion algorithms require guessing or tuning the rank parameter, which is computationally expensive and often unknown in practice. Existing approximate methods need manual rank specification.

Method: Proposes a precision-induced random re-normalization procedure that automatically determines optimal rank based on singular value distribution. The framework explicitly guides rank selection by desired precision level without requiring rank guessing.

Result: New algorithms simultaneously calculate optimal rank for desired precision and produce approximate solutions with substantially reduced computational cost compared to traditional methods.

Conclusion: The proposed approach provides an efficient alternative to traditional SVD and matrix inversion by eliminating rank tuning overhead, with promising performance supported by both theoretical analysis and numerical experiments.

Abstract: Singular value decomposition (SVD) and matrix inversion are ubiquitous in scientific computing. Both tasks are computationally demanding for large scale matrices. Existing algorithms can approximatively solve these problems with a given rank, which however is unknown in practice and requires considerable cost for tuning. In this paper, we tackle the SVD and matrix inversion problems from a new angle, where the optimal rank for the approximate solution is explicitly guided by the distribution of the singular values. Under the framework, we propose a precision-induced random re-normalization procedure for the considered problems without the need of guessing a good rank. The new algorithms built upon the procedure simultaneously calculate the optimal rank for the task at a desired precision level and lead to the corresponding approximate solution with a substantially reduced computational cost. The promising performance of the new algorithms is supported by both theory and numerical examples.

</details>


### [8] [Seepage analysis using a polygonal cell-based smoothed finite element method](https://arxiv.org/abs/2601.19357)
*Yang Yang,Mingjiao Yan,Zongliang Zhang,Yinpeng Yin,Qiang Liu,You-liang Li*

Main category: math.NA

TL;DR: Polygonal cell-based smoothed finite element method for seepage analysis with adaptive meshing and free-surface handling


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate numerical method for analyzing seepage in saturated porous media that can handle complex geometries, local refinement needs, and free-surface problems with wet-dry transitions.

Method: Combines Wachspress interpolation on convex polygonal elements with cell-based gradient smoothing, using boundary integrals instead of in-element derivatives. Employs polygonal, quadtree, and hybrid meshes with solution-driven adaptive refinement near steep gradients and wet-dry transitions. Uses fixed-mesh iterative scheme for free-surface seepage.

Result: Benchmark tests demonstrate accurate hydraulic-head and free-surface predictions. Adaptive meshing achieves similar accuracy with substantially fewer degrees of freedom and reduced CPU time compared to uniform refinement.

Conclusion: The developed polygonal cell-based smoothed finite element method provides an effective framework for seepage analysis, offering computational efficiency through adaptive meshing while maintaining accuracy for both steady-state and transient problems with free surfaces.

Abstract: This work develops a polygonal cell-based smoothed finite element method for steady-state, transient, and free-surface seepage in saturated porous media. Wachspress interpolation on convex polygonal elements is combined with cell-based gradient smoothing, so that element matrices are assembled using boundary integrals without in-element derivatives. Polygonal, quadtree, and hybrid quadtree--polygonal meshes are employed to accommodate local refinement and hanging nodes, and a solution-driven adaptive strategy further concentrates resolution near steep gradients and wet--dry transitions. Free-surface seepage is solved using a fixed-mesh iterative scheme that updates the wetted region, permeability field, and boundary conditions. Benchmark tests demonstrate accurate hydraulic-head and free-surface predictions, and show that adaptivity attains similar accuracy with substantially fewer degrees of freedom and CPU time.

</details>


### [9] [Unified Regularization of 2D Singular Integrals for Axisymmetric Galerkin BEM in Eddy-Current Evaluation](https://arxiv.org/abs/2601.19542)
*Yao Luo*

Main category: math.NA

TL;DR: Axisymmetric Galerkin BEM with unified regularization for 2D singular integrals in eddy-current modeling, validated on various geometries with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and efficient method for modeling eddy-current interactions between excitation coils and conductive objects in axisymmetric configurations, addressing the challenge of handling singular integrals in boundary element methods.

Method: Axisymmetric Galerkin boundary element method (BEM) derived from Stratton-Chu representation for azimuthal vector potential, with unified regularization framework for 2D singular integrals (logarithmic and Cauchy singularities) using common integral transformations.

Result: Method validated on cylindrical, conical, and spherical shells with consistently high accuracy and computational efficiency across broad frequency ranges and coil lift-off distances; regularization and quadrature stability proved and verified numerically.

Conclusion: The proposed axisymmetric Galerkin BEM with integral transformation technique provides a robust and efficient framework for axisymmetric eddy-current nondestructive evaluation.

Abstract: This paper presents an axisymmetric Galerkin boundary element method (BEM) for modeling eddy-current interactions between excitation coils and conductive objects. The formulation derives boundary integral equations from the Stratton-Chu representation for the azimuthal component of the vector potential in both air and conductive regions. The central contribution is a unified regularization framework for the two-dimensional (2D) singular integrals arising in Galerkin BEM. This framework handles both logarithmic and Cauchy singularities through a common set of integral transformations, eliminating the need for case-by-case analytical singularity extraction and enabling straightforward numerical quadrature. The regularization and quadrature stability are proved and verified numerically. The method is validated on several representative axisymmetric geometries, including cylindrical, conical, and spherical shells. Numerical experiments demonstrate consistently high accuracy and computational efficiency across broad frequency ranges and coil lift-off distances. The results confirm that the proposed axisymmetric Galerkin BEM, combined with the integral transformation technique, provides a robust and efficient framework for axisymmetric eddy-current nondestructive evaluation.

</details>


### [10] [A Green's Function-Based Enclosure Framework for Poisson's Equation and Generalized Sub- and Super-Solutions](https://arxiv.org/abs/2601.19682)
*Kazuaki Tanaka,Ryoga Iwanami,Kaname Matsue,Hiroyuki Ochiai*

Main category: math.NA

TL;DR: Novel framework for enclosing Poisson equation solutions using generalized sub-/super-solutions based on fundamental solutions, overcoming limitations of variational inequalities in non-convex domains.


<details>
  <summary>Details</summary>
Motivation: Conventional variational inequality approach fails for natural function classes (piecewise linear) and non-convex polygonal domains where H^2 regularity is lost due to corner singularities.

Method: Introduce "Green-representable solutions" using test functions from fundamental solutions. For 1D: explicit test function construction. For 2D polygonal domains: Method of Fundamental Solutions to generate test functions.

Result: Numerical experiments in 1D and 2D (including non-convex polygons) show the method yields strict and accurate pointwise enclosures of true solution, even for problems with discontinuous source terms or geometric singularities.

Conclusion: The proposed framework provides rigorous pointwise evaluation and reliable solution enclosures for Poisson's equation in challenging domains where conventional methods fail.

Abstract: This paper presents a novel framework for enclosing solutions of Poisson's equation based on generalized sub- and super-solutions constructed using fundamental solutions. The conventional definition of sub- and super-solutions based on variational inequalities often fails for natural function classes such as piecewise linear functions and encounters theoretical difficulties in non-convex polygonal domains, where H^2 regularity is lost because of corner singularities. To overcome these limitations, we introduce the concept of ``Green-representable solutions'' utilizing test functions constructed from fundamental solutions. This framework enables a new formulation of sub- and super-solutions that permits rigorous pointwise evaluation. For one-dimensional problems, we derive explicit constructions of the test functions. For two-dimensional polygonal domains, we employ the Method of Fundamental Solutions to generate test functions. The approach is validated through numerical experiments in both settings, including non-convex polygons. The results demonstrate that the proposed method yields strict and accurate pointwise enclosures of the true solution, even for problems with discontinuous source terms or geometric singularities.

</details>


### [11] [Error estimates of a training-free diffusion model for high-dimensional sampling](https://arxiv.org/abs/2601.19740)
*Pengjun Wang,Zezhong Zhang,Minglei Yang,Feng Bao,Yanzhao Cao,Guannan Zhang*

Main category: math.NA

TL;DR: Training-free diffusion models lack rigorous error estimates; this work provides comprehensive error analysis for such models using Gaussian mixture exact scores, recovering classical ODE convergence rates with favorable dimension dependence.


<details>
  <summary>Details</summary>
Motivation: Training-free diffusion models are attractive alternatives to score-based diffusion models but lack rigorous and numerically verifiable error estimates, creating a gap between theoretical analysis and observed numerical behavior.

Method: Develop error analysis for training-free diffusion models by exploiting exact score functions available for Gaussian mixture models, avoiding score-function approximation errors and analyzing ODE discretization schemes in reverse-time diffusion process.

Result: Recovers classical convergence rates (e.g., first-order for Euler method) with favorable dimension dependence: O(d) in ℓ₂ norm and O(log d) in ℓ∞ norm. Error estimates are fully numerically verifiable for both time-step size and dimensionality.

Conclusion: Provides comprehensive, numerically verifiable error analysis for training-free diffusion models, bridging theoretical analysis with observed numerical behavior and establishing rigorous error bounds with favorable dimension scaling.

Abstract: Score-based diffusion models are a powerful class of generative models, but their practical use often depends on training neural networks to approximate the score function. Training-free diffusion models provide an attractive alternative by exploiting analytically tractable score functions, and have recently enabled supervised learning of efficient end-to-end generative samplers. Despite their empirical success, the training-free diffusion models lack rigorous and numerically verifiable error estimates. In this work, we develop a comprehensive error analysis for a class of training-free diffusion models used to generate labeled data for supervised learning of generative samplers. By exploiting the availability of the exact score function for Gaussian mixture models, our analysis avoids propagating score-function approximation errors through the reverse-time diffusion process and recovers classical convergence rates for ODE discretization schemes, such as first-order convergence for the Euler method. Moreover, the resulting error bounds exhibit favorable dimension dependence, scaling as $O(d)$ in the $\ell_2$ norm and $O(\log d)$ in the $\ell_\infty$ norm. Importantly, the proposed error estimates are fully numerically verifiable with respect to both time-step size and dimensionality, thereby bridging the gap between theoretical analysis and observed numerical behavior.

</details>


### [12] [A refined nonlinear least-squares method for the rational approximation problem](https://arxiv.org/abs/2601.19813)
*Michael S. Ackermann,Linus Balicki,Serkan Gugercin,Steffen W. R. Werner*

Main category: math.NA

TL;DR: The paper proposes a refinement approach to the AAA algorithm that minimizes rational approximation degree while meeting error tolerance, ensuring monotonic error convergence.


<details>
  <summary>Details</summary>
Motivation: The standard AAA algorithm often produces rational approximations with degrees larger than necessary to meet a given error tolerance. There's a need for an adaptive approach that constructs interpolating rational approximations with the smallest feasible degree while satisfying the prescribed tolerance.

Method: The authors introduce refinement approaches to the linear least-squares step of the classical AAA algorithm. These refinements aim to minimize the true nonlinear least-squares error with respect to the given data. They theoretically analyze the derived approaches in terms of gradients from resulting minimization problems and propose a new greedy framework that ensures monotonic error convergence.

Result: Numerical examples from function approximation and model order reduction verify the effectiveness of the proposed algorithm to construct accurate rational approximations of small degrees.

Conclusion: The proposed refinement approach to AAA algorithm successfully constructs rational approximations with minimal degrees while meeting error tolerances, offering improved efficiency over the standard AAA method.

Abstract: The adaptive Antoulas-Anderson (AAA) algorithm for rational approximation is a widely used method for the efficient construction of highly accurate rational approximations to given data. While AAA can often produce rational approximations accurate to any prescribed tolerance, these approximations may have degrees larger than what is actually required to meet the given tolerance. In this work, we consider the adaptive construction of interpolating rational approximations while aiming for the smallest feasible degree to satisfy a given error tolerance. To this end, we introduce refinement approaches to the linear least-squares step of the classical AAA algorithm that aim to minimize the true nonlinear least-squares error with respect to the given data. Furthermore, we theoretically analyze the derived approaches in terms of the corresponding gradients from the resulting minimization problems and use these insights to propose a new greedy framework that ensures monotonic error convergence. Numerical examples from function approximation and model order reduction verify the effectiveness of the proposed algorithm to construct accurate rational approximations of small degrees.

</details>


### [13] [Galerkin-type time discretizations for parabolic and hyperbolic problems: stability and a priori error analysis](https://arxiv.org/abs/2601.19828)
*Sergio Gómez*

Main category: math.NA

TL;DR: A unified framework for analyzing space-time Galerkin methods for parabolic/hyperbolic problems using test function selection for stability without Grönwall estimates.


<details>
  <summary>Details</summary>
Motivation: To address limitations in standard energy arguments for space-time methods, particularly the gap in analysis for some methods in this class, and to provide a more robust framework.

Method: Uses a unified framework based on Galerkin-type time discretizations with careful selection of test functions to establish stability in L∞(0,T;X) norms, enabling a priori error estimates without Grönwall estimates.

Result: The approach closes analysis gaps, works for arbitrary approximation degrees, requires reduced regularity assumptions, and is robust with respect to model parameters.

Conclusion: Provides a comprehensive analysis framework for space-time methods that overcomes limitations of traditional energy arguments and offers broader applicability and robustness.

Abstract: We present a unified framework for the analysis of space-time methods based on Galerkin-type time discretizations for parabolic and hyperbolic problems. Crucially, the stability analysis relies on a suitable choice of test functions to establish the continuous dependence of the discrete solution on the data in $L^{\infty}(0, T; X)$ norms, which is then used to derive a priori error estimates. This approach closes the gap in the analysis of some methods in this class caused by the limitation of standard energy arguments, and is characterized by the absence of Grönwall estimates, applicability to arbitrary approximation degrees, reduced regularity assumptions, and robustness with respect to the model parameters.

</details>


### [14] [Modified splitting methods for Gross-Pitaevskii systems modelling Bose-Einstein condensates: Time evolution and ground state computation](https://arxiv.org/abs/2601.19838)
*Mechthild Thalhammer,Gregor Thalhammer-Thurner*

Main category: math.NA

TL;DR: Novel modified operator splitting methods overcome second-order barrier for simulating Bose-Einstein condensations using Gross-Pitaevskii equations, enabling stable higher-order approximations with adaptive time-stepping.


<details>
  <summary>Details</summary>
Motivation: Inspired by the 100th and 30th anniversaries of Bose-Einstein condensation theory and experimental realization, the paper aims to develop reliable and efficient numerical methods for simulating these quantum systems described by coupled Gross-Pitaevskii equations.

Method: Proposes modified operator splitting methods that incorporate commutators of differential and nonlinear multiplication operators to overcome the second-order stability barrier. Uses automatic time-step adjustment through local error control and presents a specific fourth-order modified operator splitting method.

Result: Numerical experiments confirm favorable performance of the fourth-order modified operator splitting method, maintaining excellent mass and energy conservation in long-term evolutions (intrinsic to geometric numerical integrators) even with varying time stepsizes. Demonstrates benefits of adaptive higher-order approximations for ground state computations.

Conclusion: The innovative approach successfully designs stable modified operator splitting methods that overcome the second-order barrier for both time evolution and imaginary time propagation in Bose-Einstein condensation simulations, enabling efficient and reliable numerical computations with adaptive higher-order accuracy.

Abstract: The year 2025 marks the 100 and 30 years anniversaries of the discovery of Bose--Einstein condensation and its successful experimental realisation. Inspired by these important research achievements, a conceptually simple approach is proposed to facilitate reliable and efficient numerical simulations. The structure of the underlying systems of coupled Gross--Pitaevskii equations suggests the use of optimised high-order operator splitting methods for dynamical evolution and ground state computation. A second-order barrier, however, prevents the applicability of standard operator splitting methods for both, time evolution as well as imaginary time propagation. An innovative alternative approach accomplishes the design of novel modified operator splitting methods that remain stable under moderate smallness assumptions on the time increments. The core idea is to incorporate commutators of the defining differential and nonlinear multiplication operators, since this permits to fulfill the basic stability requirement of positive method coefficients. Further improvements with respect to convergence at the targeted precision arise from automatic adjustments of the time stepsizes by an inexpensive local error control. The presented numerical experiments confirm the favourable performance of a specific fourth-order modified operator splitting method. Amongst others, it is demonstrated that the excellent mass and energy conservation in long-term evolutions, intrinsic attributes of geometric numerical integrators for Hamiltonian systems, is maintained for a sensible variation of the time stepsizes. Moreover, the benefits of adaptive higher-order approximations in ground state computations are illustrated.

</details>


### [15] [An Energy-Preserving Domain of Dependence Stabilization for the Linear Wave Equation on Cut-Cell Meshes](https://arxiv.org/abs/2601.19877)
*Gunnar Birke,Christian Engwer,Sandra May,Louis Petri,Hendrik Ranocha*

Main category: math.NA

TL;DR: Energy-preserving domain of dependence stabilization method for linear wave equation on cut-cell meshes using DG discretization with explicit RK time integration.


<details>
  <summary>Details</summary>
Motivation: To develop a stable numerical scheme for linear wave equations on cut-cell meshes that avoids severe time step restrictions caused by small cut cells while preserving energy properties.

Method: Combines standard discontinuous Galerkin spatial discretization with explicit strong stability preserving Runge-Kutta time integration, plus tailored stabilization terms that propagate information across small cut cells.

Result: The scheme allows time step selection based on background cell size rather than small cut cells, maintains energy stability/conservation properties, and demonstrates high accuracy and stability in numerical experiments.

Conclusion: Proposed stabilization method successfully overcomes time step limitations of cut-cell meshes while preserving desirable energy properties, enabling efficient and accurate wave equation simulations on complex geometries.

Abstract: We present an energy-preserving (either energy-conservative or energy-dissipative) domain of dependence stabilization method for the linear wave equation on cut-cell meshes. Our scheme is based on a standard discontinuous Galerkin discretization in space and an explicit (strong stability preserving) Runge Kutta method in time. Tailored stabilization terms allow for selecting the time step length based on the size of the background cells rather than the small cut cells by propagating information across small cut cells. The stabilization terms preserve the energy stability or energy conservation property of the underlying discontinuous Galerkin space discretization. Numerical results display the high accuracy and stability properties of our scheme.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [16] [Dynamic Response of a Finite Circular Plate on an Elastic Half-Space Using the Truncated Lamb Kernel](https://arxiv.org/abs/2601.19031)
*Greyson Meares,Sage Meiling,Charis Tsikkou*

Main category: math.AP

TL;DR: Exact operator formulation for dynamic interaction between finite circular elastic plate and elastic half-space, addressing limitations of classical infinite-plate assumptions.


<details>
  <summary>Details</summary>
Motivation: Classical analyses assume infinite plates and use Hankel transform diagonalization, but finite plates with radius R require handling spatially truncated Lamb operators and real-axis singularities.

Method: Represent action of truncated Lamb operator on finite-disk Bessel basis satisfying free-edge boundary conditions, derive explicit matrix elements with Cauchy principal value integrals and residue contributions for radiation damping.

Result: Obtain dense but spectrally convergent operator matrix whose inversion yields complete frequency-domain solution for finite-radius plates, reproducing experimental results and quantifying finite-radius corrections.

Conclusion: First exact operator-level treatment of finite-radius plate-half-space interaction retaining full nonlocal Lamb kernel, bridging gap between infinite-plate theory and finite-plate reality.

Abstract: We develop an exact operator formulation for the dynamic interaction between a finite circular elastic plate and an elastic half-space. Classical analyses, beginning with Lamb's representation of the half-space response, typically assume an infinite plate and rely on diagonalization of the soil operator via the continuous Hankel transform. For a plate of finite radius $R$, however, both traction and displacement are supported only on $0 \le r \le R$, leading to the spatially truncated Lamb operator \[ \mathscr{M}(ω) = χ_{[0,R]} \, T(ω)\, χ_{[0,R]}, \] where $T(ω)$ is the Hankel multiplier involving the Rayleigh denominator $Ω(ξ,ω)$. Truncation destroys the diagonal structure of $T(ω)$ and introduces real-axis singularities associated with the Rayleigh pole, in addition to square-root branch points at $ξ= k_T$ and $ξ= k_L$. We represent the action of $\mathscr{M}(ω)$ on a finite-disk Bessel basis $\{ φ_n(r) = A_{1,n} J_0(λ_n r) + A_{2,n} I_0(λ_n r)\},$ which satisfies the free-edge boundary conditions of the plate, and derive explicit expressions for the resulting matrix elements. These involve integrals of the Lamb kernel evaluated as Cauchy principal values, with residue contributions corresponding to radiation damping in the half-space. The resulting operator matrix is dense but spectrally convergent. Its inversion yields a complete frequency-domain solution for finite-radius plates. The analysis reproduces Chen et al.'s finite-radius experiments for small $R$, approaches the infinite-radius limit as $R \to \infty$, and quantifies finite-radius corrections. To our knowledge, this is the first exact operator-level treatment of finite-radius plate-half-space interaction that retains the full nonlocal Lamb kernel.

</details>


### [17] [Quantitative light-particle limit for the Vlasov-Fokker-Planck-Navier-Stokes system](https://arxiv.org/abs/2601.19110)
*Young-Pil Choi,Jinwook Jung*

Main category: math.AP

TL;DR: Quantitative convergence theory for Vlasov-Fokker-Planck-Navier-Stokes system in light particle regime with explicit convergence rates.


<details>
  <summary>Details</summary>
Motivation: Previous work by Goudon, Jabin, and Vasseur (2004) provided only qualitative compactness-based results for the light particle limit. The authors aim to develop the first quantitative convergence theory with explicit rates.

Method: Uses a relative entropy method adapted to the singular scaling of the light particle regime, where particle relaxation occurs on a fast time scale. The analysis works in both weak topologies and yields optimal convergence rates in bounded Lipschitz distance.

Result: Develops first quantitative convergence theory with explicit rates for both kinetic distribution and fluid velocity. Results apply on both torus and whole space, providing unified quantitative description of light particle hydrodynamic limit.

Conclusion: The paper extends previous qualitative results to quantitative convergence theory with optimal rates, establishing a comprehensive framework for the light particle hydrodynamic limit across different domains.

Abstract: We investigate the hydrodynamic limit of the Vlasov--Fokker--Planck--Navier--Stokes system in the light particle regime, where the particle relaxation takes place on a singularly fast time scale. Using a relative entropy method adapted to this scaling, we develop the first quantitative convergence theory for the light particle limit. Our analysis yields explicit rates for the convergence of both the kinetic distribution and the fluid velocity, extending the qualitative compactness-based result of Goudon, Jabin, and Vasseur [Indiana Univ. Math. J., 53, (2004), 1495--1515]. Moreover, we show that these quantitative estimates propagate in weak topologies and, in particular, lead to optimal convergence rates in the bounded Lipschitz distance. The results apply on both the torus and the whole space, providing a unified quantitative description of the light particle hydrodynamic limit.

</details>


### [18] [The Łojasiewicz-Simon inequality related to grain boundary motion and its applications](https://arxiv.org/abs/2601.19226)
*Masashi Mizuno,Ayumi Sakiyama,Keisuke Takasao*

Main category: math.AP

TL;DR: The paper establishes a Łojasiewicz-Simon gradient inequality for grain boundary motion models, deriving a curve shortening equation with time-dependent mobility and applying the inequality to analyze energy dissipation.


<details>
  <summary>Details</summary>
Motivation: To develop rigorous mathematical tools for analyzing grain boundary motion in materials science, specifically establishing gradient inequalities that can be used to study energy dissipation and long-time behavior of grain boundary evolution.

Method: 1. Derive a curve shortening equation with time-dependent mobility that preserves energy dissipation law for grain boundary energy. 2. Prove the Łojasiewicz-Simon gradient inequality for the grain boundary energy functional. 3. Apply the inequality to analyze the energy behavior.

Result: Successfully established the Łojasiewicz-Simon gradient inequality for grain boundary motion models, providing a mathematical framework to study energy dissipation and potentially prove convergence results for grain boundary evolution.

Conclusion: The Łojasiewicz-Simon gradient inequality provides a powerful analytical tool for studying grain boundary motion, enabling rigorous analysis of energy dissipation and potentially establishing convergence properties for grain boundary evolution in materials science applications.

Abstract: In this paper, we study the Łojasiewicz-Simon gradient inequality for the mathematical model of grain boundary motion. We first derive a curve shortening equation with time-dependent mobility, which guarantees the energy dissipation law for the grain boundary energy, including the difference between orientations of the constituent grains as a state variable. Next, we discuss the Łojasiewicz-Simon gradient inequality for the grain boundary energy. Finally, we give applications of the inequality to the energy.

</details>


### [19] [A Fokker-Planck equation with superlinear drift at infinity for Integrate-and-Fire model](https://arxiv.org/abs/2601.19282)
*Benoît Perthame,Clément Rieutord,Delphine Salort*

Main category: math.AP

TL;DR: The paper extends the classical Noisy Integrate-and-Fire model to include superlinear drift, establishes well-posedness with boundary conditions at infinity, proves entropy dissipation, and shows exponential convergence to stationary state.


<details>
  <summary>Details</summary>
Motivation: To provide a more realistic description of neuronal membrane potential dynamics by incorporating superlinear drift into the Integrate-and-Fire model, which requires extending the equation to the full line with neural activity described by flux at infinity.

Method: Extends the classical Noisy Integrate-and-Fire model to the full line with superlinear drift, establishes boundary conditions at infinity, proves well-posedness, demonstrates entropy dissipation property, and uses Doeblin's method for convergence analysis.

Result: Establishes well-posedness of the solution with proper boundary conditions at infinity, proves the entropy dissipation property, and demonstrates exponential convergence to the unique stationary state using Doeblin's method.

Conclusion: The proposed framework successfully extends the classical Integrate-and-Fire model with superlinear drift, providing rigorous mathematical foundations including well-posedness, entropy properties, and convergence to equilibrium.

Abstract: The Integrate-and-Fire model is a Fokker-Planck equation arising in neuroscience. It describes the evolution of the probability density of the neuronal membrane potential and fitting has shown that the inclusion of a em superlinear drift provides the most realistic description. To make sense of this, we propose to set the equation on the full line, the neural activity being described by the flux at infinity. This framework serves as a model extension of the classical Noisy Integrate-and-Fire model, with a fixed firing potential. We first establish the well-posedness of the solution, establish the boundary condition at infinity which is the major difficulty. Then, state rigorously the entropy dissipation property. Finally, using Doeblin's method, we prove the exponential convergence of the solution toward the unique stationary state in full generality.

</details>


### [20] [Remarks on well-posedness for linear elliptic equations via divergence-free transformation](https://arxiv.org/abs/2601.19317)
*Haesung Lee*

Main category: math.AP

TL;DR: Divergence-free transformation enables well-posedness for linear elliptic equations with L¹ coefficients, overcoming limitations of classical methods.


<details>
  <summary>Details</summary>
Motivation: Classical bilinear form methods fail to handle linear elliptic equations with zero-order coefficients c ∈ L¹(U). The paper aims to establish well-posedness in this challenging setting using a novel divergence-free transformation approach.

Method: Uses divergence-free transformation from author's previous work, compares with classical bilinear form methods, and applies Riesz-Thorin interpolation theorem between c ∈ L¹(U) and c ∈ L^{2d/(d+2)}(U) cases.

Result: Divergence-free transformation successfully establishes well-posedness for c ∈ L¹(U), while classical methods fail. Using interpolation, proves existence and uniqueness of weak solutions for c ∈ L^s(U) with s ∈ [1, 2d/(d+2)].

Conclusion: The divergence-free transformation provides a powerful alternative to classical methods, extending well-posedness results to broader coefficient spaces including L¹ coefficients, with interpolation yielding results for intermediate spaces.

Abstract: This paper investigates the well-posedness of linear elliptic equations, focusing on the divergence-free transformation introduced in the author's recent work [J. Math. Anal. Appl. 548 (2025), 129425]. By comparing this approach with classical bilinear form methods, we demonstrate that while standard techniques encounter limitations in handling zero-order coefficients $c \in L^1(U)$, the divergence-free transformation successfully establishes well-posedness in this setting. Furthermore, utilizing the Riesz-Thorin interpolation theorem between the cases $c \in L^1(U)$ and $c \in L^{\frac{2d}{d+2}}(U)$, we establish the existence and uniqueness of weak solutions under the assumption $c \in L^s(U)$ for $s \in [1, \frac{2d}{d+2}]$.

</details>


### [21] [On blow up NLS with a multiplicative noise](https://arxiv.org/abs/2601.19330)
*Chenjie Fan,Junzhe Wang*

Main category: math.AP

TL;DR: The paper analyzes the probability of noise-induced blow-up in nonlinear Schrödinger equations, showing that while multiplicative noise can accelerate blow-up, the probability of this occurring in short time intervals is actually quite small.


<details>
  <summary>Details</summary>
Motivation: To understand whether noise accelerates or prevents blow-up in nonlinear systems, specifically examining the probability of noise-induced blow-up in nonlinear Schrödinger equations where previous research showed multiplicative noise can cause blow-up in arbitrarily short times.

Method: The authors analyze the probability of blow-up under multiplicative noise conditions, providing a large deviation type upper bound to quantify how small this probability actually is.

Result: The probability of noise-induced blow-up occurring in short time intervals is shown to be quite small, with the paper providing a precise upper bound on this probability through large deviation analysis.

Conclusion: While multiplicative noise can theoretically accelerate blow-up in nonlinear Schrödinger equations, the actual probability of this occurring in practice is very small, providing important probabilistic insights into noise effects on blow-up phenomena.

Abstract: It is of significant interest to understand whether a noise will speed up or prevent blow up. Under certain nondegenerate conditions, \cite{dD2005Blowup} proved a multiplicative noise will speed up blow up of NLS, in the sense that, blow up can happen in any short time with positive probability. We prove that such probability is indeed quite small, and provide a large deviation type upper bound.

</details>


### [22] [A sharp monomial Caffarelli-Kohn-Nirenberg inequality](https://arxiv.org/abs/2601.19359)
*Francesco Pagliarin*

Main category: math.AP

TL;DR: The paper establishes the optimal constant and classifies optimizers for a monomial Caffarelli-Kohn-Nirenberg inequality under integrated curvature dimension conditions, using Γ-calculus and geometric techniques with a symmetry-breaking result.


<details>
  <summary>Details</summary>
Motivation: To extend the classical Caffarelli-Kohn-Nirenberg inequality by considering monomial versions and establishing optimal constants with complete characterization of extremal functions under geometric curvature conditions.

Method: Uses Γ-calculus (a geometric analytic framework) to apply geometric techniques, combined with regularity results to justify integration by parts procedures. The approach leverages curvature dimension conditions to analyze the inequality.

Result: Finds the optimal constant for the monomial Caffarelli-Kohn-Nirenberg inequality, classifies all optimizers under integrated curvature dimension conditions, and provides a symmetry-breaking result.

Conclusion: The paper successfully extends Caffarelli-Kohn-Nirenberg inequalities to monomial settings with geometric curvature conditions, providing complete characterization of optimal constants and extremal functions while revealing symmetry-breaking phenomena.

Abstract: We consider a monomial Caffarelli-Kohn-Nirenberg inequality, find the optimal constant and classify the optimizers under an integrated curvature dimension condition. We take advantage of the $Γ$-calculus to exploit geometrical techniques to tackle the problem and regularity results to justify some integration by parts. A symmetry-breaking result is also provided.

</details>


### [23] [Existence of Weak Solutions to a Constrained Aggregation-Diffusion-Reaction Model for Multiple Sclerosis](https://arxiv.org/abs/2601.19427)
*S. Fagioli,M. Kamath Katapady*

Main category: math.AP

TL;DR: Existence of weak solutions for an aggregation-diffusion-reaction equation with constraint, modeling multiple sclerosis macrophage dynamics.


<details>
  <summary>Details</summary>
Motivation: To mathematically model multiple sclerosis by describing activated macrophage density evolution under oligodendrocyte attraction, addressing the constraint structure in the system.

Method: Variational splitting scheme isolating transport (aggregation-diffusion) and reaction contributions; recovering oligodendrocyte density as limit of characteristic functions.

Result: Established existence result for weak solutions to the constrained aggregation-diffusion-reaction equation.

Conclusion: Successfully proved existence of solutions for the multiple sclerosis model using variational splitting and characteristic function approximation.

Abstract: We establish an existence result for weak solutions to an aggregation-diffusion-reaction equation with a constraint, arising in the modelling of multiple sclerosis. The model is derived from a general chemotaxis-type framework and describes the time evolution of the density of activated macrophages, which is subject to attraction by oligodendrocytes. The latter are governed by a constraint equation. The proof relies on a variational splitting scheme that isolates the transport (aggregation-diffusion) and reaction contributions. The structure of the constraint makes it possible to recover the oligodendrocyte density as the limit of a sequence of characteristic functions.

</details>


### [24] [Dissipative Solutions to a Compressible Non-Newtonian Korteweg System with Density-Dependent Viscous Stress Tensor](https://arxiv.org/abs/2601.19442)
*Didier Bresch,Christophe Lacave,Maja Szlenk*

Main category: math.AP

TL;DR: Proves existence of dissipative solutions for viscoplastic compressible flows with density-dependent viscosities and capillarity effects in periodic domains, extending previous Newtonian flow results to non-Newtonian flows.


<details>
  <summary>Details</summary>
Motivation: To extend the mathematical analysis of compressible flows with capillarity effects from Newtonian to non-Newtonian viscoplastic flows, establishing rigorous existence results for dissipative solutions.

Method: Uses relative entropy inequality approach to prove existence of dissipative solutions and weak-strong uniqueness for the system describing viscoplastic compressible flows with density-dependent viscosities in periodic domains.

Result: Successfully proves existence of dissipative solutions and establishes weak-strong uniqueness for the viscoplastic compressible flow system with capillarity effects, extending previous Newtonian flow results.

Conclusion: The paper successfully extends the mathematical framework for compressible flows with capillarity from Newtonian to non-Newtonian viscoplastic flows, providing rigorous existence results for dissipative solutions through relative entropy methods.

Abstract: The main objective of this paper is to prove that if capillarity effect is taken into account then there exist dissipative solutions to a system describing viscoplastic compressible flows with density dependent viscosities in a periodic domain $\T^d$ with $d=2,3$. We calculate the relative entropy inequality and in consequence show existence of dissipative solutions and the weak-strong uniqueness for this system. Our result extends the recent result concerning the link between Euler--Korteweg and Navier--Stokes--Korteweg systems for Newtonian flows (when the viscosity depends on the density) [See D.~Bresch, M. Gisclon, I. Lacroix-Violet, {\it Arch. Rational Mech. Anal.} (2019)] to non-Newtonian flows.

</details>


### [25] [Integral equation methods for scattering by general compact obstacles: wavenumber-explicit estimates](https://arxiv.org/abs/2601.19456)
*Simon N. Chandler-Wilde,Siavash Sadeghi*

Main category: math.AP

TL;DR: This paper analyzes the wavenumber dependence of boundary integral operators for Helmholtz equations, providing explicit bounds on operator norms and their inverses, with applications to various geometric settings.


<details>
  <summary>Details</summary>
Motivation: There is significant interest in understanding how boundary integral operators for Helmholtz equations depend on the wavenumber k, especially for practical numerical computations where k can be large. Recent work by Caetano et al (2025) proposed a generalized integral equation formulation applicable to arbitrary compact sets, but its k-dependence wasn't fully understood.

Method: The authors study the operator A_k and its inverse A_k^{-1}, establishing various bounds on their norms as functions of k. They use mathematical analysis techniques to prove both worst-case growth scenarios and polynomial bounds when excluding small exceptional sets. The analysis covers different geometric configurations including star-shaped domains, Lipschitz boundaries, screens, and d-sets.

Result: For star-shaped Γ, ||A_k|| ≤ ck and ||A_k^{-1}|| ≤ c'k. The paper shows that: (1) arbitrarily fast growth of ||A_k^{-1}|| is possible for some Γ, but (2) for any Γ, ||A_k^{-1}|| grows at most polynomially (as k^{2n+2+δ}) outside an arbitrarily small exceptional set. These results yield the first k-explicit bounds for single-layer operator inverses and condition numbers in various geometric settings.

Conclusion: The paper provides comprehensive understanding of wavenumber dependence for boundary integral operators in Helmholtz problems. While worst-case scenarios can be arbitrarily bad, in practice the growth is polynomial when excluding small exceptional sets. These results have important implications for numerical methods and stability analysis in wave propagation problems.

Abstract: There has been significant recent interest in understanding the dependence on the wavenumber, $k$, of boundary integral operators (BIOs), supported on some set $Γ\subset \mathbb{R}^n$, that arise in the solution of BVPs for the Helmholtz equation, $Δu + k^2 u=0$. Recently, for the Dirichlet BVP with data $g$, Caetano et al (2025) have proposed an integral equation (IE) $A_kφ=g$ that applies for arbitrary compact $Γ$. This formulation is a generalisation of standard first kind IEs, where the BIO is $S_k$, the single-layer BIO on a surface $Γ$, that apply when $Γ$ is the boundary of a Lipschitz domain or a screen.
  In this paper we study the dependence of $A_k$ on $k$, showing that, for $k\geq k_0>0$, $\|A_k\|\leq ck$ while $\|A_k^{-1}\| \leq c'k$ if $Γ$ is star-shaped, where $c, c'>0$ depend only on $k_0$ and $Γ$. Amongst other bounds we also show that: (i) on the one hand, given any mildly increasing unbounded positive sequence $(k_m)$ and any unbounded sequence $(a_m)$, there exists $Γ$, with connected complement, such that $\|A_{k_m}^{-1}\|\geq a_m$ for every $m$; (ii) on the other hand, for every $Γ\subset \mathbb{R}^n$ and $k_0,\varepsilon, δ>0$, there exists $c>0$ and $E\subset [k_0,\infty)$, with Lebesgue measure $m(E)\leq \varepsilon$, such that $\|A_{k}^{-1}\|\leq c k^{2n+2+δ}$ on $[k_0,\infty)\setminus E$, i.e., the growth of $\|A_{k}^{-1}\|$ is at worst polynomial in $k$ if one avoids a set $E$ of arbitrarily small measure.
  As a corollary of these results we obtain the first $k$-explicit bounds on $\|S_k^{-1}\|$ and the condition number of $S_k$ for the case that $Γ$ is the boundary of a Lipschitz domain, or a screen not contained in a hyperplane, and analogous estimates for the case that $Γ$ is a $d$-set (and so of Hausdorff dimension $d$), for non-integer values of $d$.

</details>


### [26] [On some nonlocal, nonlinear diffusion problems](https://arxiv.org/abs/2601.19478)
*M. M. Chipot,A. Luthra,S. A. Sauter*

Main category: math.AP

TL;DR: Nonlocal nonlinear elliptic problems solved via fixed point methods in R with error estimates and numerical experiments


<details>
  <summary>Details</summary>
Motivation: To develop computational methods for solving nonlocal, nonlinear elliptic problems, which are challenging due to their nonlocal nature and nonlinearity

Method: Reduces the problem to a fixed point argument in R, enabling computational solution through iterative methods

Result: Provides error estimates for the computational approach and includes numerical experiments to validate the method

Conclusion: The fixed point approach in R provides an effective computational framework for solving nonlocal nonlinear elliptic problems with quantifiable accuracy

Abstract: This note is devoted to some nonlocal, nonlinear elliptic problems with an emphasis on the computation of the solution of such problems, reducing it in particular to a fixed point argument in R. Errors estimates and numerical experiments are provided.

</details>


### [27] [Mode stability of self-similar wave maps without symmetry in higher dimensions](https://arxiv.org/abs/2601.19515)
*Roland Donninger,Frederick Moscatelli*

Main category: math.AP

TL;DR: The paper extends mode stability results for self-similar blowup solutions of wave maps from Minkowski space to the d-sphere to all dimensions d ≥ 4, building on previous work for d=3.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the stability properties of explicit self-similar blowup solutions for wave maps in higher dimensions. While mode stability was known for corotational functions and recently proved for d=3 without symmetry assumptions, the question remained open for dimensions d ≥ 4.

Method: The paper implements the quasi-solution method with two additional parameters, which is a technical innovation. This approach allows for analyzing mode stability without symmetry restrictions in higher dimensions.

Result: The main result is the extension of mode stability without symmetry assumptions to all dimensions d ≥ 4 for self-similar blowup solutions of wave maps from Minkowski space to the d-sphere.

Conclusion: The paper successfully demonstrates that the explicit self-similar blowup solutions are mode stable in all dimensions d ≥ 4 without requiring symmetry assumptions, representing a significant technical advancement in the application of the quasi-solution method.

Abstract: We consider wave maps from $(1+d)$-dimensional Minkowski space into the $d$-sphere. For every $d \geq 3$, there exists an explicit self-similar solution that exhibits finite time blowup. This solution is corotational and its mode stability in the class of corotational functions is known. Recently, Weissenbacher, Koch, and the first author proved mode stability without symmetry assumptions in $d =3$. In this paper we extend this result to all $d \geq 4$. On a technical level, this is the first successful implementation of the quasi-solution method where two additional parameters are present.

</details>


### [28] [Blowup stability of wave maps without symmetry](https://arxiv.org/abs/2601.19516)
*Roland Donninger,Frederick Moscatelli*

Main category: math.AP

TL;DR: The paper proves asymptotic stability of self-similar blowup solutions for wave maps from (1+d)-dimensional Minkowski space into the d-sphere under small perturbations.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of explicit self-similar blowup solutions in wave maps without symmetry assumptions, which is important for understanding singularity formation in nonlinear wave equations.

Method: The authors study wave maps from (1+d)-dimensional Minkowski space into the d-sphere without symmetry assumptions. They analyze an explicit self-similar blowup solution and prove its asymptotic stability using rigorous mathematical analysis without numerical input.

Result: The explicit self-similar blowup solution is asymptotically stable under small perturbations of the initial data. The proof is fully rigorous and requires no numerical computations.

Conclusion: Self-similar blowup solutions for wave maps are stable under small perturbations, providing important insight into singularity formation in nonlinear wave equations.

Abstract: We study wave maps from $(1+d)$-dimensional Minkowski space into the $d$-sphere without any symmetry assumptions. There exists an explicit self-similar blowup solution and we prove that this solution is asymptotically stable under small perturbations of the initial data. The proof is fully rigorous and requires no numerical input whatsoever.

</details>


### [29] [$Γ$-convergence and homogenisation for free discontinuity functionals with linear growth in the space of functions with bounded deformation](https://arxiv.org/abs/2601.19591)
*Gianni Dal Maso,Davide Donati*

Main category: math.AP

TL;DR: The paper studies Γ-convergence of free discontinuity functionals with linear growth in BD space, proving compactness, integral representation, and applying to deterministic/stochastic homogenization.


<details>
  <summary>Details</summary>
Motivation: To analyze the Γ-convergence behavior of sequences of free discontinuity functionals with linear growth defined in the space of functions with bounded deformation (BD), which is important for understanding variational limits and homogenization problems in fracture mechanics and materials science.

Method: Proves compactness with respect to Γ-convergence, establishes properties of Γ-limits leading to integral representation, obtains integrands via limits of minimization problems on small cubes, and applies these results to deterministic and stochastic homogenization problems.

Result: A compactness result for Γ-convergence is established, main properties of Γ-limits are characterized, an integral representation theorem is proved, and these results are successfully applied to solve both deterministic and stochastic homogenization problems for free discontinuity functionals in BD.

Conclusion: The paper provides a comprehensive framework for studying Γ-convergence of free discontinuity functionals in BD space, with applications to homogenization problems, establishing important mathematical tools for analyzing variational limits in fracture and material science applications.

Abstract: We study the $Γ$-convergence of sequences of free discontinuity functionals with linear growth defined in the space ${\rm BD}$ of functions with bounded deformation. We prove a compactness result with respect to $Γ$-convergence and outline the main properties of the $Γ$-limits, which lead to an integral representation result. The corresponding integrands are obtained by taking limits of suitable minimisation problems on small cubes. These results are then used to study the deterministic and stochastic homogenisation problem for a large class of free discontinuity functionals defined in ${\rm BD}$.

</details>


### [30] [On the asymptotic behavior of the Repulsive Pressureless Euler-Poisson System](https://arxiv.org/abs/2601.19733)
*Nicholas Biglin,Joseph Crachiola,Jack Curtis,Thomas Kunz,Omkar Maralappanavar,Adrian Tudorascu*

Main category: math.AP

TL;DR: Study of asymptotic behavior of distributional solutions to 1D repulsive pressureless Euler-Poisson system, focusing on sticky particle dynamics, energy conservation, existence of perfect states, and finite-time collapse conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand the asymptotic behavior of the repulsive pressureless Euler-Poisson system, which models mass distributions where particles exert outward forces on each other. This has applications in understanding collective dynamics of repulsive particle systems with sticky collisions.

Method: The authors study discrete solutions with sticky particle collisions (particles stick together upon collision and move as one). They analyze the Hamiltonian (total energy) of the system, prove existence/uniqueness of "perfect" states, derive conditions for finite-time collapse, and provide analytical proofs supported by computer simulations.

Result: Key results include: 1) Proofs about the total energy behavior, 2) Existence and uniqueness of perfect states where Hamiltonian is constant and solutions converge to equilibrium (single stationary particle), 3) Necessary and sufficient conditions for finite-time collapse, 4) A quadratic envelope bounding collapsing solutions, 5) Various counterexamples illustrating unique behavior of repulsive sticky systems.

Conclusion: The repulsive pressureless Euler-Poisson system with sticky collisions exhibits rich asymptotic behavior, including convergence to equilibrium states and finite-time collapse under specific conditions. The analysis provides fundamental understanding of how repulsive forces interact with sticky collision dynamics in particle systems.

Abstract: The main objective of this paper is a study of the asymptotic behavior of distributional solutions to the one-dimensional repulsive pressureless Euler-Poisson system. The system is a model for the dynamics of a mass distribution evolving on \mathbb{R} whose masses exert outward forces on one another. A discrete (describing the evolution of finitely many particles) solution is called sticky if, upon collision, particles stick together and move as one for all subsequent time, according to the conservation of mass and momentum principles. We prove results on the total energy (Hamiltonian) of the system and demonstrate the existence and uniqueness of so-called "perfect" states, where the Hamiltonian is constant over all time and the solution converges to equilibrium, a single stationary particle. We provide a necessary and a sufficient condition for finite-time collapse, and present a quadratic envelope within which a solution must remain in order to collapse. We demonstrate various (counter)examples that illustrate the unique behavior of the repulsive scheme with the sticky condition, analytically and with a computer simulation.

</details>


### [31] [Universality in the Low Mach number limit via a convex integration framework](https://arxiv.org/abs/2601.19744)
*Robin Ming Chen,Alexis Vasseur,Dehua Wang,Cheng Yu*

Main category: math.AP

TL;DR: The paper shows that any incompressible Euler solution can be obtained as the strong limit of convex integration solutions to compressible Euler equations as Mach number → 0, establishing incompressible system as a universal attractor.


<details>
  <summary>Details</summary>
Motivation: To understand the low Mach number limit of compressible Euler equations from the perspective of weak solution theory and convex integration, and to establish the incompressible system as a universal attractor for compressible flows.

Method: Uses a refined convex integration scheme to construct families of weak solutions to compressible Euler equations corresponding to any prescribed L^2 weak solution of incompressible Euler equations, then analyzes the limit as Mach number tends to zero.

Result: Proves that the constructed family of compressible solutions converges strongly to the given incompressible solution as Mach number → 0, demonstrating that every incompressible flow can be realized as the limit of convex integration solutions to the compressible system.

Conclusion: The incompressible Euler system acts as a universal attractor for compressible Euler equations in the low Mach number limit, revealing a new form of universality for singular limits and providing a rigorous weak solution framework for understanding the incompressible limit.

Abstract: We study the low Mach number limit of the compressible Euler equations through the lens of convex integration. For any prescribed $L^2$ weak solution of the incompressible Euler equations, we construct a corresponding family of weak solutions to the compressible Euler equations via a refined convex integration scheme. We then prove that, as the Mach number tends to zero, this family of solutions converges strongly to the given incompressible solution. This result demonstrates that the incompressible system acts as a universal attractor in this setting: every incompressible flow can be realized as the limit of convex integration solutions to the compressible system. Our approach highlights a new form of universality for singular limits and provides a rigorous framework for understanding the incompressible limit from the perspective of weak solution theory.

</details>


### [32] [Methods in studying qualitative properties of fractional equations](https://arxiv.org/abs/2601.19783)
*Wenxiong Chen,Yahong Guo,Congming Li*

Main category: math.AP

TL;DR: Systematic review of methods for studying qualitative properties of solutions to fractional equations, covering extension method, moving planes/spheres, blow-up techniques, sliding method, and regularity approaches with comparative analysis.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive handbook for researchers studying fractional equations by systematically reviewing and comparing various effective methods for analyzing qualitative properties of solutions.

Method: Systematic review approach covering: extension method, method of moving planes in integral forms, direct method of moving planes, method of moving spheres, blow-up and rescaling techniques, sliding method, regularity lifting, and interior/boundary regularity estimates. Uses simple examples to demonstrate application.

Result: Comprehensive overview of methods with comparative discussion of their respective strengths and limitations, presented as a practical handbook for researchers in fractional equations.

Conclusion: This paper serves as a useful reference guide for researchers, providing systematic coverage of methods for studying qualitative properties of solutions to fractional equations with practical examples and comparative analysis.

Abstract: In this paper, we systematically review a series of effective methods for studying the qualitative properties of solutions to fractional equations. Beginning with the pioneering extension method and the method of moving planes in integral forms, we introduce a variety of direct methods, including the direct method of moving planes, the method of moving spheres, blow-up and rescaling techniques, the sliding method, regularity lifting, and approaches for interior and boundary regularity estimates.
  To elucidate the core ideas behind these methods, we employ simple examples that demonstrate how they can be applied to investigate qualitative properties of solutions. We also provide a comparative discussion of their respective strengths and limitations. It is our hope that this paper will serve as a useful handbook for researchers engaged in the study of fractional equations.

</details>


### [33] [A general theory of nonlocal elasticity based on nonlocal gradients and connections with Eringen's model](https://arxiv.org/abs/2601.19797)
*J. C. Bellido,G. García-Sáez*

Main category: math.AP

TL;DR: Develops a general theory of nonlocal linear elasticity using nonlocal gradients with radial kernels, establishes existence/uniqueness results, connects to Eringen's model, and proves localization to classical elasticity.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive mathematical foundation for nonlocal elasticity theories by developing a general framework that unifies different approaches and connects them to classical elasticity.

Method: Starts from nonlocal hyperelastic energy functional, performs formal linearization around identity deformation to obtain nonlocal linear elasticity equations. Uses nonlocal gradients with general radial kernels, establishes Korn-type inequality for nonlocal gradients, and proves existence/uniqueness for Dirichlet and Neumann boundary conditions.

Result: Established existence and uniqueness of weak solutions for both boundary conditions, proved general Korn-type inequality for nonlocal gradients, showed Eringen's model is a special case, and proved localization results showing convergence to classical elasticity as interaction horizon vanishes or fractional parameter approaches one.

Conclusion: Provides a unified mathematical foundation for nonlocal elasticity theories that encompasses existing models, establishes rigorous mathematical properties, and demonstrates connection to classical elasticity through localization results.

Abstract: We develop a general theory of nonlocal linear elasticity based on nonlocal gradients with general radial kernels. Starting from a nonlocal hyperelastic energy functional, we perform a formal linearization around the identity deformation to obtain a system of nonlocal linear elasticity equations. We establish the existence and uniqueness of weak solutions for both Dirichlet and Neumann boundary conditions, proving a general Korn-type inequality for nonlocal gradients. We show that this framework encompasses Eringen's nonlocal elasticity model as a particular case, establishing an explicit connection between the two formulations. Finally, we prove localization results demonstrating that solutions to the nonlocal problems converge to their classical local counterparts in two different regimes: as the interaction horizon vanishes and, in the fractional case, as the fractional parameter approaches one. These results provide a comprehensive and unified mathematical foundation for nonlocal elasticity theories.

</details>


### [34] [A priori estimates of stable and finite Morse index solutions to elliptic equations that arise in Physics](https://arxiv.org/abs/2601.19801)
*J. Silverio Martínez-Baena*

Main category: math.AP

TL;DR: Thesis studies qualitative properties of solutions to nonlinear elliptic equations, focusing on regularity, stability, and multiplicity. Key contributions include a counterexample showing bounded radial Morse index doesn't prevent singularities in dimensions 3-9, optimal regularity for Hardy-Hénon equations, and existence/multiplicity results for equations with vanishing coefficients.


<details>
  <summary>Details</summary>
Motivation: To understand qualitative properties of solutions to nonlinear elliptic equations arising from physical phenomena, particularly how stability and Morse index influence regularity, and to address open problems in the field including the Brezis-Vázquez regularity conjecture.

Method: Uses modern framework of solution stability and Morse index theory, constructs counterexamples, establishes optimal regularity results for radial solutions, and applies variational and topological methods for existence and multiplicity proofs.

Result: 1) Constructed counterexample showing bounded radial Morse index doesn't prevent singular behavior in dimensions 3-9, challenging extension of Brezis-Vázquez conjecture. 2) Established optimal regularity for radial solutions of non-autonomous Hardy-Hénon equation. 3) Proved existence of multiple distinct solutions for equations with vanishing coefficients.

Conclusion: The thesis advances understanding of regularity-stability relationships in nonlinear elliptic equations, provides important counterexamples to natural conjectures, establishes optimal regularity results, and demonstrates existence of multiple solutions. Future directions include extending techniques to non-autonomous problems and applications to theoretical physics field theories.

Abstract: This thesis studies qualitative properties of solutions to nonlinear elliptic equations of Poisson type with Dirichlet boundary conditions that arise from some physical phenomena, with a particular focus on regularity, stability, and multiplicity of solutions. Building on the modern framework of solution stability and Morse index theory, the work investigates how these notions influence regularity in nonlinear elliptic problems. A central contribution is the construction of a counterexample showing that bounded radial Morse index does not prevent singular behavior of solutions in dimensions three through nine, challenging a natural extension of the Brezis-Vázquez regularity conjecture. In addition, optimal regularity results are established for radial solutions of a non-autonomous Hardy-Hénon equation, identifying the precise range of dimensions for which regularity holds. The thesis also addresses existence and multiplicity results for elliptic equations involving nonlinearities with spatially vanishing coefficients. Under suitable assumptions, the existence of multiple distinct solutions is proved using variational and topological methods. Finally, the thesis outlines several directions for future research, including extensions of stability-based regularity techniques to non-autonomous problems and potential applications of these techniques to field theories arising in theoretical physics.

</details>


### [35] [On $α$-entropy solutions of a nonlocal thin film equation: existence and finite speed of propagation](https://arxiv.org/abs/2601.19808)
*Antonio Segatti,Roman Taranets*

Main category: math.AP

TL;DR: First α-entropy estimate for nonlocal thin film equations with spectral fractional Laplacian, proving finite speed of propagation and waiting time phenomenon.


<details>
  <summary>Details</summary>
Motivation: To understand the interaction between nonlocal effects and classical thin film dynamics, particularly how nonlocal diffusion affects solution behavior like support propagation and waiting time phenomena.

Method: Developed α-entropy estimate for nonlocal thin film equations with spectral fractional Laplacian and homogeneous Neumann boundary conditions, then created a localized version of this estimate.

Result: Established first α-entropy estimate providing essential a priori bounds, proved finite speed of propagation (compact support for positive times), and found sufficient condition for waiting time phenomenon.

Conclusion: Nonlocal thin film equations exhibit new features in solution behavior compared to classical models, with the α-entropy estimate being a key tool for analyzing regularity and long-time dynamics.

Abstract: We consider an initial-boundary value problem for a class of nonlocal thin film equations governed by the spectral fractional Laplacian with homogeneous Neumann boundary conditions. We were the first to establish an $α$-entropy estimate for nonlocal thin film equations, which yields essential a priori bounds for the regularity and long-time behavior of weak solutions. By developing a localized version of this estimate, we prove finite speed of propagation, showing that the support of nonnegative solutions remains compact for positive times. Furthermore, we find a sufficient condition for a waiting time phenomenon, whereby the solution remains identically zero in a region for a nontrivial time interval. These results highlight new features in the interaction between nonlocal effects and classical thin film dynamics.

</details>


### [36] [Convergence of a two-parameter hyperbolic relaxation system toward the incompressible Navier-Stokes equations](https://arxiv.org/abs/2601.19846)
*Qian Huang,Christian Rohde,Ruixi Zhang*

Main category: math.AP

TL;DR: Two-parameter hyperbolic relaxation method for incompressible Navier-Stokes with convergence proofs for velocity and pressure under different initial perturbation regimes.


<details>
  <summary>Details</summary>
Motivation: To develop a robust hyperbolic relaxation approximation for incompressible Navier-Stokes equations that can handle different initial perturbation regimes while ensuring convergence to the Navier-Stokes limit.

Method: Combines first-order relaxation and artificial compressibility method. Uses intermediate affine system for pressure error estimates, modulated energy structure, and delicate bootstrap arguments for convergence analysis.

Result: Proved simultaneous convergence of fluid velocity and pressure to Navier-Stokes limit in 3D with vanishing initial perturbations. Extended velocity convergence to O(1) initial perturbations with global-in-time recovery in both 2D and 3D.

Conclusion: The hyperbolic relaxation method provides rigorous convergence guarantees for incompressible Navier-Stokes approximations under various initial conditions, establishing a solid theoretical foundation for this numerical approach.

Abstract: We investigate a two-parameter hyperbolic relaxation approximation to the incompressible Navier-Stokes equations, incorporating a first-order relaxation and the artificial compressibility method. With vanishingly small perturbations of initial velocity, we rigorously prove the simultaneous convergence of fluid velocity and pressure toward the Navier-Stokes limit in the three-dimensional case by constructing an intermediate affine system to obtain the necessary error estimates for the pressure. Furthermore, we extend the velocity convergence analysis to the case of $\mathcal O(1)$ initial velocity perturbations, and establish the global-in-time recovery of the velocity field using a modulated energy structure and delicate bootstrap arguments in both two- and three-dimensional settings.

</details>


### [37] [Viscosity Solutions in Martinet Spaces](https://arxiv.org/abs/2601.19864)
*Thomas Bieske,Frederic Bowen*

Main category: math.AP

TL;DR: Study of viscosity solutions in Martinet spaces, proving uniqueness for monotone elliptic PDEs and infinite Laplace equation


<details>
  <summary>Details</summary>
Motivation: Martinet spaces lack the algebraic structure of Carnot groups and triangular vector fields of Grushin-type spaces, creating a gap in understanding viscosity solutions in these more general sub-Riemannian settings

Method: Establish properties of viscosity solutions in Martinet spaces, then prove uniqueness results for two classes of equations: strictly monotone elliptic PDEs and the infinite Laplace equation

Result: Successfully established viscosity solution properties in Martinet spaces and proved uniqueness for both strictly monotone elliptic PDEs and the infinite Laplace equation in this setting

Conclusion: The paper extends viscosity solution theory to Martinet spaces, filling an important gap in sub-Riemannian analysis by handling spaces without Carnot group or Grushin-type structure

Abstract: In this paper, we establish the properties of viscosity solutions in Martinet spaces, which lack both the algebraic group law of Carnot groups and the triangular vector fields of Grushin-type spaces. We then prove the uniqueness of viscosity solutions to strictly monotone elliptic PDEs and to the infinite Laplace equation.

</details>


### [38] [Nonlocal Boundary Value Problems Governed by Symmetric Nonlocal Operators](https://arxiv.org/abs/2601.19872)
*Leonhard Frerick,Julia Huschens,Michael Vu*

Main category: math.AP

TL;DR: The paper develops a Hilbert space framework for nonlocal boundary value problems with general symmetric transition kernels, extending beyond standard nonlocal operators.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of nonlocal boundary value problems from operators with kernel functions to more general operators with symmetric transition kernels, which allows for broader applications including discrete settings.

Method: Develops a classical Hilbert space approach for solving weak formulations of nonlocal Dirichlet and Neumann problems with general symmetric transition kernels, using a variational framework.

Result: Establishes a theoretical framework for nonlocal boundary value problems with symmetric transition kernels, including existence and uniqueness results, and demonstrates the approach with the discrete Poisson problem on the unit cube.

Conclusion: The paper successfully extends nonlocal boundary value theory to operators with symmetric transition kernels, providing a flexible framework that encompasses both continuous and discrete settings, with the discrete Poisson problem serving as a concrete example.

Abstract: Nonlocal boundary value problems with Dirichlet or Neumann boundary are well-studied for nonlocal operators of the type $\mathcal{L}_γu = \operatorname{PV} \int_{\mathbb{R}^d} \big(u(\cdot)-u(y)\big) γ(\cdot,y) \, \mathrm{d}y$ where the underlying kernel function $γ: \mathbb{R}^d \times \mathbb{R}^d \rightarrow [0,\infty)$ is assumed to be measurable and symmetric. In this paper, a theory is introduced for problems whose governing operator is of the more general type \[\mathcal{L}u:= \operatorname{PV} \int_{\mathbb{R}^d}\big(u(\cdot)-u(y)\big) \, K(\cdot, \mathrm{d}y)\] where ${K: \mathbb{R}^d \times \mathcal{B}(\mathbb{R}^d) \rightarrow [0,\infty]}$ is a symmetric transition kernel. Our main focus is on nonlocal Dirichlet and Neumann problems and a classical Hilbert space approach is developed for solving designated weak formulations. As an example, the discrete Poisson problem on $Ω=(0,1)^d$ is discussed.

</details>


### [39] [Lane--Emden Systems with Singular Nonlinearities for the Fully Nonlinear Elliptic Operator](https://arxiv.org/abs/2601.19874)
*Karan Rathore,Mohan Mallick,Ram Baran Verma*

Main category: math.AP

TL;DR: The paper studies existence, uniqueness, non-existence, and regularity of positive solutions to a fully nonlinear elliptic system with singular nonlinearities and Dirichlet boundary conditions.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by extending the classical Lane-Emden system results to more general fully nonlinear elliptic operators with singular nonlinearities, where the nonlinearities involve reciprocal powers of the solution components.

Method: The authors analyze the system using techniques from fully nonlinear elliptic equations, variational methods, and comparison principles. They establish conditions on parameters p, q, r, s that determine solution behavior through careful analysis of the singular nonlinear structure.

Result: The paper establishes precise conditions on parameters p, q, r, s that guarantee existence, uniqueness, or non-existence of positive solutions. It also provides regularity results for solutions when they exist.

Conclusion: The parameter relationships p, q, r, s play a crucial role in determining solution behavior, with the results extending and generalizing known results for Lane-Emden type systems to fully nonlinear elliptic operators with singular nonlinearities.

Abstract: Consider \[ \begin{cases} F(D^2 u,Du,u,x) = u^{-p}v^{-q},~\text{in}~Ω\\ F(D^2 v,Dv,v,x)=u^{-r}v^{-s},~~\text{in}~~Ω\\ u,v>0~~\text{in}~~Ω\\ u=v=0~\quad~\text{on}~~\partialΩ, \end{cases} \] where $Ω$ is an open connected subset of $\mathbb{R}^{N}$ and $p,s$ are two non-negative and $q,r$ are positive real numbers. This article discuses the conditions in terms of the relations among $p,q,r$ and $s$ which lead to existence, uniqueness and non-existence of positive solutions to the system. Furthermore, we also have studied some regularity properties of solution of the system. These results are inspired by the study of Lane-Emden system of equations as in \cite{busca2002liouville,ghergu2010lane}.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [40] [Tensorized Discontinuous Isogeometric Analysis Method for the 2-D Time-Independent Linearized Boltzmann Transport Equation](https://arxiv.org/abs/2601.18925)
*Patrick A. Myers,Joseph A. Bogdan,Majdi I. Radaideh,Brian C. Kiedrowski*

Main category: physics.comp-ph

TL;DR: TDIGA method combines tensor train format with discontinuous isogeometric analysis for neutron transport problems, achieving massive storage compression but with computational trade-offs requiring mixed TT/CSR formats.


<details>
  <summary>Details</summary>
Motivation: To enable high-fidelity neutron transport simulations on expensive high-order IGA meshes while addressing the massive storage requirements of traditional sparse matrix formats through tensor compression techniques.

Method: Tensorized Discontinuous Isogeometric Analysis (TDIGA) with tensor train (TT) format for operator assembly, applied to 2-D linearized Boltzmann transport equation with discrete ordinates, multigroup energy, and IGA in space. Uses mixed TT/CSR formats for interior/boundary operators.

Result: TT format compresses interior operators from petabytes to megabytes (vs CSR gigabytes), but boundary operators remain challenging. Mixed TT/CSR format enables high-fidelity transport, though CSR remains <10× faster. Validated against Monte Carlo and analytic solutions.

Conclusion: TDIGA enables high-fidelity transport for expensive IGA meshes through tensor compression, but computational trade-offs require mixed formats. While slower than CSR, it makes previously infeasible problems tractable by reducing storage requirements dramatically.

Abstract: We present the novel Tensorized Discontinuous Isogeometric Analysis (TDIGA) method applied to the discontinuous Galerkin (DG) time-independent 2-D linearized Boltzmann transport equation (LBTE) with higher-order scattering, discretized with discrete ordinates in angle, multigroup in energy, and isogeometric analysis (IGA) in space. We formulate operator assembly in the tensor train (TT) format, producing seven-dimensional operators for both fixed-source and $k$-eigenvalue neutron transport problems solved using the restarted Generalized Minimum Residual Method (GMRES) and power iteration with an uncompressed solution vector. Our results on single-patch homogeneous and multi-patch heterogeneous problems, including a cruciform-shaped fuel array inspired by advanced reactor fuel designs, demonstrate the TT format's ability to compress interior operators from petabytes to megabytes, whereas the Compressed Sparse Row (CSR) matrix format requires gigabytes of storage. However, highly coupled boundary operators present a significant challenge for TT. Despite the storage savings, TT formatted operators increase time-to-solution relative to CSR as an uncompressed solution vector forces operator-vector product scaling of $O(dr^2N^d\log(N))$ for TT while CSR scales at $O(\text{nnz})$. We mitigate this discrepancy by using mixed formats with interior operators in TT, while high-rank boundary operators remain in CSR format. We compare all results to Monte Carlo (MC) and analytic reference solutions. While CSR remains $<10\times$ faster than this mixed format, the TDIGA method enables high-fidelity transport for expensive high-order IGA meshes.

</details>


### [41] [qNEP: A highly efficient neuroevolution potential with dynamic charges for large-scale atomistic simulations](https://arxiv.org/abs/2601.19034)
*Zheyong Fan,Benrui Tang,Esmée Berger,Ethan Berger,Erik Fransson,Ke Xu,Zihan Yan,Zhoulin Liu,Zichen Song,Haikuan Dong,Shunda Chen,Lei Li,Ziliang Wang,Yizhou Zhu,Julia Wiktor,Paul Erhart*

Main category: physics.comp-ph

TL;DR: The paper introduces qNEP, a charge-aware extension of neuroevolution potentials that enables efficient large-scale simulations with explicit electrostatics for dielectric properties, IR spectra, and field-matter coupling.


<details>
  <summary>Details</summary>
Motivation: Existing machine-learned interatomic potentials with electrostatics are computationally demanding, limiting large-scale, long-time simulations of electrostatics-driven phenomena like dielectric response, infrared activity, and field-matter coupling.

Method: Extends neuroevolution potential (NEP) to qNEP by introducing explicit, environment-dependent partial charges represented by neural networks as functions of local descriptor vectors. Derives consistent expressions for forces and virials accounting for position-dependent charges, implemented in GPUMD with Ewald summation and particle-particle particle-mesh treatments.

Result: qNEP enables direct prediction of Born effective charge tensors and polarization, allowing evaluation of dielectric properties, infrared spectra, and coupling to external electric fields. Demonstrated accuracy and efficiency on water, Li7La3Zr2O12, BaTiO3, and magnesium-water interface systems.

Conclusion: qNEP enables accurate atomistic simulations with explicit long-range electrostatics, scalable to million-atom systems on nanosecond time scales using consumer-grade GPUs, providing a unified framework for electrostatics-driven phenomena.

Abstract: Although electrostatics can be incorporated into machine-learned interatomic potentials, existing approaches are computationally very demanding, limiting large-scale, long-time simulations of electrostatics-driven phenomena such as dielectric response, infrared activity, and field-matter coupling. Here, we extend the neuroevolution potential (NEP), a highly efficient machine-learned interatomic potential, to a charge-aware framework (qNEP) by introducing explicit, environment-dependent partial charges. Each ionic partial charge is represented by a neural network as a function of the local descriptor vector, analogous to the NEP site-energy model. This formulation enables the direct prediction of the Born effective charge tensor for each ion and, consequently, the polarization. As a result, dielectric properties, infrared spectra, and coupling to external electric fields can be evaluated within a unified framework. We derive consistent expressions for the forces and virials that explicitly account for the position dependence of the partial charges. The qNEP method has been implemented in the free-and-open-source GPUMD package, with support for both Ewald summation and particle-particle particle-mesh treatments of electrostatics. We demonstrate the accuracy and efficiency of the qNEP approach through representative applications to water, Li7La3Zr2O12, BaTiO3, and a magnesium-water interface. These results show that qNEP enables accurate atomistic simulations with explicit long-range electrostatics, scalable to million-atom systems on nanosecond time scales using consumer-grade GPUs.

</details>


### [42] [Transformer Learning of Chaotic Collective Dynamics in Many-Body Systems](https://arxiv.org/abs/2601.19080)
*Ho Jang,Gia-Wei Chern*

Main category: physics.comp-ph

TL;DR: Transformers with self-attention effectively model chaotic many-body dynamics by learning non-Markovian reduced descriptions that capture statistical climate despite pointwise prediction divergence.


<details>
  <summary>Details</summary>
Motivation: Chaotic many-body systems present fundamental challenges for reduced descriptions: microscopic equations are Markovian but collective observables exhibit strong memory effects and exponential sensitivity to initial conditions, making conventional approaches inadequate.

Method: A self-attention-based transformer framework is used to model chaotic collective dynamics directly from time-series data. The transformer selectively reweights long-range temporal correlations to learn a non-Markovian reduced description, overcoming limitations of conventional recurrent architectures.

Result: Applied to the 1D semiclassical Holstein model with interaction quenches inducing strongly nonlinear chaotic dynamics of charge-density-wave order parameter. While pointwise predictions inevitably diverge at long times, the transformer faithfully reproduces the statistical "climate" of the chaos, including temporal correlations and characteristic decay scales.

Conclusion: Self-attention provides a powerful mechanism for learning effective reduced dynamics in chaotic many-body systems, establishing transformers as effective tools for capturing statistical properties of chaotic collective behavior despite inherent prediction limitations.

Abstract: Learning reduced descriptions of chaotic many-body dynamics is fundamentally challenging: although microscopic equations are Markovian, collective observables exhibit strong memory and exponential sensitivity to initial conditions and prediction errors. We show that a self-attention-based transformer framework provides an effective approach for modeling such chaotic collective dynamics directly from time-series data. By selectively reweighting long-range temporal correlations, the transformer learns a non-Markovian reduced description that overcomes intrinsic limitations of conventional recurrent architectures. As a concrete demonstration, we study the one-dimensional semiclassical Holstein model, where interaction quenches induce strongly nonlinear and chaotic dynamics of the charge-density-wave order parameter. While pointwise predictions inevitably diverge at long times, the transformer faithfully reproduces the statistical "climate" of the chaos, including temporal correlations and characteristic decay scales. Our results establish self-attention as a powerful mechanism for learning effective reduced dynamics in chaotic many-body systems.

</details>


### [43] [Rimu.jl: Random integrators for many-body quantum systems](https://arxiv.org/abs/2601.19505)
*Matija Čufar,C. J. Bradly,Ray Yang,Elke Pahl,Joachim Brand*

Main category: physics.comp-ph

TL;DR: Rimu.jl is a Julia package for solving many-body quantum problems using FCIQMC and exact diagonalization methods with high-performance computing capabilities.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient, high-performance computing solution for solving many-body quantum problems with both stochastic and exact methods in a unified Julia package.

Method: Matrix-free implementation of Hamiltonians, compact Fock state representation, implementation of FCIQMC algorithm (projector QMC), inclusion of model Hamiltonians, and interface for exact diagonalization via external eigenvalue solvers.

Result: A comprehensive Julia package (Rimu.jl) that enables efficient solution of many-body quantum problems through both stochastic FCIQMC and exact diagonalization methods with CommonSolve.jl interface.

Conclusion: Rimu.jl provides a powerful, efficient tool for quantum many-body calculations with both stochastic and exact methods, suitable for high-performance computing applications in quantum physics.

Abstract: Rimu.jl is a Julia package for solving many-body quantum problems. The core of the package is a matrix-free implementation of Hamiltonians and other operators and compact representation of Fock states, which together allow for efficient methods suitable for high-performance computing. Rimu.jl includes a Julia implementation of the full configuration interaction quantum Monte Carlo (FCIQMC) algorithm which is a type of projector QMC algorithm for stochastically solving the time-independent Schrödinger equation. It also includes many well-known model Hamiltonians, and an interface for exact diagonalisation based on external eigenvalue solvers. Both the stochastic and exact diagonalisation methods are accessed with a CommonSolve.jl interface. We describe the FCIQMC algorithm and how to obtain estimators of observables as well as the key features of the implementation.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [44] [Energy partition in collisionless counterstreaming plasmas](https://arxiv.org/abs/2601.18988)
*Alexis Marret,Frederico Fiuza*

Main category: physics.plasm-ph

TL;DR: 3D kinetic simulations show counter-streaming plasmas undergo two-stage magnetic field amplification via Weibel instability and filament kinking, with electron heating via magnetic pumping. Final electron-ion temperature ratio depends on mass ratio, with electrons gaining only a few percent of initial ion kinetic energy in electron-proton flows.


<details>
  <summary>Details</summary>
Motivation: Understanding energy redistribution between plasma species in collisionless astrophysical environments (supernova remnants, AGN jets) where counter-streaming outflows drive magnetic field amplification, heating, and particle acceleration.

Method: 3D fully-kinetic simulations of weakly magnetized counter-propagating plasmas to investigate energy partition between species.

Result: Two-stage magnetic field amplification: early Weibel instability and late filament kinking via dynamo mechanism. Electron heating primarily during late phase via magnetic pumping. Final temperature ratio T_e/T_i and energy partition depend on ion-to-electron mass ratio. For electron-proton flows, electron thermal energy reaches only a few percent of initial ion kinetic energy.

Conclusion: Energy partition in collisionless counter-streaming plasmas involves complex interplay of instabilities and mechanisms, with electron heating limited and dependent on mass ratios, explaining observed energy distributions in astrophysical environments.

Abstract: Fast, counter-streaming plasma outflows drive magnetic field amplification, plasma heating, and particle acceleration in numerous astrophysical environments, from supernova remnant shocks to active galactic nuclei jets. Understanding how, in the absence of Coulomb collisions, energy is redistributed between the different plasma species remains a fundamental open question. We use 3D fully-kinetic simulations to investigate energy partition in weakly magnetized counter-propagating plasmas. Our results reveal a complex interplay between different processes, where at early times the Weibel instability drives a first stage of magnetic field amplification and at late times the kinking of current filaments drives a second amplification stage via a dynamo-type mechanism. Electrons are heated primarily during the latter phase through magnetic pumping. By the time the flows thermalize, we observe that the final temperature ratio $T_e/T_i$ and energy partition depend on the ion-to-electron mass ratio. For electron-proton flows, the electron thermal energy only reaches up to a few percent of the initial ion kinetic energy.

</details>


### [45] [Effect of Controlled Magnetic Island Bifurcation on Electron Diffusion](https://arxiv.org/abs/2601.19073)
*Jessica Eskew,D. M. Orlov,B. Andrew,E. Bursch,M. Koepke,F. Skiff,M. E. Austin,T. Cote,C. Marini,E. G. Kostadinova*

Main category: physics.plasm-ph

TL;DR: Magnetic island bifurcations alter electron transport pathways in plasmas, with topological changes between q=2/1 and q=4/2 structures creating different diffusion regimes (classical, subdiffusive, superdiffusive) depending on launch location.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic island bifurcations affect cross-field electron transport in magnetized plasmas, particularly how changes in island topology (number/location of O-points, X-points, separatrix boundaries) alter diffusion pathways and particle confinement.

Method: Used field line tracing code TRIP3D with collisional operator to simulate electron transport. Launched thermal tracer electrons from different locations (O-points, X-points, outside separatrix boundaries) during island bifurcation experiments on DIII-D where external magnetic perturbations rotated and periodically bifurcated islands on q=2 surface.

Result: Distinct diffusion regimes observed: classical, subdiffusive, and superdiffusive behavior depending on both dominant island mode (q=2/1 vs q=4/2) and electron launch location. Island bifurcation alters electron diffusion across rational surfaces, affecting particle confinement.

Conclusion: Magnetic island bifurcation significantly influences electron transport by modifying diffusion pathways. The findings provide insight into conditions where electron trapping into islands or separatrix stochastization can enable additional mechanisms like energetic electron generation.

Abstract: Magnetic islands strongly influence cross-field electron transport in magnetized plasmas. In particular, bifurcations of the island topology modify the number and location of O-points, X-points, and separatrix boundaries, thereby altering diffusion pathways. In recent DIII-D experiments, external magnetic perturbations were used to rotate and periodically bifurcate the island on the q = 2 surface, causing a switchback between a q = 2/1-dominated structure and a narrower q = 4/2-dominated structure. To investigate how this topological change affects electron transport, we employ the field line tracing code TRIP3D with an implemented collisional operator. Thermal, tracer electrons launched from O-points, X-points, and outside separatrix boundaries reveal distinct diffusion regimes, including classical, subdiffusive, and superdiffusive behavior, depending on both the dominant island mode and launch location. These results suggest that island bifurcation can alter electron diffusion across rational surfaces, with direct implications for particle confinement. While the present work emphasizes diffusion as a general framework, the findings provide insight into the conditions under which electron trapping into an island or stochastization of the island's separatrix can enable additional mechanisms, such as the generation of energetic electrons.

</details>


### [46] [Magnetic Compression of Compact Tori Experiment and Simulation](https://arxiv.org/abs/2601.19291)
*Carl Dunlea*

Main category: physics.plasm-ph

TL;DR: A magnetic compression experiment using compact torus plasma was conducted with improved levitation fields that reduced impurities and enhanced compression performance, despite kink instabilities. A new computational framework (DELiTE) was developed for MHD modeling.


<details>
  <summary>Details</summary>
Motivation: To study plasma physics applicable to magnetic target fusion compression through repetitive non-destructive testing, with the goal of improving compact torus formation, levitation, and compression performance for fusion applications.

Method: Formed compact torus plasma using coaxial gun into hour-glass shaped containment region with insulating outer wall. Used external coil currents for radial levitation and rapid compression. Developed DELiTE computational framework for spatial discretization of PDEs on unstructured triangular grids in axisymmetric geometry, implementing single-fluid two-temperature MHD model.

Result: Optimal coil configuration improved levitated CT lifetime and recurrence rate of shots with good compressional flux conservation. Reduced plasma impurities by suppressing plasma-wall interaction. Observed significant increases in magnetic field, electron density, and ion temperature during compression despite external kink mode instability. Matching decay rates of levitation and CT currents reduced MHD activity and increased probability of long-lived CTs.

Conclusion: The experiment successfully demonstrated improved magnetic compression performance through optimized levitation fields and coil configurations, while the DELiTE framework provides a computational tool for modeling such plasma systems. The approach shows promise for magnetic target fusion applications despite remaining challenges with plasma instabilities.

Abstract: The magnetic compression experiment at General Fusion was a repetitive non-destructive test to study plasma physics applicable to magnetic target fusion compression. A compact torus (CT) is formed with a co-axial gun into a containment region with an hour-glass shaped inner flux conserver, and an insulating outer wall. External coil currents keep the CT off the outer wall (radial levitation) and then rapidly compress it inwards. The optimal external coil configuration greatly improved both the levitated CT lifetime and the recurrence rate of shots with good compressional flux conservation. As confirmed by spectrometer data, the improved levitation field profile reduced plasma impurity levels by suppressing the interaction between plasma and the insulating outer wall during the formation process. Significant increases in magnetic field, electron density, and ion temperature were routinely observed at magnetic compression in the final external coil configuration tested, despite the prevalence of an instability, thought be an external kink mode, at compression. Matching the decay rate of the levitation currents to that of the CT currents resulted in a reduced level of MHD activity associated with unintentional compression by the levitation field, and a higher probability of long-lived CTs. The DELiTE (Differential Equations on Linear Triangular Elements) framework was developed for spatial discretisation of partial differential equations on an unstructured triangular grid in axisymmetric geometry. The framework is based on discrete differential operators in matrix form, which are derived using linear finite elements and mimic some of the properties of their continuous counterparts. A single-fluid two-temperature MHD model is implemented in this framework.

</details>


### [47] [Sensitivity of External Magnetic Field on the Change in Cross-section of a Toroidal Current](https://arxiv.org/abs/2601.19401)
*Suman Aich,Joydeep Ghosh,Rakesh L. Tanna,D. Raju,Sameer Kumar,Aditya-U team*

Main category: physics.plasm-ph

TL;DR: The study validates numerical predictions about magnetic field sensitivity to toroidal current column cross-sectional area using experimental measurements from Aditya Upgrade tokamak plasma.


<details>
  <summary>Details</summary>
Motivation: To experimentally validate previous numerical findings that magnetic field sensitivity to toroidal current column cross-sectional area depends on location, and to confirm the existence of an angle of invariance in toroidal geometries.

Method: Using measured magnetic field data from Aditya Upgrade tokamak plasma to test numerical predictions about magnetic field behavior in toroidal current columns.

Result: Experimental validation of numerical predictions regarding location-dependent magnetic field sensitivity to cross-sectional area and confirmation of angle of invariance in toroidal geometries.

Conclusion: The study provides experimental evidence supporting previous numerical observations about magnetic field behavior in toroidal current columns, confirming theoretical predictions with tokamak plasma measurements.

Abstract: Due to any toroidal current column, the magnetic field is found to be sensitive as well as insensitive to its cross-sectional area depending on location of subject point, as predicted by numerical approaches [S. Aich, J. Thakkar, and J. Ghosh, Plasma Fusion Res. 17, 2403055 (2022)], and hence the presence of an angle of invariance is found to be present for any toroidal geometry. Present study aims to validate those numerical observations using the measured magnetic field due to Aditya Upgrade tokamak plasma.

</details>


### [48] [Advanced Shaping of Quasi-Bessel Beams for High-Intensity Applications](https://arxiv.org/abs/2601.19554)
*Jérôme Touguet,Igor Andriyash,Ronan Lahaye,Guillaume Chapelant,Julien Gautier,Lucas Rovige,Cédric Thaury*

Main category: physics.plasm-ph

TL;DR: Researchers identify the cause of oscillations in quasi-Bessel beams from axiparabolas and develop a method to control longitudinal intensity profiles for high-intensity laser applications.


<details>
  <summary>Details</summary>
Motivation: Quasi-Bessel beams produced by axiparabolas are used in high-intensity laser applications but suffer from unwanted oscillations in their longitudinal profiles, limiting their effectiveness in applications like laser-plasma acceleration and advanced photon sources.

Method: Combined analytical insight with numerical and experimental validation to understand the physical origin of distortions and develop a general strategy to control on-axis intensity of extended focal lines.

Result: Both smooth and sharply structured longitudinal profiles can be reliably produced, establishing a robust framework for tailoring quasi-Bessel beams.

Conclusion: The research provides a practical solution for controlling quasi-Bessel beam profiles, making them more effective for high-field applications including laser-plasma acceleration and advanced photon sources.

Abstract: Quasi-Bessel beams produced by axiparabolas are increasingly used in high-intensity laser applications, yet their longitudinal profiles exhibit unwanted oscillations that limit their effectiveness. Here we identify the physical origin of these distortions and develop a general strategy to control the on-axis intensity of extended focal lines. By combining analytical insight with numerical and experimental validation, we show how both smooth and sharply structured longitudinal profiles can be reliably produced. This establishes a robust framework for tailoring quasi-Bessel beams in regimes relevant to laser-plasma acceleration, advanced photon sources, and other high-field applications.

</details>


### [49] [Detecting Solenoidal Plasma Turbulence via Laser Polarization Rotation](https://arxiv.org/abs/2601.19890)
*Kenan Qu,Nathaniel J. Fisch*

Main category: physics.plasm-ph

TL;DR: A diagnostic method using cross-polarization laser scattering to measure solenoidal turbulence energy and structure in high-energy-density plasmas, applicable to NIF conditions.


<details>
  <summary>Details</summary>
Motivation: Solenoidal turbulence may enhance fusion reactivity, but no standard diagnostic exists to directly measure these solenoidal flows or distinguish them from compressional turbulence in high-energy-density plasmas.

Method: Proposes using cross-polarization scattering of a probe laser that couples to plasma vorticity, generating a cross-polarized signal proportional to turbulent vorticity, effectively acting as a calorimeter for shear flows. Identifies a diffractive scattering signature analogous to "Debye-Scherrer ring" that reveals eddy size distribution.

Result: The technique can directly diagnose the energy and spatial structure of rotational turbulence, with the scattering signature revealing eddy size distribution.

Conclusion: This diagnostic method is applicable to National Ignition Facility (NIF) implosion conditions and other high-energy-density scenarios, providing a way to measure solenoidal turbulence that could enhance fusion reactivity.

Abstract: Recent theoretical studies suggest that solenoidal turbulence can significantly enhance fusion reactivity, yet no standard diagnostic exists to directly measure these solenoidal flows in high-energy-density plasmas, nor to distinguish between solenoidal and compressional turbulence. We propose a method that directly diagnoses the energy and spatial structure of this rotational turbulence using the cross-polarization scattering of a probe laser. By coupling to the plasma vorticity, the scattering generates a cross-polarized signal proportional to the turbulent vorticity, effectively acting as a calorimeter for shear flows. We identify a diffractive scattering signature analogous to ``Debye-Scherrer ring'' that reveals the eddy size distribution. We show that this technique is applicable to National Ignition Facility (NIF) implosion conditions and other high-energy-density scenarios.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [50] [Pseudo-relativistic fermionic systems with attractive Yukawa potential](https://arxiv.org/abs/2601.19751)
*Bin Chen,Yujin Guo,Phan Thành Nam,Dong Hao Ou Yang*

Main category: math-ph

TL;DR: Study of Hartree-Fock and Hartree-Fock-Bogoliubov theories for fermionic systems with pseudo-relativistic kinetic energy and attractive Yukawa potential, showing stability only below critical mass threshold.


<details>
  <summary>Details</summary>
Motivation: To understand the stability conditions and ground state properties of large fermionic systems with pseudo-relativistic dynamics and attractive Yukawa interactions, which are relevant for relativistic quantum systems.

Method: Theoretical analysis using Hartree-Fock and Hartree-Fock-Bogoliubov theories applied to fermionic systems with pseudo-relativistic kinetic energy and attractive Yukawa potential, examining stability conditions and ground state existence.

Result: Proved that system stability depends on total mass: stable only when mass doesn't exceed critical value. Investigated existence and properties of ground states in both sub-critical and critical mass regimes.

Conclusion: The system exhibits a critical mass threshold for stability, with distinct ground state behaviors in sub-critical and critical regimes, providing insights into relativistic fermionic systems with attractive interactions.

Abstract: We study the Hartree-Fock and Hartree-Fock-Bogoliubov theories for a large fermionic system with the pseudo-relativistic kinetic energy and an attractive Yukawa interaction potential. We prove that the system is stable if and only if the total mass does not excess a critical value, and investigate the existence and properties of ground states in both sub-critical and critical mass regimes.

</details>


### [51] [Non-Hermitian Fabry-Pérot Resonances](https://arxiv.org/abs/2601.19855)
*Habib Ammari,Erik Orvehed Hiltunen,Bowen Li,Ping Liu,Jiayu Qiu,Yingjie Shao,Alexander Uhlmann*

Main category: math-ph

TL;DR: Analysis of non-Hermitian Fabry-Pérot resonances in high-contrast resonator systems, focusing on exceptional point degeneracy and skin effects from imaginary gauge potentials using propagation matrix formalism.


<details>
  <summary>Details</summary>
Motivation: To understand non-Hermitian effects in optical resonator systems beyond the subwavelength regime, particularly exceptional point degeneracy and skin effects induced by imaginary gauge potentials in high-contrast resonator systems.

Method: Using propagation matrix formalism to analyze continuous differential models of non-Hermitian Fabry-Pérot resonances, characterizing exceptional points and skin effects beyond subwavelength regime.

Result: Established existence of exceptional points purely from radiation conditions, and proved that non-Hermitian skin effect applies uniformly across resonant modes, yielding broadband edge localization.

Conclusion: The propagation matrix formalism successfully characterizes non-Hermitian effects in high-contrast resonator systems, providing insights into exceptional point degeneracy and broadband edge localization through skin effects.

Abstract: We characterise non-Hermitian Fabry-Pérot resonances in high-contrast resonator systems and study the properties of their associated resonant modes from continuous differential models. We consider two non-Hermitian effects: the exceptional point degeneracy and the skin effect induced by imaginary gauge potentials. Using the propagation matrix formalism, we characterise these two non-Hermitian effects beyond the subwavelength regime. This analysis allows us to (i) establish the existence of exceptional points purely from radiation conditions and to (ii) prove that the non-Hermitian skin effect applies uniformly across resonant modes, yielding broadband edge localisation.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [52] [Computing the density of the Kesten-Stigum limit in supercritical Galton-Watson processes](https://arxiv.org/abs/2601.19633)
*Alice Cortinovis,Sophie Hautphenne,Stefano Massei*

Main category: math.PR

TL;DR: A new numerical method for computing the density of the limit random variable in supercritical Galton-Watson processes using Laplace-Stieltjes transform and Laguerre polynomial approximations.


<details>
  <summary>Details</summary>
Motivation: While the Kesten-Stigum theorem guarantees existence of a non-trivial limit random variable for supercritical Galton-Watson processes (which captures early demographic fluctuations and determines random amplitude of long-term exponential growth), computing its density in a stable and efficient manner for arbitrary offspring laws remains a significant challenge.

Method: The approach leverages a functional equation characterizing the Laplace-Stieltjes transform of the limit distribution and combines it with a moment-matching method to obtain accurate approximations within a class of linear combinations of Laguerre polynomials with exponential damping.

Result: The effectiveness of the approach is validated on several examples where the offspring generating function is a polynomial of bounded degree.

Conclusion: The proposed method provides a novel numerical approach for computing the density of the limit random variable in supercritical Galton-Watson processes, addressing a significant computational challenge in population dynamics modeling.

Abstract: This paper proposes a novel numerical method for computing the density of the limit random variable associated with a supercritical Galton-Watson process. This random variable captures the effect of early demographic fluctuations and determines the random amplitude of long-term exponential population growth. While the existence of a non-trivial limit is ensured by the Kesten-Stigum theorem, computing its density in a stable and efficient manner for arbitrary offspring laws remains a significant challenge. The proposed approach leverages a functional equation that characterizes the Laplace-Stieltjes transform of the limit distribution and combines it with a moment-matching method to obtain accurate approximations within a class of linear combinations of Laguerre polynomials with exponential damping. The effectiveness of the approach is validated on several examples in which the offspring generating function is a polynomial of bounded degree.

</details>


### [53] [Stochastic Persistence in Infinite Dimensions](https://arxiv.org/abs/2601.19145)
*Juraj Foldes,Declan Stacy*

Main category: math.PR

TL;DR: The paper develops a general criteria for stochastic persistence in infinite-dimensional systems like SPDEs, extending finite-dimensional results, and applies it to ecological and biological models.


<details>
  <summary>Details</summary>
Motivation: Motivated by infinite-dimensional ecological and biological models such as reaction-diffusion SPDEs and stochastic functional differential equations, which require analysis of stochastic persistence (coexistence) in infinite-dimensional settings.

Method: Develops a general criteria for stochastic persistence using an average Lyapunov function; analyzes projective process for SPDEs; combines mild (stochastic convolution) and variational (Lyapunov function) techniques; proves nontrivial well-posedness and nonnegativity results for reaction-diffusion SPDEs.

Result: Extends stochastic persistence criteria to infinite dimensions; provides well-posedness and nonnegativity results for reaction-diffusion SPDEs; demonstrates applications to ecological models (Lotka-Volterra), epidemic model (SIR), and turbulence model; shows coexistence in Lotka-Volterra is determined by invasion rates as in SDE case.

Conclusion: The paper successfully extends stochastic persistence theory to infinite-dimensional systems, providing a general framework and specific applications to ecological and biological models, with the Lotka-Volterra results showing consistency with finite-dimensional counterparts.

Abstract: Motivated by infinite-dimensional ecological and biological models such as reaction-diffusion SPDEs and stochastic functional differential equations, we develop a general criteria for stochastic persistence (coexistence) in terms of an average lyapunov function, which was previously known only in finite dimensions. To apply our results to SPDEs we analyze the projective process, and we employ a combination of mild (stochastic convolution) and variational (lyapunov function) techniques. Our analysis also requires some nontrivial well-posedness and nonnegativity results for reaction-diffusion SPDEs, which we state and prove in great generality, extending the known results in the literature. Finally, we present several examples including ecological models (Lotka-Volterra), an epidemic model (SIR), and a model for turbulence. Notably we show that, as in the SDE case, coexistence in the Lotka-Volterra model is determined by the invasion rates.

</details>


### [54] [Mass generation for the two dimensional O(N) Linear Sigma Model in the large N limit](https://arxiv.org/abs/2601.19630)
*Matías G. Delgadino,Scott A. Smith*

Main category: math.PR

TL;DR: The paper studies the O(N) Linear Sigma Model in 2D Euclidean space, showing that in the large N limit, correlations decay exponentially and marginals converge to a massive Gaussian Free Field without restrictions on coupling constants.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of the O(N) Linear Sigma Model on ℝ² in the large N limit, particularly how correlations decay and whether the model converges to a Gaussian Free Field without the restrictions on coupling constants that were present in previous torus-based studies.

Method: Uses the Feyel/Üstünel extension of Talagrand's inequality combined with classical tools from Euclidean Quantum Field Theory, analyzing the model under a scaling dictated by the formal 1/N expansion.

Result: In the large N limit, correlations decay exponentially fast with an acquired mass that decays exponentially in inverse temperature. Each marginal converges to a massive Gaussian Free Field on ℝ², quantified in the 2-Wasserstein distance with a weighted H¹(ℝ²) cost function. Results hold without restrictions on coupling constants.

Conclusion: The O(N) Linear Sigma Model on ℝ² converges to a massive Gaussian Free Field in the large N limit without coupling constant restrictions, extending previous results from the torus and providing a more complete understanding of the model's behavior.

Abstract: This work studies the $O(N)$ Linear Sigma Model on $\mathbb{R}^{2}$ under a scaling dictated by the formal $1/N$ expansion. We show that in the large $N$ limit, correlations decay exponentially fast, where the acquired mass decays exponentially in the inverse temperature. In fact, each marginal converges to a massive Gaussian Free Field (GFF) on $\mathbb{R}^{2}$, quantified in the $2$-Wasserstein distance with a weighted $H^{1}(\mathbb{R}^{2})$ cost function. In contrast to prior work on the torus via parabolic stochastic quantization, our results hold without restrictions on the coupling constants, allowing us to also obtain a massive GFF in a suitable double scaling limit. Our proof combines the Feyel/Üstünel extension of Talagrand's inequality with some classical tools in Euclidean Quantum Field Theory.

</details>


### [55] [Sharp bounds for non-trace class noise and applications to SPDEs](https://arxiv.org/abs/2601.19639)
*Antonio Agresti,Fabian Germ,Mark Veraar*

Main category: math.PR

TL;DR: The paper establishes necessary and sufficient conditions for convergence of Gaussian series in Bessel potential spaces, with applications to stochastic PDEs with non-trace class noise.


<details>
  <summary>Details</summary>
Motivation: In stochastic PDEs with colored, non-trace class space-time noise, researchers frequently encounter Gaussian series of the form ∑γ_nμ_nf_n. Understanding when these series converge in appropriate function spaces (Bessel potential spaces H^{-s,q}) is crucial for analyzing solutions to such equations.

Method: The authors develop a main theorem using weighted sequence spaces that encode the L^∞-growth of the orthonormal system (f_n). This approach captures the relationship between the orthonormal system's growth behavior and convergence properties in Sobolev-type spaces.

Result: The paper establishes necessary and sufficient conditions for convergence of Gaussian series in Bessel potential spaces H^{-s,q}. These conditions can be interpreted as a Sobolev embedding for Gaussian series. The results are applied to the stochastic heat equation with additive non-trace class noise, where the conditions capture the scaling relationship between the heat operator and noise coloring.

Conclusion: The paper provides a complete characterization of convergence for Gaussian series in Bessel potential spaces, with applications to stochastic PDEs. The weighted sequence space approach that tracks L^∞-growth of orthonormal systems is essential for obtaining sharp estimates in these problems.

Abstract: In the study of stochastic PDEs with colored, non-trace class space-time noise, one frequently encounters Gaussian series of the form $$ \sum_{n\geq 1} γ_n μ_n f_n, $$ where $(γ_n)_{n}$ is a sequence of standard independent Gaussian variables, $g$ is an $L^η(\mathcal{O})$ function, $(μ_n)_{n}$ is a sequence of scalars, and $(f_n)_n$ is an orthonormal system in $L^2(\mathcal{O})$ where $\mathcal{O} \subseteq \mathbb{R}^d$ is an open set. In this manuscript, we establish necessary and sufficient conditions for the above sum to converge in Bessel potential spaces $H^{-s,q}(\mathcal{O})$. The latter can be interpreted as a Sobolev embedding for Gaussian series. Our main theorem is formulated using weighted sequence spaces that encode the $L^\infty$-growth of the orthonormal system $(f_n)_{n}$, a feature that is crucial for obtaining sharp estimates. We apply our results to the stochastic heat equation with additive non-trace class noise. In this case, our conditions capture the scaling relationship between the heat operator and the coloring of the noise.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [56] [Induced Scattering of Fast Radio Bursts in Magnetar Magnetospheres](https://arxiv.org/abs/2601.18865)
*Rei Nishiura,Shoma F. Kamijima,Kunihito Ioka*

Main category: astro-ph.HE

TL;DR: Induced Compton/Brillouin scattering in magnetized pair plasma affects FRB propagation in magnetar magnetospheres, with scattering evolution bifurcating based on density thresholds.


<details>
  <summary>Details</summary>
Motivation: To understand how induced Compton/Brillouin scattering affects fast radio burst (FRB) propagation in magnetar magnetospheres, particularly to resolve tension with observations of compact emission regions and explain FRB diversity.

Method: Combined kinetic theory verification with Particle-in-Cell (PIC) simulations to investigate electromagnetic wave scattering in magnetized electron-positron pair plasma, applied to FRB propagation scenarios.

Result: Scattering inevitably enters linear growth stage despite magnetic field suppression. Evolution bifurcates: full scattering occurs above critical density, while below it scattering saturates allowing FRB escape.

Conclusion: This scattering mechanism eases tension with observations of compact FRB emission regions and may explain diversity in FRB phenomena, including presence/absence of FRBs associated with X-ray bursts.

Abstract: We investigate induced Compton/Brillouin scattering of electromagnetic waves in magnetized electron and positron pair plasma by verifying kinetic theory with Particle-in-Cell simulations. Applying this to fast radio bursts (FRBs) in magnetar magnetospheres, we find that the scattering--although suppressed by the magnetic field--inevitably enters the linear growth stage. The subsequent evolution bifurcates: full scattering occurs when the density exceeds a critical value, whereas below it the scattering saturates and the FRB can escape. This eases the tension with observations of compact emission regions and may explain the observed diversity, including the presence or absence of FRBs associated with X-ray bursts.

</details>


### [57] [Maximum Energy of Particles Accelerated in GRB Afterglow Shocks](https://arxiv.org/abs/2601.19135)
*Zhao-Feng Wu,Sofía Guevara-Montoya,Paz Beniamini,Dimitrios Giannios,Daniel Grošelj,Lorenzo Sironi*

Main category: astro-ph.HE

TL;DR: Particle acceleration in relativistic shocks is poorly understood. PIC simulations predict electron acceleration via small-angle scattering with energy limits below Bohm limit, observable as synchrotron cutoffs in GRB afterglows. Modeling shows short GRBs in low-density environments exhibit GeV cutoffs within hours. Current observations can't distinguish PIC predictions from Bohm limit due to Fermi-LAT uncertainties, but future MeV-TeV observations could constrain acceleration physics.


<details>
  <summary>Details</summary>
Motivation: Particle acceleration in relativistic collisionless shocks remains an unsolved problem in high-energy astrophysics. Understanding the acceleration mechanisms is crucial for interpreting astrophysical phenomena like gamma-ray burst afterglows, which serve as natural laboratories for testing theoretical predictions about shock physics.

Method: The researchers model the spectral evolution of GRB afterglows during relativistic deceleration phase, incorporating PIC-motivated acceleration prescriptions. They self-consistently compute synchrotron and synchrotron self-Compton emission. They apply their framework to specific GRBs (190114C and 130427A) and analyze observational constraints.

Result: Low-energy bursts in low-density environments (typical of short GRBs) show pronounced synchrotron cutoff in GeV band within minutes to hours after trigger. Current observations from Fermi-LAT are insufficient to discriminate between PIC-motivated acceleration and Bohm limit due to large uncertainties. However, the modeling framework demonstrates that future MeV-TeV afterglow observations could break model degeneracies.

Conclusion: While current observations cannot yet distinguish between different particle acceleration models in relativistic shocks, the study provides a framework for future tests. Upcoming MeV-TeV observations of GRB afterglows offer promising opportunities to place substantially tighter constraints on particle acceleration physics in relativistic collisionless shocks.

Abstract: Particle acceleration in relativistic collisionless shocks remains an open problem in high-energy astrophysics. Particle-in-cell (PIC) simulations predict that electron acceleration in weakly magnetized shocks proceeds via small-angle scattering, leading to a maximum electron energy significantly below the Bohm limit. This upper bound manifests observationally as a characteristic synchrotron cutoff, providing a direct probe of the underlying acceleration physics. Gamma-ray burst (GRB) afterglows offer an exceptional laboratory for testing these predictions. Here, we model the spectral evolution of GRB afterglows during the relativistic deceleration phase, incorporating PIC-motivated acceleration prescriptions and self-consistently computing synchrotron and synchrotron self-Compton emission. We find that low-energy bursts in low-density environments, typical of short GRBs, exhibit a pronounced synchrotron cutoff in the GeV band within minutes to hours after the trigger. Applying our framework to GRB 190114C and GRB 130427A, we find that current observations are insufficient to discriminate between PIC-motivated acceleration and the Bohm limit, primarily due to large uncertainties in the Fermi-LAT band. Nevertheless, future MeV-TeV afterglow observations can break model degeneracies and place substantially tighter constraints on particle acceleration in relativistic shocks.

</details>


### [58] [On the rarity of rocket-driven Penrose extraction in Kerr spacetime](https://arxiv.org/abs/2601.19616)
*An T. Le*

Main category: astro-ph.HE

TL;DR: Monte Carlo study shows Penrose process energy extraction from rotating black holes via rocket propulsion is statistically rare (~1%), requiring high black hole spin (a/M ≳ 0.89) and ultra-relativistic exhaust velocities (v_e ≈ 0.91c), with single-impulse thrust being more efficient than continuous thrust.


<details>
  <summary>Details</summary>
Motivation: To quantify the feasibility and constraints of energy extraction from rotating black holes via the Penrose process using rocket propulsion, and understand why electromagnetic mechanisms dominate astrophysically.

Method: Monte Carlo simulation of over 250,000 trajectories analyzing rocket propulsion in Kerr black holes, examining conditions for successful energy extraction and escape to infinity, comparing single-impulse vs continuous thrust strategies.

Result: Successful extraction is rare (~1% success rate), requires high black hole spin (a/M ≳ 0.89) and ultra-relativistic exhaust (v_e ≈ 0.91c). Single-impulse thrust at periapsis achieves 19% cumulative efficiency vs 2-4% for continuous thrust. Highly tuned "sweet spot" conditions can reach 88.5% success rate.

Conclusion: Material-based Penrose extraction requires extreme fine-tuning, explaining astrophysical dominance of electromagnetic mechanisms. The process is statistically rare and constrained to specific parameter regimes, with single-impulse strategies being more efficient than continuous thrust.

Abstract: We present a Monte Carlo study of energy extraction from rotating (Kerr) black holes via the Penrose process using rocket propulsion. Through over 250,000 trajectory simulations, we establish sharp constraints on when Penrose extraction with escape to infinity succeeds. The mechanism requires that exhaust ejected inside the ergosphere carries negative Killing energy, which is kinematically accessible only via ultra-relativistic ejection deep within the ergosphere. We find that successful extraction with escape is statistically rare ($\sim$1% in broad parameter scans) and is governed by strict thresholds: it requires high black hole spin (empirically $a/M \gtrsim 0.89$) and ultra-relativistic exhaust velocity (onset at $v_e \approx 0.91c$). When conditions are highly tuned to a specific "sweet spot," success rates can reach 88.5%, representing a narrow extraction window rather than generic behavior. Furthermore, single-impulse thrust at periapsis achieves significantly higher cumulative efficiency ($η_{\rm cum} \approx 19\%$) compared to continuous thrust ($\sim$2--4%) due to path-averaging penalties. These constraints quantify the extreme fine-tuning required for material-based Penrose extraction, consistent with the astrophysical dominance of electromagnetic mechanisms. Simulation code is available at https://github.com/anindex/penrose_process.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [59] [Mixture-Weighted Ensemble Kalman Filter with Quasi-Monte Carlo Transport](https://arxiv.org/abs/2601.18992)
*Ilja Klebanov,Claudia Schillings,Dana Wrischnig*

Main category: stat.ME

TL;DR: The paper proposes a hybrid filtering approach combining Ensemble Kalman Filter (EnKF) transport with importance sampling correction, developing six IS-EnKF schemes with theoretical guarantees and TQMC-enhanced variants for reduced sampling error.


<details>
  <summary>Details</summary>
Motivation: BPF suffers from weight degeneracy while EnKF is only exact for linear-Gaussian systems. The authors aim to combine their strengths: EnKF's scalability with BPF's asymptotic exactness through principled importance sampling.

Method: 1) Develop general importance sampling theory for mixture targets/proposals. 2) Interpret stochastic EnKF analysis as sampling from Gaussian-mixture proposals. 3) Create six IS-EnKF schemes with importance sampling correction. 4) Construct transported quasi-Monte Carlo (TQMC) point sets for Gaussian-mixture laws to reduce sampling error.

Result: Proposed filters show improved accuracy over BPF, EnKF, and standard weighted EnKF. Weighted schemes eliminate EnKF's error plateau caused by analysis-target mismatch. TQMC variants substantially reduce sampling error without changing filtering pipeline.

Conclusion: The hybrid IS-EnKF approach successfully combines EnKF's scalability with BPF's asymptotic exactness, with theoretical guarantees and practical improvements demonstrated through numerical experiments.

Abstract: The Bootstrap Particle Filter (BPF) and the Ensemble Kalman Filter (EnKF) are two widely used methods for sequential Bayesian filtering: the BPF is asymptotically exact but can suffer from weight degeneracy, while the EnKF scales well in high dimension yet is exact only in the linear-Gaussian case. We combine these approaches by retaining the EnKF transport step and adding a principled importance-sampling correction. Our first contribution is a general importance-sampling theory for mixture targets and proposals, including variance comparisons between individual- and mixture-based estimators. We then interpret the stochastic EnKF analysis as sampling from explicit Gaussian-mixture proposals obtained by conditioning on the current or previous ensemble, which leads to six self-normalized IS-EnKF schemes. We embed these updates into a broader class of ensemble-based filters and prove consistency and error bounds, including weight-variance comparisons and sufficient conditions ensuring finite-variance importance weights. As a second contribution, we construct transported quasi-Monte Carlo (TQMC) point sets for the Gaussian-mixture laws arising in prediction and analysis, yielding TQMC-enhanced variants that can substantially reduce sampling error without changing the filtering pipeline. Numerical experiments on benchmark models compare the proposed mixture-weighted and TQMC-enhanced filters, showing improved filtering accuracy relative to BPF, EnKF, and the standard weighted EnKF, and that the weighted schemes eliminate the EnKF error plateau often caused by analysis-target mismatch.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [60] [Approximation by linear sampling operators in Banach spaces](https://arxiv.org/abs/2601.19012)
*Yurii Kolomoitsev*

Main category: math.FA

TL;DR: The paper establishes comprehensive approximation theory for linear sampling operators in general Banach lattices, extending classical results from L_p spaces to a much broader functional framework.


<details>
  <summary>Details</summary>
Motivation: To extend classical approximation theory results for sampling operators from L_p spaces to general Banach lattices, addressing limitations in existing literature and substantially enlarging the class of functions that can be analyzed in this framework.

Method: Theoretical analysis of linear sampling operators in general Banach lattices, developing matching direct and inverse approximation estimates, convergence criteria, equivalence results involving special K-functionals and their realizations by sampling operators, as well as strong converse inequalities.

Result: Established comprehensive approximation properties including direct/inverse estimates, convergence criteria, equivalence results, and strong converse inequalities for sampling operators in Banach lattices - results not previously known even for classical L_p spaces.

Conclusion: The work significantly extends classical approximation theory by providing a unified framework for sampling operators in general Banach lattices, substantially broadening the applicability of sampling operator theory beyond traditional L_p spaces.

Abstract: This paper studies approximation properties of linear sampling operators in general Banach lattices $X$. We obtain matching direct and inverse approximation estimates, convergence criteria, equivalence results involving special $K$-functionals and their realizations by sampling operators, as well as strong converse inequalities, which, to the best of our knowledge, have not been previously established for sampling operators even in the classical spaces $L_p$. The results extend several classical theorems previously known mainly in $L_p$ and apply to all functions $f\in X$ for which the corresponding sampling operator is well defined, thereby substantially enlarging the class of functions that can be considered in this framework.

</details>


### [61] [Strip-type operators and abstract Cauchy problems](https://arxiv.org/abs/2601.19075)
*Nikolaos Roidos*

Main category: math.FA

TL;DR: The paper establishes well-posedness of classical solutions for non-homogeneous abstract linear Schrödinger and wave equations with zero initial conditions in Banach spaces, using operator theory and extending results to R-bounded operators.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous mathematical framework for studying non-homogeneous abstract linear Schrödinger and wave equations in Banach spaces, extending classical results to more general operator settings including R-bounded operators.

Method: The authors use operator theory in Banach spaces, specifically working with operators of strip-type and parabola-type. They establish well-posedness in vector-valued Sobolev-Slobodetskii spaces and extend results by replacing boundedness properties with R-boundedness conditions.

Result: Proves well-posedness of classical solutions for non-homogeneous abstract linear Schrödinger and wave equations with zero initial conditions. Extends these results to operators with R-boundedness properties. Applies the theory to an abstract semilinear wave equation, establishing existence and uniqueness of classical solutions for short times.

Conclusion: The paper provides a comprehensive operator-theoretic framework for analyzing abstract linear and semilinear evolution equations in Banach spaces, with applications to Schrödinger and wave equations, demonstrating the utility of R-boundedness conditions in extending classical results.

Abstract: We consider the non-homogeneous abstract linear Schrödinger and wave equations with zero initial conditions, defined by operators of strip-type and parabola-type in Banach spaces, respectively, and establish the well-posedness of classical solutions in appropriate vector-valued Sobolev-Slobodetskii spaces. We obtain analogous results for two extensions of these equations by replacing the previously mentioned boundedness properties of the associated operators with $R$-boundedness. As an application, we consider an abstract semilinear wave equation and establish the existence and uniqueness of classical solutions to this problem for short times.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [62] [Polyhedral design with blended $n$-sided interpolants](https://arxiv.org/abs/2601.19322)
*Péter Salvi*

Main category: cs.CG

TL;DR: A new parametric surface representation that interpolates mesh vertices using smoothly connecting quadrilateral patches blended from local multi-sided quadratic interpolants.


<details>
  <summary>Details</summary>
Motivation: To create a smooth parametric surface representation that can interpolate vertices of closed meshes with arbitrary topology, addressing the challenge of handling non-four-sided cases and complex mesh structures.

Method: Blending local multi-sided quadratic interpolants to create smoothly connecting quadrilateral patches. For non-four-sided cases, uses a special parameterization technique involving rational curves. Also discusses handling of triangular subpatches and alternative subpatch representations.

Result: Proposes a new parametric surface representation capable of interpolating vertices of arbitrary topology closed meshes through smoothly connected quadrilateral patches.

Conclusion: The method provides a viable approach for creating smooth parametric surfaces from arbitrary topology meshes, with special techniques for handling non-four-sided cases and triangular subpatches.

Abstract: A new parametric surface representation is proposed that interpolates the vertices of a given closed mesh of arbitrary topology. Smoothly connecting quadrilateral patches are created by blending local, multi-sided quadratic interpolants. In the non-four-sided case, this requires a special parameterization technique involving rational curves. Appropriate handling of triangular subpatches and alternative subpatch representations are also discussed.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [63] [Convolutional causal learning for aerodynamic flows](https://arxiv.org/abs/2601.19104)
*Ryo Koshikawa,Ryo Araki,Qiong Liu,Kai Fukami*

Main category: physics.flu-dyn

TL;DR: Information-theoretic machine learning extracts time-dependent vortical structures that causally influence aerodynamic coefficients from snapshot data, enabling interpretable flow analysis and control.


<details>
  <summary>Details</summary>
Motivation: To develop a data-driven approach for capturing aerodynamic causality from snapshot data, enabling identification of time-varying vortical structures that influence future aerodynamic behavior in unsteady flows.

Method: Information-theoretic machine learning with convolutional neural networks for spatial continuous mode extraction, combined with autoencoder-based data compression for low-order representation of informative vortical structures and aerodynamic coefficients.

Result: Successfully applied to extreme vortex-gust airfoil interactions, experimental transverse jet-wing interaction, and turbulent separated wake; extracted time-varying gust effects on lift response interpretably; identified relationship between large-scale vortical motion and lift force without spatial length-scale information.

Conclusion: The approach provides a foundation for data-driven causal modeling and control in unsteady flows by extracting interpretable, time-dependent aerodynamic causality from snapshot data.

Abstract: This study considers capturing aerodynamic causality from snapshot data with a time-varying mode decomposition technique referred to as information-theoretic machine learning. The current approach extracts time-dependent informative vortical structures, contributing to the future evolution of the aerodynamic coefficients. The present decomposition is employed with a convolutional neural network, enabling the identification of the spatial continuous mode. In addition, a low-order representation, characterizing the informative vortical structures and their corresponding aerodynamic coefficients, can also be identified by considering autoencoder-based data compression. The present technique is applied to a range of aerodynamic examples, including extreme vortex-gust airfoil interactions, experimentally measured transverse jet-wing interaction, and a turbulent separated wake. For the cases of gust-wing interaction, the time-varying gust effect on the lift response is extracted in an interpretable manner. With the example of a turbulent wake, the relationship between large-scale vortical motion and lift force is identified without any spatial length-scale information. The proposed approach could serve as a foundation for data-driven causal modeling and control for a range of unsteady flows.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [64] [Ricci Flow on CP1-bundles over a Product of Kähler-Einstein Manifolds](https://arxiv.org/abs/2601.18931)
*Frederick Tsz-Ho Fong,Hung Tran*

Main category: math.DG

TL;DR: Study of Ricci flow on CP1-bundles over Kähler-Einstein product manifolds using Wang's ansatz, showing the ansatz is preserved and Type I singularities occur in Kähler case.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of Ricci flow on CP1-bundles over product manifolds, particularly investigating singularity formation and preservation of geometric structure under the flow.

Method: Use the ansatz developed by M. Wang et al. for initial metrics on CP1-bundles over products of Kähler-Einstein manifolds, then analyze Ricci flow evolution preserving this ansatz structure.

Result: The ansatz is preserved along the Ricci flow, and in the Kähler case, Type I finite-time singularities must occur under this ansatz.

Conclusion: The geometric structure defined by Wang's ansatz remains invariant under Ricci flow, leading to predictable singularity formation patterns in Kähler settings.

Abstract: In this paper, we study the Ricci flow on CP1-bundles over a product of Kähler-Einstein manifolds whose initial metric is constructed by the ansatz used in works by M. Wang et. al. We prove that the ansatz is preserved along the Ricci flow. Furthermore, in the Kähler case, we proved that Type I finite-time singularity must occur under such an ansatz.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [65] [Jacobi-Piñeiro Multiple Orthogonal Polynomials on the simplex](https://arxiv.org/abs/2601.19416)
*Lidia Fernández,Ana Foulquié-Moreno,Juan Antonio Villegas*

Main category: math.CA

TL;DR: The paper introduces bivariate multiple polynomials on the simplex using Rodrigues formulas, generalizing Jacobi-Piñeiro polynomials to multivariate setting, and applies them to bivariate Hermite-Padé approximation on triangles.


<details>
  <summary>Details</summary>
Motivation: Rodrigues formulas are powerful for computing orthogonal polynomials with classical weights, but there's a need to extend this approach to multivariate settings and apply it to approximation problems.

Method: Develops bivariate multiple polynomials on the simplex via Rodrigues formulas, providing a natural generalization of Jacobi-Piñeiro polynomials to multivariate cases.

Result: Successfully constructs bivariate multiple polynomials using Rodrigues approach and applies them to study the bivariate Hermite-Padé problem on triangles.

Conclusion: The Rodrigues formula approach effectively extends classical orthogonal polynomial theory to multivariate settings and provides useful tools for bivariate approximation problems.

Abstract: It is known that Rodrigues formulas provide a very powerful tool to compute orthogonal polynomials with respect to classical weights. We provide an example of bivariate multiple polynomials on the simplex defined via a Rodrigues formula. This approach offers a natural generalization of Jacobi--Piñeiro polynomials to the multivariate setting. Moreover, we apply these polynomials to the study of the bivariate Hermite--Padé problem on the triangle.

</details>


### [66] [Quantitative FUP and spectral gap for quasi-Fuchsian group](https://arxiv.org/abs/2601.19663)
*Long Jin,An Zhang,Hong Zhang*

Main category: math.CA

TL;DR: The paper provides an explicit formula for the exponent β in higher-dimensional fractal uncertainty principle (FUP), quantifying its dependence on porosity parameter ν, and applies this to get explicit essential spectral gaps for convex co-compact hyperbolic 3-manifolds.


<details>
  <summary>Details</summary>
Motivation: To quantify the dependence of the FUP exponent β on the porosity parameter ν in higher dimensions, extending previous 2D results to higher dimensions, and to refine spectral gap results for hyperbolic manifolds.

Method: Derivation of explicit formula for β in higher-dimensional FUP using mathematical analysis techniques, then application to convex co-compact hyperbolic 3-manifolds from quasi-Fuchsian groups.

Result: Explicit formula for β(ν) in higher-dimensional FUP, leading to explicit essential spectral gap for convex co-compact hyperbolic 3-manifolds, refining Tao's 2025 result and extending Jin-Zhang 2020 to higher dimensions.

Conclusion: The paper successfully quantifies the FUP exponent dependence on porosity in higher dimensions and provides explicit spectral gap estimates for hyperbolic 3-manifolds, advancing both fractal uncertainty principles and spectral theory applications.

Abstract: We derive an explicit formula for the exponent $β$ in the higher-dimensional fractal uncertainty principle (FUP) established by Cohen 2023, quantifying its dependence on the porosity parameter $ν$ of the Fourier support. This quantitative version of FUP yields an explicit essential spectral gap for convex co-compact hyperbolic 3-manifolds arising from quasi-Fuchsian groups, thereby refining the result of Tao 2025. Our result extends the earlier work of Jin-Zhang 2020 to higher dimensions.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [67] [Variable Elliptic Structures on the Plane: Transport Dynamics, Rigidity, and Function Theory](https://arxiv.org/abs/2601.19274)
*Daniel Alayón-Solarz*

Main category: math.CV

TL;DR: The paper studies variable elliptic structures in the plane defined by smoothly varying quadratic relations, deriving transport equations for structure coefficients and establishing connections to complex analysis tools in variable coefficient settings.


<details>
  <summary>Details</summary>
Motivation: To understand variable elliptic structures in the plane and develop a theory that extends classical complex analysis tools to variable coefficient settings, revealing the interplay between transport dynamics, rigidity, and function theory.

Method: Differentiate the structure relation i² + β(x,y)i + α(x,y) = 0 to obtain explicit expressions for derivatives of i(x,y) in terms of α and β, leading to a universal transport system. In the elliptic regime, this reduces to a forced complex Burgers equation for a scalar spectral parameter encoding structure coefficients.

Result: Identifies a rigidity condition where transport becomes conservative, shows that in this regime the generalized Cauchy-Riemann operator satisfies a Leibniz rule and admits factorization of associated second order operators into first order components, enabling classical complex analysis tools to reappear in variable coefficient settings.

Conclusion: The theory extends classical planar complex analysis tools (Cauchy-Pompeiu formulas, integral representations, elliptic operators) to variable coefficient settings with explicit structure, emphasizing transparency of integrability mechanisms and connections between transport dynamics, rigidity, and function theory.

Abstract: We study variable elliptic structures in the plane defined by a smoothly varying quadratic relation i^2 + beta(x,y) i + alpha(x,y) = 0, and the associated first order operator dbar = 1/2 (dx + i dy). Differentiating the structure relation yields explicit expressions for the derivatives of i(x,y) in terms of the coefficient functions alpha and beta, leading to a universal transport system governing their admissible variations. In the elliptic regime this system reduces to a forced complex Burgers equation for a scalar spectral parameter encoding the structure coefficients. We identify a rigidity condition under which the transport becomes conservative, and show that in this regime the generalized Cauchy Riemann operator satisfies a Leibniz rule and admits a factorization of the associated second order operator into first order components. As a consequence, classical tools of planar complex analysis, including Cauchy Pompeiu type formulas, integral representations, and elliptic second order operators, reappear in a variable coefficient setting with explicit structure. The theory is developed at the level of direct computation, emphasizing transparency of the integrability mechanism and the interplay between transport dynamics, rigidity, and function theory.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [68] [Approximate controllability of a bilinear wave equation and minimum time](https://arxiv.org/abs/2601.19544)
*Karine Beauchard,Thomas Perrin,Eugenio Pozzoli*

Main category: math.OC

TL;DR: The paper studies global approximate controllability of Klein-Gordon wave equations on tori with bilinear controls, establishing minimum control times based on dimension and initial state zero sets.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limits of controllability for Klein-Gordon wave equations with bilinear controls, particularly how the geometry of initial state zero sets and spatial dimension affect minimum control times.

Method: Combines Lie bracket techniques (à la Agrachev-Sarychev) with propagation of well-prepared positive states to analyze controllability properties.

Result: In dimensions 1-2: minimum control time equals maximum radius of ball contained in initial state zero set. In dimensions ≥3: zero minimum time if zero set has measure zero, and controllability in sufficiently large time from all nonzero initial states.

Conclusion: The dimension and geometry of initial state zero sets crucially determine minimum control times for Klein-Gordon wave equations with bilinear controls, with fundamental differences between low and high dimensions.

Abstract: We study the global approximate controllability (GAC) of a Klein-Gordon wave equation, posed on the torus $\mathbb{T}^d$ of arbitrary dimension $d\in \mathbb{N}^*$, with bilinear control potentials supported on the first $(2d+1)$-Fourier modes. Let $Z(W_0)\subset \mathbb{T}^d$ be the set of essential zeroes of the initial state $W_0\in H^1\times L^2(\mathbb{T}^d)$, and $r(W_0)\geq 0$ be the maximum radius of a ball of $\mathbb{T}^d$ contained in $Z(W_0)$. Due to finite speed of propagation, the minimum control time starting from $W_0$ is necessarily larger than or equal to $r(W_0)$. We prove the following three facts.
  In low dimensions $d \in \{1,2\}$: the minimum time for GAC from $W_0 \neq 0$ is equal to $r(W_0)$.
  In any dimensions $d\geq 3$: the minimum time for GAC from $W_0$ is zero if $Z(W_0)$ has zero Lebesgue measure; and the GAC in sufficiently large time from all $W_0\neq 0$.
  The proof strategy consists in combining Lie bracket techniques \emph{à la Agrachev-Sarychev} with the propagation of well-prepared positive states.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [69] [MAD-SURF: a machine learning interatomic potential for molecular adsorption on coinage metal surfaces](https://arxiv.org/abs/2601.18852)
*Manuel González Lastre,Joakim S. Jestilä,Rubén Pérez,Adam S. Foster*

Main category: cond-mat.mtrl-sci

TL;DR: MAD-SURF is a machine learning interatomic potential for simulating molecular adsorption on coinage metal surfaces with DFT-level accuracy but orders of magnitude faster.


<details>
  <summary>Details</summary>
Motivation: First-principles simulations of molecular adsorption on metal surfaces are computationally expensive, limiting their application to large systems needed for interpreting scanning probe microscopy and advancing surface chemistry and molecular electronics.

Method: Developed MAD-SURF, a machine learning interatomic potential trained on a broad dataset including diverse molecules, adsorption motifs, surfaces, molecular dynamics trajectories, and non-covalent aggregates on coinage metals.

Result: MAD-SURF achieves accuracy comparable to DFT reference while enabling simulations orders of magnitude faster. It reliably reproduces energies, forces, and adsorption geometries across three coinage metal substrates and works on experimentally characterized systems including organic monolayers, polycyclic aggregates, flexible biomolecules, and gold's herringbone reconstruction.

Conclusion: MAD-SURF provides a practical framework for accelerating atomistic simulations and advancing data-driven workflows in surface science by merging accuracy, speed, and generalizability for molecular adsorption on coinage metal surfaces.

Abstract: Predicting how organic molecules adsorb, assemble, and interact on metal surfaces is central to surface chemistry and molecular electronics, particularly in the context of interpreting high-resolution scanning probe microscopy. Yet, the application of first-principles simulations to interfaces is hampered by the computational cost for evaluating the electronic structure for the large number of atoms typically involved. We hereby present MAD-SURF, a machine learning interatomic potential specifically tailored for molecular adsorption on coinage metal surfaces. Trained on a broad dataset spanning diverse molecules, adsorption motifs, surfaces, molecular dynamics trajectories and non-covalent aggregates, MAD-SURF achieves accuracy comparable to the underlying DFT reference while enabling simulations orders of magnitude faster than density functional theory. The model reliably reproduces energies, forces and adsorption geometries across the three coinage metal substrates. We demonstrate its capabilities on experimentally characterized systems, including organic monolayers, polycyclic aggregates, flexible biomolecules and the long-range herringbone reconstruction of gold. By merging accuracy, speed, and generalizability, MAD-SURF offers a practical framework for accelerating atomistic simulations and advancing data-driven workflows in surface science.

</details>


### [70] [Symmetry Adapted Analysis of Screw Dislocation: Electronic Structure and Carrier Recombination Mechanisms in GaN](https://arxiv.org/abs/2601.19240)
*Yuncheng Xie,Haozhe Shi,Menglin Huang,Weibin Chu,Shiyou Chen,Xin-Gao Gong*

Main category: cond-mat.mtrl-sci

TL;DR: The paper develops a rigorous symmetry-based approach to analyze screw dislocations in GaN, revealing suppressed radiative recombination due to piezoelectrical effects at dislocation cores.


<details>
  <summary>Details</summary>
Motivation: Screw dislocations fundamentally alter energy landscapes and carrier dynamics in crystalline materials, but conventional treatments lack rigorous symmetry constraints needed for accurate electronic structure analysis.

Method: Restores exact algebra of screw dislocation group to unveil latent symmetry constraints, applies to GaN to derive band-connectivity constraints and rigorous dipole selection rules, combines with computed Hamiltonian matrix for symmetry-filtered calculations.

Result: Reveals piezoelectrical effect at dislocation core that strongly suppresses radiative recombination, shows pronounced dominance of non-radiative capture over radiative recombination in GaN.

Conclusion: Screw dislocations have detrimental impact on luminous efficiency of GaN, providing theoretical foundation for optimizing dislocation-limited optoelectronic devices.

Abstract: As fundamental one-dimensional defects, screw dislocations profoundly reshape the energy landscape and carrier dynamics of crystalline materials. By restoring the exact algebra of the screw dislocation group, we unveil the latent symmetry constraints that govern the electronic structure, providing a more rigorous physical picture than the conventional treatments. When applied to GaN, the method yields a band-connectivity constraint and rigorous dipole selection rules for polarization-resolved transitions. Combined with computed Hamiltonian matrix, the approach gives symmetry-filtered radiative and dielectric calculations and reveals a piezoelectrical effect at the dislocation core that strongly suppresses radiative recombination. The pronounced dominance of non-radiative capture over radiative recombination highlights the detrimental impact of screw dislocations on the luminous efficiency of GaN, providing a theoretical foundation for optimizing dislocation-limited optoelectronic devices.

</details>


### [71] [Comparative Analysis of Plasticity-based GND Density Estimation Methods in Crystal Plasticity Finite Element Models](https://arxiv.org/abs/2601.19763)
*Michael Pilipchuk,Chaitali Patil,Veera Sundararaghavan*

Main category: cond-mat.mtrl-sci

TL;DR: Comparison of two methods for quantifying geometrically necessary dislocations (GNDs) in CPFE simulations: projection technique vs. slip gradient method, showing projection yields lower GND densities in polycrystals.


<details>
  <summary>Details</summary>
Motivation: Accurate quantification of GNDs is critical for capturing strain gradients in polycrystal CPFE simulations, but different methods exist with unclear comparative performance.

Method: Compare projection technique (decomposes Nye tensor into dislocation components via L2 minimization) with slip gradient method (directly computes dislocation densities from shear gradients) across varying grain sizes, strains, and grain neighborhoods including multigrain junctions.

Result: Both methods match analytical GND densities for simple cases, but projection techniques yield significantly lower GND densities than slip gradient methods in polycrystals. Using only active dislocation systems in projection technique resolves this mismatch.

Conclusion: Projection technique underestimates GND densities in polycrystals compared to slip gradient method, but can be improved by considering only active dislocation systems, highlighting the importance of method selection for accurate GND quantification.

Abstract: In crystal plasticity finite element (CPFE) simulations, accurately quantifying geometrically necessary dislocations (GNDs) is critical for capturing strain gradients in polycrystals. We compare different methods for quantifying GNDs, all of which originate from the Nye tensor, which is computed as the curl of the plastic deformation gradient. The projection technique directly decomposes the Nye tensor onto individual screw and edge dislocation components to compute GNDs. This approach requires converting a nine-component Nye tensor into densities for a larger number of dislocation systems, a fundamentally underdetermined (non-unique) process, which is resolved using $L2$ minimization. In contrast, when employing CPFE analysis, one could directly compute dislocation densities on each slip system using shear gradients. Projection and slip gradient methods are compared with respect to their prediction of GNDs with changing grain size, strain, and grain neighborhoods, including multigrain junctions. Although these techniques match analytical GND densities for single slip, single crystal deformation, and are consistent with anticipated overall GND trends, we find that the GND densities from projection techniques are significantly lower than those predicted from CPFE-based slip gradients in polycrystals. A suggested improvement of only using the active dislocation systems in the projection technique almost entirely resolved this mismatch.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [72] [Contrast-Source-Based Physics-Driven Neural Network for Inverse Scattering Problems](https://arxiv.org/abs/2601.19243)
*Yutong Du,Zicheng Liu*

Main category: cs.LG

TL;DR: Proposes CSPDNN - a contrast-source-based physics-driven neural network for efficient inverse scattering problems that predicts induced currents with adaptive total variation loss for robust reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing DNN solvers for inverse scattering problems require large datasets (limiting generalization), while untrained neural networks (UNNs) suffer from long inference times. Need efficient physics-driven approach that doesn't require large datasets.

Method: Contrast-source-based physics-driven neural network (CSPDNN) that predicts induced current distribution directly, incorporates adaptive total variation loss for regularization, and uses measured electric fields with prior physical knowledge for weight updates without requiring large datasets.

Result: Improved imaging performance validated through comprehensive numerical simulations and experimental data. Method achieves efficient reconstruction under varying contrast and noise conditions.

Conclusion: CSPDNN provides an efficient physics-driven solution to inverse scattering problems that overcomes limitations of supervised DNNs (dataset requirements) and UNNs (slow inference), offering robust reconstruction performance.

Abstract: Deep neural networks (DNNs) have recently been applied to inverse scattering problems (ISPs) due to their strong nonlinear mapping capabilities. However, supervised DNN solvers require large-scale datasets, which limits their generalization in practical applications. Untrained neural networks (UNNs) address this issue by updating weights from measured electric fields and prior physical knowledge, but existing UNN solvers suffer from long inference time. To overcome these limitations, this paper proposes a contrast-source-based physics-driven neural network (CSPDNN), which predicts the induced current distribution to improve efficiency and incorporates an adaptive total variation loss for robust reconstruction under varying contrast and noise conditions. The improved imaging performance is validated through comprehensive numerical simulations and experimental data.

</details>


### [73] [Learn and Verify: A Framework for Rigorous Verification of Physics-Informed Neural Networks](https://arxiv.org/abs/2601.19818)
*Kazuaki Tanaka,Kohei Yatabe*

Main category: cs.LG

TL;DR: Proposes a "Learn and Verify" framework that provides mathematically rigorous error bounds for neural network solutions of differential equations, combining a novel Doubly Smoothed Maximum loss with interval arithmetic verification.


<details>
  <summary>Details</summary>
Motivation: Neural network solutions for differential equations (like PINNs) lack rigorous error bounds and convergence guarantees compared to classical numerical methods, making it difficult to mathematically certify their accuracy due to non-deterministic optimization.

Method: Uses a "Learn and Verify" framework combining: 1) Novel Doubly Smoothed Maximum (DSM) loss for training neural networks, and 2) Interval arithmetic for verification to compute rigorous a posteriori error bounds as machine-verifiable proofs.

Result: Numerical experiments on nonlinear ODEs (including problems with time-varying coefficients and finite-time blow-up) demonstrate successful construction of rigorous enclosures of true solutions.

Conclusion: The framework establishes a foundation for trustworthy scientific machine learning by providing computable, mathematically rigorous error bounds for neural network solutions of differential equations.

Abstract: The numerical solution of differential equations using neural networks has become a central topic in scientific computing, with Physics-Informed Neural Networks (PINNs) emerging as a powerful paradigm for both forward and inverse problems. However, unlike classical numerical methods that offer established convergence guarantees, neural network-based approximations typically lack rigorous error bounds. Furthermore, the non-deterministic nature of their optimization makes it difficult to mathematically certify their accuracy. To address these challenges, we propose a "Learn and Verify" framework that provides computable, mathematically rigorous error bounds for the solutions of differential equations. By combining a novel Doubly Smoothed Maximum (DSM) loss for training with interval arithmetic for verification, we compute rigorous a posteriori error bounds as machine-verifiable proofs. Numerical experiments on nonlinear Ordinary Differential Equations (ODEs), including problems with time-varying coefficients and finite-time blow-up, demonstrate that the proposed framework successfully constructs rigorous enclosures of the true solutions, establishing a foundation for trustworthy scientific machine learning.

</details>
