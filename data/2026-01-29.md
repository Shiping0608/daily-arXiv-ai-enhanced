<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 22]
- [math.AP](#math.AP) [Total: 22]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.DC](#cs.DC) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [math.DG](#math.DG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [New Adaptive Numerical Methods Based on Dual Formulation of Hyperbolic Conservation Laws](https://arxiv.org/abs/2601.20000)
*Alina Chertock,Qingcheng Fu,Alexander Kurganov,Lorenzo Micalizzi*

Main category: math.NA

TL;DR: Adaptive high-order method for hyperbolic conservation laws using dual conservative/nonconservative formulations to create smoothness indicators, enabling selective discretization strategies for different flow features.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency and resolution of complex flow features in hyperbolic systems by adaptively selecting appropriate numerical discretizations based on local solution smoothness.

Method: Dual formulation approach evolving both conservative and nonconservative solutions simultaneously, using their differences to construct smoothness indicators that distinguish smooth regions, contact discontinuities, and other nonsmooth regions, then applying adaptive discretization strategies accordingly.

Result: Numerical results for 1D and 2D compressible Euler equations show improved computational efficiency and better resolution of complex flow features compared to non-adaptive fifth-order A-WENO schemes.

Conclusion: The adaptive method successfully leverages dual formulation differences to create effective smoothness indicators, enabling selective application of appropriate numerical techniques that balance accuracy and efficiency for different flow features.

Abstract: In this paper, we propose an adaptive high-order method for hyperbolic systems of conservation laws. The proposed method is based on a dual formulation approach: Two numerical solutions, corresponding to conservative and nonconservative formulations of the same system, are evolved simultaneously. Since nonconservative schemes are known to produce nonphysical weak solutions near discontinuities, we exploit the difference between these two solutions to construct a smoothness indicator (SI). In smooth regions, the difference between the conservative and nonconservative solutions is of the same order as the truncation error of the underlying discretization, whereas in nonsmooth regions, it is ${\cal O}(1)$. We apply this idea to the Euler equations of gas dynamics and define the SI using differences in the momentum and pressure variables. This choice allows us to further distinguish neighborhoods of contact discontinuities from other nonsmooth parts of the computed solution. The resulting classification is used to adaptively select numerical discretizations. In the vicinities of contact discontinuities, we employ the low-dissipation central-upwind numerical flux and a second-order piecewise linear reconstruction with the slopes computed using an overcompressive SBM limiter. Elsewhere, we use an alternative weighted essentially non-oscillatory (A-WENO) framework with the central-upwind finite-volume numerical fluxes and either unlimited (in smooth regions) or Ai-WENO-Z (in the nonsmooth regions away from contact discontinuities) fifth-order interpolation. Numerical results for the one- and two-dimensional compressible Euler equations show that the proposed adaptive method improves both the computational efficiency and resolution of complex flow features compared with the non-adaptive fifth-order A-WENO scheme.

</details>


### [2] [Least-Squares Neural Network (LSNN) Method for Scalar Hyperbolic Partial Differential Equations](https://arxiv.org/abs/2601.20013)
*Min Liu,Zhiqiang Cai*

Main category: math.NA

TL;DR: The paper introduces the least-squares neural network (LSNN) method for solving first-order hyperbolic PDEs, using ReLU networks instead of finite elements, with physics-preserved numerical differentiation and no artificial penalization techniques.


<details>
  <summary>Details</summary>
Motivation: To develop a method for solving hyperbolic PDEs that can capture shock features without oscillations or overshooting, while avoiding traditional penalization techniques like artificial viscosity, entropy conditions, or total variation.

Method: LSNN method uses an equivalent least-squares formulation on an admissible solution set for discontinuous solutions, employs ReLU neural networks as approximating functions, implements physics-preserved numerical differentiation, and avoids penalization techniques.

Result: The method successfully captures shock features in solutions without oscillations or overshooting, though efficiently solving the resulting non-convex optimization problem remains an open challenge.

Conclusion: The LSNN method provides an effective approach for hyperbolic PDEs, and future work involves applying the structure-guided Gauss-Newton method to solve the optimization challenges in shallow neural network approximations.

Abstract: This chapter offers a comprehensive introduction to the least-squares neural network (LSNN) method introduced in [14,16], for solving scalar first-order hyperbolic partial differential equations, specifically linear advection-reaction equations and nonlinear hyperbolic conservation laws. The LSNN method is built on an equivalent least-squares formulation of the underlying problem on an admissible solution set that accommodates discontinuous solutions. It employs ReLU neural networks (in place of finite elements) as the approximating functions, uses a carefully designed physics-preserved numerical differentiation, and avoids penalization techniques such as artificial viscosity, entropy condition, and/or total variation. This approach captures shock features in the solution without oscillations or overshooting. Efficiently and reliably solving the resulting non-convex optimization problem posed by the LSNN method remains an open challenge. This chapter concludes with a brief discussion on application of the structure-guided Gauss-Newton (SgGN) method developed recently in [21] for solving shallow NN approximation.

</details>


### [3] [A mixed virtual element discretization for the generalized Oseen problem](https://arxiv.org/abs/2601.20050)
*Felipe Lepe,Gonzalo Rivera*

Main category: math.NA

TL;DR: A mixed virtual element method for 2D generalized Oseen problems using pseudostress as an additional unknown, with pressure recovery via post-processing, proven stable with a priori error estimates validated numerically.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving the two-dimensional generalized Oseen problem, which is important in fluid dynamics, by introducing pseudostress to eliminate pressure from the system and using virtual elements for approximation.

Method: Mixed virtual element method with pseudostress as additional unknown, eliminating pressure from system (recovered via post-processing), using fixed point argument for existence/uniqueness, developing VEM for tensor and velocity fields on polygonal meshes.

Result: Proved existence and uniqueness of continuous solution, developed stable virtual element method, provided a priori error estimates, and validated method through numerical tests on different polygonal meshes.

Conclusion: The proposed mixed virtual element method is effective for approximating solutions to 2D generalized Oseen problems, offering stability, theoretical error bounds, and practical validation on various mesh types.

Abstract: In this paper we introduce a mixed virtual element method to approximate the solution for the two dimensional generalized Oseen problem. We introduce the pseudostress as an additional unknown, which allows to eliminate the pressure from the system; the pressure can be recovered via a post-process of the pseudostress tensor. We prove existence and uniqueness of the continuous solution via a fixed point argument. Under standard mesh assumptions, we develop a virtual element method to approximate both the tensor and the velocity field, and we show that it is stable. Furthermore, we provide a priori error estimates for the method and validate them through a series of numerical tests using different polygonal meshes.

</details>


### [4] [Improving Smoothed Aggregation AMG Robustness on Stretched Mesh Applications](https://arxiv.org/abs/2601.20119)
*Chris Siefert,Raymond Tuminaro,Daniel Sunderland*

Main category: math.NA

TL;DR: The paper analyzes strength-of-connection algorithms in algebraic multigrid, identifies limitations in current approaches, and proposes four alternative methods to improve robustness on stretched meshes.


<details>
  <summary>Details</summary>
Motivation: Current strength-of-connection schemes in smoothed aggregation AMG can fail on certain problems, particularly matrices from linear finite elements on stretched meshes. The paper aims to develop more robust and inexpensive alternatives to address these failure cases.

Method: The paper proposes four alternative approaches: 1) Using a distance Laplacian strength-of-connection matrix instead of the original linear system matrix, 2) Non-symmetric scaling of matrix values, 3) Alternative classification criteria based on identifying gaps in scaled entry values, and 4) Alternative lumping procedure that modifies all retained matrix entries (not just diagonals) to preserve row sums.

Result: Numerical results demonstrate trade-offs between the proposed alternatives, showing notably more robust convergence on matrices from linear finite elements on stretched meshes in some cases.

Conclusion: The paper identifies weaknesses in traditional strength-of-connection algorithms and presents alternative approaches that can improve robustness for challenging problems like those arising from stretched meshes, though trade-offs exist between different methods.

Abstract: Strength-of-connection algorithms play a key role in algebraic multigrid (AMG). Specifically, they determine which matrix nonzeros are classified as weak and so ignored when coarsening matrix graphs and defining interpolation sparsity patterns. The general goal is to encourage coarsening only in directions where error can be smoothed and to avoid coarsening across sharp problem variations. Unfortunately, developing robust and inexpensive strength-of-connection schemes is challenging.
  The classification of matrix nonzeros involves four aspects: (a) choosing a strength-of-connection matrix, (b) scaling its values, (c) choosing a criterion to classify scaled values as strong or weak, and (d) dropping weak entries which includes adjusting matrix values to account for dropped terms. Typically, smoothed aggregation AMG uses the linear system being solved as a strength-of-connection matrix. It scales values symmetrically using square-roots of the matrix diagonal. It classifies based on whether scaled values are above or below a threshold. Finally, it adjusts matrix values by modifying the diagonal so that the sum of entries within each row of the dropped matrix matches that of the original. While these procedures can work well, we illustrate failure cases that motivate alternatives. The first alternative uses a distance Laplacian strength-of-connection matrix. The second centers on non-symmetric scaling. We then investigate alternative classification criteria based on identifying gaps in the values of the scaled entries. Finally, an alternative lumping procedure is proposed where row sums are preserved by modifying all retained matrix entries (as opposed to just diagonal entries). A series of numerical results illustrates trade-offs demonstrating in some cases notably more robust convergence on matrices coming from linear finite elements on stretched meshes.

</details>


### [5] [Error estimates of $hp$-finite element method for elliptic optimal control problems with robin boundary](https://arxiv.org/abs/2601.20145)
*Xingyuan Lin,Xiuxiu Lin,Xuesong Chen*

Main category: math.NA

TL;DR: A priori and a posteriori error analysis of hp FEM for elliptic control problems with Robin boundary conditions and boundary observation.


<details>
  <summary>Details</summary>
Motivation: To develop comprehensive error analysis for hp finite element methods applied to elliptic optimal control problems with Robin boundary conditions, addressing both theoretical error bounds and practical error estimation.

Method: Uses Clément-type approach and auxiliary system construction for a priori estimates; employs Scott-Zhang-type quasi-interpolation and coupled state-control approximations for residual-based a posteriori error estimates.

Result: Derived both a priori and a posteriori error estimates for hp FEM applied to elliptic optimal control problems with Robin boundary conditions, establishing a complete error analysis framework.

Conclusion: The paper provides a comprehensive error analysis framework for hp FEM in elliptic control problems with Robin boundaries, validated by numerical examples demonstrating accurate error estimation.

Abstract: A priori and a posteriori error analysis of $hp$ finite element method for elliptic control problem with Robin boundary condition and boundary observation are presented. are presented. Through the Clément-type approach and the construction of an auxiliary system, we derived a priori error estimates for the elliptic optimal control problem. Residual-based a posteriori error estimates are derived based on the well-known Scott-Zhang-type quasi-interpolation and coupled state-control approximations, thus establishing an a posteriori error estimator for the $hp$ finite element method. The numerical example demonstrates the accuracy of error estimation for the elliptic optimal control problems with Robin boundary.

</details>


### [6] [A direct sampling method for magnetic induction tomography](https://arxiv.org/abs/2601.20191)
*Junqing Chen,Chengzhe Jiang*

Main category: math.NA

TL;DR: Direct sampling method for magnetic induction tomography using point spread functions with explicit expressions for fast imaging.


<details>
  <summary>Details</summary>
Motivation: To develop a simple and fast imaging method for magnetic induction tomography inverse problems that provides theoretical guarantees.

Method: Defines class of point spread functions with explicit expressions computed via inner products, proves their distance decay properties, derives specific expressions for special cases.

Result: Numerical experiments confirm the efficiency and accuracy of the proposed algorithm.

Conclusion: The proposed direct sampling method provides a theoretically sound, computationally efficient approach to magnetic induction tomography imaging with proven decay properties.

Abstract: This paper proposes a direct sampling method for the inverse problem of magnetic induction tomography (MIT). Our approach defines a class of point spread functions with explicit expressions, which are computed via inner products, leading to a simple and fast imaging process. We then prove that these point spread functions decay with distance, establishing the theoretical basis of the algorithm. Specific expressions for special cases are also derived to visually demonstrate their attenuation pattern. Numerical experimental results further confirm the efficiency and accuracy of the proposed algorithm.

</details>


### [7] [Local Regularity Estimation through Sobolev-Scale Norm Profile](https://arxiv.org/abs/2601.20207)
*Xiaobin Li,Leevan Ling,Yizhong Sun*

Main category: math.NA

TL;DR: Kernel-based method to estimate spatially varying Sobolev regularity of multivariate functions from scattered data using local interpolation and norm profile analysis.


<details>
  <summary>Details</summary>
Motivation: Need to quantify local differentiability (Sobolev regularity) of unknown multivariate functions from scattered sampling data, which is important for understanding function behavior, guiding approximation methods, and analyzing complex data like turbulent flows.

Method: Constructs sequence of Sobolev-space reproducing kernel interpolants with varying smoothness orders, evaluates native-space norms to create "Sobolev-scale norm profile," identifies elbow point as regularity estimate. Includes stencil-shift to avoid discontinuities and norm-sweep comparison for efficiency.

Result: Method accurately recovers spatially varying regularity in synthetic test functions and turbulent-flow data, demonstrating robustness for kernel-based approximation and differentiation.

Conclusion: Proposed kernel-based approach provides effective local regularity estimation from scattered data, with practical enhancements for handling discontinuities and computational efficiency on large datasets.

Abstract: We develop a kernel-based approach for estimating the spatially varying Sobolev regularity~$s$ of an unknown $d$-variate function~$f$ from scattered sampling data, which quantifies the degree of local differentiability supported by the data. Relying only on neighborhood data near the point of interest $z\in Ω_z$, our method constructs a sequence of Sobolev-space reproducing kernel interpolants whose kernel smoothness order is specified by an index~$m > d/2$. The native-space norms of these interpolants are evaluated over a bounded range of~$m$, producing a \emph{Sobolev-scale norm profile}. The elbow of this profile serves as a quantitative probe of the underlying local regularity~$s(Ω_z)$. In particular, when $m > s(Ω_z)$, the profile exhibits rapid, near-worst-case growth governed by the classical upper bound associated with the conditioning of the kernel matrix. A band-limited surrogate analysis explains this transition and establishes a lower-bound relation linking native-norm growth to the Sobolev regularity of~$f$. Two complementary strategies are incorporated for further enhancement: (i)~a \emph{stencil-shift} subroutine, which repositions local neighborhoods to avoid crossing discontinuities whenever possible, thereby suppressing artifacts in the norm estimates; and (ii)~a local--global \emph{norm-sweep comparison} strategy that combines short two-point local tails with an optional one-point global screen to detect outlier $Ω_z$ of low Sobolev regularity and accelerate evaluation on large datasets. Numerical experiments on synthetic test functions and turbulent-flow data demonstrate accurate recovery of spatially varying regularity and confirm the robustness of the proposed characterization for kernel-based approximation and differentiation.

</details>


### [8] [A low regularity exponential-type integrator for the derivative nonlinear Schrödinger equation](https://arxiv.org/abs/2601.20212)
*Lun Ji,Hang Li,Alexander Ostermann*

Main category: math.NA

TL;DR: First-order unfiltered exponential integrator for 1D derivative nonlinear Schrödinger equation with low regularity, achieving convergence for s>½ with H^{s+1} initial data, plus a symmetrized version with improved performance.


<details>
  <summary>Details</summary>
Motivation: There is a need for low regularity integrators for the derivative nonlinear Schrödinger equation, as existing methods may not handle low regularity cases effectively. The authors aim to develop numerical methods that work well even when the solution lacks high regularity.

Method: Developed a first-order unfiltered exponential integrator specifically for the one-dimensional derivative nonlinear Schrödinger equation. Also constructed a symmetrized version of this method to improve performance. The analysis focuses on low regularity cases where s>½.

Result: The method converges with first-order in H^s space for initial data in H^{s+1} when s>½. The symmetrized version performs better in terms of both global error and conservation behavior. Numerical experiments confirm the theoretical findings.

Conclusion: These are the first low regularity integrators developed for the derivative nonlinear Schrödinger equation, providing effective numerical methods for cases with limited regularity. The symmetrized version offers improved accuracy and conservation properties.

Abstract: In this work, we present a first-order unfiltered exponential integrator for the one-dimensional derivative nonlinear Schrödinger equation with low regularity. Our analysis shows that for any $s>\frac12$, the method converges with first-order in $H^s(\mathbb{T})$ for initial data $u_0\in H^{s+1}(\mathbb{T})$. Moreover, we constructed a symmetrized version of this method that performs better in terms of both global error and conservation behavior. To the best of our knowledge, these are the first low regularity integrators for the derivative nonlinear Schrödinger equation. Numerical experiments illustrate our theoretical findings.

</details>


### [9] [A Unified Variational Functional for Equidistribution and Alignment in Moving Mesh Adaptation](https://arxiv.org/abs/2601.20235)
*Wenbin Wang,Yunqing Huang,Huayi Wei*

Main category: math.NA

TL;DR: New variational functional for adaptive moving mesh generation using A-pullback formulation that enforces equidistribution and alignment without empirical parameters.


<details>
  <summary>Details</summary>
Motivation: Existing variational mesh functionals suffer from strong nonlinearity or dependence on empirical parameters, limiting their robustness and theoretical foundation.

Method: Propose a new variational functional using A-pullback formulation (A=J⁻¹M⁻¹J⁻ᵀ) that combines trace-based term with logarithmic determinant term for balanced control of mesh size and anisotropy.

Result: Established theoretical properties: coercivity, polyconvexity, existence of minimizers, geodesic convexity. Developed efficient moving mesh algorithm via simplified geometric discretization.

Conclusion: The proposed functional provides robust adaptive mesh generation without empirical parameters, demonstrated through numerical experiments including function-induced meshes and Rayleigh-Taylor instability simulations.

Abstract: Existing variational mesh functionals often suffer from strong nonlinearity or dependence on empirical parameters.We propose a new variational functional for adaptive moving mesh generation that enforces equidistribution and alignment through an $\boldsymbol A$-pullback formulation, where $\boldsymbol A=\boldsymbol J^{-1}\boldsymbol M^{-1}\boldsymbol J^{-T}$. The functional combines a trace-based term with a logarithmic determinant term, achieving balanced control of mesh size and anisotropy without empirical parameters. We establish coercivity, polyconvexity, existence of minimizers, and geodesic convexity with respect to the inverse Jacobian, and derive a simplified geometric discretization leading to an efficient moving mesh algorithm. Numerical experiments confirm the theoretical properties and demonstrate robust adaptive behavior for function-induced meshes and Rayleigh-Taylor instability simulations.

</details>


### [10] [Element-based B-spline basis function spaces: construction and application in isogeometric analysis](https://arxiv.org/abs/2601.20243)
*Peng Yang,Maodong Pan,Falai Chen,Zhimin Zhang*

Main category: math.NA

TL;DR: This paper presents a unified framework for constructing B-spline basis functions equivalent to finite element spaces, enabling Hermite interpolation at nodes without solving global systems, with applications in isogeometric analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a theoretical framework that bridges B-spline basis functions with finite element spaces, enabling efficient computation in isogeometric analysis while maintaining optimal approximation properties and allowing direct imposition of boundary conditions.

Method: The method constructs B-spline basis function spaces as explicit linear combinations of B-spline element bases. It uses an element-wise formulation that enables Hermite interpolation at nodes using function values and derivatives without solving global linear systems. The framework decomposes geometric mappings into element-level representations for efficient computation.

Result: Numerical tests demonstrate optimal convergence rates and superconvergence properties in 2D isogeometric analysis under uniform knot configurations, and improved computational efficiency in 3D IgA with non-uniform knot distributions. The method handles both domain parameterization and IgA solutions simultaneously.

Conclusion: The paper establishes a unified theoretical framework that successfully bridges B-spline and finite element spaces, enabling efficient Hermite interpolation without global system solving, with demonstrated optimal convergence and computational efficiency in isogeometric analysis applications.

Abstract: This paper develops a unified theoretical framework for constructing B-spline basis function spaces with structural equivalence to finite element spaces. The theory rigorously establishes that these bases emerge as explicit linear combinations of B-spline element bases. For any prescribed smoothness requirements, this element-wise formulation enables the Hermite interpolation at nodes, which directly utilizes function values and derivatives without solving global linear systems. By focusing on explicit interpolation properties, element-wise analysis establishes optimal approximation errors, even when the space smoothness attains its theoretical maximum for the space degree. In isogeometric analysis (IgA), the construction naturally decomposes geometric mappings into element-level representations, allowing efficient computations across elements regardless of node distribution. Notably, the same Hermite interpolation framework simultaneously handles domain parameterization and IgA solutions, allowing direct imposition of boundary conditions through function and derivative matching. Numerical tests demonstrate optimal convergence rates and superconvergence properties in 2D IgA under uniform knot configurations, and improved computational efficiency in 3D IgA with non-uniform knot distributions.

</details>


### [11] [A note on approximation in weighted Korobov spaces via multiple rank-1 lattices](https://arxiv.org/abs/2601.20290)
*Mou Cai,Takashi Goda*

Main category: math.NA

TL;DR: Extends multiple rank-1 lattice algorithms to handle lower smoothness (1/2<α≤1) and general weights in weighted Korobov spaces, achieving optimal convergence rates and providing tractability conditions.


<details>
  <summary>Details</summary>
Motivation: Previous results for multiple rank-1 lattice algorithms in weighted Korobov spaces required smoothness parameter α>1 and were restricted to product weights. This paper aims to extend these results to cover lower smoothness (1/2<α≤1) and general weights, addressing a broader class of periodic functions with low smoothness and variable importance.

Method: Extends multiple rank-1 lattice-based algorithms to handle the case where 1/2<α≤1 and general weights. Also incorporates random shifts into multiple rank-1 lattice algorithms to create randomized algorithms. Provides summability conditions on weights to ensure strong polynomial tractability.

Result: Successfully extends multiple rank-1 lattice algorithms to lower smoothness parameters (1/2<α≤1) and general weights. Shows that with appropriate summability conditions on weights, strong polynomial tractability can be achieved for any α>1/2. Demonstrates that randomized algorithms with random shifts achieve nearly optimal convergence rates for worst-case root mean squared L₂ error while maintaining tractability properties.

Conclusion: The paper significantly broadens the applicability of multiple rank-1 lattice algorithms to include functions with lower smoothness and more general weight structures. The results provide both deterministic and randomized algorithms with optimal convergence rates and tractability properties for a wider range of problems in multivariate approximation.

Abstract: This paper studies the multivariate approximation of functions in weighted Korobov spaces using multiple rank-1 lattice rules. It has been shown by Kämmerer and Volkmer (2019) that algorithms based on multiple rank-1 lattices achieve the optimal convergence rate for the $L_{\infty}$ error in Wiener-type spaces, up to logarithmic factors. While this result was translated to weighted Korobov spaces in the recent monograph by Dick, Kritzer, and Pillichshammer (2022), the analysis requires the smoothness parameter $α$ to be greater than $1$ and is restricted to product weights. In this paper, we extend this result for multiple rank-1 lattice-based algorithms to the case where $1/2<α\le 1$ and for general weights, covering a broader range of periodic functions with low smoothness and general relative importance of variables. We also provide a summability condition on the weights to ensure strong polynomial tractability for any $α>1/2$. Furthermore, by incorporating random shifts into multiple rank-1 lattice-based algorithms, we prove that the resulting randomized algorithm achieves a nearly optimal convergence rate in terms of the worst-case root mean squared $L_2$ error, while retaining the same tractability property.

</details>


### [12] [Refined rates of convergence for target-data dependent greedy generalized interpolation with Sobolev kernels](https://arxiv.org/abs/2601.20407)
*Bernard Haasdonk,Gabriele Santin,Tizian Wenzel,Daniel Winkle*

Main category: math.NA

TL;DR: This paper removes a spurious logarithmic term from convergence rate estimates for greedy kernel interpolation methods, improving theoretical guarantees for high-dimensional problems.


<details>
  <summary>Details</summary>
Motivation: Greedy methods for kernel interpolation show promising convergence rates, especially for high-dimensional problems, but current theoretical estimates contain an unnecessary logarithmic term that limits their beneficial effects.

Method: The authors use estimates on metric entropy numbers to refine the convergence rate analysis of greedy kernel interpolation methods, specifically targeting the removal of the spurious logarithmic factor.

Result: The paper successfully removes the logarithmic term from convergence rate estimates, providing cleaner theoretical guarantees for greedy kernel interpolation methods in Sobolev spaces.

Conclusion: By eliminating the spurious logarithmic factor through metric entropy number estimates, the paper provides improved theoretical convergence rates for greedy kernel interpolation methods, particularly beneficial for high-dimensional applications.

Abstract: Greedy methods have recently been successfully applied to generalized kernel interpolation, or the recovery of a function from data stemming from the evaluation of linear functionals, including the approximation of solutions of linear PDEs by symmetric collocation. When applied to kernels generating Sobolev spaces as their native Hilbert spaces, some of these greedy methods can provide the same error guarantee of generalized interpolation on quasi-uniform points. More importantly, certain target-data-adaptive methods even give a dimension- and smoothness-independent improvement in the speed of convergence over quasi-uniform points, thus offering advantages for high-dimensional problems. These convergence rates however contain a spurious logarithmic term that limits this beneficial effect. The goal of this note is to remove this factor, and this is possible by using estimates on metric entropy numbers.

</details>


### [13] [Fokker--Planck Dynamics on Star Graphs with Variable Drift: Well-Posedness, Adjoint Analysis, and Numerical Approximation](https://arxiv.org/abs/2601.20456)
*Ritu Kumari,Cyrille Kenne,Landry Djomegne,Mani Mehra*

Main category: math.NA

TL;DR: Optimal control of Fokker-Planck equations on star graphs with bilinear drift control, establishing well-posedness, existence of optimal controls, deriving adjoint systems, and proposing wavelet-based numerical scheme.


<details>
  <summary>Details</summary>
Motivation: Stochastic transport processes on networked domains (metric graphs) arise in various applications where diffusion and drift interact with graph structures. While Fokker-Planck equations on metric graphs have been studied analytically, their optimal control remains largely unexplored, especially when control acts through the drift term.

Method: Investigates optimal control problem governed by Fokker-Planck equation on a star graph with bilinear control in drift. Establishes well-posedness of state equation, proves existence of optimal controls, derives adjoint system, formulates first-order necessary optimality conditions, and proposes wavelet-based numerical scheme for approximation.

Result: Established well-posedness of state equation, proved existence of at least one optimal control, derived associated adjoint system, formulated first-order necessary optimality conditions, and demonstrated performance of wavelet-based numerical scheme through representative experiments.

Conclusion: Results contribute to analytical and computational understanding of controlled stochastic dynamics on network-like domains, providing theoretical foundation and numerical methods for optimal control of Fokker-Planck equations on metric graphs.

Abstract: Stochastic transport processes on networked domains (modelled on metric graphs) arise in a variety of applications where diffusion and drift mechanisms interact with an underlying graph structure. The Fokker--Planck equation provides a natural framework for describing the evolution of probability densities associated with such dynamics. While Fokker--Planck equations on metric graphs have been studied from an analytical viewpoint, their optimal control remains largely unexplored, particularly in settings where the control acts through the drift term. In this paper, we investigate an optimal control problem governed by the Fokker--Planck equation on a star graph, with a bilinear control appearing in the drift. We establish the well-posedness of the state equation and prove the existence of at least one optimal control. The associated adjoint system is derived, and first-order necessary optimality conditions are formulated. A wavelet-based numerical scheme is proposed to approximate the optimal solution, and its performance is illustrated through representative numerical experiments. These results contribute to the analytical and computational understanding of controlled stochastic dynamics on network-like domains.

</details>


### [14] [Monotone-based Numerical Schemes for Two-Dimensional Systems of Nonlocal Conservation Laws](https://arxiv.org/abs/2601.20494)
*Anika Beckers,Jan Friedrich*

Main category: math.NA

TL;DR: Monotone-based numerical schemes for 2D nonlocal conservation laws with error estimates and convergence proofs.


<details>
  <summary>Details</summary>
Motivation: To develop robust numerical methods for solving two-dimensional systems of nonlocal conservation laws, which are weakly coupled through nonlocal terms and have broad applicability to various nonlocal models.

Method: Utilize well-known monotone numerical flux functions with suitable approximations of nonlocal terms, providing sufficient conditions for convergence to weak entropy solutions with error estimates.

Result: Convergence of monotone-based schemes to unique weak entropy solution with convergence rate O(√Δt), plus existence and uniqueness proof for the nonlocal system.

Conclusion: The proposed numerical schemes are effective for 2D nonlocal conservation laws, with theoretical convergence guarantees and practical validation through numerical experiments.

Abstract: We present a class of numerical schemes for two-dimensional systems of nonlocal conservation laws, which are based on utilizing well-known monotone numerical flux functions after suitably approximating the nonlocal terms. The considered systems are weakly coupled by the nonlocal terms and the underlying flux function is rather general to guarantee that our results are applicable to a wide range of common nonlocal models. We state sufficient conditions to ensure the convergence of the monotone-based numerical schemes to the unique weak entropy solution. Moreover, we provide an error estimate that yields the convergence rate of $\mathcal{O}(\sqrt{Δt})$ for the numerical approximations of the solution. Our results include an existence and uniqueness proof of the nonlocal system, too. Numerical results illustrate our theoretical findings.

</details>


### [15] [Local convergence analysis of a linearized Alikhanov scheme for the time fractional sine-Gordon equation](https://arxiv.org/abs/2601.20566)
*Chang Hou,Hu Chen*

Main category: math.NA

TL;DR: A fully discrete linearized scheme for time fractional sine-Gordon equation with weak singularity, achieving optimal temporal convergence order min{2,r} in H^1-seminorm on graded meshes.


<details>
  <summary>Details</summary>
Motivation: The time fractional sine-Gordon equation exhibits weak singularities (t^α type) in its solutions, requiring specialized numerical methods that can handle these singularities while maintaining accuracy.

Method: Uses Alikhanov formula to derive a fully discrete linearized scheme, employs general regularity assumptions for sharp truncation-error bounds, proves key inequality and stability results valid on general graded temporal meshes.

Result: Achieves temporal local convergence order of min{2, r} in H^1-seminorm, where r is the degree of grading; numerical experiments show optimal rate is attained with r = 2.

Conclusion: The proposed method effectively handles weak singularities in time fractional sine-Gordon equation through graded temporal meshes, achieving optimal convergence rates with moderate grading (r=2).

Abstract: This paper investigates the time fractional sine-Gordon equation whose solution exhibits a weak singularity of type t^α. By means of the Alikhanov formula we derive a fully discrete, linearized scheme. Using the more general regularity assumption, we derive a sharp truncation-error bound for the fractional derivative. Furthermore, we prove a key inequality and a less restrictive stability result that is valid on general graded temporal meshes. Consequently, the temporal local convergence order is shown to be min{2, r} in H^1-seminorm, where r is the degree of grading; numerical experiments confirm that the optimal rate is already attained as soon as r = 2.

</details>


### [16] [Quantitative synthetic aperture radar inversion](https://arxiv.org/abs/2601.20583)
*Liliana Borcea,Josselin Garnier,Alexander V. Mamonov,Jörn Zimmerling*

Main category: math.NA

TL;DR: Two-step inverse scattering method for monostatic SAR: first computes internal wave from measurements non-iteratively, then optimizes to match Maxwell's equations iteratively.


<details>
  <summary>Details</summary>
Motivation: Standard nonlinear least squares approach to SAR inverse scattering is difficult due to oscillatory nature of high-frequency measurements and multiple scattering effects. Need more efficient method for wave speed estimation in heterogeneous media.

Method: Two-step approach: 1) Non-iterative computation of internal wave (approximate electric field) from measurements for each antenna location; 2) Iterative optimization to minimize discrepancy between internal wave and Maxwell's equations solution across all antenna locations.

Result: First step provides imaging function with computational cost comparable to standard SAR but better target support estimation. Further iterations improve quantitative wave speed estimation. Numerical simulations show performance advantages over standard inversion.

Conclusion: Proposed two-step method offers practical alternative to standard nonlinear least squares for SAR inverse scattering, with better target localization and improved quantitative wave speed estimation through iterative refinement.

Abstract: We study an inverse scattering problem for monostatic synthetic aperture radar (SAR): Estimate the wave speed in a heterogeneous, isotropic and nonmagnetic medium probed by waves emitted and measured by a moving antenna. The forward map, from the wave speed to the measurements, is derived from Maxwell's equations. It is a nonlinear map that accounts for multiple scattering and it is very oscillatory at high frequencies. This makes the standard, nonlinear least squares data fitting formulation of the inverse problem difficult to solve. We introduce an alternative, two-step approach: The first step computes the nonlinear map from the measurements to an approximation of the electric field inside the unknown medium aka, the internal wave. This is done for each antenna location in a non-iterative manner.
  The internal wave fits the data by construction, but it does not solve Maxwell's equations. The second step uses optimization to minimize the discrepancy between the internal wave and the solution of Maxwell's equations, for all antenna locations. The optimization is iterative. The first step defines an imaging function whose computational cost is comparable to that of standard SAR imaging, but it gives a better estimate of the support of targets. Further iterations improve the quantitative estimation of the wave speed. We assess the performance of the method with numerical simulations and compare the results with those of standard inversion.

</details>


### [17] [Unconditional full linear convergence and quasi-optimal complexity of smoothed adaptive finite element methods](https://arxiv.org/abs/2601.20677)
*Philipp Bringmann,Christoph Lietz,Dirk Praetorius*

Main category: math.NA

TL;DR: S-AFEM achieves comparable mesh adaptation to classical AFEM at lower cost by using periodic accurate solves with cheap smoothing iterations between levels, with proven R-linear convergence and optimal computational rates.


<details>
  <summary>Details</summary>
Motivation: To reduce computational cost of adaptive finite element methods while maintaining their convergence properties, by replacing expensive discrete solves at every level with cheaper smoothing operations for most levels.

Method: Smoothed adaptive finite element method (S-AFEM) that performs accurate discrete solves only on periodically determined mesh levels, while intermediate levels use a fixed number of cheap smoothing iterations (Richardson, Gauss-Seidel, conjugate gradient, or multigrid).

Result: Proves unconditional full R-linear convergence of a quasi-error quantity and optimal convergence rates with respect to overall computational cost for sufficiently small adaptivity parameters, requiring only mild uniform stability assumption on the smoother.

Conclusion: S-AFEM retains all desired abstract convergence guarantees of classical AFEM while substantially reducing cumulative computational time, validated by numerical experiments showing potential for speed-up in AFEM computations.

Abstract: We present the first rigorous convergence analysis of the smoothed adaptive finite element method (S-AFEM) proposed in [Mulita, Giani, Heltai: SIAM J. Sci. Comput. 43, 2021]. S-AFEM modifies the classical adaptive finite element method (AFEM) by performing accurate discrete solves only on periodically determined mesh levels, while the intermediate levels employ a fixed number of cheap smoothing iterations. Numerical experiments in that work showed that this strategy generates adapted meshes comparable to those of AFEM at substantially lower computational cost. In this paper, we prove unconditional full R-linear convergence of a suitable quasi-error quantity and, for sufficiently small adaptivity parameters, optimal convergence rates with respect to the overall computational cost. The analysis requires only a mild uniform stability assumption on the employed smoother, satisfied by standard methods such as Richardson, Gauss-Seidel, conjugate gradient, and multigrid schemes. Our results apply to general second-order linear elliptic PDEs and show that S-AFEM retains all desired abstract convergence guarantees of AFEM while reducing the cumulative computational time. Numerical experiments validate the theory, analyze runtime performance, and underline the potential of S-AFEM for speed-up in AFEM computations.

</details>


### [18] [Adaptive domain decomposition method for time-dependent problems with applications in fluid dynamics](https://arxiv.org/abs/2601.20750)
*Vit Dolejsi,Jakub Sistek*

Main category: math.NA

TL;DR: Adaptive space-time DG method for PDEs with domain decomposition preconditioning and cost-optimized adaptive domain decomposition.


<details>
  <summary>Details</summary>
Motivation: Need efficient numerical solution of time-dependent PDEs using adaptive DG methods, where nonlinear systems require iterative solvers with effective preconditioning for varying system sizes due to mesh adaptation.

Method: Adaptive space-time DG discretization, Newton-like iterative solver with GMRES, additive/hybrid two-level Schwarz domain decomposition preconditioners, cost model based on FLOPs, computation speed, and communication time, adaptive domain decomposition to optimize subdomains and coarse grid elements.

Result: Studied convergence dependence on subdomains and coarse grid elements, proposed cost model, developed adaptive domain decomposition method to minimize computational costs, demonstrated efficiency on compressible flow benchmark problems.

Conclusion: The adaptive domain decomposition method with optimized preconditioner selection effectively reduces computational costs for adaptive DG simulations of compressible flows.

Abstract: We deal with the numerical solution of the time-dependent partial differential equations using the adaptive space-time discontinuous Galerkin (DG) method. The discretization leads to a nonlinear algebraic system at each time level, the size of the system is varying due to mesh adaptation. A Newton-like iterative solver leads to a sequence of linear algebraic systems which are solved by GMRES solver with a domain decomposition preconditioner. Particularly, we consider additive and hybrid two-level Schwarz preconditioners which are efficient and easy to implement for DG discretization. We study the convergence of the linear solver in dependence on the number of subdomains and the number of element of the coarse grid. We propose a simplified cost model measuring the computational costs in terms of floating-point operations, the speed of computation, and the wall-clock time for communications among computer cores. Moreover, the cost model serves as a base of the presented adaptive domain decomposition method which chooses the number of subdomains and the number of element of the coarse grid in order to minimize the computational costs. The efficiency of the proposed technique is demonstrated by two benchmark problems of compressible flow simulations.

</details>


### [19] [Optimal Sensor Placement in Gaussian Processes via Column Subset Selection](https://arxiv.org/abs/2601.20781)
*Jessie Chen,Hangjie Ji,Arvind K. Saibaba*

Main category: math.NA

TL;DR: The paper proposes algorithms for optimal sensor placement in Gaussian process regression using column subset selection and Nyström approximations to solve the NP-hard D-optimal sensor selection problem efficiently.


<details>
  <summary>Details</summary>
Motivation: When deploying limited sensors for Gaussian process regression, optimal placement is crucial to minimize reconstruction uncertainty. The D-optimal sensor selection problem is NP-hard, requiring efficient approximation algorithms.

Method: Model sensor placement as column subset selection on covariance matrix; use Golub-Klema-Stewart framework for selection; propose Nyström approximation methods (randomized Nyström, random pivoted Cholesky, greedy pivoted Cholesky) to reduce computational cost.

Result: Developed algorithms with analysis of lower bounds on D-optimality; demonstrated performance on thin liquid film dynamics and sea surface temperature applications.

Conclusion: The proposed methods provide efficient solutions for optimal sensor placement in Gaussian process regression with theoretical guarantees and practical applicability to real-world problems.

Abstract: Gaussian process regression uses data measured at sensor locations to reconstruct a spatially dependent function with quantified uncertainty. However, if only a limited number of sensors can be deployed, it is important to determine how to optimally place the sensors to minimize a measure of the uncertainty in the reconstruction. We consider the Bayesian D-optimal criterion to determine the optimal sensor locations by choosing sensors from a candidate set of sensors. Since this is an NP-hard problem, our approach models sensor placement as a column subset selection problem (CSSP) on the covariance matrix, computed using the kernel function on the candidate sensor points. We propose an algorithm that uses the Golub-Klema-Stewart framework (GKS) to select sensors and provide an analysis of lower bounds on the D-optimality of these sensor placements. To reduce the computational cost in the GKS step, we propose and analyze algorithms for the D-optimal sensor placements using Nyström approximations on the covariance matrix. Moreover, we propose several algorithms that select sensors via Nyström approximation of the covariance matrix, utilizing the randomized Nyström approximation, random pivoted Cholesky and greedy pivoted Cholesky. We demonstrate the performance of our method on two applications: thin liquid film dynamics and sea surface temperature.

</details>


### [20] [Jacobi Hamiltonian Integrators: construction and applications](https://arxiv.org/abs/2601.20799)
*Adérito Araújo,Gonçalo Inocêncio Oliveira,João Nuno Mestre*

Main category: math.NA

TL;DR: A framework for building geometric integrators for Hamiltonian systems on Jacobi manifolds using Poissonization and homogeneous symplectic bi-realizations.


<details>
  <summary>Details</summary>
Motivation: To develop structure-preserving numerical integrators for Hamiltonian systems on Jacobi manifolds, which include contact Hamiltonian systems and classical models, to better preserve geometric structure and improve long-time behavior.

Method: Combine Poissonization of Jacobi structures with homogeneous symplectic bi-realizations to lift Jacobi dynamics to homogeneous Poisson Hamiltonian systems, enabling construction of structure-preserving Jacobi Hamiltonian integrators.

Result: Explicit construction of Jacobi Hamiltonian integrators with demonstrated qualitative advantages over standard integrators, including better preservation of geometric structure and improved long-time behavior in numerical experiments.

Conclusion: The proposed systematic framework successfully constructs geometric integrators for Hamiltonian systems on Jacobi manifolds that preserve geometric structure and exhibit superior long-time numerical behavior.

Abstract: We propose a systematic framework for constructing geometric integrators for Hamiltonian systems on Jacobi manifolds. By combining Poissonization of Jacobi structures with homogeneous symplectic bi-realizations, Jacobi dynamics are lifted to homogeneous Poisson Hamiltonian systems, enabling the construction of structure-preserving Jacobi Hamiltonian integrators. The resulting schemes are constructed explicitly and applied to a range of examples, including contact Hamiltonian systems and classical models. Numerical experiments highlight their qualitative advantages over standard integrators, including better preservation of geometric structure and improved long-time behavior.

</details>


### [21] [A locking-free mixed virtual element discretization for the elasticity eigenvalue problem](https://arxiv.org/abs/2601.20807)
*Felipe Leppe,Gonzalo Rivera*

Main category: math.NA

TL;DR: Mixed virtual element method for 2D elasticity eigenvalue problems with proven convergence and error estimates, validated by numerical tests showing locking-free behavior.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for approximating eigenvalues and eigenfunctions of two-dimensional elasticity eigenvalue problems that works well on polygonal meshes and avoids locking issues.

Method: Mixed virtual element method for 2D elasticity eigenvalue problems, using compact operator theory to analyze convergence and derive error estimates for eigenvalues and eigenfunctions.

Result: Proven convergence of discrete solution operator to continuous operator as mesh size tends to zero, with error estimates for eigenvalues and eigenfunctions. Numerical tests confirm convergence orders and show method is locking-free and accurate across different polygonal mesh shapes.

Conclusion: The mixed virtual element method is effective for elasticity eigenvalue problems, providing accurate approximations that are robust to mesh geometry and free from locking phenomena.

Abstract: In this paper, we introduce a mixed virtual element method to approximate the eigenvalues and eigenfunctions of the two-dimensional elasticity eigenvalue problem. Under standard assumptions on the meshes, we prove the convergence of the discrete solution operator to the continuous one as the mesh size tends to zero. Using the theory of compact operators, we analyze the convergence of the method and derive error estimates for both the eigenvalues and eigenfunctions. We validate our theoretical results with a series of numerical tests, in which we compute convergence orders and show that the method is locking-free and capable of accurately approximating the spectrum independently of the shape of the polygons on the meshes.

</details>


### [22] [Fast Solvers for the Reynolds Equation on Piecewise Linear Geometries](https://arxiv.org/abs/2601.20841)
*Sarah Dennis,Thomas G. Fai*

Main category: math.NA

TL;DR: Exact solutions for Reynolds equation in piecewise linear domains using Schur complement methods, with applications to validating lubrication theory limits.


<details>
  <summary>Details</summary>
Motivation: To develop efficient exact solution methods for Reynolds equation in piecewise linear domains, enabling accurate validation of lubrication theory assumptions by comparing with full Stokes solutions.

Method: Formulate exact solutions for Reynolds equation by coupling piecewise component solutions using Schur complement inversion. Two formulations: piecewise constant heights and piecewise linear heights, with linear-time complexity for the latter. Methods can approximate non-linear heights with second-order accuracy.

Result: Developed exact solution methods for Reynolds equation in piecewise linear domains with linear time complexity. Applied methods to validate lubrication theory limits by comparing Reynolds and Stokes solutions for various textured slider geometries.

Conclusion: The proposed Schur complement methods provide efficient exact solutions for Reynolds equation in piecewise domains, enabling systematic validation of lubrication theory assumptions and revealing its limits of validity for textured surfaces.

Abstract: The Reynolds equation is derived from the incompressible Navier Stokes equations under the lubrication assumptions of a long and thin domain geometry and a small scaled Reynolds number. The Reynolds equation is an elliptic differential equation and a dramatic simplification from the governing equations. When the fluid domain is piecewise linear, the Reynolds equation has an exact solution that we formulate by coupling the exact solutions of each piecewise component. We consider a formulation specifically for piecewise constant heights, and a more general formulation for piecewise linear heights; in both cases the linear system is inverted using the Schur complement. These methods can also be applied in the case of non-linear heights by approximating the height as piecewise constant or piecewise linear, in which case the methods achieve second order accuracy. We assess the time complexity of the two methods, and determine that the method for piecewise linear heights is linear time for the number of piecewise components. As an application of these methods, we explore the limits of validity for lubrication theory by comparing the solutions of the Reynolds and the Stokes equations for a variety of linear and non-linear textured slider geometries.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [23] [The Verigin problem with phase transition as a Wasserstein flow](https://arxiv.org/abs/2601.20001)
*Anna Kubin,Tim Laux,Alice Marveggio*

Main category: math.AP

TL;DR: Novel variational framework for compressible two-phase flow in porous media using Wasserstein gradient flow structure to construct weak solutions via minimizing movement scheme.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous mathematical framework for modeling compressible two-phase flow in porous media (Verigin problem with phase transition) that reveals the underlying gradient-flow structure and enables construction of weak solutions.

Method: Introduces variational framework using minimizing movement scheme with Wasserstein distance to reveal gradient-flow structure. Proves convergence of scheme to obtain "relaxed" distributional solutions satisfying optimal energy-dissipation rate.

Result: Convergence of minimizing movement scheme to relaxed distributional solutions. Under additional assumptions (d≥3 and uniform Muckenhoupt weights), limit is characteristic function of finite perimeter set in non-vacuum region.

Conclusion: The paper establishes a rigorous mathematical foundation for compressible two-phase flow in porous media through Wasserstein gradient flow approach, providing existence of weak solutions with optimal energy dissipation properties.

Abstract: We study the modeling of a compressible two-phase flow in a porous medium. The governing free boundary problem is known as the Verigin problem with phase transition. We introduce a novel variational framework to construct weak solutions. Our approach reveals the gradient-flow structure of the system by adopting a minimizing movement scheme using the Wasserstein distance. We prove the convergence of the scheme, obtaining ``relaxed" distributional solutions in the limit that satisfy an optimal energy-dissipation rate. Under the additional assumptions that $d \geq 3$ and that the discrete mass densities are uniformly Muckenhoupt weights, we show that the limit is the characteristic function of a set of finite perimeter in the region where there is no vacuum.

</details>


### [24] [Positive normalized solutions to a singular elliptic equation with a $L^2$-supercritical nonlinearity](https://arxiv.org/abs/2601.20200)
*Siyu Chen,Xiaojun Chang,Jiazheng Zhou*

Main category: math.AP

TL;DR: The paper proves existence of positive normalized solutions to a singular elliptic equation with Dirichlet boundary conditions and L² normalization constraint for small mass ρ.


<details>
  <summary>Details</summary>
Motivation: To study existence of positive normalized solutions (solutions with prescribed L² norm) to singular elliptic equations combining singular nonlinearity (u^{-r}) and superlinear nonlinearity (u^{p-1}), which arise in various physical contexts including nonlinear optics and quantum mechanics.

Method: Variational approach using a regularized functional to handle the singularity, followed by careful limiting process analysis. The method involves working in H₀¹(Ω) with the normalization constraint ∫u² = ρ.

Result: For sufficiently small ρ > 0, the problem admits a positive solution (λ,u) ∈ ℝ × H₀¹(Ω), where λ is the Lagrange multiplier arising from the normalization constraint.

Conclusion: Positive normalized solutions exist for small mass parameters, demonstrating that the combined singular and superlinear nonlinearities can be balanced under Dirichlet boundary conditions with prescribed L² norm.

Abstract: This paper studies the existence of positive normalized solutions to the singular elliptic equation \[ -Δu + λu = u^{-r} + u^{p-1} \quad \text{in } Ω, \] with the Dirichlet boundary condition $u=0$ on $\partialΩ$ and the normalization constraint $\int_Ωu^2\,dx = ρ$. Here $Ω\subset\mathbb{R}^N$ ($N\ge3$) is a smooth bounded domain, $0<r<1$, $2+\frac{4}{N}<p<2^*$, where $2^*$ is the critical Sobolev exponent, and $λ\in\mathbb{R}$ is a Lagrange multiplier. We obtain that for sufficiently small $ρ>0$, the problem admits a positive solution $(λ,u)\in\mathbb{R}\times H_0^1(Ω)$. The proof is based on a variational approach using a regularized functional and a careful analysis of the limiting process.

</details>


### [25] [Mode-Wise Spectral Criteria for Coupled Mass Transport in Hybrid PDE--ODE Tumor Microenvironments](https://arxiv.org/abs/2601.20204)
*Jiguang Yu,Louis Shuo Wang,Zonghao Liu,Jingfeng Liu*

Main category: math.AP

TL;DR: The paper analyzes a tumor-microenvironment model with motile (S,R) and non-motile (P,A) cell populations, showing global existence, stability properties, and conditions for pattern formation via chemotaxis-induced cross-diffusion rather than classical Turing instability.


<details>
  <summary>Details</summary>
Motivation: To understand pattern formation mechanisms in tumor-microenvironment systems where classical Turing instability is excluded, and to analyze how chemotaxis coupling can induce effective cross-diffusion leading to spatial patterning.

Method: Develops a coupled mass transport model with motile densities (S,R) and non-motile states (P,A) with switching. Uses maximum principle bounds, pointwise invariants, Neumann eigenmode reduction, and linear stability analysis to derive dispersion relations and instability criteria.

Result: The base (S,R) reaction-diffusion system remains stable for all nonconstant modes, excluding classical Turing instability. Chemotaxis coupling induces effective cross-diffusion that can lead to pattern formation, with explicit trace/determinant criteria for unstable Laplacian modes and instability thresholds.

Conclusion: Pattern formation in tumor-microenvironment systems can occur through chemotaxis-induced cross-diffusion mechanisms rather than classical Turing instability, with the model exhibiting global existence, positivity, and long-term convergence to a unique globally attracting coexistence state.

Abstract: We study coupled mass transport in a tumor--microenvironment setting with two motile densities $(S,R)$ and non-motile state switching $(P,A)$. The populations diffuse and undergo chemotactic drift; $(P,A)$ follow pointwise ODE switching. A decoupled inhibitory field $D$ satisfies a damped Neumann heat equation, giving maximum-principle bounds and exponential decay. Together with the pointwise invariant $P+A$, these identities yield global existence, positivity, and long-time reduction to limiting $(S,R)$ kinetics with a unique globally attracting coexistence state. Neumann eigenmode reduction gives closed dispersion relations. The base $(S,R)$ reaction--diffusion block remains stable for all nonconstant modes for any $d_S,d_R>0$, excluding classical Turing destabilization. Chemotaxis is posed via a diffusive cue $c$, since $\nabla A$ is undefined for non-diffusive $A$. In one-way damped coupling, the linearized mode matrix is block triangular and leaves the $(S,R)$ spectrum unchanged. Two-way coupling adds a feedback rank-one mobility correction, induces effective cross-diffusion, and admits mode growth. We give explicit trace/determinant criteria for unstable Laplacian modes and the resulting instability thresholds.

</details>


### [26] [The scattering map for the Schrodinger operator on curved spaces](https://arxiv.org/abs/2601.20225)
*Andrew Hassell,Qiuye Jia*

Main category: math.AP

TL;DR: The paper shows that the scattering map for a Schrödinger operator with compactly supported perturbations is a 1-cusp Fourier integral operator, connecting scattering theory to Vasy and Zachos's pseudodifferential calculus.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric structure of scattering maps for Schrödinger operators with compactly supported perturbations, and to connect scattering theory to the 1-cusp pseudodifferential calculus developed by Vasy and Zachos for inverse problems on asymptotically conic manifolds.

Method: The authors study the scattering map S that relates asymptotic data of global solutions to Pu=0 as t→±∞. They analyze this map using the 1-cusp pseudodifferential calculus introduced by Vasy and Zachos, showing that 1-cusp geometry provides the natural framework for studying asymptotic data of Schrödinger equation solutions.

Result: The main result proves that the scattering map S is a 1-cusp Fourier integral operator, establishing a connection between scattering theory for Schrödinger operators and the 1-cusp calculus originally developed for different inverse problems.

Conclusion: 1-cusp geometry provides the natural setting for studying asymptotic data of solutions to Schrödinger's equation, and the scattering map inherits this geometric structure as a 1-cusp Fourier integral operator.

Abstract: Let $P$ be a Schrödinger operator $D_t+Δ_g$ with metric and potential perturbation that are compactly supported in spacetime $\mathbb{R}^{n+1}$.
  Here $D_t = -i \partial_t$ and $Δ_g$ is the positive Laplacian.
  We consider the scattering map $S$ defined previously by the first author with Gell-Redman and Gomes arXiv:2201.03140, which relates the asymptotic data, as $t \to \pm \infty$, of global solutions $u$ to $Pu = 0$. We show that $S$ is a `1-cusp' Fourier integral operator, where `1-cusp' refers to a pseudodifferential calculus introduced by Vasy and Zachos arXiv:2204.11706 in the completely different setting of inverse problems on asymptotically conic manifolds. Our viewpoint is that 1-cusp geometry is the natural setting for studying the asymptotic data of solutions to Schrödinger's equation.

</details>


### [27] [On the Damped Euler--Monge--Ampère equations with Radial Symmetry: Critical Thresholds and Large-Time Behavior](https://arxiv.org/abs/2601.20266)
*Kunhui Luan*

Main category: math.AP

TL;DR: The paper establishes critical thresholds for global regularity vs. singularity formation in the pressureless Euler-Monge-Ampère system with velocity damping under radial symmetry, showing damping removes density lower bounds and enables exponential decay to equilibrium.


<details>
  <summary>Details</summary>
Motivation: To understand the global well-posedness and large-time dynamics of the pressureless Euler-Monge-Ampère system with velocity damping in multiple dimensions, particularly how damping affects the critical thresholds for regularity and whether it allows solutions to exist globally even with vacuum or low density.

Method: Two approaches: 1) refined spectral dynamics method based on Liu (2002), and 2) comparison principle using Lyapunov functions from Bhatnagar (2020). Both methods are applied to radially symmetric initial data to establish critical thresholds.

Result: 1) Critical thresholds exist where subcritical initial data yield global regularity while supercritical data lead to finite-time singularities. 2) Linear damping removes the initial density lower bound required in undamped cases, allowing global regularity even with vacuum/arbitrarily low density. 3) Subcritical data exhibit exponential decay to equilibrium.

Conclusion: The work unifies and extends existing theories for 1D Euler-Poisson systems and undamped multidimensional EMA systems with radial symmetry, demonstrating that velocity damping fundamentally changes the system's behavior by eliminating density constraints and enabling stable equilibrium convergence.

Abstract: We investigate the global well-posedness and large-time dynamics of the pressureless Euler--Monge--Ampère (EMA) system with velocity damping in multidimensions, subject to radially symmetric initial data. We first establish the phenomenon of critical thresholds, where subcritical initial data maintain global regularity, and supercritical initial data lead to finite time singularity formation. We provide two methods for constructing these thresholds: a refined spectral dynamics approach based on \cite{liu2002spectral} and a comparison principle based on Lyapunov functions introduced in \cite{bhatnagar2020critical2}.
  A key finding of this work is that the inclusion of linear damping effectively removes the initial density lower bound previously required in the undamped case \cite{tadmor2022critical} in certain regimes, allowing for global regularity even in the presence of vacuum or arbitrarily low density. Furthermore, for subcritical initial data, we prove an exponential decay rate to the equilibrium state. Our results unify and extend existing theories for 1D Euler--Poisson system and undamped multidimensional EMA system with radial symmetry.

</details>


### [28] [Norm inflation for quadratic derivative fractional nonlinear Schrödinger equations](https://arxiv.org/abs/2601.20294)
*Toshiki Kondo,Mamoru Okamoto*

Main category: math.AP

TL;DR: The paper studies quadratic derivative fractional nonlinear Schrödinger equations, determining sharp fractional derivative exponents for well-posedness and establishing norm inflation with infinite loss of regularity.


<details>
  <summary>Details</summary>
Motivation: To understand the well-posedness properties of quadratic derivative fractional nonlinear Schrödinger equations on both the real line and torus, specifically determining the critical exponents for Sobolev space well-posedness and exploring ill-posedness phenomena.

Method: Leverages global well-posedness results from Nakanishi and Wang (2025) to expand solutions as sums of iterated terms, then derives estimates for each iterated term to establish norm inflation with infinite loss of regularity.

Result: Determines sharp exponents of fractional derivatives for well-posedness in Sobolev spaces and establishes norm inflation with infinite loss of regularity, which implies ill-posedness for certain parameter ranges.

Conclusion: The Cauchy problem for quadratic derivative fractional nonlinear Schrödinger equations exhibits a sharp threshold for well-posedness, with norm inflation and infinite loss of regularity occurring beyond this threshold, demonstrating ill-posedness in certain regimes.

Abstract: We consider the Cauchy problem for quadratic derivative fractional nonlinear Schrödinger equations on $\mathbb{R}$ or $\mathbb{T}$. We determine the sharp exponents of the fractional derivatives for which the Cauchy problem is well-posed in the Sobolev space. Thanks to the global well-posedness result established by Nakanishi and Wang (2025), we can expand the solution as a sum of iterated terms. By deriving estimates for each iterated term, we establish norm inflation with infinite loss of regularity, which in particular implies ill-posedness.

</details>


### [29] [On a two-season faecal-oral model with impulsive intervention](https://arxiv.org/abs/2601.20372)
*Qi Zhou,Zhigui Lin,Carlos Alberto Santos*

Main category: math.AP

TL;DR: A two-season switching faecal-oral disease model with impulsive intervention and free boundaries is developed to study rainfall-driven waterborne disease outbreaks, showing that dry season duration and intervention intensity positively correlate with effective disease control.


<details>
  <summary>Details</summary>
Motivation: Rainfall drives waterborne faecal-oral disease outbreaks, prompting human interventions. The paper aims to mathematically model how seasonal rainfall variations and intervention strategies affect disease spread and control.

Method: Develops a two-season switching model with impulsive intervention and free boundaries: fixed boundaries in dry season, moving boundaries in wet season, with interventions at wet season ends. Uses novel analytical techniques to overcome mathematical challenges from simultaneous impulsive intervention and seasonal switching.

Result: Establishes a spreading-vanishing dichotomy with sharp criteria governing disease outcomes. Numerical simulations validate theoretical results and visually show seasonal switching and intervention effects. Mathematical analysis reveals dry season duration and intervention intensity are positively correlated with effective disease control.

Conclusion: The model successfully explains how seasonal rainfall patterns and timed interventions affect waterborne disease dynamics. Both longer dry seasons and stronger impulsive interventions enhance disease control effectiveness, providing mathematical justification for intervention strategies.

Abstract: Rainfall is associated with the outbreak of certain waterborne faecal-oral diseases, driving the implementation of various human interventions for their control and prevention. Taking into account human intervention and temporal variation in rainfall, this paper develops a two-season switching faecal-oral model with impulsive intervention and free boundaries. In this model, the infection fronts are represented by fixed boundaries during the dry season and by moving boundaries during the wet season, with impulsive intervention occurring at the end of each wet season. The simultaneous introduction of impulsive intervention and seasonal switching creates new difficulties for mathematical analysis. We overcome these challenges through novel analytical techniques, resulting in a spreading-vanishing dichotomy and a sharp criteria governing this dichotomy. Finally, numerical simulations are presented to validate the theoretical results and to visually illustrate the influence of seasonal switching and impulsive intervention. Our results mathematically explain that two factors, the duration of the dry season and the intensity of impulsive intervention are both positively correlated with effective disease control.

</details>


### [30] [Long-time Strichartz estimates on 3D waveguide with applications](https://arxiv.org/abs/2601.20392)
*Yangkendi Deng,Boning Di,Jiao Ma,Dunyan Yan,Kailong Yang*

Main category: math.AP

TL;DR: Long-time Strichartz estimates for Schrödinger equation on waveguide manifolds used to bound Sobolev norm growth for 3D nonlinear Schrödinger equation.


<details>
  <summary>Details</summary>
Motivation: To understand and control the long-time behavior of solutions to nonlinear Schrödinger equations on waveguide manifolds, particularly the growth of Sobolev norms which indicates energy transfer between Fourier modes.

Method: Develop long-time Strichartz estimates for the linear Schrödinger equation on waveguide manifolds, then apply these estimates to analyze the nonlinear Schrödinger equation in three-dimensional waveguides.

Result: Establish upper bounds on the growth of Sobolev norms for the nonlinear Schrödinger equation on three-dimensional waveguides using the derived Strichartz estimates.

Conclusion: Long-time Strichartz estimates provide effective tools for controlling Sobolev norm growth in nonlinear Schrödinger equations on waveguide geometries, offering insights into long-time dynamics.

Abstract: We study long-time Strichartz estimates for the Schrödinger equation on waveguide manifolds, and use them to establish upper bounds on the growth of Sobolev norms for the nonlinear Schrödinger equation on three-dimensional waveguides.

</details>


### [31] [On Eigenvalues of Logarithmic Potential Operator in the Hyperbolic Space](https://arxiv.org/abs/2601.20431)
*Jiya Rose Johnson,Sheela Verma*

Main category: math.AP

TL;DR: The paper studies the hyperbolic logarithmic potential operator on bounded domains in the Poincaré disk, establishing polarization properties, a reverse Faber-Krahn inequality for its largest eigenvalue, eigenfunction representation, and positivity.


<details>
  <summary>Details</summary>
Motivation: To analyze spectral properties of the hyperbolic logarithmic potential operator on bounded domains in the Poincaré disk, extending Euclidean potential theory to hyperbolic geometry.

Method: Extends polarization concept to hyperbolic geometry, proves associated properties, establishes reverse Faber-Krahn inequality under polarization, provides eigenfunction representation formula, and shows operator positivity.

Result: Successfully extends polarization to Poincaré disk, proves reverse Faber-Krahn inequality for largest eigenvalue, obtains eigenfunction representation, and demonstrates operator positivity on L²(Ω).

Conclusion: The hyperbolic logarithmic potential operator shares similar spectral properties with its Euclidean counterpart, with polarization providing a useful tool for establishing extremal eigenvalue inequalities in hyperbolic geometry.

Abstract: Let $Ω$ be a bounded open set in the Poincaré hyperbolic disk, $\mathbb{D}$. In this article, we consider the hyperbolic logarithmic potential operator $\mathcal{L}_h : L^2(Ω) \to L^2(Ω)$, defined by \begin{equation*}
  \mathcal{L}_h u(z)=\frac{1}{2}\int_Ω\log\frac{1}{[z,w]}\,u(w)\, {\,\rm d}(w), \end{equation*} and the associated eigenvalue problem on $Ω$ \begin{equation}
  \mathcal{L}_h u=τu. \end{equation} We first extend the notion of polarization with respect to hyperplanes in the Poincaré disk and prove the associated properties. Then we establish a reverse Faber-Krahn inequality for the largest eigenvalue, $τ_{h}$ of $\mathcal{L}_h$, under polarization. Further, we provide a representation formula for the eigenfunctions of $\mathcal{L}_h$. In addition, we show that the operator $\mathcal{L}_h$ is a positive operator on $L^2(Ω)$.

</details>


### [32] [Fast reaction limits and convergence rate for nonlinear bulk-surface reaction-diffusion systems modeling reversible chemical reactions](https://arxiv.org/abs/2601.20453)
*The Tuan Hoang,Nhu Phong Tham,Bao Quoc Tang*

Main category: math.AP

TL;DR: Analysis of fast reaction limit for bulk-surface reaction-diffusion system with reversible reactions, proving convergence to heat equation with nonlinear dynamical boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of coupled bulk-surface reaction-diffusion systems when reaction rates become infinitely fast, which is relevant for modeling chemical reactions where species are distributed differently in bulk and on surfaces.

Method: Prove strong convergence by establishing uniform a-priori estimates in reaction rate constants, use product spaces to handle bulk-surface coupling, apply Aubin-Lions lemma for compactness, and derive convergence rates for equal stoichiometric coefficients.

Result: Solution converges strongly to heat equation with nonlinear dynamical boundary condition in fast reaction limit; convergence rates obtained for equal stoichiometric coefficients.

Conclusion: Fast reaction limit rigorously justified for bulk-surface systems, providing mathematical foundation for modeling rapid surface reactions with bulk-surface coupling.

Abstract: The fast reaction limit for a nonlinear bulk-surface reaction-diffusion system is investigated. The system model describes a reversible reaction with arbitrary stoichiometric coefficients, where one chemical is present in a bounded vessel $Ω$ and the other chemical lies only on the boundary $\partialΩ$ where the reaction takes place. In the limit as the reaction rate constant tends to infinity, we prove that the solution converges strongly to that of a heat equation with nonlinear dynamical boundary condition. This is obtained by showing a-priori estimates of solutions which are uniform in the reaction rate constants. In order to overcome the difficulty caused by the bulk-surface coupling, we consider the limit in suitable product spaces where the Aubin-Lions lemma is applicable. Moreover, in the case of equal stoichiometric coefficients, we obtain the convergence rate of the fast reaction limit by exploiting suitable estimates of the limiting system.

</details>


### [33] [Existence and selection of solutions in the energy-variational framework with applications in fluid dynamics](https://arxiv.org/abs/2601.20455)
*Thomas Eiter,Robert Lasarzik,Marcel Śliwiński*

Main category: math.AP

TL;DR: The paper presents a generalized existence theory for energy-variational solutions to evolutionary PDEs, with two key extensions: relaxed regularity assumptions and allowance for energies with linear growth. Applications include Euler-Korteweg system and binormal curvature flow, plus selection criteria for specific solutions.


<details>
  <summary>Details</summary>
Motivation: Previous energy-variational solution concepts had restrictive assumptions on regularity weights and required specific energy growth conditions. The authors aim to develop a more general framework that can handle broader classes of evolutionary PDEs, particularly those requiring relaxed regularity or energies with only linear growth.

Method: The authors develop an abstract existence theory for energy-variational solutions with two main generalizations: 1) relaxation of assumptions on the regularity weight, and 2) allowing energies with merely linear growth. They apply this abstract framework to specific PDE examples - the Euler-Korteweg system (benefiting from the first generalization) and binormal curvature flow (benefiting from the second generalization). They also discuss selection criteria for choosing particular solutions from potentially multi-valued solution sets.

Result: The paper establishes novel existence results for energy-variational solutions under more general conditions than previous works. The abstract theory is successfully applied to two important PDE examples: the Euler-Korteweg system demonstrates the utility of relaxed regularity assumptions, while binormal curvature flow illustrates the need for handling energies with linear growth. Additionally, the paper provides criteria for selecting specific solutions from potentially non-unique solution sets.

Conclusion: The generalized energy-variational solution framework significantly extends the applicability of this solution concept to broader classes of evolutionary PDEs. The successful applications to Euler-Korteweg system and binormal curvature flow demonstrate the practical utility of the theoretical generalizations. The discussion of selection criteria addresses the important issue of solution uniqueness in this variational framework.

Abstract: We provide a novel existence result for energy-variational solutions to a general class of evolutionary partial differential equations. Compared to previous works on this solution concept, the generalization is mainly twofold: a relaxation of the assumptions on the regularity weight and the admissibility of energies with merely linear growth. We apply the abstract theory to the Euler--Korteweg system and to the equation for binormal curvature flow, which serve as examples that require the first and second generalization, respectively. Moreover, we discuss criteria that are suitable for the selection of particular energy-variational solutions in the possibly multi-valued solution set.

</details>


### [34] [Regularity of the trace of nonlocal minimal graphs](https://arxiv.org/abs/2601.20484)
*Serena Dipierro,Ovidiu Savin,Enrico Valdinoci*

Main category: math.AP

TL;DR: Nonlocal minimal graphs have C^{1,γ} regularity at sticky points, implying boundary continuity leads to boundary differentiability.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of nonlocal minimal graphs at boundary points where they "stick" to the boundary, and to establish connections between continuity and differentiability at boundaries.

Method: Mathematical proof establishing that the trace of nonlocal minimal graphs at points of stickiness belongs to the Hölder class C^{1,γ}, using techniques from nonlocal calculus and regularity theory.

Result: Proved that nonlocal minimal graphs have C^{1,γ} regularity at sticky boundary points, and consequently showed that boundary continuity implies boundary differentiability for such graphs.

Conclusion: Nonlocal minimal graphs exhibit improved regularity at boundary stickiness points, establishing an important connection between continuity and differentiability in nonlocal geometric problems.

Abstract: We prove that the trace of nonlocal minimal graphs at points of stickiness is of class~$C^{1,γ}$.
  As a result, we show that boundary continuity implies boundary differentiability for nonlocal minimal graphs.

</details>


### [35] [Regularity of Lipschitz free boundaries for weak solutions of Alt-Caffarelli type problems](https://arxiv.org/abs/2601.20493)
*Joan Domingo-Pasarin,Xavier Ros-Oton*

Main category: math.AP

TL;DR: The paper proves that for Lipschitz domains, weak solutions to the generalized Alt-Caffarelli problem are actually smooth, extending previous viscosity solution results to weak solutions.


<details>
  <summary>Details</summary>
Motivation: Motivated by the Serrin problem, the authors study weak solutions of the generalized Alt-Caffarelli problem, seeking to extend regularity results from viscosity solutions to weak solutions in Lipschitz domains.

Method: The authors study weak solutions of the generalized Alt-Caffarelli problem (-Δu = f in Ω, u = 0 on ∂Ω, ∂_νu = Q on ∂Ω) and establish regularity results through analysis of boundary behavior and Poisson kernel characterization.

Result: Main result: If Ω is Lipschitz, then weak solutions are actually C^∞ (provided f and Q are smooth). This extends previous viscosity solution results to weak solutions. As a corollary, an alternative solution to Serrin's problem is obtained for Lipschitz domains.

Conclusion: The paper successfully extends regularity results from viscosity to weak solutions for the generalized Alt-Caffarelli problem in Lipschitz domains, providing new insights into Serrin's problem and connecting domain regularity to Poisson kernel properties.

Abstract: Motivated by the Serrin problem, we study weak solutions of the generalised Alt-Caffarelli problem $-Δu = f$ in $Ω$, $u = 0$ on $\partialΩ$, $\partial_νu = Q$ on $\partialΩ$. Our main result establishes that if $Ω$ is Lipschitz, then it is actually $C^{\infty}$ (provided that $f$ and $Q$ are smooth). This was known before only for viscosity solutions. As a corollary, we obtain an alternative solution of Serrin's problem in the case of Lipschitz domains. We also discuss the characterisation of the regularity of Lipschitz domains in terms of their Poisson kernel.

</details>


### [36] [Normalized Solutions for a Weighted Laplacian Problem with the Caffarelli-Kohn-Nirenberg Critical Exponent](https://arxiv.org/abs/2601.20513)
*Divya Goel,Asmita Rai*

Main category: math.AP

TL;DR: Existence and multiplicity of normalized solutions for a weighted nonlinear Schrödinger-type equation with Caffarelli-Kohn-Nirenberg operator under mass constraint, covering subcritical, critical, and supercritical regimes.


<details>
  <summary>Details</summary>
Motivation: Study normalized solutions (solutions with prescribed L² norm) for weighted Schrödinger equations with Caffarelli-Kohn-Nirenberg operator, addressing challenges from critical nonlinearity and noncompactness in unbounded domains.

Method: Constrained variational techniques, refined estimates on best constants in Caffarelli-Kohn-Nirenberg inequalities, and a bespoke concentration-compactness lemma to handle noncompactness.

Result: Existence of mass-subcritical ground states, multiple constrained critical points, and high-energy ground state solutions in mass-critical and supercritical regimes despite noncompactness.

Conclusion: Comprehensive existence and multiplicity results for normalized solutions across different regimes, overcoming challenges from critical nonlinearity and domain noncompactness through advanced variational methods.

Abstract: This article establishes the existence and multiplicity of normalized solutions to the weighted nonlinear Schrödinger-type equation governed by the Caffarelli-Kohn-Nirenberg operator, $$ -\text{div}(|x|^{-2a}\nabla u)=λ\frac{u}{|x|^{2a}}+β\frac{|u|^{q-2}u}{|x|^{bq}} +\frac{|u|^{2^{\sharp}-2}u}{|x|^{b{2^{\sharp}}}}\quad \text{in}~\mathbb{R}^N,$$ $$\int_{\mathbb{R}^N}\frac{|u|^2}{|x|^{2a}}dx=ρ^2,$$ where $λ\in \mathbb{R}$, $β,~ρ>0$, $0< a<\frac{N-2}{2}$, $a<b<a+1$, $2^{\sharp}:=\frac{2N}{N-2(1+a-b)}$ and $2<q<{2^{\sharp}}$.
  Through constrained variational techniques, refined estimates on the best constants in the Caffarelli-Kohn-Nirenberg inequalities, and a bespoke concentration-compactness lemma, the study secures mass-subcritical ground states alongside multiple constrained critical points, together with high-energy ground state solutions in the mass-critical and supercritical regimes -- notwithstanding the noncompactness arising from the critical Caffarelli-Kohn-Nirenberg nonlinearity over the unbounded domain.

</details>


### [37] [Refined Strichartz estimates and their orthornomal counterparts for Schrödinger equations on torus](https://arxiv.org/abs/2601.20515)
*Divyang G. Bhimani,Subhash. R. Choudhary,S. S. Mondal*

Main category: math.AP

TL;DR: This paper establishes refined Strichartz estimates for Schrödinger equations on tori using partial regularity framework, showing improved regularity in mixed Lebesgue spaces and extending results to infinite orthonormal systems.


<details>
  <summary>Details</summary>
Motivation: To extend classical Strichartz estimates by incorporating partial regularity (regularity with respect to only some spatial variables) rather than full Sobolev regularity, and to generalize these estimates to infinite systems of orthonormal functions for applications in quantum mechanics.

Method: Develops harmonic analysis tools for mixed Lebesgue spaces including Fourier multiplier transference principle, vector-valued Bernstein inequality, and vector-valued Littlewood-Paley theory for operator densities. Applies these to establish refined Strichartz estimates within partial regularity framework.

Result: 1) Shows Schrödinger equation solutions have better regularity in mixed Lebesgue spaces with partial regularity; 2) Proves local well-posedness for non-gauge-invariant nonlinearities with partially regular initial data; 3) Extends refined Strichartz estimates to infinite orthonormal systems; 4) Establishes well-posedness for Hartree equation for infinitely many fermions in Schatten spaces.

Conclusion: The paper successfully develops a partial regularity framework for Strichartz estimates on tori, complementing classical full-regularity theory and enabling applications to nonlinear Schrödinger equations with non-gauge-invariant terms and many-body quantum systems. The developed harmonic analysis tools have independent value beyond the immediate applications.

Abstract: The aim of the paper is twofold. We establish refined Strichartz estimates for the Schrödinger equation on tori within the framework of partial regularity. As a result, we reveal that the solution of the free Schrödinger equation has better regularity in mixed Lebesgue spaces. This complements the well-established theory over the past few decades, where initial data comes from the Sobolev space with respect to all spatial variables. As an application, we obtain local well-posedness for non-gauge-invariant nonlinearities with partially regular initial data. On the other hand, we extend refined Strichartz estimates for infinite systems of orthonormal functions, which generalizes the classical orthonormal Strichartz estimates on the torus by Nakamura [41] . As an application, we establish well-posedness for the Hartree equation for infinitely many fermions in some Schatten spaces. In the process, we develop several harmonic analysis tools for mixed Lebesgue spaces, e.g. Fourier multiplier transference principle, vector-valued Bernstein inequality, and vector-valued Littlewood--Paley theory for densities of operators, which may be of independent interest and complement the results of [45,55].

</details>


### [38] [$Γ$-convergence of free discontinuity problems for circle-valued maps in the linear regime](https://arxiv.org/abs/2601.20612)
*Giovanni Bellettini,Roberta Marziani,Riccardo Scala*

Main category: math.AP

TL;DR: The paper studies Γ-convergence of Ambrosio-Tortorelli type functionals for circle-valued functions with linear growth volume terms, showing emergence of non-local Γ-limits due to topological structure.


<details>
  <summary>Details</summary>
Motivation: To extend previous analysis of the quadratic case to linear growth volume terms for circle-valued functions, investigating how topological structure of the target space affects Γ-convergence.

Method: Using Γ-convergence analysis of Ambrosio-Tortorelli type functionals, examining volume terms with linear growth for circle-valued functions, and studying compactness properties of minimal liftings.

Result: Shows emergence of non-local Γ-limits due to the topological structure of the circle target space, extends previous quadratic case results to linear growth, and discusses compactness of minimal liftings.

Conclusion: The topological structure of circle-valued target spaces leads to non-local Γ-limits in Ambrosio-Tortorelli type functionals with linear growth, extending previous quadratic case analysis and providing insights into compactness properties.

Abstract: We investigate the $Γ$-convergence of Ambrosio-Tortorelli type-functionals for circle valued functions, in the case of volume terms with linear growth. We show the emergence of a non-local $Γ$-limit, which is due to the topological structure of the target space, and discuss compactness of minimal liftings. Our results extend the analysis of a previous work on the quadratic case.

</details>


### [39] [Existence results for Leibenson's equation on Riemannian manifolds](https://arxiv.org/abs/2601.20640)
*Philipp Sürig*

Main category: math.AP

TL;DR: Existence of weak solutions for the Leibenson equation (doubly nonlinear evolution) on arbitrary Riemannian manifolds under conditions p>1, q>0, pq≥1.


<details>
  <summary>Details</summary>
Motivation: To establish existence results for the Leibenson equation (∂_t u = Δ_p u^q) on general Riemannian manifolds, which is a doubly nonlinear evolution equation combining p-Laplacian and power nonlinearity.

Method: Analysis of the Cauchy problem on arbitrary Riemannian manifold M, considering weak solutions for initial data in L¹(M)∩L^∞(M). The proof likely involves functional analysis, energy estimates, and approximation techniques for doubly nonlinear equations.

Result: Proves existence of weak solutions for the Leibenson equation when p>1, q>0, and pq≥1, for any initial data u₀ in L¹(M)∩L^∞(M).

Conclusion: The Leibenson equation admits weak solutions on arbitrary Riemannian manifolds under the specified parameter conditions, extending existence theory for doubly nonlinear evolution equations to general geometric settings.

Abstract: We consider on an arbitrary Riemannian manifold $M$ the \textit{Leibenson equation} $\partial _{t}u=Δ_{p}u^{q}$, that is also known as a \textit{doubly nonlinear evolution equation}. We prove that if $p>1, q>0$ and $pq\geq 1$ then the Cauchy-problem \begin{equation*} \left\{\begin{array}{ll}\partial _{t}u=Δ_{p}u^{q} &\text{in}~M\times (0, \infty), \\u(x, 0)=u_{0}(x)& \text{in}~M,
  \end{array}%
  \right. \end{equation*} has a weak solution for any $u_{0}\in L^{1}(M)\cap L^{\infty}(M)$.

</details>


### [40] [Large positive solutions for a class of 1-D diffusive logistic problems with general boundary conditions](https://arxiv.org/abs/2601.20651)
*Julián López-Gómez,Alejandro Sahuquillo,Andrea Tellini*

Main category: math.AP

TL;DR: This paper establishes existence and uniqueness of positive solutions for singular boundary value problems with general boundary operators, including non-classical cases where β can be any real value, not just β≥0.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend classical Sturm-Liouville theory by allowing more general boundary conditions (β can be any real value, not just β≥0) and to address the challenge of proving uniqueness when the nonlinearity f(u)=au^p-λu is not increasing for λ>0.

Method: The method involves analyzing singular boundary value problems with general boundary operators (Dirichlet, Neumann, or Robin type), establishing existence of positive solutions, then proving uniqueness for constant coefficient a(x), characterizing the asymptotic behavior of solutions as λ→±∞, and finally establishing uniqueness for non-increasing a(x) under specific conditions.

Result: The paper proves: (1) existence of positive solutions for general boundary operators, (2) uniqueness when a(x) is constant, (3) characterization of asymptotic behavior of solutions as λ→±∞, and (4) uniqueness when a(x) is non-increasing with λ≥0 and β<0 for certain boundary conditions.

Conclusion: The paper successfully extends classical boundary value problem theory to more general boundary conditions, establishes existence and uniqueness results for positive solutions, and characterizes their asymptotic behavior, providing a comprehensive analysis of singular boundary value problems with non-increasing nonlinearities.

Abstract: The first goal of this paper is to establish the existence of a positive solution for the singular boundary value problem (1.1), where $\mathcal{B}$ is a general boundary operator of Dirichlet, Neumann or Robin type, either classical or non-classical; in the sense that, as soon as $\mathcal{B}u(0)=-u'(0)+βu(0)$, the coefficient $β$ can take any real value, not necessarily $β\geq 0$ as in the classical Sturm--Liouville theory. Since the function $f(u):=au^p -λu$, $u\geq 0$, is not increasing if $λ>0$, the uniqueness of the positive solution of (1.1) is far from obvious, in general, even for the simplest case when $a(x)$ is a positive constant. The second goal of this paper is to establish the uniqueness of the positive solution of (1.1) in that case. At a later stage, denoting by $L_λ$ the unique positive solution of (1.1) when $a(x)$ is a positive constant, we will characterize the point-wise behavior of $L_λ$ as $λ\to \pm \infty$. It turns out that any positive solution of (1.1) mimics the behavior of $L_λ$ as $λ\to \pm\infty$. Finally, we will establish the uniqueness of the positive solution of (1.1) when $a(x)$ is non-increasing in $[0,R]$, $λ\geq 0$, and $β<0$ if $-u'(0)+βu(0)=0$.

</details>


### [41] [Spectral stability of shock profiles for the Navier-Stokes-Poisson system](https://arxiv.org/abs/2601.20684)
*Wanyong Shim*

Main category: math.AP

TL;DR: The paper analyzes spectral stability of small-amplitude shock profiles in the 1D isothermal Navier-Stokes-Poisson system, establishing bounds on essential/point spectra and proving simplicity of the zero eigenvalue using an extended Evans function approach.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral stability of shock profiles in plasma physics, specifically for the isothermal Navier-Stokes-Poisson system describing ion dynamics in collision-dominated plasmas, where the zero eigenvalue is embedded in essential spectrum preventing standard Evans function analysis.

Method: Developed an extended Evans-function framework that works within regions of the essential spectrum, allowing computation of the Evans function derivative at the origin. This derivative is factorized into two components: one related to profile transversality and another to hyperbolic stability of the corresponding quasi-neutral Euler shock.

Result: Established: (i) bounds on essential spectrum, (ii) bounds on point spectrum, and (iii) proved simplicity of the zero eigenvalue for the linearized operator in L^2. Showed both factors in the Evans function derivative factorization are nonzero, confirming eigenvalue simplicity.

Conclusion: Successfully resolved the challenge of analyzing spectral stability when the zero eigenvalue is embedded in essential spectrum by extending Evans function methods, providing a framework applicable to similar problems in plasma physics and hyperbolic-parabolic systems.

Abstract: We investigate the spectral stability of small-amplitude shock profiles for the one-dimensional isothermal Navier-Stokes-Poisson system, which describes ion dynamics in a collision-dominated plasma. Specifically, we establish (i) bounds on the essential spectrum, (ii) bounds on the point spectrum, and (iii) simplicity of the zero eigenvalue for the linearized operator about the profile in $L^2$. The result in (i) shows that the zero eigenvalue arising from translation invariance is embedded in the essential spectrum. Consequently, the standard Evans function approach cannot be applied directly to prove (iii). To resolve this, we employ an Evans-function framework that extends into regions of the essential spectrum, thereby enabling us to compute the derivative of the Evans function at the origin. Our result establishes that this derivative admits a factorization into two factors: one associated with transversality of the connecting profile and the other with hyperbolic stability of the corresponding shock of the quasi-neutral Euler system. We further show that both factors are nonzero, which implies simplicity of the zero eigenvalue.

</details>


### [42] [Schrödinger system with quintic nonlinearity: spectral stability of multiple sign-changing periodic waves](https://arxiv.org/abs/2601.20733)
*Guilherme de Loreno,Gabriel E. Bittencourt Moraes*

Main category: math.AP

TL;DR: Analysis of existence and spectral stability of multiple periodic standing wave solutions for a nonlinear Schrödinger system using cnoidal and snoidal profiles.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence and spectral stability properties of periodic standing wave solutions in nonlinear Schrödinger systems, which are important for understanding wave propagation phenomena in various physical contexts.

Method: Uses cnoidal and snoidal wave profiles, applies Floquet theory and comparison theorems for spectral analysis of linearized operators, and employs spectral stability theory via Krein signature.

Result: Derives stability results under periodic perturbations with the same period as the underlying standing waves, obtaining both spectral stability and instability results.

Conclusion: Provides comprehensive spectral analysis of multiple periodic standing wave solutions, establishing conditions for their stability and instability in nonlinear Schrödinger systems.

Abstract: This manuscript investigates the existence and spectral stability of multiple periodic standing wave solutions for a nonlinear Schrödinger system. By considering both cnoidal and snoidal profiles, we provide a comprehensive spectral analysis of the associated linearized operators, employing the Floquet theory and comparison theorems. Stability results are derived under periodic perturbations with the same period as the underlying standing waves. Furthermore, we apply the spectral stability theory via Krein signature to determine the spectral stability and instability results.

</details>


### [43] [Double phase meets Muckenhoupt](https://arxiv.org/abs/2601.20736)
*Daviti Adamadze,Lars Diening,Tengiz Kopaliani,Jihoon Ok*

Main category: math.AP

TL;DR: Generalizes FKS result to double phase model with minimal assumptions using Muckenhoupt-type condition in generalized Orlicz spaces, develops complete theory, and proves Hölder continuity via De Giorgi technique.


<details>
  <summary>Details</summary>
Motivation: To extend the classical FKS result to the double phase model while working with minimal assumptions on the modulating coefficient, addressing the need for a more general framework in non-standard growth problems.

Method: Introduces a Muckenhoupt-type condition on generalized Orlicz spaces, develops complete theory including maximal operator boundedness and Sobolev-Poincare estimates, and applies De Giorgi technique.

Result: Establishes Hölder continuity of solutions for the double phase model under minimal assumptions, providing a complete theory equivalent to classical Muckenhoupt weights in the generalized Orlicz setting.

Conclusion: Successfully generalizes the FKS result to double phase problems, creating a comprehensive framework that handles minimal coefficient assumptions through Muckenhoupt-type conditions in generalized Orlicz spaces.

Abstract: In this paper we generalize the famous result of [FKS] to the double phase model. In particular, we work with minimal assumptions on the modulating coefficient by introducing a Muckenhoupt-type condition on generalized Orlicz spaces. We develop a complete theory equivalent to that of classical Muckenhoupt weights, including the boundedness of the maximal operator and Sobolev-Poincare estimates. We combine this with the De~Giorgi technique to show Hölder continuity of the solutions.

</details>


### [44] [Continuum of finite point blowup rates for the critical generalized Korteweg-de Vries equation](https://arxiv.org/abs/2601.20801)
*Yvan Martel,Didier Pilod*

Main category: math.AP

TL;DR: Existence of H^1 solutions to mass critical gKdV equation with blowup at finite point x=0 at time t=0, with blowup rate ∥∂_x u(t,x)∥_{L^2} ≈ t^{-ν} for ν∈(3/7,1/2), associated with blowup residue r_α(x)=x^{α-1/2} for x>0.


<details>
  <summary>Details</summary>
Motivation: To construct blowup solutions for mass critical gKdV equation that blow up at a finite spatial point (x=0) with specific blowup rates, extending previous work that only covered the special case ν=2/5. This contrasts with most blowup solutions for this equation which typically blow up at spatial infinity.

Method: Proving existence of H^1 solutions with prescribed blowup behavior by analyzing blowup residues of the form r_α(x)=x^{α-1/2} for x>0, where α=(3ν-1)/(2-4ν). The condition ν∈(3/7,1/2) ensures α>1, which corresponds to the full range where the residue belongs to H^1.

Result: Existence of H^1 solutions to mass critical gKdV that blow up at finite point x=0 at time t=0 with blowup rate ∥∂_x u(t,x)∥_{L^2} ≈ t^{-ν} for any ν∈(3/7,1/2). This extends the previous special case ν=2/5 to a continuous range of blowup rates.

Conclusion: The paper constructs blowup solutions at finite spatial points for mass critical gKdV with various blowup rates, filling a gap in the understanding of blowup phenomena for this equation. The authors also present open problems regarding blowup for mass critical gKdV.

Abstract: For any $ν\in(\frac 37,\frac12)$, we prove the existence of an $H^1$ solution $u$ of the mass critical generalized Korteweg-de Vries equation on the time interval $(0,T_0]$, for some $T_0>0$, which blows up at the time $t=0$ and at the point $x=0$ with the rate $\|\partial_x u (t,x)\|_{L^2} \approx t^{-ν}$. Such a blowup rate is associated to a blowup residue of the form $r_α(x)= x^{α-\frac 12}$ for $x>0$ close to the blowup point, where $α=\frac{3ν-1}{2-4ν}$. The condition $ν\in(\frac37,\frac12)$ is equivalent to $α>1$, which corresponds to the full range for which the residue $r_α$ belongs to $H^1$.
  Such blowup at a finite point is in contrast with all the blowup solutions constructed for this equation, except the one constructed previously by the authors corresponding to the special value $ν=\frac 25$.
  Finally, we present some open problems regarding the blowup phenomenon for the mass critical gKdV equation.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [45] [Two-Step Diffusion: Fast Sampling and Reliable Prediction for 3D Keller--Segel and KPP Equations in Fluid Flows](https://arxiv.org/abs/2601.20024)
*Zhenda Shen,Zhongjian Wang,Jack Xin,Zhiwen Zhang*

Main category: physics.comp-ph

TL;DR: Two-stage pipeline for fast generative transport in 3D KS/KPP equations: Stage I uses Meanflow for one-step global transport, Stage II uses Deep Particle corrector with mini-batch W₂ optimization for refinement.


<details>
  <summary>Details</summary>
Motivation: To achieve fast and reliable generative transport for 3D KS and KPP equations with fluid flows, approximating the map between initial and terminal distributions under Wasserstein metric while minimizing inaccuracy of direct Wasserstein solvers.

Method: Two-stage pipeline: Stage I - Meanflow-style regressor provides deterministic one-step global transport; Stage II - frozen initializer with near-identity corrector (Deep Particle) that directly minimizes mini-batch W₂ objective using warm-started optimal transport couplings computed on Meanflow outputs.

Result: The method stabilizes high-dimensional W₂ computation after one-step transport concentrates mass on approximated correct support, validated in 3D KS and KPP equations with ordered and chaotic streamline fluid flows.

Conclusion: Proposed pipeline retains one-step efficiency while reinstating explicit W₂ objective where tractable, enabling reliable generative transport for complex 3D PDE systems with varying physical parameters.

Abstract: We study fast and reliable generative transport for the 3D KS (Keller-Segel) and KPP (Kolmogorov-Petrovsky-Piskunov) equations in the presence of fluid flows with the goal to approximate the map between initial and terminal distributions for a range of physical parameters $σ$ under the Wasserstein metric. To minimize the inaccuracy of direct Wasserstein solver, we propose a two-stage pipeline that retains one-step efficiency while reinstating an explicit $W_2$ objective where it is tractable. In Stage I, a Meanflow-style regressor yields a deterministic, one-step global transport that moves particles close to their terminal states. In Stage II, we freeze this initializer and train a near-identity corrector (Deep Particle, DP) that directly minimizes a mini-batch $W_2$ objective using warm-started optimal transport couplings computed on the Meanflow outputs. Crucially, after the one-step transport (from Stage I) concentrating mass on the approximated correct support, the induced geometry stabilizes high-dimensional $W_2$ computation of the direct Wasserstein solver. We validate our construction in the 3D KS and KPP equations subject to fluid flows with ordered and chaotic streamlines.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [46] [Superelastic Heating in Treanor-Gordiets Plasmas: A Unified Analytic Closure](https://arxiv.org/abs/2601.20734)
*Bernard Parent*

Main category: physics.plasm-ph

TL;DR: New anharmonic gain function resolves underestimation of superelastic electron heating in non-equilibrium plasmas by accounting for overpopulated high-lying states in Treanor-Gordiets distributions.


<details>
  <summary>Details</summary>
Motivation: Conventional harmonic models fail to accurately predict superelastic electron heating rates in non-equilibrium plasmas where vibrational temperature exceeds gas temperature, underestimating rates by orders of magnitude due to artificial decoupling of energy modes.

Method: Derived a closed-form, thermodynamically consistent anharmonic gain function based on detailed balance and second-order Dunham expansion, providing a unified governing equation that identifies kinetic crossover between V-V up-pumping and V-T relaxation.

Result: The model accurately predicts the Treanor minimum and recovers the accuracy of full state-to-state benchmarks at a fraction of the computational cost, resolving the limitations of conventional harmonic models.

Conclusion: The approach provides a robust closure for predicting electron temperature evolution in applications ranging from hypersonic flows to plasma-assisted combustion, offering significant computational efficiency while maintaining accuracy.

Abstract: In non-equilibrium plasmas where the vibrational temperature exceeds the gas temperature, conventional harmonic models underestimate superelastic electron heating rates by an order of magnitude or more. This failure stems from the artificial decoupling of energy modes, which ignores the exponential heating contributions from overpopulated high-lying states characteristic of Treanor-Gordiets distributions. We resolve this limitation by deriving a closed-form, thermodynamically consistent anharmonic gain function based on detailed balance and a second-order Dunham expansion. This formulation serves as a unified governing equation that naturally identifies the kinetic crossover between vibrational-vibrational (V-V) up-pumping and vibrational-translational (V-T) relaxation. This approach accurately predicts the Treanor minimum and recovers the accuracy of full state-to-state benchmarks at a fraction of the computational cost. The model provides a robust closure for predicting electron temperature evolution in applications ranging from hypersonic flows to plasma-assisted combustion.

</details>


### [47] [Compressible Turbulence as a Source of Particle Beams and Ion Bernstein Waves in Collisionless Plasmas](https://arxiv.org/abs/2601.20842)
*Chuanpeng Hou,Huirong Yan,Siqi Zhao*

Main category: physics.plasm-ph

TL;DR: Compressible plasma turbulence naturally generates suprathermal particles and proton beams via transit-time damping at MHD scales and ion Bernstein waves at sub-ion scales, explaining solar wind observations.


<details>
  <summary>Details</summary>
Motivation: To understand the source of particle beams and ion Bernstein waves in collisionless plasmas, particularly explaining super-Alfvénic proton beams observed in the solar wind, and to investigate the role of compressive fluctuations in cross-scale energy transfer.

Method: High-resolution particle-in-cell (PIC) simulation of compressible turbulence in collisionless plasmas, analyzing processes at both magnetohydrodynamic (MHD) scales and sub-ion scales.

Result: Compressible turbulence is damped by transit-time damping at MHD scales, generating suprathermal electrons and proton beams. At sub-ion scales, multiple branches of ion Bernstein waves are excited and contribute to proton suprathermal tails. These processes remain efficient under realistic solar wind conditions.

Conclusion: Compressive fluctuations, though often understudied, are essential for cross-scale energy transfer and dissipation in collisionless plasma turbulence, providing a natural explanation for observed super-Alfvénic proton beams in the solar wind.

Abstract: We investigate the source of particle beams and ion Bernstein waves in collisionless plasmas using a high-resolution particle-in-cell simulation of compressible turbulence. At magnetohydrodynamic (MHD) scales, compressible turbulence is damped by transit-time damping, naturally generating suprathermal electrons and proton beams. As the energy cascade reaches sub-ion scales, multiple branches of ion Bernstein waves are excited and contribute to the formation of proton suprathermal tails. Under realistic conditions such as those in the solar wind, these processes remain efficient and provide a natural explanation for the super-Alfvénic proton beams observed in situ. We show that compressive fluctuations, though often understudied, are essential for cross-scale energy transfer and dissipation in collisionless plasma turbulence.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [48] [Quantum statistics from classical simulations via generative Gibbs sampling](https://arxiv.org/abs/2601.20228)
*Weizhou Wang,Xuanxi Zhang,Jonathan Weare,Aaron R. Dinner*

Main category: physics.chem-ph

TL;DR: GG-PI: A generative modeling framework using Gibbs sampling to efficiently simulate nuclear quantum effects from classical MD data, reducing computational cost compared to path integral MD.


<details>
  <summary>Details</summary>
Motivation: Path integral molecular dynamics (PIMD) is accurate for nuclear quantum effects but computationally expensive. There's a need for more efficient methods that can leverage existing classical simulation data.

Method: GG-PI combines generative modeling of single-bead conditional density with Gibbs sampling. It uses ring-polymer framework to recover quantum statistics from classical simulation data, allowing transfer across temperatures without retraining.

Result: GG-PI significantly reduces wall clock time compared to PIMD on standard test systems while maintaining accuracy for nuclear quantum effects simulation.

Conclusion: The GG-PI framework provides an efficient alternative to PIMD for nuclear quantum effects, leverages existing classical simulation data, and can be extended to various problems with similar Markov structure.

Abstract: Accurate simulation of nuclear quantum effects is essential for molecular modeling but expensive using path integral molecular dynamics (PIMD). We present GG-PI, a ring-polymer-based framework that combines generative modeling of the single-bead conditional density with Gibbs sampling to recover quantum statistics from classical simulation data. GG-PI uses inexpensive standard classical simulations or existing data for training and allows transfer across temperatures without retraining. On standard test systems, GG-PI significantly reduces wall clock time compared to PIMD. Our approach extends easily to a wide range of problems with similar Markov structure.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [49] [Global Plane Waves From Local Gaussians: Periodic Charge Densities in a Blink](https://arxiv.org/abs/2601.19966)
*Jonas Elsborg,Felix Ærtebjerg,Luca Thiede,Alán Aspuru-Guzik,Tejs Vegge,Arghya Bhowmik*

Main category: cond-mat.mtrl-sci

TL;DR: ELECTRAFI is a fast, differentiable model for predicting periodic charge densities in crystals using anisotropic Gaussians and analytic Fourier transforms, achieving state-of-the-art accuracy with 633× speedup over competitors.


<details>
  <summary>Details</summary>
Motivation: Current methods for predicting periodic charge densities in crystalline materials are computationally expensive, involving explicit real-space grid probing, periodic image summation, and spherical harmonic expansions. There's a need for faster, more efficient models that can accelerate DFT calculations without sacrificing accuracy.

Method: ELECTRAFI constructs anisotropic Gaussians in real space and exploits their closed-form Fourier transforms to analytically evaluate plane-wave coefficients via the Poisson summation formula. This delegates non-local and periodic behavior to analytic transforms, enabling reconstruction of full periodic charge density with a single inverse FFT, avoiding explicit grid probing and spherical harmonic expansions.

Result: ELECTRAFI matches or exceeds state-of-the-art accuracy across periodic benchmarks while being up to 633× faster than the strongest competing method, reconstructing crystal charge densities in a fraction of a second. When used to initialize DFT calculations, it reduces total DFT compute cost by up to ~20%.

Conclusion: Accuracy and inference cost jointly determine end-to-end DFT speedups, motivating the focus on efficiency. ELECTRAFI demonstrates that fast, accurate charge density prediction can significantly accelerate materials discovery workflows by reducing DFT initialization costs.

Abstract: We introduce ELECTRAFI, a fast, end-to-end differentiable model for predicting periodic charge densities in crystalline materials. ELECTRAFI constructs anisotropic Gaussians in real space and exploits their closed-form Fourier transforms to analytically evaluate plane-wave coefficients via the Poisson summation formula. This formulation delegates non-local and periodic behavior to analytic transforms, enabling reconstruction of the full periodic charge density with a single inverse FFT. By avoiding explicit real-space grid probing, periodic image summation, and spherical harmonic expansions, ELECTRAFI matches or exceeds state-of-the-art accuracy across periodic benchmarks while being up to $633 \times$ faster than the strongest competing method, reconstructing crystal charge densities in a fraction of a second. When used to initialize DFT calculations, ELECTRAFI reduces total DFT compute cost by up to ~20%, whereas slower charge density models negate savings due to high inference times. Our results show that accuracy and inference cost jointly determine end-to-end DFT speedups, and motivate our focus on efficiency.

</details>


### [50] [Multiscale Numerical Modelling of Ultrafast Laser-Matter Interactions: Maxwell Two Temperature Model Molecular Dynamics (M-TTM-MD)](https://arxiv.org/abs/2601.20763)
*Othmane Benhayoun,Martin E. Garcia*

Main category: cond-mat.mtrl-sci

TL;DR: A unified numerical framework (M-TTM-MD) combining FDTD for Maxwell's equations, Molecular Dynamics, and Two-Temperature Model to simulate ultrafast laser-matter interactions in metals at atomic scale.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between electromagnetic field propagation, electron-phonon energy exchange, and atomic motion in ultrafast laser-metal interactions, enabling self-consistent treatment of energy absorption, transport, and structural response.

Method: Coupled Maxwell-Two-Temperature Model-Molecular Dynamics (M-TTM-MD) framework using FDTD with ADE technique for dispersive dielectric properties, TTM for electron-phonon coupling, and MD for atomic motion, with dynamic grid updates.

Result: Developed a self-consistent numerical framework that provides insights into laser-induced phenomena in metals, including energy transport and surface dynamics under extreme nonequilibrium conditions.

Conclusion: The M-TTM-MD model successfully integrates electromagnetic, electronic, and atomic-scale physics to enable comprehensive simulation of ultrafast laser-matter interactions in metallic systems.

Abstract: In this work, we present a comprehensive numerical framework that couples numerical solutions of Maxwell's equations using the Finite-Difference Time-Domain (FDTD) approach, Molecular Dynamics (MD), and the Two-Temperature Model (TTM) to describe ultrafast laser-matter interactions in metallic systems at the atomic scale. The proposed Maxwell-Two-Temperature Model-Molecular Dynamics (M-TTM-MD) bridges the gap between electromagnetic field propagation, electron-phonon energy exchange, and atomic motion, allowing for a self-consistent treatment of energy absorption, transport, and structural response within a unified simulation environment. The calculated electromagnetic fields incorporate dispersive dielectric properties derived using the Auxiliary Differential Equation (ADE) technique, while the electronic and lattice subsystems are dynamically coupled through spatially and temporally resolved energy exchange terms. The changes in the material topography are then reflected in the updated grid for the FDTD scheme. The developed M-TTM-MD model provides a self-consistent numerical framework that offers insights into laser-induced phenomena in metals, including energy transport and surface dynamics under extreme nonequilibrium conditions.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [51] [Rate-induced tipping in a solvable model with the Allee effect](https://arxiv.org/abs/2601.20128)
*Hidekazu Yoshioka*

Main category: math.DS

TL;DR: A novel exactly solvable ODE model for rate-induced tipping with Allee effect, featuring explicit solutions, extinction thresholds, and an unconditionally stable cubature method.


<details>
  <summary>Details</summary>
Motivation: To develop a tractable mathematical tool for studying rate-induced tipping phenomena in dynamical systems, particularly focusing on population extinction scenarios where time-dependent parameters trigger stability transitions.

Method: An exactly solvable ordinary differential equation model incorporating Allee effect that induces saddle points, with explicit solutions for extinction thresholds. The authors derive an integral inequality as necessary condition for rate-induced tipping and propose an unconditionally stable cubature method superior to classical forward Euler.

Result: The model successfully handles population extinction where solutions vanish in finite time, provides explicit analytical solutions, and demonstrates practical application to Japanese inland fisheries data showing rise and fall patterns from modern times to present.

Conclusion: The proposed model serves as an effective and tractable mathematical framework for studying rate-induced tipping phenomena, particularly in ecological contexts like population dynamics and fisheries management, with practical computational advantages.

Abstract: We present a novel exactly solvable ordinary differential equation model for rate-induced tipping: a dynamic phenomenon of dynamical systems where a time-dependent parameter triggers the transition of stability of a system. Our model contains an Allee effect that induces a saddle point and admits an explicit solution along with the extinction threshold of a time-dependent Allee parameter. More specifically, we derive an integral inequality that serves as a necessary condition for the occurrence of rate-induced tipping. A remarkable point in the proposed model is that it can handle population extinction such that the solution completely vanishes in a finite amount of time. An unconditionally stable cubature method suitable for our model is proposed, and its superiority over the classical forward Euler method is discussed. We also discuss a fisheries application where inland fisheries rose and fell from modern times to the present in Japan. The proposed model serves as a tractable mathematical tool for studying rate-induced tipping phenomena.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [52] [CM-GAI: Continuum Mechanistic Generative Artificial Intelligence Theory for Data Dynamics](https://arxiv.org/abs/2601.20462)
*Shan Tang,Ziwei Cao,Zhenling Yang,Jiachen Guo,Yicheng Lu,Wing Kam Liu,Xu Guo*

Main category: cs.CE

TL;DR: A continuum mechanics-based theoretical framework generalizes optimal transport theory to enable generative AI tasks with limited data, successfully applied to mechanical problems at material, structure, and system levels.


<details>
  <summary>Details</summary>
Motivation: Generative AI has limited capability in specialized domains due to data scarcity. The paper aims to develop a theoretical framework that can enable generative tasks with small amounts of data by leveraging mechanics principles.

Method: Develops a continuum mechanics-based theoretical framework that generalizes optimal transport theory from pure mathematics to describe data dynamics. This framework is applied to solve three typical mechanical problems: stress-strain response generation, temperature-dependent stress fields, and plastic strain fields under dynamic loading.

Result: The proposed theory successfully completes generative tasks at all three levels (material, structure, system), demonstrating its potential to solve difficult engineering problems beyond mechanics, such as image generation.

Conclusion: Mechanics can provide new tools for computer science, enabling data-efficient generative AI in specialized domains. The work also acknowledges limitations of the proposed theory.

Abstract: Generative artificial intelligence (GAI) plays a fundamental role in high-impact AI-based systems such as SORA and AlphaFold. Currently, GAI shows limited capability in the specialized domains due to data scarcity. In this paper, we develop a continuum mechanics-based theoretical framework to generalize the optimal transport theory from pure mathematics, which can be used to describe the dynamics of data, realizing the generative tasks with a small amount of data. The developed theory is used to solve three typical problem involved in many mechanical designs and engineering applications: at material level, how to generate the stress-strain response outside the range of experimental conditions based on experimentally measured stress-strain data; at structure level, how to generate the temperature-dependent stress fields under the thermal loading; at system level, how to generate the plastic strain fields under transient dynamic loading. Our results show the proposed theory can complete the generation successfully, showing its potential to solve many difficult problems involved in engineering applications, not limited to mechanics problems, such as image generation. The present work shows that mechanics can provide new tools for computer science. The limitation of the proposed theory is also discussed.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis](https://arxiv.org/abs/2601.20079)
*Paul Seurin,Dean Price*

Main category: cs.LG

TL;DR: Multi-objective optimization of heat-pipe microreactors using PEARL algorithm to simultaneously minimize power peaking factor and levelized cost of electricity across three cost scenarios.


<details>
  <summary>Details</summary>
Motivation: Extend previous single-objective optimization framework to address both economic (LCOE) and safety/operational (power peaking factor) considerations for heat-pipe microreactors, which are promising for remote deployment but require balancing cost and performance.

Method: Used Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm for multi-objective optimization, incorporating surrogate modeling and evaluating three cost scenarios for axial and drum reflectors. Analyzed design parameters including solid moderator radius, pin pitch, drum coating angle, and fuel height.

Result: Identified consistent design strategies: reducing solid moderator radius, pin pitch, and drum coating angle while increasing fuel height lowers power peaking factor; minimizing axial reflector contribution, reducing control drum reliance, substituting TRISO fuel with cheaper graphite, and maximizing fuel burnup optimizes LCOE across all scenarios.

Conclusion: PEARL shows promise for navigating design trade-offs, but discrepancies between surrogate models and full simulations remain. Future improvements through constraint relaxation and better surrogate development are needed for more accurate optimization.

Abstract: Heat-pipe microreactors (HPMRs) are compact and transportable nuclear power systems exhibiting inherent safety, well-suited for deployment in remote regions where access is limited and reliance on costly fossil fuels is prevalent. In prior work, we developed a design optimization framework that incorporates techno-economic considerations through surrogate modeling and reinforcement learning (RL)-based optimization, focusing solely on minimizing the levelized cost of electricity (LCOE) by using a bottom-up cost estimation approach. In this study, we extend that framework to a multi-objective optimization that uses the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm. The objectives include minimizing both the rod-integrated peaking factor ($F_{Δh}$) and LCOE -- subject to safety and operational constraints. We evaluate three cost scenarios: (1) a high-cost axial and drum reflectors, (2) a low-cost axial reflector, and (3) low-cost axial and drum reflectors. Our findings indicate that reducing the solid moderator radius, pin pitch, and drum coating angle -- all while increasing the fuel height -- effectively lowers $F_{Δh}$. Across all three scenarios, four key strategies consistently emerged for optimizing LCOE: (1) minimizing the axial reflector contribution when costly, (2) reducing control drum reliance, (3) substituting expensive tri-structural isotropic (TRISO) fuel with axial reflector material priced at the level of graphite, and (4) maximizing fuel burnup. While PEARL demonstrates promise in navigating trade-offs across diverse design scenarios, discrepancies between surrogate model predictions and full-order simulations remain. Further improvements are anticipated through constraint relaxation and surrogate development, constituting an ongoing area of investigation.

</details>


### [54] [Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization](https://arxiv.org/abs/2601.20172)
*James Amarel,Robyn Miller,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: cs.LG

TL;DR: The paper introduces an influence-based diagnostic to measure how neural PDE emulators internalize physical symmetries by analyzing gradient coherence along symmetry orbits during training.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating symmetry in neural PDE emulators rely on forward-pass equivariance tests, which don't assess whether learning dynamics properly couple physically equivalent configurations. There's a need for diagnostic tools that go beyond surface-level symmetry tests to understand how symmetries are internalized during training.

Method: Develops an influence-based diagnostic that measures propagation of parameter updates between symmetry-related states using metric-weighted overlap of loss gradients evaluated along group orbits. This probes local geometry of learned loss landscape and assesses whether learning dynamics couple physically equivalent configurations.

Result: Applied to autoregressive fluid flow emulators, shows that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects symmetry-compatible basins. The diagnostic successfully evaluates whether surrogate models have internalized symmetry properties of known solution operators.

Conclusion: The paper presents a novel technique for evaluating if neural PDE emulators have properly internalized symmetry properties, going beyond forward-pass tests by analyzing training dynamics and gradient coherence along symmetry orbits.

Abstract: We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.

</details>


### [55] [An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems](https://arxiv.org/abs/2601.20637)
*Panayiotis Ioannou,Pietro Liò,Pietro Cicuta*

Main category: cs.LG

TL;DR: NODEs can extrapolate to new boundary conditions with dynamic similarity, SR recovers equations from noisy data with correct variable selection, and SR partially recovers equations from NODE-generated data trained on limited samples.


<details>
  <summary>Details</summary>
Motivation: To explore how Neural ODEs and Symbolic Regression can work together for scientific discovery - specifically for modeling complex system dynamics and discovering governing differential equations from noisy data.

Method: Used noisy synthetic data from two damped oscillatory systems to test: 1) NODE extrapolation to new boundary conditions, 2) SR's ability to recover equations from noisy ground-truth data, and 3) SR's performance on data generated by NODEs trained on limited (10%) simulation data.

Result: 1) NODEs extrapolate effectively when trajectories share dynamic similarity with training data. 2) SR successfully recovers equations from noisy ground-truth data with correct variable selection. 3) SR recovers 2/3 governing equations plus approximation for third from NODE-generated data trained on 10% of full simulation.

Conclusion: Using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery, though there's room for improvement in recovering all equations from NODE-generated data.

Abstract: Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.

</details>


### [56] [ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance](https://arxiv.org/abs/2601.20227)
*Zichao Yu,Ming Li,Wenyi Zhang,Difan Zou,Weiguo Gao*

Main category: cs.LG

TL;DR: ProFlow is a zero-shot physics-consistent sampling framework that uses pre-trained generative models to infer physical fields from sparse observations while strictly satisfying PDE constraints without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing deep generative models for inverse problems struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. There's a need for a sampling mechanism that reconciles strict physical consistency and observational fidelity with pre-trained statistical priors.

Method: ProFlow uses a proximal guidance framework with a two-step scheme: (1) terminal optimization step that projects flow predictions onto physically and observationally consistent sets via proximal minimization, and (2) interpolation step that maps refined states back to the generative trajectory to maintain consistency with the learned flow probability path.

Result: Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations show ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics compared to state-of-the-art diffusion- and flow-based baselines.

Conclusion: ProFlow provides an effective zero-shot framework for physics-consistent sampling that strictly satisfies PDE constraints while leveraging pre-trained generative priors without task-specific retraining, offering a Bayesian interpretation as a sequence of local MAP updates.

Abstract: Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.

</details>


### [57] [TINNs: Time-Induced Neural Networks for Solving Time-Dependent PDEs](https://arxiv.org/abs/2601.20361)
*Chen-Yang Dai,Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai*

Main category: cs.LG

TL;DR: TINNs improve PINNs by making network weights time-dependent, enabling evolving spatial representations and achieving 4x better accuracy with 10x faster convergence.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs use shared weights across all times, forcing the same features to represent different dynamics, which degrades accuracy and destabilizes training when enforcing PDE, boundary, and initial constraints jointly.

Method: Propose Time-Induced Neural Networks (TINNs) that parameterize network weights as a learned function of time, allowing effective spatial representation to evolve over time while maintaining shared structure. Formulated as nonlinear least-squares problem optimized with Levenberg-Marquardt method.

Result: Experiments on various time-dependent PDEs show up to 4x improved accuracy and 10x faster convergence compared to PINNs and strong baselines.

Conclusion: TINNs provide a novel architecture that addresses limitations of standard PINNs by enabling time-dependent weight evolution, resulting in significantly better performance for solving time-dependent PDEs.

Abstract: Physics-informed neural networks (PINNs) solve time-dependent partial differential equations (PDEs) by learning a mesh-free, differentiable solution that can be evaluated anywhere in space and time. However, standard space--time PINNs take time as an input but reuse a single network with shared weights across all times, forcing the same features to represent markedly different dynamics. This coupling degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints jointly. We propose Time-Induced Neural Networks (TINNs), a novel architecture that parameterizes the network weights as a learned function of time, allowing the effective spatial representation to evolve over time while maintaining shared structure. The resulting formulation naturally yields a nonlinear least-squares problem, which we optimize efficiently using a Levenberg--Marquardt method. Experiments on various time-dependent PDEs show up to $4\times$ improved accuracy and $10\times$ faster convergence compared to PINNs and strong baselines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [58] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: Discontinuous DLS is a data-driven error-bounded lossy compression method that uses localized spatial-temporal subspaces informed by data structure to improve compression efficiency while preserving scientific data fidelity.


<details>
  <summary>Details</summary>
Motivation: The growing volume of scientific simulation data presents significant storage and transfer challenges, requiring efficient compression solutions that maintain data validity for scientific analysis.

Method: Discontinuous DLS uses data-informed localized spatial and temporal subspaces to enhance compression efficiency. It's implemented in a distributed computing environment using MPI and is applicable to various scientific datasets including fluid dynamics and environmental simulations.

Result: The method significantly reduces storage requirements without compromising critical data fidelity, showing improved compression-to-error ratios compared to data-agnostic compressors when evaluated against state-of-the-art error-bounded compression methods.

Conclusion: Discontinuous DLS is a promising approach for large-scale scientific data compression in HPC environments, providing a robust solution for managing growing data demands of modern scientific simulations.

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [59] [Uniqueness of invariant measures for stochastic damped anisotropic Navier--Stokes equations](https://arxiv.org/abs/2601.20616)
*Siyu Liang*

Main category: math.PR

TL;DR: Proves uniqueness of invariant measures for 2D Navier-Stokes with anisotropic viscosity, damping, and additive noise on ℝ² when damping is sufficiently large relative to noise intensity.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness of invariant measures for anisotropic Navier-Stokes systems on unbounded domains where traditional methods (like Poincaré inequality) fail, and to handle general additive noise without non-degeneracy conditions.

Method: Uses asymptotic coupling method with anisotropic energy estimates and exponential-type estimates for H¹-energy. The approach works without Poincaré inequality by leveraging the damping term.

Result: Proves uniqueness of invariant measures when damping coefficient is sufficiently large compared to noise intensity. Result applies to general additive noise without non-degeneracy conditions and remains valid even in deterministic case (σ≡0).

Conclusion: Damping is essential for existence of invariant measures on ℝ² where Poincaré inequality fails. The asymptotic coupling method with anisotropic estimates successfully establishes uniqueness under appropriate damping-to-noise ratio conditions.

Abstract: We study a two-dimensional Navier--Stokes system with anisotropic viscosity, linear damping term, and an additive noise on the whole space $\mathbb{R}^2$. For this model we prove uniqueness of invariant measures when the damping coefficient is sufficiently large compared to the noise intensity. The argument is based on an asymptotic coupling method and relies on anisotropic energy estimates together with exponential-type estimates for the $H^1$-energy. Since no Poincaré inequality is available on $\mathbb{R}^2$, the damping term is essential even for the existence of invariant measures. Our result applies to general additive noise without any non-degeneracy condition and remains valid even in the deterministic case $σ\equiv0$.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [60] [A counterexample to the Berger--Coburn conjecture](https://arxiv.org/abs/2601.20859)
*Sam Looi*

Main category: math.FA

TL;DR: The paper shows that Berger and Coburn's endpoint boundedness criterion for Toeplitz operators on Bargmann-Fock space fails for general measurable symbols in all complex dimensions n≥1.


<details>
  <summary>Details</summary>
Motivation: To investigate the validity of Berger and Coburn's criterion which uses the heat transform of symbols at time t=1/4 as a boundedness test for Toeplitz operators on Bargmann-Fock space.

Method: Construct a counterexample by summing translated bounded "blocks" whose Toeplitz norms are summable while their t=1/4 heat profiles have fixed size. The blocks are produced by combining Hilbert-Schmidt estimates for Weyl quantization with the Bargmann correspondence between Weyl and Toeplitz operators.

Result: Successfully constructed a measurable symbol g∈L²(ℂⁿ,dμ) such that: (1) gkₐ∈L²(dμ) for every normalized reproducing kernel kₐ, (2) the associated Toeplitz form extends to a bounded operator on H²(ℂⁿ,dμ), but (3) the heat transform g^(1/4) is unbounded on ℂⁿ.

Conclusion: Berger and Coburn's endpoint boundedness criterion fails for general measurable symbols in every complex dimension n≥1, demonstrating that the heat transform condition at t=1/4 is not necessary for boundedness of Toeplitz operators.

Abstract: Berger and Coburn proposed an endpoint boundedness criterion for Toeplitz operators on the Bargmann--Fock space in which the decisive quantity is the heat transform of the symbol at the borderline time $t=\tfrac14$, the time naturally singled out by the Weyl calculus under the Bargmann transform. We show that this criterion fails for general measurable symbols in every complex dimension $n\ge 1$. Concretely, we construct a measurable symbol $g\in L^2(\mathbb C^n,dμ)$ such that $gk_a\in L^2(dμ)$ for every normalized reproducing kernel $k_a$, and the associated Toeplitz form extends to a bounded operator on $H^2(\mathbb C^n,dμ)$, but the heat transform $g^{(1/4)}$ is unbounded on $\mathbb C^n$. The example is obtained by summing translated bounded "blocks" whose Toeplitz norms are summable while their $t=\tfrac14$ heat profiles have fixed size. The blocks are produced by combining a Hilbert--Schmidt estimate for Weyl quantization with the Bargmann correspondence between Weyl and Toeplitz operators.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [61] [A paradox concerning the numerical simulation of Navier-Stokes turbulence](https://arxiv.org/abs/2510.11220)
*Shijie Qin,Kun Xu,Shijun Liao*

Main category: physics.flu-dyn

TL;DR: The paper reveals a paradox in 2D Rayleigh-Bénard convection simulations where final flow types (vortical vs zonal) alternate unpredictably with time-step changes, suggesting numerical noise fundamentally influences NS turbulence outcomes despite NS equations requiring negligible stochastic disturbances.


<details>
  <summary>Details</summary>
Motivation: To investigate the fundamental relationship between numerical simulation parameters (time-step) and the resulting flow types in 2D Rayleigh-Bénard convection, and to uncover potential paradoxes in how Navier-Stokes equations model turbulence versus how they are numerically simulated.

Method: Traditional direct numerical simulation (DNS) of two-dimensional Rayleigh-Bénard convection using double precision arithmetic with varying time-steps, analyzing how different time-step choices affect the final flow type (vortical or zonal flow) and their statistics.

Result: The final flow type alternates between vortical and zonal flow as time-step is reduced to very small values, with statistics completely different between the two flow types. This suggests time-step values corresponding to each turbulent flow type are densely distributed, indicating stochastic numerical noise significantly influences NS turbulence simulation outcomes.

Conclusion: There exists a logical paradox: NS equations require negligible stochastic disturbances, yet numerical simulations show stochastic numerical noise (via time-step variations) fundamentally determines flow types. This highlights essential differences between exact NS solutions, numerical simulations, and actual turbulence, with implications for understanding turbulence and the fourth Millennium problem.

Abstract: Two-dimensional Rayleigh-Bénard convection governed by the Navier-Stokes (NS) equations is predicted by traditional direct numerical simulation (DNS) using double precision arithmetic and a range of different time-steps. It is found that the the final flow type tends either to vortical flow or zonal flow, whose statistics are completely different. Notably, these two flow types frequently alternate as the time-step is reduced to a very small value, suggesting that the time-step corresponding to each turbulent flow type should be densely distributed. Thus, stochastic numerical noise exerts a huge influence on the final flow type and statistics of numerically simulated NS turbulence (i.e., turbulence governed by the NS equations) because the time-step has a close relationship with numerical noise. However, the NS equations, as a turbulence model per se, require all small stochastic disturbances for $t>0$ to be negligible. This leads to a logical paradox in the underlying theory. Further investigations are recommended to reveal the essential differences between the exact solution of NS turbulence, numerical simulations of such NS turbulence, and the corresponding actual turbulence. It should be emphasized that NS turbulence is closely related to the fourth Millennium problem, and turbulence is an unsolved fundamental problem in classic mechanics. Hopefully this paradox would be helpful to deepen our understandings about the turbulence and the fourth Millennium problem.

</details>


### [62] [A Practical Computational Hemolysis Model Incorporating Biophysical Properties of the Red Blood Cell Membrane](https://arxiv.org/abs/2601.19994)
*Nico Dirkes,Marek Behr*

Main category: physics.flu-dyn

TL;DR: Simple strain-based RBC model with pore formation hemoglobin release achieves accurate hemolysis predictions within experimental error, outperforming stress-based power law models by orders of magnitude.


<details>
  <summary>Details</summary>
Motivation: Current computational hemolysis models for blood-handling medical devices are either inaccurate (deviating by orders of magnitude from experiments) or computationally expensive, creating a need for accurate yet simple and efficient models.

Method: Compared three red blood cell models: stress-based (Bludszuweit), simple strain-based (Kelvin-Voigt), and complex tensor-based (TTM). Compared two hemoglobin release models: power-law approach and biophysical pore formation model. Evaluated in FDA blood pump and FDA nozzle benchmarks.

Result: Simple strain-based model combined with pore formation model achieved absolute hemolysis predictions within standard deviation of experimental measurements. Stress-based power law models deviated by several orders of magnitude.

Conclusion: Strain-based pore modeling approach, incorporating RBC membrane viscoelastic properties and hemoglobin release through membrane pores, provides significantly improved hemolysis predictions that can be easily integrated into common CFD workflows.

Abstract: Purpose: Hemolysis is a key issue in the design of blood-handling medical devices. Computational prediction of this phenomenon is challenging due to the complex multiscale nature of blood. As a result, conventional approaches often fail to predict hemolysis accurately, commonly showing deviations of multiple orders of magnitude compared to experimental data. More accurate models are typically computationally expensive and thus impractical for real-world applications. This work aims to fill this gap by presenting accurate yet simple and efficient computational hemolysis models.
  Methods: Hemolysis modeling relies on two key components: a red blood cell model and a hemoglobin release model. In this work, we compare three red blood cell models: a common stress-based model (Bludszuweit), a simple strain-based model based on the Kelvin-Voigt constitutive law, and a more complex tensor-based model (TTM). Further, we compare two hemoglobin release models: the widely used power-law approach and a biophysical pore formation model.
  Results: We evaluate these models in two benchmark cases: the FDA blood pump and the FDA nozzle. In both benchmarks, the simple strain-based model combined with the pore formation model achieves absolute predictions of hemolysis within the standard deviation of experimental measurements. In contrast, stress-based power law models deviate by several orders of magnitude.
  Conclusion: The strain-based pore modeling approach takes into account the biophysical properties of red blood cell membranes, in particular their viscoelastic deformation behavior and hemoglobin release through membrane pores. This leads to significantly improved hemolysis predictions in a framework that can easily be integrated into common CFD workflows.

</details>


### [63] [Numerically Consistent Non-Boussinesq Subgrid-scale Stress Model with Enhanced Convergence](https://arxiv.org/abs/2601.20265)
*Yuenong Ling,Adrián Lozano-Durán*

Main category: physics.flu-dyn

TL;DR: ML-based SGS models for LES with scheme consistency, non-Boussinesq formulation, and monotonic convergence via multi-task learning, tested on APG turbulent boundary layers.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of linear eddy-viscosity closures in complex flows (especially with two inhomogeneous directions) and address lack of monotonic convergence with grid refinement in conventional SGS models.

Method: Extends data-assimilation approach with: 1) non-Boussinesq SGS formulation with dissipation-matching training loss, 2) multi-task learning strategy to explicitly promote monotonic convergence with grid refinement, 3) consistency with numerical scheme of flow solver.

Result: A posteriori tests show improved predictions of mean velocity and wall-shear stress compared to Dynamic Smagorinsky model, while achieving monotonic convergence with grid refinement.

Conclusion: The proposed ML-based SGS model successfully addresses key limitations of traditional models, providing better accuracy and desirable convergence properties for complex flows like APG turbulent boundary layers.

Abstract: We extend the data-assimilation approach of Ling and Lozano-Durán (AIAA 2025-1280) to develop machine-learning-based subgrid-scale stress (SGS) models for large-eddy simulation (LES) that are consistent with the numerical scheme of the flow solver. The method accounts for configurations with two inhomogeneous directions and is applied to turbulent boundary layers (TBL) under adverse pressure gradients (APG). To overcome the limitations of linear eddy-viscosity closures in complex flows, we adopt a non-Boussinesq SGS formulation along with a dissipation-matching training loss. A second improvement is the integration of a multi-task learning strategy that explicitly promotes monotonic convergence with grid refinement, a property that is often absent in conventional SGS models. A posteriori tests show that the proposed model improves predictions of the mean velocity and wall-shear stress relative to the Dynamic Smagorinsky model (DSM), while also achieving monotonic convergence with grid refinement.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [64] [Lipschitz regularity of harmonic map heat flows into $CAT(0)$ spaces](https://arxiv.org/abs/2601.20579)
*Hui-Chun Zhang,Xi-Ping Zhu*

Main category: math.DG

TL;DR: The paper shows that semi-group weak solutions of harmonic map heat flow into CAT(0) spaces are Lipschitz continuous in space and time, and establishes a Bochner inequality.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the historical development of harmonic map heat flow theory, from Eells-Sampson's work on Riemannian manifolds to recent extensions to CAT(0) spaces. While recent work obtained suitable weak solutions with Lipschitz regularity, semi-group weak solutions (developed earlier) lacked known regularity properties despite having favorable long-time behavior.

Method: The authors analyze semi-group weak solutions of harmonic map heat flow into CAT(0) spaces, building on previous work by Mayer and Jost who extended Crandall-Liggett's gradient flow theory to CAT(0) spaces. They investigate the regularity properties of these solutions.

Result: The main result proves that semi-group weak solutions of harmonic map heat flow into CAT(0) spaces are Lipschitz continuous in both space and time. Additionally, the authors establish an Eells-Sampson-type Bochner inequality.

Conclusion: The paper successfully establishes Lipschitz regularity for semi-group weak solutions, addressing an open question about whether these solutions possess the same regularity properties as the recently developed suitable weak solutions. This connects the two approaches to harmonic map heat flow in singular metric spaces.

Abstract: In 1964, Eells and Sampson proved the famous long-time existence and convergence for the harmonic map heat flow into non-positively curved Riemannian manifolds. Subsequently, Hamilton investigated the corresponding initial-boundary problem. In 1992, Gromov and Schoen developed a variational theory of harmonic maps into $CAT(0)$ metric spaces. This progress naturally motivated the study of the harmonic map heat flow into singular metric spaces.
  In the 1990s, Mayer and Jost independently studied convex functionals on $CAT(0)$ spaces and extended Crandall-Liggett's theory of gradient flows from Banach spaces to $CAT(0)$ spaces to obtain the weak solutions, called semi-group weak solutions, for the harmonic map heat flow into $CAT(0)$ spaces. Very recently, Lin, Segatti, Sire, and Wang used an elliptic approximation method to obtain another class of weak solutions, called suitable weak solutions in the sense of the Evolution Variational Inequality (EVI), to the harmonic map heat flow into $CAT(0)$ spaces. They proved that these solutions are Lipschitz in space and $\frac{1}{2}$-Hölder continuous in time.
  Since the semi-group weak solutions of the harmonic map heat flow enjoy the favorable long-time existence, uniqueness and well established long-time behaviors, it is natural to ask if the semi-group solutions possess the Lipschitz regularity. In the present paper, we answer this question. We show that the semi-group weak solutions of the harmonic map heat flow into CAT(0) spaces are Lipschitz continuous in both space and time. We also establish an Eells-Sampson-type Bochner inequality.

</details>
