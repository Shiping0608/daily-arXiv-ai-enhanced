<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 14]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 4]
- [math.DG](#math.DG) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Solution of Advection Equation with Discontinuous Initial and Boundary Conditions via Physics-Informed Neural Networks](https://arxiv.org/abs/2601.20978)
*Omid Khosravi,Mehdi Tatari*

Main category: math.NA

TL;DR: PINNs with Fourier features, two-stage training, adaptive weighting, median filtering, and bounded mapping improve advection equation modeling for discontinuous problems.


<details>
  <summary>Details</summary>
Motivation: Physics-informed neural networks struggle with spectral bias and excessive smoothing when modeling advection equations with discontinuous initial/boundary conditions.

Method: Fourier feature mapping, two-stage training (Fourier parameters then network weights), adaptive loss weighting, spatial median filtering, bounded linear mapping, and modified upwind-inspired loss for nonlinear problems.

Result: Improved modeling of discontinuous solutions in advection equations with reduced spectral bias and better preservation of discontinuities.

Conclusion: The proposed techniques effectively enhance PINN performance for advection equations with discontinuities by addressing spectral bias and smoothing issues.

Abstract: In this paper, we investigate several techniques for modeling the one-dimensional advection equation for a specific class of problems with discontinuous initial and boundary conditions using physics-informed neural networks (PINNs). To mitigate the spectral bias phenomenon, we employ a Fourier feature mapping layer as the input representation, adopt a two-stage training strategy in which the Fourier feature parameters and the neural network weights are optimized sequentially, and incorporate adaptive loss weighting. To further enhance the approximation accuracy, a median filter is applied to the spatial data, and the predicted solution is constrained through a bounded linear mapping. Moreover, for certain nonlinear problems, we introduce a modified loss function inspired by the upwind numerical scheme to alleviate the excessive smoothing of discontinuous solutions typically observed in neural network approximations.

</details>


### [2] [Identification of space-dependent coefficients in two competing terms of a nonlinear subdiffusion equation](https://arxiv.org/abs/2601.21018)
*Barbara Kaltenbacher,William Rundell*

Main category: math.NA

TL;DR: The paper develops a fixed-point scheme for reconstructing spatially varying coefficients in nonlinear diffusion equations from interior observations, proving convergence and local uniqueness.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inverse problem of reconstructing spatially varying coefficients (p and q) in nonlinear diffusion equations, which is important for applications involving equations like Fisher-KPP, Frank-Kamenetskii-Zeldovich, and Allen-Cahn equations.

Method: The authors devise a fixed-point scheme for reconstructing coefficients from interior observations using two approaches: a) observations at final time under two different excitations, and b) observations at two different time instances under a single excitation.

Result: The paper proves convergence of the reconstruction scheme and demonstrates local uniqueness of the coefficients. Numerical experiments illustrate the performance of the reconstruction method.

Conclusion: The proposed fixed-point scheme successfully reconstructs spatially varying coefficients in nonlinear diffusion equations with proven convergence and uniqueness properties, validated through numerical experiments.

Abstract: We consider a (sub)diffusion equation with a nonlinearity of the form $pf(u)-qu$, where $p$ and $q$ are space dependent functions. Prominent examples are the Fisher-KPP, the Frank-Kamenetskii-Zeldovich and the Allen-Cahn equations. We devise a fixed point scheme for reconstructing the spatially varying coefficients from interior observations a) at final time under two different excitations b) at two different time instances under a single excitation. Convergence of the scheme as well as local uniqueness of these coefficients is proven. Numerical experiments illustrate the performance of the reconstruction scheme.

</details>


### [3] [Parametric Hyperbolic Conservation Laws: A Unified Framework for Conservation, Entropy Stability, and Hyperbolicity](https://arxiv.org/abs/2601.21080)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: SymCLaw: A parametric hyperbolic conservation law framework that learns hyperbolic systems from data while guaranteeing conservation, entropy stability, and hyperbolicity by design.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for learning hyperbolic systems from data typically enforce only conservation or rely on prior knowledge of governing equations, lacking guarantees for hyperbolicity and entropy stability which are fundamental physical properties.

Method: Parameterizes flux functions to guarantee real eigenvalues and complete eigenvectors of flux Jacobian (preserving hyperbolicity). Jointly learns convex entropy function and associated flux potential to ensure entropy dissipation. Provides entropy-stable numerical flux scheme compatible with standard discretizations.

Result: Numerical experiments on Burgers, shallow water, Euler, and KPP equations show SymCLaw generalizes to unseen initial conditions, maintains stability under noisy training data, and achieves accurate long-time predictions.

Conclusion: SymCLaw provides a principled foundation for data-driven modeling of hyperbolic conservation laws by ensuring fundamental physical properties (conservation, entropy stability, hyperbolicity) are preserved by design.

Abstract: We propose a parametric hyperbolic conservation law (SymCLaw) for learning hyperbolic systems directly from data while ensuring conservation, entropy stability, and hyperbolicity by design. Unlike existing approaches that typically enforce only conservation or rely on prior knowledge of the governing equations, our method parameterizes the flux functions in a form that guarantees real eigenvalues and complete eigenvectors of the flux Jacobian, thereby preserving hyperbolicity. At the same time, we embed entropy-stable design principles by jointly learning a convex entropy function and its associated flux potential, ensuring entropy dissipation and the selection of physically admissible weak solutions. A corresponding entropy-stable numerical flux scheme provides compatibility with standard discretizations, allowing seamless integration into classical solvers. Numerical experiments on benchmark problems, including Burgers, shallow water, Euler, and KPP equations, demonstrate that SymCLaw generalizes to unseen initial conditions, maintains stability under noisy training data, and achieves accurate long-time predictions, highlighting its potential as a principled foundation for data-driven modeling of hyperbolic conservation laws.

</details>


### [4] [An efficient implicit scheme for the multimaterial Euler equations in Lagrangian coordinates](https://arxiv.org/abs/2601.21241)
*Simone Chiocchetti,Giovanni Russo*

Main category: math.NA

TL;DR: Implicit Lagrangian method for stratified fluid flows with high density/stiffness ratios to overcome time step restrictions while avoiding interface smearing.


<details>
  <summary>Details</summary>
Motivation: Stratified fluids (fluid metamaterials) with alternating layers have unique macroscopic properties, but numerical simulation faces challenges: explicit Lagrangian schemes have severe time step restrictions, while Eulerian methods suffer from artificial interface smearing, especially problematic for high density/stiffness ratio systems like water-air or air-granular media.

Method: Implicit numerical method for multimaterial Euler equations in Lagrangian coordinates that exploits the mathematical structure to solve via a single implicit discrete wave equation for pressure field, yielding symmetric positive definite system for efficiency. Includes filtering strategies to suppress pressure/density oscillations common in multimaterial flows.

Result: The method demonstrates robustness, accuracy, and performance improvements, particularly for stratified media with high density and stiffness ratios, overcoming prohibitive time step restrictions while maintaining sharp interfaces.

Conclusion: The proposed implicit Lagrangian approach provides an efficient solution for simulating stratified fluid metamaterials with high density/stiffness ratios, combining the interface-preserving benefits of Lagrangian methods with the computational efficiency of implicit time stepping.

Abstract: Stratified fluids composed of a sequence of alternate layers show interesting macroscopic properties, which may be quite different from those of the individual constituent fluids. On a macroscopic scale, such systems can be considered a sort of fluid metamaterial. In many cases each fluid layer can be described by Euler equations following the stiffened gas equation of state. The computation of detailed numerical solutions of such stratified material poses several challenges, first and foremost the issue of artificial smearing of material parameters across interface boundaries. Lagrangian schemes completely eliminate this issue, but at the cost of rather stringent time step restrictions. In this work we introduce an implicit numerical method for the multimaterial Euler equations in Lagrangian coordinates. The implicit discretization is aimed at bypassing the prohibitive time step restrictions present in flows with stratified media, where one of the materials is particularly dense, or rigid (or both). This is the case for flows of water-air mixtures, air-granular media, or similar high density ratio systems. We will present the novel discretisation approach, which makes extensive use of the remarkable structure of the governing equations in Lagrangian coordinates to find the solution by means of a single implicit discrete wave equation for the pressure field, yielding a symmetric positive definite structure and thus a particularly efficient algorithm. Additionally, we will introduce simple filtering strategies for counteracting the emergence of pressure or density oscillations typically encountered in multimaterial flows, and will present results concerning the robustness, accuracy, and performance of the proposed method, including applications to stratified media with high density and stiffness ratios.

</details>


### [5] [Natural superconvergence points for splines](https://arxiv.org/abs/2601.21368)
*Peng Yang,Zhimin Zhang*

Main category: math.NA

TL;DR: The paper develops a unified theory of natural superconvergence points for polynomial spline approximations to second-order elliptic problems, showing that superconvergence occurs at local symmetric centers when polynomial degree and derivative order share parity.


<details>
  <summary>Details</summary>
Motivation: To establish a comprehensive theoretical framework for understanding and predicting superconvergence phenomena in polynomial spline approximations, particularly identifying where and why superconvergence occurs naturally in finite element methods.

Method: Develops theory starting from 1D case, showing superconvergence at local symmetric centers when k and s share parity, extends to higher dimensions on simplicial and tensor-product meshes, and uses asymptotic error expansions to characterize all superconvergence points.

Result: Establishes systematic distribution patterns of superconvergence points, demonstrates persistence even in localized symmetric regions, and shows that superconvergence points are readily attainable and follow predictable patterns across dimensions.

Conclusion: The paper provides a unified theoretical framework that explains natural superconvergence phenomena in spline approximations, revealing systematic distribution patterns that can be leveraged for improved numerical accuracy in practical computations.

Abstract: This paper develops a unified theory of natural superconvergence points for polynomial spline approximations to second-order elliptic problems. Beginning with the one-dimensional case, we establish that when a point $x_0$ is a local symmetric center of the partition, the numerical error $(u-u_h)^{(s)}(x_0)$ exhibits superconvergence whenever the polynomial degree $k$ and the derivative order $s$ share the same parity. In particular, for the smoothest spline (B-spline) solution, the abundance of superconvergence points allows us to construct asymptotic expansion of the error within the element that fully characterize all superconvergence points, for both function values and derivatives. The theoretical framework is then extended to higher-dimensional settings on simplicial and tensor-product meshes, and the essential conclusions are preserved, with one-dimensional derivatives generalized to mixed derivatives. Numerical experiments demonstrate that superconvergence persists even in extremely localized symmetric regions, revealing that superconvergence points are both readily attainable and follow systematic distribution patterns.

</details>


### [6] [Higher-Order Finite Difference Methods for the Tempered Fractional Laplacian](https://arxiv.org/abs/2601.21388)
*Mingyi Wang,Dongling Wang*

Main category: math.NA

TL;DR: A general framework for high-order finite difference schemes for tempered fractional Laplacian using new generating functions achieves orders p=4,6,8 with Toeplitz stiffness matrices enabling fast computations.


<details>
  <summary>Details</summary>
Motivation: To develop efficient high-order numerical methods for tempered fractional Laplacian equations, which are important in various applications but challenging to discretize accurately and efficiently.

Method: Develops high-order finite difference schemes based on new generating functions from discrete symbols for tempered fractional Laplacian. The method produces Toeplitz stiffness matrices that enable efficient matrix-vector multiplications via fast algorithms.

Result: The discretizations achieve high-order convergence with orders p=4, 6, 8 for sufficiently smooth functions. The methods demonstrate stability and convergence through rigorous analysis and numerical simulations confirm effectiveness with excellent agreement to theoretical predictions.

Conclusion: The proposed high-order finite difference framework provides an effective and efficient approach for solving tempered fractional Laplacian equations, combining high accuracy with computational efficiency through Toeplitz matrix structure and fast algorithms.

Abstract: This paper presents a general framework of high-order finite difference (HFD) schemes for the tempered fractional Laplacian (TFL) based on new generating functions obtained from the discrete symbols. Specifically, for sufficiently smooth functions, the resulting discretizations achieve high-order convergence with orders $p=4, 6, 8$. The discrete operators lead to Toeplitz stiffness matrices, allowing efficient matrix-vector multiplications via fast algorithms. Building on these approximations, HFD methods are formulated for solving TFL equations, and their stability and convergence are rigorously analyzed. Numerical simulations confirm the effectiveness of the proposed methods, showing excellent agreement with the theoretical predictions.

</details>


### [7] [Numerical Methods for Dynamical Low-Rank Approximations of Stochastic Differential Equations -- Part I: Time discretization](https://arxiv.org/abs/2601.21428)
*Yoshihito Kazashi,Fabio Nobile,Fabio Zoccolan*

Main category: math.NA

TL;DR: This paper analyzes three time-discretization methods for Dynamical Low-Rank Approximation of high-dimensional SDEs using the Dynamically Orthogonal approach, comparing stability and convergence properties.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop and analyze effective numerical time-discretization procedures for the Dynamical Low-Rank Approximation (DLRA) of high-dimensional stochastic differential equations, specifically focusing on the Dynamically Orthogonal method to handle the computational challenges of high-dimensional SDEs.

Method: Three time-discretization strategies are studied: 1) Standard forward discretization of both deterministic and stochastic modes, 2-3) Two staggered algorithms that alternately update deterministic and stochastic modes in half steps. The analysis focuses on convergence properties and stability conditions.

Result: The standard forward discretization converges under a time-step restriction dependent on the smallest singular value of the Gram matrix, which remains positive for non-degenerate noise. The staggered algorithms are more stable and converge without this time-step restriction. Computational experiments support the theoretical findings.

Conclusion: Staggered time-discretization algorithms for DLRA of SDEs offer superior stability compared to standard forward discretization, allowing convergence without restrictive time-step conditions. This work focuses only on time discretization, leaving probability discretization for Part II.

Abstract: In this work (Part I), we study three time-discretization procedures of the Dynamical Low-Rank Approximation (DLRA) of high-dimensional stochastic differential equations (SDEs). Specifically, we consider the Dynamically Orthogonal (DO) method for DLRA proposed and analyzed in arXiv:2308.11581v4, which consists of a linear combination of products between deterministic orthonormal modes and stochastic modes, both time-dependent. The first strategy we consider for numerical time-integration is very standard, consisting in a forward discretization in time of both deterministic and stochastic components. Its convergence is proven subject to a time-step restriction dependent on the smallest singular value of the Gram matrix associated to the stochastic modes. Under the same condition on the time-step, this smallest singular value is shown to be always positive, provided that the SDE under study is driven by a non-degenerate noise. The second and the third algorithms, on the other hand, are staggered ones, in which we alternately update the deterministic and the stochastic modes in half steps. These approaches are shown to be more stable than the first one and allow us to obtain convergence results without the aforementioned restriction on the time-step. Computational experiments support theoretical results. In this work we do not consider the discretization in probability, which will be the topic of Part II.

</details>


### [8] [Numerical analysis of a locking-free primal hybrid method for linear elasticity with $H(\mathrm{div})$-conforming stress recovery](https://arxiv.org/abs/2601.21635)
*Giovanni Taraschi,Maicon Ribeiro Correa*

Main category: math.NA

TL;DR: A hybrid finite element method for linear elasticity with displacement, pressure, and traction Lagrange multiplier, providing locking-free approximations for nearly incompressible materials with H(div)-conforming stress recovery.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for linear elasticity problems that avoids locking phenomena, particularly for nearly incompressible materials, while providing stable approximations and proper stress recovery.

Method: Primal hybrid finite element method using displacement, auxiliary pressure field, and traction Lagrange multiplier. General analysis for discrete solution existence/uniqueness, construction of stable approximation spaces on triangular/quadrilateral meshes, and element-wise stress recovery strategy.

Result: Method achieves optimal convergence orders, is locking-free for nearly incompressible problems, and produces H(div)-conforming, locally equilibrated, weakly symmetric stress approximations robust to locking.

Conclusion: The proposed hybrid finite element method provides a robust, locking-free approach for linear elasticity with proper stress recovery, making it suitable for nearly incompressible materials.

Abstract: In this work, we study a primal hybrid finite element method for the approximation of linear elasticity problems, posed in terms of displacement, an auxiliary pressure field, and a Lagrange multiplier related to the traction. We develop a general analysis for the existence and uniqueness of the solution for the discrete problem, which is applied to the construction of stable approximation spaces on triangular and quadrilateral meshes. The use of these spaces lead to optimal convergence orders, resulting in a locking-free method capable of providing robust approximations for nearly incompressible problems. Finally, we propose a strategy for recovering the stress field from the hybrid solution by solving element-wise sub-problems. The resulting stress approximation is $H(\mathrm{div})$-conforming, locally equilibrated, weakly symmetric, and robust to locking.

</details>


### [9] [A Hybrid semi-Lagrangian Flow Mapping Approach for Vlasov Systems: Combining Iterative and Compositional Flow Maps](https://arxiv.org/abs/2601.21668)
*Philipp Krah,Zetao Lin,R. -Paul Wilhelm,Fabio Bacchini,Jean-Christophe Nave,Virginie Grandgirard,Kai Schneider*

Main category: math.NA

TL;DR: Hybrid semi-Lagrangian scheme combining Numerical Flow Iteration (NuFI) and Characteristic Mapping Method (CMM) for Vlasov-Poisson equation, balancing accuracy, conservation, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical scheme for the Vlasov-Poisson equation that addresses the limitations of existing methods: NuFI has good conservation properties but quadratic computational cost scaling, while CMM has low computational cost but different structural properties. The hybrid approach aims to combine their strengths.

Method: Proposes a hybrid method that uses NuFI for accurate and conservative local time stepping (preserving symplectic structure and invariants), while employing CMM for efficient solution propagation through submap composition. Both methods exploit the semi-group property of the diffeomorphic flow to reconstruct solutions through flow maps.

Result: The hybrid scheme reduces storage requirements, maintains accuracy, and improves structural properties. Numerical experiments demonstrate effectiveness and highlight trade-offs between memory usage and computational cost. Benchmarked against semi-Lagrangian predictor-corrector schemes used in modern gyrokinetic codes.

Conclusion: The hybrid NuFI-CMM approach successfully combines the strengths of both methods, achieving a balance between computational efficiency, conservation properties, and accuracy for solving the Vlasov-Poisson equation.

Abstract: We propose a hybrid semi-Lagrangian scheme for the Vlasov--Poisson equation that combines the Numerical Flow Iteration (NuFI) method with the Characteristic Mapping Method (CMM). Both approaches exploit the semi-group property of the underlying diffeomorphic flow, enabling the reconstruction of solutions through flow maps that trace characteristics back to their initial positions. NuFI builds this flow map iteratively, preserving symplectic structure and conserving invariants, but its computational cost scales quadratically with time. Its advantage lies in a compact, low-dimensional representation depending only on the electric field. In contrast, CMM achieves low computational costs when remapping by composing the global flow map from explicitly stored submaps. The proposed hybrid method merges these strengths: NuFi is employed for accurate and conservative local time stepping, while CMM efficiently propagates the solution through submap composition. This approach reduces storage requirements, maintains accuracy, and improves structural properties. Numerical experiments demonstrate the effectiveness of the scheme and highlight the trade-offs between memory usage and computational cost. We benchmark against a semi-Lagrangian predictor-corrector scheme used in modern gyrokinetic codes, evaluating accuracy and conservation properties.

</details>


### [10] [Adaptive Kernel Methods](https://arxiv.org/abs/2601.21707)
*Tam√°s D√≥zsa,Andrea Angino,Zolt√°n Szab√≥,J√≥zsef Bokor,Matthias Voigt*

Main category: math.NA

TL;DR: The paper introduces adaptive kernel methods with learnable solution spaces that depend on parameters independent of the dataset, enabling efficient large-scale kernel models without approximating infinite-dimensional RKHSs.


<details>
  <summary>Details</summary>
Motivation: Traditional kernel methods use fixed ambient RKHSs determined solely by the kernel and dataset, limiting flexibility. The authors aim to create more adaptive kernel methods with learnable parameters that can improve efficiency and scalability for large datasets.

Method: Two main contributions: 1) Efficient approximation of kernels associated with infinite-dimensional RKHSs for reducing solution-space dimension; 2) Construction of fixed-dimensional, parameter-dependent solution spaces that enable efficient kernel models without needing to approximate infinite-dimensional RKHSs. The methods create variable projection operators that depend on loss function, dataset, and learnable parameters.

Result: The proposed adaptive kernel methods generalize earlier approaches like Random Fourier Features and demonstrate effectiveness through several numerical experiments. The methods enable highly efficient kernel models suitable for large-scale problems.

Conclusion: The novel family of adaptive kernel methods with learnable solution spaces provides a flexible and efficient framework for large-scale kernel learning, overcoming limitations of traditional fixed RKHS approaches while maintaining theoretical foundations.

Abstract: Kernel methods approximate nonlinear maps in a data-driven manner by projecting the target map onto a finite-dimensional Hilbert space called the solution space. Traditionally, this space is a subspace of a fixed ambient reproducing kernel Hilbert space (RKHS), determined solely by the chosen kernel and the dataset, whose elements identify the basis elements. Consequently, the projection operator underlying the kernel method depends on the loss function, the dataset, and the choice of ambient RKHS. In this study, we consider kernel methods whose solution spaces also depend on learnable parameters that are independent of the dataset. The resulting methods can be viewed as variable projection operators that depend on the loss function, the dataset, and the new learnable parameters instead of a fixed RKHS. This work has two main contributions. First, we propose an efficient approximation of kernels associated with infinite-dimensional RKHSs, commonly used to reduce the solution-space dimension for large datasets. Second, we construct fixed-dimensional, parameter-dependent solution spaces that enable highly efficient kernel models suitable for large-scale problems without the need to approximate kernels of infinite-dimensional RKHSs. Our novel family of adaptive kernel methods generalizes earlier approaches, including Random Fourier Features, and we demonstrate their effectiveness through several numerical experiments.

</details>


### [11] [A reduced basis method for parabolic PDEs based on a space-time least squares formulation](https://arxiv.org/abs/2601.21736)
*Michael Hinze,Christian Kahle,Michael Stahl*

Main category: math.NA

TL;DR: POD-greedy reduced basis method for parameter-dependent parabolic PDEs using least squares space-time formulation with minimal regularity assumptions


<details>
  <summary>Details</summary>
Motivation: Extend the least squares space-time approach for parabolic PDEs to parameter-dependent cases, enabling efficient reduced-order modeling for parameterized parabolic problems with minimal regularity requirements

Method: Apply reduced basis method to parameter-dependent least squares space-time formulation, using POD-greedy algorithm with offline-online decomposition and error certification through absolute and relative error bounds

Result: Method successfully handles parameter-dependent parabolic PDEs with minimal regularity, provides certification with error bounds, and demonstrates performance through numerical examples

Conclusion: The POD-greedy reduced basis method effectively extends least squares space-time formulation to parameter-dependent parabolic PDEs, providing certified reduced-order models with error bounds for problems with minimal regularity

Abstract: In this work, we present a POD-greedy reduced basis method for parabolic partial differential equations (PDEs), based on the least squares space-time formulation proposed in [Hinze, Kahle, Stahl, A least-squares space-time approach for parabolic equations, 2023, arXiv:2305.03402] that assumes only minimal regularity. We extend this approach to the parameter-dependent case. The corresponding variational formulation then is based on a parameter-dependent, symmetric, uniformly coercive, and continuous bilinear form. We apply the reduced basis method to this formulation, following the well-developed techniques for parameterized coercive problems, as seen e.g. in reduced basis methods for parameterized elliptic PDEs. We present an offline-online decomposition and provide certification with absolute and relative error bounds. The performance of the method is demonstrated using selected numerical examples.

</details>


### [12] [Solving Hamilton-Jacobi equations by minimizing residuals of monotone discretizations](https://arxiv.org/abs/2601.21764)
*Olivier Bokanowski,Carlos Esteve-Yag√ºe,Richard Tsai*

Main category: math.NA

TL;DR: Residual minimization for monotone finite-difference discretizations yields well-posed discrete solutions for nonlinear equations, enabling neural network solutions for high-dimensional Hamilton-Jacobi equations.


<details>
  <summary>Details</summary>
Motivation: To solve fully nonlinear Hamilton-Jacobi equations in high dimensions using neural networks trained by minimizing residuals from monotone discretizations, addressing analytical challenges of solvability and uniqueness of local minima that don't follow from monotonicity alone.

Method: Derive sufficient conditions under which residual minimization yields well-posed discrete solutions for nonlinear equations defined by monotone finite-difference discretizations, establishing well-posedness of optimization-based solvers.

Result: Framework enables adaptation of Level Set Methods to high-dimensional settings for applications like high-dimensional segmentation and interface tracking, with arguments extending to degenerate elliptic/parabolic PDEs on graphs with monotone graph Laplacians.

Conclusion: Residual minimization with monotone discretizations provides well-posed optimization problems for high-dimensional PDEs, overcoming analytical hurdles and enabling neural network solutions for previously intractable problems.

Abstract: We derive sufficient conditions under which residual minimization yields well-posed discrete solutions for nonlinear equations defined by monotone finite--difference discretizations. Our analysis is motivated by the challenge of solving fully nonlinear Hamilton--Jacobi (HJ) equations in high dimensions by means of a Neural Network, which is trained by minimizing residuals arising from monotone discretizations of the Hamiltonian. While classical theory ensures that consistency and monotonicity imply convergence to the viscosity solution, treating these discrete systems as optimization problems introduces new analytical hurdles: solvability and the uniqueness of local minima do not follow from monotonicity alone.
  By establishing the well--posedness of these optimization--based solvers, our framework enables the adaptation of Level Set Methods to high--dimensional settings, unlocking new capabilities in applications such as high--dimensional segmentation and interface tracking. Finally, we observe that these arguments extend almost directly to degenerate elliptic or parabolic PDEs on graphs equipped with monotone graph Laplacians.

</details>


### [13] [A novel Krylov subspace method for approximating Fr√©chet derivatives of large-scale matrix functions](https://arxiv.org/abs/2601.21799)
*Daniel Kressner,Peter Oehme*

Main category: math.NA

TL;DR: A novel Krylov subspace method for approximating Fr√©chet derivative matrix-vector products of large-scale matrix functions, improving convergence over standard approaches.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need to compute L_f(A,E)b (Fr√©chet derivative of matrix function f(A) in direction E) for sensitivity analysis of network centrality measures and gradient-based optimization involving matrix functions. Standard Krylov methods applied to the block triangular representation have poor spectral properties and convergence issues.

Method: Proposes a novel modification of the Arnoldi algorithm that better preserves the block triangular structure of the representation f([A E; 0 A])[0; b] = [L_f(A,E)b; f(A)b]. This modified approach allows bounding convergence by polynomial approximation of f' on the numerical range of A.

Result: The modified Arnoldi method avoids the convergence difficulties of standard approaches. Numerical experiments demonstrate improved performance and validate the theoretical convergence bounds based on polynomial approximation of f' on the numerical range of A.

Conclusion: The proposed Krylov subspace method provides an efficient and theoretically sound approach for approximating Fr√©chet derivative matrix-vector products of large-scale matrix functions, with better convergence properties than direct application of standard methods to the block triangular representation.

Abstract: We present a novel Krylov subspace method for approximating $L_f(A, E) \vc{b}$, the matrix-vector product of the Fr√©chet derivative $L_f(A, E)$ of a large-scale matrix function $f(A)$ in direction $E$, a task that arises naturally in the sensitivity analysis of quantities involving matrix functions, such as centrality measures for networks. It also arises in the context of gradient-based methods for optimization problems that feature matrix functions, e.g., when fitting an evolution equation to an observed solution trajectory. In principle, the well-known identity \[
  f\left( \begin{bmatrix}
  A & E \\ 0 & A
  \end{bmatrix} \right) \begin{bmatrix}
  0 \\ \vc{b}
  \end{bmatrix} = \begin{bmatrix}
  L_f(A, E) \vc{b} \\ f(A) \vc{b}
  \end{bmatrix}, \] allows one to directly apply any standard Krylov subspace method, such as the Arnoldi algorithm, to address this task. However, this comes with the major disadvantage that the involved block triangular matrix has unfavorable spectral properties, which impede the convergence analysis and, to a certain extent, also the observed convergence. To avoid these difficulties, we propose a novel modification of the Arnoldi algorithm that aims at better preserving the block triangular structure. In turn, this allows one to bound the convergence of the modified method by the best polynomial approximation of the derivative $f^\prime$ on the numerical range of $A$. Several numerical experiments illustrate our findings.

</details>


### [14] [Quotient geometry of tensor ring decomposition](https://arxiv.org/abs/2601.21874)
*Bin Gao,Renfeng Peng,Ya-xiang Yuan*

Main category: math.NA

TL;DR: The paper establishes the quotient geometry of tensor ring decomposition by addressing gauge invariance and full-rank conditions, extending to uniform TR decomposition, with validation through tensor completion experiments.


<details>
  <summary>Details</summary>
Motivation: Tensor ring decomposition has practical success but its intrinsic geometry remains poorly understood due to the ring structure and nontrivial gauge invariance, creating a gap between practical applications and theoretical foundations.

Method: Established quotient geometry of TR decomposition by imposing full-rank conditions on all unfolding matrices of core tensors and capturing gauge invariance. Extended results to uniform TR decomposition where all core tensors are identical.

Result: Successfully developed the geometric framework for TR decomposition, providing theoretical foundations for efficient numerical methods. Numerical experiments validated the developed geometries through tensor ring completion tasks.

Conclusion: The paper bridges the theoretical gap in understanding TR decomposition geometry, providing a solid mathematical foundation that enables more efficient numerical methods for tensor computations and applications.

Abstract: Differential geometries derived from tensor decompositions have been extensively studied and provided the foundations for a variety of efficient numerical methods. Despite the practical success of the tensor ring (TR) decomposition, its intrinsic geometry remains less understood, primarily due to the underlying ring structure and the resulting nontrivial gauge invariance. We establish the quotient geometry of TR decomposition by imposing full-rank conditions on all unfolding matrices of the core tensors and capturing the gauge invariance. Additionally, the results can be extended to the uniform TR decomposition, where all core tensors are identical. Numerical experiments validate the developed geometries via tensor ring completion tasks.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Global oscillatory solutions for the Yang-Mills heat flow](https://arxiv.org/abs/2601.21017)
*Yannick Sire,Juncheng Wei,Youquan Zheng,Yifu Zhou*

Main category: math.AP

TL;DR: The paper analyzes long-time dynamics of SO(4)-equivariant Yang-Mills heat flow with SU(2) structure group in 4D, revealing oscillatory asymptotic behavior as t‚Üí‚àû.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time asymptotic behavior of Yang-Mills heat flow solutions, particularly investigating whether they exhibit novel dynamic patterns like oscillations at infinity.

Method: Study SO(4)-equivariant Yang-Mills heat flow with SU(2) structure group in 4D space, focusing on initial data with specific decay conditions at spatial infinity.

Result: Proved that long-time dynamics can be described by initial data in a unified manner, revealing three types of asymptotic behavior: blow-up, blow-down, and oscillatory behavior as t‚Üí‚àû.

Conclusion: This appears to be the first example of Yang-Mills heat flows exhibiting oscillatory asymptotic behavior at time infinity, demonstrating rich dynamic possibilities beyond traditional blow-up/blow-down scenarios.

Abstract: We investigate the long-time dynamics for the global solution of the $SO(4)$-equivariant Yang-Mills heat flow (YMHF) with structure group $SU(2)$ in space dimension $4$. For a class of initial data with specific decay at spatial infinity, we prove that the long-time dynamics of YMHF can be described by the initial data in a unified manner. As a consequence, the global solutions can exhibit blow-up, blow-down, and more exotically, {\it oscillatory} asymptotic behavior at time infinity. This seems to be the first example of Yang-Mills heat flows with oscillatory behavior as $t\to \infty$.

</details>


### [16] [Decay rates to equilibrium in a nonlinear subdiffusion equation with two counteracting terms](https://arxiv.org/abs/2601.21038)
*Barbara Kaltenbacher*

Main category: math.AP

TL;DR: Proves convergence to steady state for subdiffusion equations with exponential or power law rates under mild conditions on coefficients, nonlinearity, and elliptic operator.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous convergence results for subdiffusion equations with nonlinear terms, which are important in modeling anomalous diffusion phenomena in physics, biology, and engineering.

Method: Mathematical analysis of subdiffusion equations using functional analysis techniques, likely involving fractional calculus, semigroup theory, and energy methods to prove convergence to steady states.

Result: Proves that solutions converge to steady states as time goes to infinity for both exponential (Œ±=1) and power law (Œ±‚àà[0,1)) subdiffusion rates under mild conditions on the coefficients p, q, nonlinearity f, source r, and elliptic operator ùïÉ.

Conclusion: The paper establishes convergence to steady states for a broad class of subdiffusion equations with nonlinear terms, providing important mathematical foundations for understanding long-term behavior in anomalous diffusion systems.

Abstract: In this paper we prove convergence to a steady state as $t\to\infty$ for solutions to the subdiffusion equation \[ \partial_t^Œ±u - \mathbb{L} u = q(x)u - p(x)f(u) + r \] with the exponential ($Œ±=1$) or power law ($Œ±\in[0,1)$) rates under mild conditions on the coefficients $p$, $q$, the nonlinearity $f$, the source $r$, and the elliptic operator $\mathbb{L}$.

</details>


### [17] [Classical solutions to the Boltzmann equations for gas mixture with unequal molecular masses](https://arxiv.org/abs/2601.21213)
*Gaofeng Wang,Weike Wang,Tianfang Wu*

Main category: math.AP

TL;DR: Global existence of classical solutions for multi-component Boltzmann equations with unequal molecular masses and soft potentials near Maxwellians in periodic domains.


<details>
  <summary>Details</summary>
Motivation: Most Boltzmann equation research focuses on single species, while gas mixtures with unequal molecular masses (like Earth's atmosphere with N‚ÇÇ:O‚ÇÇ mass ratio 7:8) have limited studies despite broader applications.

Method: Analyzes Boltzmann equations for mixtures with unequal molecular masses (m·¥¨‚â†m·¥Æ), establishes global existence of classical solutions near Maxwellians for soft potentials (-3<Œ≥<0) in periodic spatial domains, with detailed characterization of linear collision operator structure and estimates for nonlinear terms.

Result: Proves global in time existence of classical solutions for arbitrary molecular mass ratios, providing detailed analysis of linear collision operator structure and nonlinear term estimates under unequal mass conditions.

Conclusion: The results advance spectral analysis for soft potentials and L¬≤,L^‚àû frameworks for multi-component Boltzmann equations, enabling future studies of gas mixtures with unequal molecular masses.

Abstract: The Boltzmann equation is essential for gas thermodynamics,as it models how the molecular density distribution $F(t,x,v)$ changes over time. However, existing research primarily focuses on the single species Boltzmann equation, while investigations into gas mixtures with unequal molecular masses remain relatively limited. Notably, mixed gas studies have broader applications exemplified by Earth's atmosphere, composed of 78\% nitrogen, 21\% oxygen, and 1\% trace gases, where the $N_2$ to $O_2$ molecular mass ratio is 28:32 (simplified as 7:8). This work addresses the Boltzmann equations for such mixtures with unequal molecular masses $(m^A\neq m^B)$, establishing the global in time existence of classical solutions near Maxwellians for soft potentials ($-3<Œ≥<0$) in a periodic spatial domain. Our analysis encompasses arbitrary molecular mass ratios. Our analysis encompasses arbitrary molecular mass ratios. The main contribution of this paper lies in the detailed characterization of the linear collision operator's structure and establishing estimates for the nonlinear terms under unequal mass conditions. Consequently, these results may help advance spectral analysis for soft potentials as well as $L^2,L^{\infty}$ frameworks in future studies of multi-component Boltzmann equations.

</details>


### [18] [Hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixture](https://arxiv.org/abs/2601.21245)
*Yeping Li,Gaofeng Wang,Tianfang Wu*

Main category: math.AP

TL;DR: The paper studies the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixtures using Hilbert expansion, deriving bi-Maxwellian distributions and proving convergence as Knudsen number tends to zero with validity time estimates depending on potential range.


<details>
  <summary>Details</summary>
Motivation: To rigorously justify the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixtures with arbitrary particle masses and charges, which introduces asymmetric effects making the system difficult to decouple and analyze.

Method: Uses Hilbert expansion method, calculates first 2k-1 terms (k‚â•6), truncates expansion, expresses solution as sum of expansion terms plus remainder. Employs L¬≤-W¬π,‚àû interplay framework with new weight function, uses vector-valued functions to handle asymmetric collision operators, analyzes velocity decay rate to eliminate singularity from small parameters.

Result: Derives bi-Maxwellian determined by Euler-Poisson system of two fluids. Proves solution validity time is O(Œµ^{-y}) where y = -(2k-3)/[2(2k-1)] for -1‚â§Œ≥‚â§1, and y = -(2k-3)/[(1-Œ≥)(2k-1)] for -3<Œ≥<-1.

Conclusion: The results provide rigorous justification of hydrodynamic limit for gas mixtures with physical realism, applicable to analyzing gas flow dynamics in daytime ionosphere at high altitudes above Earth.

Abstract: In this paper, we study the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for a gas mixture in the whole space $(x \in \mathbb{R}^3)$ with the potential range of $Œ≥\in\left(-3, 1\right]$. Using the method of Hilbert expansion, we first derive a bi-Maxwellian determined by the Euler-Poisson system of two fluids. To justify the convergence of the solution rigorously as the Knudsen number tends to zero, we sequentially calculate the first $2k-1$ terms of the expansion series $(k \geq 6)$, and then truncate it, and express the solution as the sum of these first $2k-1$ terms and a remainder term. Within the framework of the $L_{x,v}^2-W_{x,v}^{1,\infty}$ interplay established by Guo and Jang \cite{[ininp]Guo2010CMP}, we construct a new weight function to estimate the remainder term in four different cases regarding the potential $Œ≥$. Here, the particle masses $m^A, m^B > 0$ and their charges $e^A, e^B$ can be given arbitrarily. This causes the collision operator to exhibit asymmetric effects ($m^A \neq m^B$), rendering the system of equations impossible to decouple. So, it adds difficulties to both $L^2$, $L^{\infty}$ estimates for the remainder. Therefore, we adopt the framework of vector-valued functions and analyze the velocity decay rate of the operator $K_{M,2,w}^{Œ±,c}$ to eliminate the singularity induced by small parameters in characteristic line iterations. Our results show that the validity time of the solution is $O(\varepsilon^{-y})$, where $y$ is $-\frac{2k-3}{2(2k-1)}$ when $-1 \leq Œ≥\leq 1$, and it becomes $-\frac{2k-3}{(1-Œ≥)(2k-1)}$, when $-3 < Œ≥< -1$. These results possess strong physical realism and can be applied to analyze gas flow dynamics in the daytime ionosphere at high altitudes above the Earth.

</details>


### [19] [Blow-up phemomenon for the Geng-Xue system and related models](https://arxiv.org/abs/2601.21295)
*Song Liu,Zhaoyang Yin*

Main category: math.AP

TL;DR: The paper analyzes blow-up criteria and phenomena for the Geng-Xue system with cubic nonlinearity, extending results to the b-family of two-component systems.


<details>
  <summary>Details</summary>
Motivation: To study the Cauchy problem for the Geng-Xue system with cubic nonlinearity, particularly focusing on understanding blow-up behavior without relying on conservation laws.

Method: 1. Prove blow-up criteria in low Besov spaces. 2. Use a method that doesn't require conservation laws to establish blow-up phenomena. 3. Extend results to the b-family of two-component systems with cubic nonlinearity.

Result: Established blow-up criteria for the Geng-Xue system in low Besov spaces and demonstrated blow-up phenomena using conservation-law-independent methods, with extensions to the b-family of systems.

Conclusion: The paper successfully analyzes blow-up behavior for cubic nonlinear Geng-Xue systems, providing criteria and phenomena proofs without conservation law dependence, with broader applicability to related b-family systems.

Abstract: In this paper, we consider the Cauchy problem of the Geng-Xue system with cubic nonlinearity. Firstly, we prove a blow-up criteria in the low besov space. Secondly, we prove the blow-up phenomenon by using the method which does not require any conservation law. Finally, we extend our results to the b-family of two-component system with cubic nonlinearity.

</details>


### [20] [Wellposedness and dynamics of two types of reaction--nonlocal diffusion systems under the inhomogeneous spectral fractional Laplacian](https://arxiv.org/abs/2601.21422)
*Pu Yuan,Paul A. Zegeling*

Main category: math.AP

TL;DR: The paper studies reaction-nonlocal diffusion equations with fractional Laplacian operators, establishing local wellposedness, maximum principles, and analyzing specific prototypes including bistable RNDE and nonlocal Gray-Scott system with numerical simulations.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous analytical framework for reaction-nonlocal diffusion equations that model anomalous diffusion beyond Brownian motion, particularly focusing on boundary value problems with constant Dirichlet conditions and understanding the impact of fractional diffusion orders on solution behavior and pattern formation.

Method: Uses harmonic lifting to handle boundary constraints, analytic contraction semigroup theory in C‚ÇÄ(Œ©) to obtain Duhamel formula and smoothing properties, derives maximum principles via L‚àû-contractivity and positivity preservation, analyzes specific prototypes (bistable RNDE and nonlocal Gray-Scott system), and employs numerical simulations with sine pseudospectral discretization and ETDRK4 time-stepping.

Result: Established local wellposedness for locally Lipschitz reactions with blow-up alternative, proved L‚àû-contractivity and positivity preservation, derived invariant-range property for bistable RNDE solutions, proved positivity preservation for nonlocal Gray-Scott system, identified explicit L‚àû invariant set ensuring global boundedness, and performed numerical simulations showing impact of fractional orders on pattern formation.

Conclusion: The paper provides a comprehensive analytical framework for reaction-nonlocal diffusion equations with fractional Laplacians, establishing fundamental properties including wellposedness, maximum principles, and stability results, with applications to specific reaction-diffusion systems and numerical validation of theoretical findings.

Abstract: Reactio-nonlocal diffusion equations model nonlocal transport and anomalous diffusion by replacing the Laplacian with a fractional power, capturing diffusion mechanisms beyond Brownian motion. We primarily study the semilinear problem \[ \partial_t u + Œµ^2(-Œî)_g^Œ±u = \mathcal{N}(u) \] allowing constant inhomogeneous Dirichlet boundary condition $u|_{\partialŒ©}=g$. To handle the boundary constraint, we use a harmonic lifting to reformulate the problem as an equivalent homogeneous system with a shifted nonlinearity. Working in \(C_0(Œ©)\), analytic contraction semigroup theory yields the Duhamel formula and quantitative smoothing, implying local wellposedness for locally Lipschitz reactions and a blow-up alternative. The semigroup viewpoint also provides $L^\infty$-contractivity and positivity preservation, which drive pointwise maximum principles and stability bounds. Furthermore, we analyze two prototypes. For the bistable RNDE, we derive an energy dissipation identity and, using a fractional weak maximum principle, obtain an invariant-range property that confines solutions between the two stable steady states. For the nonlocal Gray-Scott system with possibly different fractional diffusion orders, we prove that solutions preserve positivity. Moreover, we identify an explicit \(L^\infty\) invariant set ensuring global boundedness, and derive an eigenfunction-weighted interior \(L^2\) bound. Finally, we perform numerical simulations using a sine pseudospectral discretization and ETDRK4 time-stepping, which the impact of fractional orders on pattern formation, consistent with our analytical results.

</details>


### [21] [Multistatic anisotropic travel-time as a tensor tomography problem](https://arxiv.org/abs/2601.21640)
*Naeem Desai,Oliver Graham,William R. B. Lionheart*

Main category: math.AP

TL;DR: The paper analyzes travel-time imaging problems for anisotropic reflectivity reconstruction using multistatic measurements, relating them to generalized Radon transforms and tensor ray transforms.


<details>
  <summary>Details</summary>
Motivation: To understand how to reconstruct anisotropic reflectivity in travel-time imaging problems where reflectivity depends on direction (incoming and outgoing), particularly in multistatic configurations where transmitters and receivers are not co-located.

Method: Formulates travel-time problems as generalized Radon transforms integrating over isochrones (ellipses in planar case, spheroids in volumetric case). Relates simplified distant transmitter/receiver case to tensor ray transforms (specifically Sharafutdinov's longitudinal ray transform) and discusses implications of its null-space.

Result: Establishes mathematical connections between travel-time imaging for anisotropic reflectivity and established transform theory, providing theoretical framework for understanding reconstruction limitations due to null-spaces in tensor transforms.

Conclusion: Travel-time imaging for anisotropic reflectivity in multistatic configurations can be mathematically formulated using generalized Radon transforms and tensor ray transforms, with important implications from known null-spaces that affect what can be reconstructed.

Abstract: Travel-time imaging problems seek to reconstruct an image of reflectivity of a scene by measuring travel time (and amplitude, phase) of electromagnetic or acoustic signals, such as radar and sonar. Multistatic, in this context, means that the transmitters and receivers need not be co-located. The reflectivity is anisotropic if it depends on direction, and in the multistatic case this means incoming and outgoing direction. Travel-time problems can be formulated as generalized Radon transforms of integrals over isochrones, in the planar case ellipses with transmitter and receivers at foci. In a simplified case where transmitters and receivers are distant from the scene, isochrones can be approximated by straight lines. We relate this to tensor ray transforms, specifically the longitudinal ray transform of Sharafutdinov, and discuss the implication of its known null-space. In the volumetric case isochrones are spheroids and we relate the problem to the normal Radon transform of tensor fields.

</details>


### [22] [The Kolmogorov forward equation for a distributed model of regime-switching diffusions](https://arxiv.org/abs/2601.21659)
*Alexander S. Bratus,Olga S. Rozanova*

Main category: math.AP

TL;DR: The paper proposes integro-differential equations for regime-switching diffusion processes with/without advection, develops constructive algorithms for solving the Cauchy problem, finds explicit solutions for some initial distributions, and discusses approximating discrete hidden states with continuous distributions.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical tools for analyzing regime-switching diffusion processes with continuously distributed states, which can approximate models with discrete hidden states and provide more flexible modeling frameworks for stochastic systems with state transitions.

Method: Proposes integro-differential equations describing state densities for regime-switching diffusion processes; develops constructive algorithms for solving the Cauchy problem; finds explicit solutions for specific initial distributions; discusses approximation of discrete hidden state models by continuous state distributions.

Result: Existence of constructive algorithms for solving the Cauchy problem; explicit solutions obtained for certain initial distributions; demonstration of how discrete hidden state models can be approximated by continuous state distributions.

Conclusion: The proposed integro-differential equation framework provides a powerful mathematical approach for analyzing regime-switching diffusion processes with continuously distributed states, offering both computational algorithms and analytical solutions while enabling approximation of discrete-state models.

Abstract: For the regime-switching diffusion process with and without advection term we propose an integro-differential equation describing the densities of states continuously distributed over a segment. We demonstrate that there exists a constructive algorithm for solving the Cauchy problem. We then show that for some initial distributions of states, the solution can be found explicitly. We also discuss how a model with a discrete number of hidden states can be approximated by a model with continuously distributed states.

</details>


### [23] [Nonhomogeneous boundary condition for spectral non-local operators](https://arxiv.org/abs/2601.21674)
*Ivan Bioƒçiƒá,Vanja Wagner*

Main category: math.AP

TL;DR: Study of semilinear non-local elliptic problems with spectral-type operators in bounded domains, focusing on nonhomogeneous boundary conditions and establishing existence results for general nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of non-local elliptic problems beyond the fractional Laplacian framework, particularly addressing the challenge of formulating and analyzing nonhomogeneous boundary conditions in non-local settings where traditional boundary value problems are ill-posed.

Method: Combines stochastic process techniques (killed L√©vy processes), potential theory, and spectral analysis. Introduces a weak L¬π trace-like boundary operator and analyzes boundary behavior using renewal functions and distance to boundary.

Result: Establishes sharp boundary estimates for Green and Poisson potentials, develops a weak boundary trace operator, and proves existence results for solutions under general nonlinearities including sign-changing and non-monotone cases.

Conclusion: Provides a unified framework for semilinear boundary problems in non-local settings that extends beyond fractional Laplacian theory, with boundary behavior characterized by renewal functions and distance to boundary.

Abstract: We study semilinear non-local elliptic problems driven by spectral-type operators of the form $œà(-L_{|D})$ in a bounded $C^{1,1}$ domain $D\subset \mathbb{R}^d$ with a nonhomogeneous boundary condition. Here $œà$ is a Bernstein function satisfying a weak scaling condition at infinity, and $L_{|D}$ is the generator of a killed L√©vy process. This general framework covers and extends the theory of the interpolated fractional Laplacian. A key novelty in this setting is the analysis of the nonhomogeneous boundary condition formulated in terms of the Poisson potential with respect to the $d-1$ Hausdorff measure on $\partial D$. We establish sharp boundary estimates for Green and Poisson potentials, introduce a weak $L^1$ trace-like boundary operator, and provide existence results for solutions under quite general nonlinearities, including sign-changing and non-monotone cases. The methodology combines stochastic process techniques, potential theory, and spectral analysis, and expresses the boundary behavior of the solution in terms of the renewal function and the distance to the boundary, suggesting a possible unified treatment of semilinear boundary problems in non-local settings.

</details>


### [24] [Localized Big Bang Stability of Spacetime Dimensions $n\geq4$](https://arxiv.org/abs/2601.21677)
*Weihang Zheng*

Main category: math.AP

TL;DR: The paper proves nonlinear stability of sub-critical Kasner-scalar field solutions in higher dimensions (n‚â•4), showing perturbed solutions remain asymptotically Kasner and terminate at crushing singularities.


<details>
  <summary>Details</summary>
Motivation: To generalize previous stability results for Kasner-scalar field solutions from specific dimensions to all higher dimensional spacetimes (n‚â•4), extending the understanding of cosmological singularities in Einstein-scalar field theory.

Method: Mathematical analysis of Einstein-scalar field equations on truncated cone domains, proving nonlinear stability through rigorous PDE techniques and geometric analysis methods.

Result: Perturbed sub-critical Kasner-scalar field solutions are asymptotically pointwise Kasner, geodesically incomplete in contracting direction, and terminate at quiescent crushing singularities with curvature blow-up.

Conclusion: The stability result extends to all dimensions n‚â•4, confirming Kasner-scalar field solutions as robust attractors in higher-dimensional cosmology with similar singularity structure as lower dimensions.

Abstract: We prove the past nonlinear stability of the sub-critical Kasner-scalar field solutions to the Einstein-scalar field equations on a truncated cone domain in spacetime dimensions $n\geq4$. Our analysis demonstrates that the perturbed solutions are asymptotically pointwise Kasner, geodesically incomplete in the contracting direction and terminate at quiescent and crushing singularities characterized by the blow-up of curvature invariants. This work generalizes the result of Beyer-Oliynyk-Zheng in [arXiv:2502.09210v2] to all higher dimensional spacetimes.

</details>


### [25] [Unique Continuation Property for Stochastic Wave Equations](https://arxiv.org/abs/2601.21854)
*Qi L√º,Zhonghua Liao*

Main category: math.AP

TL;DR: Stochastic wave equations restore unique continuation across characteristic surfaces, unlike deterministic waves where it fails.


<details>
  <summary>Details</summary>
Motivation: To investigate whether stochastic effects can restore the unique continuation property (UCP) that fails for deterministic wave equations across characteristic hypersurfaces.

Method: Develop novel stochastic Carleman estimates where the It√¥ diffusion term provides positive energy contributions absent in deterministic models.

Result: Proved UCP restoration: if solution vanishes on one side of characteristic surface with non-degenerate stochastic diffusion, it vanishes in full neighborhood. Extended to non-homogeneous sources and global continuation from narrow characteristic cones.

Conclusion: Stochastic hyperbolic dynamics qualitatively differ from deterministic ones, opening new avenues for stochastic control theory and inverse problems.

Abstract: This paper establishes a fundamental and surprising phenomenon in the theory of stochastic wave equations: the restoration of the unique continuation property (UCP) across characteristic hypersurfaces, a property that is known to fail generically in the deterministic setting. We prove that if a solution to a linear stochastic wave equation vanishes on one side of a characteristic surface $Œì$, then it must vanish in a full neighborhood of any point on $Œì$, provided the stochastic diffusion coefficient is non-degenerate. This result stands in sharp contrast to the classical H√∂rmander-type counterexamples for deterministic waves.
  Furthermore, we extend the UCP to equations with non-homogeneous stochastic sources and establish a global unique continuation result from the interior of an arbitrarily narrow characteristic cone. Our proofs rely on a novel stochastic Carleman estimate, where the It√¥ diffusion term introduces a crucial positive energy contribution that is absent in deterministic models.
  These findings demonstrate a qualitative difference between deterministic and stochastic hyperbolic dynamics and open new avenues for control theory and inverse problems in stochastic setting.

</details>


### [26] [Probabilistically Strong Solutions to Stochastic Euler Equations](https://arxiv.org/abs/2601.22073)
*Benjamin Gess,Robert Lasarzik*

Main category: math.AP

TL;DR: The paper establishes existence of probabilistically strong, measure-valued solutions for stochastic Navier-Stokes equations and proves their convergence to probabilistically strong solutions for stochastic Euler equations in vanishing viscosity limit.


<details>
  <summary>Details</summary>
Motivation: To solve the open problem of constructing probabilistically strong solutions for stochastic Euler equations that satisfy energy inequality for general L¬≤ initial data, and to extend results to fluids driven by transport noise.

Method: Introduces the concept of energy-variational solutions in stochastic context to treat nonlinearities without changing probability space. Uses vanishing viscosity limit approach from Navier-Stokes to Euler equations.

Result: Establishes existence of probabilistically strong, measure-valued solutions for stochastic incompressible Navier-Stokes equations and proves their convergence to probabilistically strong solutions for stochastic Euler equations in vanishing viscosity limit.

Conclusion: Successfully solves the open problem of constructing probabilistically strong solutions for stochastic Euler equations with energy inequality for general L¬≤ initial data, with extension to transport noise-driven fluids.

Abstract: In this paper, we establish the existence of probabilistically strong, measure-valued solutions for the stochastic incompressible Navier--Stokes equations and prove their convergence, in the vanishing viscosity limit, to probabilistically strong solutions for the stochastic incompressible Euler equations. In particular, this solves the open problem of constructing probabilistically strong solutions for the stochastic Euler equations that satisfy the energy inequality for general $L^2$ initial data. We introduce the concept of energy-variational solutions in the stochastic context in order to treat the nonlinearities without changing the probability space. Furthermore, we extend these results to fluids driven by transport noise.

</details>


### [27] [On Global Weak Solutions for the Magnetic Two-Component Hunter-Saxton System](https://arxiv.org/abs/2601.22088)
*Levin Maier*

Main category: math.AP

TL;DR: The paper provides analytical foundations for the magnetic two-component Hunter-Saxton system (M2HS), deriving explicit solution formulas and constructing global conservative weak solutions.


<details>
  <summary>Details</summary>
Motivation: The M2HS was recently derived as a magnetic geodesic equation on an infinite-dimensional configuration space, but the geometric framework and global weak flow were only outlined. This paper aims to provide the analytical foundations from the PDE perspective.

Method: 1) Derive explicit solution formula in Lagrangian variables via Riccati reduction; 2) Rigorously construct global conservative weak solutions by developing analytic theory of relaxed configuration space and associated weak magnetic geodesic flow.

Result: Obtained explicit solution formula yielding alternative proof of blow-up criterion with explicit expression for blow-up time. Successfully constructed global conservative weak solutions, realizing the geometric program proposed in previous work.

Conclusion: The paper establishes analytical foundations for M2HS, providing both explicit solution formulas and rigorous construction of global weak solutions, completing the geometric program outlined in previous research.

Abstract: We study the magnetic two-component Hunter-Saxton system (M2HS), which was recently derived in \cite{M24} as a magnetic geodesic equation on an infinite-dimensional configuration space. While the geometric framework and the global weak flow were outlined there, the present paper provides the analytical foundations of this construction from the PDE perspective.
  First, we derive an explicit solution formula in Lagrangian variables via a Riccati reduction, yielding an alternative proof of the blow-up criterion together with an explicit expression for the blow-up time. Second, we rigorously construct global conservative weak solutions by developing the analytic theory of the relaxed configuration space and the associated weak magnetic geodesic flow, thereby realizing the geometric program proposed in \cite{M24}.

</details>


### [28] [Microlocal maximal hypoellipticity from the geometric viewpoint: I](https://arxiv.org/abs/2601.22122)
*Omar Mohsen*

Main category: math.AP

TL;DR: Develops a bi-graded pseudo-differential calculus for sub-Riemannian manifolds that unifies classical calculus with sub-Riemannian adapted calculus, using geometric resolutions and operator algebras.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive pseudo-differential calculus for H√∂rmander vector fields that bridges classical theory with sub-Riemannian geometry, enabling analysis of operators in constrained geometric settings.

Method: Uses geometric constructions (resolution of singularities) combined with operator algebra methods to develop a bi-graded calculus including Sobolev spaces, wavefront sets, and principal symbols.

Result: Proves that invertibility of the principal symbol implies microlocal maximal hypoellipticity, resolving affirmatively the microlocal version of Helffer and Nourrigat's conjecture.

Conclusion: The developed calculus provides a unified framework for analyzing operators on sub-Riemannian manifolds and establishes fundamental connections between symbol invertibility and hypoelliptic properties.

Abstract: Given some vector fields on a smooth manifold satisfying H√∂rmander's condition, we define a bi-graded pseudo-differential calculus which contains the classical pseudo-differential calculus and a pseudo-differential calculus adapted to the sub-Riemannian structure induced by the vector fields.
  Our approach is based on geometric constructions (resolution of singularities) together with methods from operators algebras. We develop this calculus in full generality, including Sobolev spaces, the wavefront set, and the principal symbol, etc.
  In particular, using this calculus, we prove that invertibility of the principal symbol implies microlocal maximal hypoellipticity. This allows us to resolve affirmatively the microlocal version of a conjecture of Helffer and Nourrigat.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Semi-implicit Lax-Wendroff kinetic scheme for hydrodynamic phonon transport](https://arxiv.org/abs/2601.21161)
*Shijie Li,Hong Liang,Songze Chen,Chuang Zhang*

Main category: physics.comp-ph

TL;DR: A semi-implicit Lax-Wendroff kinetic scheme for hydrodynamic phonon transport using double relaxation time approximation, enabling larger time steps and cell sizes than phonon mean free path/relaxation time at small Knudsen numbers.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for multi-scale thermal conduction in solid materials that can handle both normal and resistive scattering processes in phonon transport, allowing for larger computational time steps and cell sizes than traditional methods.

Method: Semi-implicit Lax-Wendroff kinetic scheme based on Boltzmann transport equation with double relaxation time approximation. Uses trapezoidal rule for scattering terms and midpoint rule for migration terms in finite volume framework. Solves kinetic equation for interfacial flux reconstruction instead of direct interpolation, coupling phonon migration and scattering within time steps. Employs second-order upwind or central schemes for distribution function reconstruction.

Result: The method enables cell sizes and time steps larger than phonon mean free path and relaxation time at small Knudsen numbers. Numerical tests show accurate capture of multi-scale thermal conduction phenomena across different normal/resistive scattering rates.

Conclusion: The developed semi-implicit kinetic scheme provides an efficient and accurate approach for simulating hydrodynamic phonon transport across multiple scales, successfully handling both scattering processes while allowing computationally favorable discretization parameters.

Abstract: A semi-implicit Lax-Wendroff kinetic scheme is developed for hydrodynamic phonon transport in solid materials based on the Boltzmann transport equation under the double relaxation time approximation, in which both the normal and resistive scattering processes are accounted. The trapezoidal and midpoint rules are adopted for the temporal integration of the scattering and migration terms under the framework of finite volume method, respectively. Instead of direct numerical interpolation, the kinetic equation is solved again when reconstructing the interfacial flux, in order to realize the coupling of phonon migration and scattering within a numerical time step. Specifically, the finite difference scheme is introduced and the second-order upwind or central schemes are used for the reconstruction of the interfacial distribution function and its spatial gradient. Consequently, the cell size and time step of the present method could be larger than the phonon mean free path and relaxation time in the limit of small Knudsen numbers. Numerical tests demonstrate that the present method can accurately capture multi-scale thermal conduction phenomena within different normal or resistive scattering rates.

</details>


### [30] [Acquiring Human-Like Mechanics Intuition from Scarce Observations via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21881)
*Jingruo Peng,Shuze Zhu*

Main category: physics.comp-ph

TL;DR: A reinforcement learning framework using episodic switching across related physical observations enables agents to develop accurate mechanics intuition from just 2-3 observations, generalizing well beyond training data.


<details>
  <summary>Details</summary>
Motivation: Humans can infer accurate mechanical outcomes from few observations (mechanics intuition), but the mechanisms behind such data-efficient learning remain unclear. The paper aims to understand and replicate this capability in artificial agents.

Method: Proposes a reinforcement learning framework where agents encode continuous physical observation parameters into their state and are trained via episodic switching across closely related observations. The agent learns from just 2-3 observations.

Result: The agent acquires robust mechanics intuition that generalizes accurately over wide parameter ranges, substantially beyond training data, demonstrated on brachistochrone and large-deformation elastic plate problems. Generalization emerges when learned value function enforces Bellman consistency across neighboring task parameters.

Conclusion: Episodic switching provides a principled route to artificial mechanics intuition, with theoretical explanation linking to biological generalization abilities through smooth policies capturing low-dimensional solution manifolds.

Abstract: Humans can infer accurate mechanical outcomes from only a few observations, a capability known as mechanics intuition. The mechanisms behind such data-efficient learning remain unclear. Here, we propose a reinforcement learning framework in which an agent encodes continuous physical observation parameters into its state and is trained via episodic switching across closely related observations. With merely two or three observations, the agent acquires robust mechanics intuition that generalizes accurately over wide parameter ranges, substantially beyond the training data, as demonstrated on the brachistochrone and a large-deformation elastic plate. We explain this generalization through a unified theoretical view: it emerges when the learned value function enforces Bellman consistency across neighboring task parameters, rendering the Bellman residual stationary with respect to physical variations. This induces a smooth policy that captures a low-dimensional solution manifold underlying the continuum of tasks. Our work establishes episodic switching as a principled route to artificial mechanics intuition and offers a theoretical link to similar generalization abilities in biological learners.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [A joint diffusion approach to multi-modal inference in inertial confinement fusion](https://arxiv.org/abs/2601.21006)
*Michael S. Jones,Justin Kunimune,Daniel Casey,Bogdan Kustowski,Eugene Kur,Kelli Humbird*

Main category: physics.plasm-ph

TL;DR: JointDiff is a generative framework using joint diffusion to predict conditional simulation input/output distributions from partial multi-modal observations in inertial confinement fusion, unifying forward modeling, inverse inference, and output imputation.


<details>
  <summary>Details</summary>
Motivation: In inertial confinement fusion (ICF), only a subset of simulated data is available experimentally despite both simulation and experiment producing scalar and image outputs. There's a need to bridge this gap between simulation and experimental data for better understanding and design acceleration.

Method: JointDiff uses joint diffusion to unify forward surrogate modeling, inverse inference, and output imputation into one architecture. It's trained on large ensembles of 3D Multi-Rocket Piston simulations and applied to National Ignition Facility experiments.

Result: The model demonstrates high accuracy, statistical robustness, and transferability to NIF experiments. It establishes JointDiff as a flexible generative surrogate for multi-modal scientific tasks in ICF.

Conclusion: JointDiff provides a powerful framework for understanding diagnostic constraints, aligning simulation to experiment, and accelerating ICF design through its unified approach to handling multi-modal scientific data.

Abstract: A combination of physics-based simulation and experiments has been critical to achieving ignition in inertial confinement fusion (ICF). Simulation and experiment both produce a mixture of scalar and images outputs, however only a subset of simulated data are available experimentally. We introduce a generative framework, called JointDiff, which enables predictions of conditional simulation input and output distributions from partial, multi-modal observations. The model leverages joint diffusion to unify forward surrogate modeling, inverse inference, and output imputation into one architecture. We train our model on a large ensemble of three-dimensional Multi-Rocket Piston simulations and demonstrate high accuracy, statistical robustness, and transferability to experiments performed at the National Ignition Facility (NIF). This work establishes JointDiff as a flexible generative surrogate for multi-modal scientific tasks, with implications for understanding diagnostic constraints, aligning simulation to experiment, and accelerating ICF design.

</details>


### [32] [Initial observations in X-point target divertor discharges on MAST-U](https://arxiv.org/abs/2601.21840)
*N. Lonigro,K. Verhaegh,J. Harrison,B. Lipschultz,C. Bowman,F. Federici,J. Flanagan,D. Greenhouse,D. Moulton,P. Ryan,R. Scannell,S. Silburn,T. Wijkamp,D. Brida,C. Theiler,the EUROfusion Tokamak Exploitation Team,the MAST Upgrade Team*

Main category: physics.plasm-ph

TL;DR: MAST-U experiments show double-null X-point-target (XPT) divertor configuration improves exhaust performance over Super-X divertor by combining large strike point radius with additional X-point, enhancing plasma-neutral interactions and reducing heat loads.


<details>
  <summary>Details</summary>
Motivation: To address challenging exhaust conditions in future fusion reactors by developing improved divertor configurations that can handle high power (>3 MW) H-mode operations with better momentum, power, and particle losses.

Method: Performed first high-power H-mode experiments on MAST-U using double-null X-point-target (XPT) divertor configuration, which combines large strike point radius (similar to Super-X divertor) with an additional X-point near separatrix in baffled outer divertor chambers.

Result: XPT configuration shows additional exhaust benefits over Super-X divertor: broader electron density profile near secondary X-point enhances plasma-neutral interactions, leading to broader hydrogenic emission, larger power/ion sinks, lower target electron temperatures, and reduced heat fluxes. Preliminary evidence suggests improved ELM buffering.

Conclusion: Combining multiple alternative divertor configuration strategies (XPT) can significantly improve exhaust performance, which may be essential for meeting the demanding exhaust requirements of future fusion reactors.

Abstract: The first high-power (> 3 MW) H-mode experiments using a double-null X-point-target (XPT) divertor configuration have been performed on MAST-U. The XPT geometry is obtained by combining a large strike point radius, similar to the Super-X divertor (SXD), with an additional X-point near the separatrix in the baffled outer divertor chambers and leads to additional exhaust benefits over the SXD. The broader electron density profile near the secondary X-point leads to additional plasma-neutral interactions, evidenced by a broader hydrogenic emission profile, and resulting in larger power and ion sinks. The increase in plasma-neutral interactions also leads to lower target electron temperatures and heat fluxes. These benefits appear to extend to transients, and preliminary evidence of improved ELM buffering in the XPT is presented. These results showcase how multiple alternative divertor configuration strategies can be combined to improve momentum, power, and particle losses, which may be required for the challenging exhaust conditions of future reactors.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [33] [Weighted Sobolev Spaces and Distributional Spectral Theory for Generalized Aging Operators via Transmutation Methods](https://arxiv.org/abs/2601.21497)
*Gustavo Dorrego*

Main category: math.FA

TL;DR: Develops a distributional theory for Weighted Weyl-Sonine operators using transmutation, constructing weighted Schwartz spaces and tempered distributions to analyze spectral properties in heterogeneous/aging media.


<details>
  <summary>Details</summary>
Motivation: Spectral analysis in heterogeneous and aging media requires functional frameworks beyond standard Hilbert spaces. Need rigorous distributional theory for non-local operators to handle complex media dynamics.

Method: Uses structure-preserving transmutation method to construct Weighted Schwartz Space S_{œà,œâ} and its dual S'_{œà,œâ} with Fr√©chet topology consistent with aging dynamics generator. Extends Weighted Fourier Transform, characterizes weighted Dirac delta, and introduces Weighted Sobolev Spaces H^{s}_{œà,œâ} via spectral multipliers.

Result: Derives sharp embedding theorem |u(t)| ‚â§ C œâ(t)^{-1} ||u||_{H^s_{œà,œâ}} connecting spectral energy to pointwise decay. Provides unified geometric characterization of fractional regimes (Hadamard, Riemann-Liouville) within single operator-theoretic architecture.

Conclusion: Establishes comprehensive functional framework for spectral analysis of non-local operators in heterogeneous/aging media, enabling rigorous treatment of weighted distributions and unifying various fractional calculus approaches.

Abstract: The spectral analysis of operators in heterogeneous and aging media typically requires a functional framework that extends beyond the standard Hilbertian setting. In this paper, we establish a rigorous distributional theory for a class of non-local operators, termed Weighted Weyl-Sonine operators, by employing a structure-preserving transmutation method. We construct the Weighted Schwartz Space $\mathcal{S}_{œà,œâ}$ and its topological dual, the space of Weighted Tempered Distributions $\mathcal{S}'_{œà,œâ}$, ensuring that the underlying Fr√©chet topology is consistent with the infinitesimal generator of the aging dynamics. This topological foundation allows us to: (i) extend the Weighted Fourier Transform to generalized functions as a unitary isomorphism; (ii) provide an explicit spectral characterization of the weighted Dirac delta $Œ¥_{œà,œâ}$ and its scaling laws under geometric dilations; and (iii) introduce a scale of Weighted Sobolev Spaces $H^{s}_{œà,œâ}$ defined via spectral multipliers. A central result is the derivation of a sharp embedding theorem, $|u(t)| \le C œâ(t)^{-1} \|u\|_{H^s_{œà,œâ}}$, which rigorously connects abstract spectral energy to the pointwise decay induced by the weight $œâ$. This framework provides a unified geometric characterization of several fractional regimes, including the Hadamard and Riemann-Liouville cases, within a single operator-theoretic architecture.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [34] [Solving the Offline and Online Min-Max Problem of Non-smooth Submodular-Concave Functions: A Zeroth-Order Approach](https://arxiv.org/abs/2601.21243)
*Amir Ali Farzin,Yuen-Man Pun,Philipp Braun,Tyler Summers,Iman Shames*

Main category: math.OC

TL;DR: Zeroth-order method for max-min problems with non-smooth submodular-concave objectives, using Lov√°sz extension and Gaussian smoothing, with convergence guarantees to Œµ-saddle points.


<details>
  <summary>Details</summary>
Motivation: Address max-min and min-max optimization problems with complex objective functions that are non-smooth, submodular with respect to the minimizer, and concave with respect to the maximizer, which arise in various applications but lack efficient solution methods.

Method: Zeroth-order method combining: 1) subgradient of Lov√°sz extension for the submodular minimizer part, 2) Gaussian smoothing to estimate smoothed function gradient for the concave maximizer part. The algorithm handles both offline and online settings.

Result: Proves convergence to Œµ-saddle point in expectation for offline case. For online setting, achieves O(‚àö(N¬∑PÃÑ_N)) online duality gap in expectation, where N is iterations and PÃÑ_N is path length of optimal decisions. Complexity analysis and hyperparameter selection provided.

Conclusion: The proposed zeroth-order method effectively solves challenging max-min problems with submodular-concave objectives, with theoretical convergence guarantees in both offline and online settings, validated by numerical experiments.

Abstract: We consider max-min and min-max problems with objective functions that are possibly non-smooth, submodular with respect to the minimiser and concave with respect to the maximiser. We investigate the performance of a zeroth-order method applied to this problem. The method is based on the subgradient of the Lov√°sz extension of the objective function with respect to the minimiser and based on Gaussian smoothing to estimate the smoothed function gradient with respect to the maximiser. In expectation sense, we prove the convergence of the algorithm to an $Œµ$-saddle point in the offline case. Moreover, we show that, in the expectation sense, in the online setting, the algorithm achieves $O(\sqrt{N\bar{P}_N})$ online duality gap, where $N$ is the number of iterations and $\bar{P}_N$ is the path length of the sequence of optimal decisions. The complexity analysis and hyperparameter selection are presented for all the cases. The theoretical results are illustrated via numerical examples.

</details>


### [35] [On Approximate Computation of Critical Points](https://arxiv.org/abs/2601.21917)
*Amir Ali Ahmadi,Georgina Hall*

Main category: math.OC

TL;DR: Proving that even coarse approximations of critical points for simple nonconvex functions is NP-hard, challenging the common belief that approximate critical point computation is tractable in nonconvex optimization.


<details>
  <summary>Details</summary>
Motivation: To challenge the commonly-held belief that approximate computation of critical points in nonconvex optimization is tractable, by showing that even very coarse approximations are computationally intractable for simple function classes.

Method: Proving computational hardness results through complexity theory reductions. Specifically, showing that if a polynomial-time algorithm exists that can find points with gradient norm at most 2^n for polynomials of constant degree (as low as three), then P=NP. Also proving hardness under additional structural assumptions like guaranteed existence/uniqueness of critical points, lower bounded functions, and distance-based approximations.

Result: Demonstrated that computing even extremely coarse approximations (gradient norm ‚â§ 2^n) of critical points for simple polynomial functions is NP-hard. The hardness persists even under favorable structural conditions including guaranteed critical point existence/uniqueness and lower bounded functions.

Conclusion: Approximate computation of critical points in nonconvex optimization is fundamentally intractable for even simple function classes, contradicting the widespread assumption that this task is computationally feasible.

Abstract: We show that computing even very coarse approximations of critical points is intractable for simple classes of nonconvex functions. More concretely, we prove that if there exists a polynomial-time algorithm that takes as input a polynomial in $n$ variables of constant degree (as low as three) and outputs a point whose gradient has Euclidean norm at most $2^n$ whenever the polynomial has a critical point, then P=NP. The algorithm is permitted to return an arbitrary point when no critical point exists. We also prove hardness results for approximate computation of critical points under additional structural assumptions, including settings in which existence and uniqueness of a critical point are guaranteed, the function is lower bounded, and approximation is measured in terms of distance to a critical point. Overall, our results stand in contrast to the commonly-held belief that, in nonconvex optimization, approximate computation of critical points is a tractable task.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [36] [Wave generation via oscillatory reconnection at a three-dimensional magnetic null point](https://arxiv.org/abs/2601.21520)
*Luiz A. C. A. Schiavo,Gert J. J. Botha,James A. McLaughlin*

Main category: astro-ph.SR

TL;DR: 3D MHD simulation shows oscillatory reconnection at magnetic null point generates both slow magnetoacoustic waves (propagating in all directions) and Alfv√©n waves (propagating perpendicular to spine motion) with constant period P.


<details>
  <summary>Details</summary>
Motivation: To investigate wave generation and time-dependent reconnection around a 3D magnetic null point, particularly understanding the types of waves produced and their propagation characteristics for coronal seismology applications.

Method: Three-dimensional nonlinear magnetohydrodynamic (MHD) simulation with non-periodic perturbation in xz-plane triggering oscillatory reconnection at 3D null point. Used three wave proxies (compressible parallel, compressible transverse, incompressible parallel) and Spectral Proper Orthogonal Decomposition to analyze MHD wave behavior.

Result: Oscillatory reconnection generates: 1) slow magnetoacoustic wave of period P propagating outwards in all directions along spine and fan plane; 2) propagating Alfv√©n wave of period P exclusively along y-axis in fan plane (perpendicular to spine motion).

Conclusion: The study provides new insights into wave generation from 3D null points, revealing distinct propagation patterns for different wave modes, with implications for coronal seismology and understanding magnetic reconnection processes.

Abstract: This work conducts a three-dimensional (3D), nonlinear magnetohydrodynamic (MHD) simulation to investigate wave generating, time-dependent reconnection around a magnetic null point. A non-periodic perturbation (in the $xz$-plane) triggers oscillatory reconnection (OR) at the 3D null, resulting in a self-sustained oscillation with a constant period $P$. We investigate the response of the system using three distinct wave proxies (compressible parallel, compressible transverse and incompressible parallel) as well as Spectral Proper Orthogonal Decomposition for decoupling and analyzing the resultant MHD wave behavior. We find that OR generates a slow magnetoacoustic wave of period $P$ that propagates outwards in all directions along the spine and fan plane of the 3D null point. We also find the generation of a propagating Alfv√©n wave of period $P$, exclusively along the $y$-axis in the fan plane, i.e. in the direction perpendicular to the spine motion. These findings provide new insights into waves generated from a 3D null point and their implications for coronal seismology.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [37] [Towards regularized learning from functional data with covariate shift](https://arxiv.org/abs/2601.21019)
*Markus Holzleitner,Sergiy Pereverzyev,Sergei V. Pereverzyev,Vaibhav Silmana,S. Sivananthan*

Main category: math.ST

TL;DR: The paper proposes a general regularization framework for unsupervised domain adaptation in vector-valued regression under covariate shift, using vector-valued RKHS and developing an aggregation-based approach for parameter selection.


<details>
  <summary>Details</summary>
Motivation: Covariate shift (different input distributions between training and test data) poses significant challenges for reliable learning in domain adaptation, especially for vector-valued regression problems with functional outputs.

Method: Develops a regularization framework using vector-valued reproducing kernel Hilbert spaces (vRKHS), restricts hypothesis space for practical operator learning, and proposes an aggregation-based approach that combines estimators from different regularization parameters and kernels.

Result: Establishes optimal convergence rates under general source conditions, provides theoretical justification for the aggregation approach, and demonstrates robustness and effectiveness on real-world face image datasets for mitigating distributional discrepancies.

Conclusion: The proposed framework provides a theoretically sound and practical solution for unsupervised domain adaptation in vector-valued regression under covariate shift, with effective parameter selection through aggregation and demonstrated real-world applicability.

Abstract: This paper investigates a general regularization framework for unsupervised domain adaptation in vector-valued regression under the covariate shift assumption, utilizing vector-valued reproducing kernel Hilbert spaces (vRKHS). Covariate shift occurs when the input distributions of the training and test data differ, introducing significant challenges for reliable learning. By restricting the hypothesis space, we develop a practical operator learning algorithm capable of handling functional outputs. We establish optimal convergence rates for the proposed framework under a general source condition, providing a theoretical foundation for regularized learning in this setting. We also propose an aggregation-based approach that forms a linear combination of estimators corresponding to different regularization parameters and different kernels. The proposed approach addresses the challenge of selecting appropriate tuning parameters, which is crucial for constructing a good estimator, and we provide a theoretical justification for its effectiveness. Furthermore, we illustrate the proposed method on a real-world face image dataset, demonstrating robustness and effectiveness in mitigating distributional discrepancies under covariate shift.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [38] [Rapid estimation of global sea surface temperatures from sparse streaming in situ observations](https://arxiv.org/abs/2601.21913)
*Cassidy All,Kevin Ho,Maya Magnuski,Christopher Nicolaides,Louisa B. Ebby,Mohammad Farazmand*

Main category: physics.ao-ph

TL;DR: S-DEIM combines empirical interpolation with RNNs to reconstruct high-resolution sea surface temperatures from sparse observations, achieving 40% better accuracy than previous methods with real-time computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate reconstruction of high-resolution sea surface temperatures from sparse measurements is crucial for weather forecasting and climate projections, but existing methods produce inaccurate results when measurements are sparse.

Method: S-DEIM uses a two-term approach: empirical interpolation from instantaneous in situ observations plus RNN-learned patterns from historical time series. Trained on NOAA's weekly high-resolution SST data (1989-2021) and tested on 2022-2023 data using only 0.2% of grid points.

Result: 40% more accurate than DEIM/Q-DEIM, 91% of estimates within ¬±1¬∞C of true SST, robust to sensor placement (only 1-2% degradation with random sensors), computationally efficient (1 min offline training, <1 sec reconstruction).

Conclusion: S-DEIM enables rapid, accurate SST reconstruction from sparse streaming data in real time, offering significant improvements over previous empirical interpolation methods for operational oceanography and climate applications.

Abstract: Reconstructing high-resolution sea surface temperatures (SST) from staggered SST measurements is essential for weather forecasting and climate projections. However, when SST measurements are sparse, the resulting inferred SST fields are rather inaccurate. Here, we demonstrate the ability of Sparse Discrete Empirical Interpolation Method (S-DEIM) to reconstruct the high-resolution SST field from sparse in situ observations, without using a model. The S-DEIM estimate consists of two terms, one computed from instantaneous in situ observations using empirical interpolation, and the other learned from the historical time series of observations using recurrent neural networks (RNNs). We train the RNNs using the National Oceanic and Atmospheric Administration's weekly high-resolution SST dataset spanning the years 1989-2021 which constitutes the training data. Subsequently, we examine the performance of S-DEIM on the test data, comprising January 2022 to January 2023. For this test data, S-DEIM infers the high-resolution SST from 100 in situ observations, constituting only 0.2% of the high-resolution spatial grid. We show that the resulting S-DEIM reconstructions are about 40% more accurate than earlier empirical interpolation methods, such as DEIM and Q-DEIM. Furthermore, 91% of S-DEIM estimates fall within $\pm 1^\circ$C of the true SST. We also demonstrate that S-DEIM is robust with respect to sensor placement: even when the sensors are distributed randomly, S-DEIM reconstruction error deteriorates only by 1-2%. S-DEIM is also computationally efficient. Training the RNN, which is performed only once offline, takes approximately one minute. Once trained, the S-DEIM reconstructions are computed in less than a second. As such, S-DEIM can be used for rapid SST reconstruction from sparse streaming observational data in real time.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [39] [Phase analysis of Ising machines and their implications on optimization](https://arxiv.org/abs/2507.08533)
*Shu Zhou,K. Y. Michael Wong,Juntao Wang,David Shui Wing Hui,Daniel Ebler,Jie Sun*

Main category: cond-mat.stat-mech

TL;DR: Analyzing phase diagrams reveals optimal Ising machine design: ground states emerge in binary spin distribution phase, with best solutions at binary-gapless phase coexistence. Adding digitization expands this region, creating superior Ising machines like digCIM.


<details>
  <summary>Details</summary>
Motivation: Ising machines offer computational advantages for combinatorial optimization but suffer from heuristic parameter tuning and lack systematic design principles, limiting solution quality.

Method: Analyze phase diagrams of spin distributions in the Sherrington-Kirkpatrick model to understand optimal operating conditions. Identify that ground states emerge in binary spin distribution phase, with optimal solutions at coexistence of binary and gapless phases.

Result: Found that ground states are achieved in binary spin distribution phase, and optimal solutions are produced where binary phase and gapless phase coexist. Showed that this coexistence region can be expanded by strategically placing a digitization operation.

Conclusion: Systematic analysis of phase diagrams enables optimal Ising machine design. Adding digitization expands the optimal operating region, leading to superior Ising machines like the proposed digCIM algorithm.

Abstract: Ising machines, which are dynamical systems designed to operate in a parallel and iterative manner, have emerged as a new paradigm for solving combinatorial optimization problems. Despite computational advantages, the quality of solutions depends heavily on the form of dynamics and tuning of parameters, which are in general set heuristically due to the lack of systematic insights. Here, we focus on optimal Ising machine design by analyzing phase diagrams of spin distributions in the Sherrington-Kirkpatrick model. We find that that the ground state can be achieved in the phase where the spin distribution becomes binary, and optimal solutions are produced where the binary phase and gapless phase coexist. Our analysis shows that such coexistence phase region can be expanded by carefully placing a digitization operation, giving rise to a family of superior Ising machines, as illustrated by the proposed algorithm digCIM.

</details>


### [40] [High-precision Dynamic Monte Carlo Study of Rigidity Percolation](https://arxiv.org/abs/2601.21399)
*Mingzhong Lu,Yufeng Song,Qiyuan Shi,Ming Li,Youjin Deng*

Main category: cond-mat.stat-mech

TL;DR: Dynamic rigidity percolation on triangular lattice reveals temporal self-similarity and large-scale cascade events, with improved critical exponents.


<details>
  <summary>Details</summary>
Motivation: While static rigidity percolation on triangular lattices has been studied, the dynamics of the rigidity transition remain less explored. The authors aim to investigate how rigid clusters emerge and evolve during bond addition.

Method: Developed a dynamic pebble game algorithm that tracks rigid cluster formation as bonds are sequentially added to an empty lattice. Used event-based ensemble approach for high-precision measurements.

Result: Discovered temporal self-similarity in cluster dynamics, identified large-scale cascade events where single bond additions trigger extensive cluster mergers scaling with system size. Obtained improved critical values: p_c = 0.6602778(10), 1/ŒΩ = 0.850(3), d_f = 1.850(2).

Conclusion: The dynamic approach reveals previously overlooked temporal scaling behaviors in rigidity percolation and provides substantially improved precision for critical exponents, advancing understanding of mechanical stability transitions in disordered materials.

Abstract: Rigidity percolation provides an important basis for understanding the onset of mechanical stability in disordered materials. While most studies on the triangular lattice have focused on static properties at fixed bond~(site) occupation probabilities, the dynamics of the rigidity transition remain less explored. In this work, we formulate a dynamic pebble game algorithm that monitors how rigid clusters emerge and evolve as bonds are added sequentially to an empty lattice, with computational efficiency comparable to the standard static pebble game. We uncover a previously overlooked temporal self-similarity exhibited in multiple quantities, including the cluster size changes and merged cluster sizes during bond addition, as well as the number of simultaneously merging clusters. We identify large-scale cascade events in which a single bond addition triggers the merger of an extensive number of clusters that scales with system size with inverse correlation-length exponent. Using an event-based ensemble approach, we obtain high-precision estimates of the critical point $p_c = 0.660\,277\,8(10)$, the inverse correlation-length exponent $1/ŒΩ= 0.850(3)$, and the fractal dimension $d_f = 1.850(2)$, representing substantial improvements over existing values.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [41] [Accelerated Inorganic Electrides Discovery by Generative Models and Hierarchical Screening](https://arxiv.org/abs/2601.21077)
*Shuo Tao,Qiang Zhu*

Main category: cond-mat.mtrl-sci

TL;DR: AI-driven discovery framework identifies 264 new electron-rich compounds including 13 stable electrides from screening over 8,000 binary/ternary compositions.


<details>
  <summary>Details</summary>
Motivation: Electrides have exceptional properties (low work function, high electron mobility, strong catalytic activity) but are difficult to discover due to challenges in achieving energetically favorable electron localization in crystal cavities.

Method: Combined physical principles with diffusion-based materials generation and hierarchical thermodynamic/electronic structure screening. Systematically explored 1,510 binary and 6,654 ternary compositions with excess valence electrons from electropositive metals, followed by high-throughput validation of thermodynamic stability and electronic structure analysis.

Result: Identified 264 new electron-rich compounds within 0.05 eV/atom above the convex hull at DFT level, including 13 thermodynamically stable electrides.

Conclusion: The approach demonstrates a generalizable strategy for targeted materials discovery in vast chemical spaces, enabling systematic identification of electrides and other electron-rich compounds.

Abstract: Electrides are exotic compounds in which excess electrons occupy interstitial regions of the crystal lattice and serve as anions, exhibiting exceptional properties such as low work function, high electron mobility, and strong catalytic activity. Although they show promise for diverse applications, identifying new electrides remains challenging due to the difficulty of achieving energetically favorable electron localization in crystal cavities. Here, we present an accelerated materials discovery framework that combines physical principles, diffusion-based materials generation with hierarchical thermodynamic and electronic structure screening. Using this workflow, we systematically explored 1,510 binary and 6,654 ternary chemical compositions containing excess valence electrons from electropositive alkaline, alkaline-earth, and early transition metals, and then filtered them with a high throughput validation on both thermodynamical stability and electronic structure analysis. As a result, we have identified 264 new electron rich compounds within 0.05 eV/atom above the convex hull at the density functional theory (DFT) level, including 13 thermodynamically stable electrides. Our approach demonstrates a generalizable strategy for targeted materials discovery in a vast chemical space.

</details>


### [42] [Dynamically training machine-learning-based force fields for strongly anharmonic materials](https://arxiv.org/abs/2601.21311)
*Martin Callsen,Tai-Ting Lee,Mei-Yin Chou*

Main category: cond-mat.mtrl-sci

TL;DR: Dynamic training framework for ML force fields using Bayesian error estimation to guide adaptive data acquisition during MD simulations, improving robustness and transferability across materials with varying anharmonicity.


<details>
  <summary>Details</summary>
Motivation: ML force fields offer computational efficiency but suffer from reliability issues when trained on static datasets that fail to generalize to unseen atomic configurations during MD simulations, limiting their practical application.

Method: Dynamic training framework building on conventional lattice dynamics expansion of total energy, incorporating Bayesian error estimation to guide adaptive data acquisition during simulation. Uses trajectory-averaged Bayesian errors for efficient configuration space exploration.

Result: Demonstrated effectiveness across materials with varying anharmonicity (c-BAs, Si, SnSe). Bayesian error estimation enables targeted exploration, enhances force field robustness/transferability, and determines training convergence without additional ab initio data.

Conclusion: Proposed framework provides practical, easily implementable scheme to improve ML force field training - the most critical step in developing reliable force fields for materials property computation at finite temperatures.

Abstract: Machine learning (ML) force fields have emerged as a powerful tool for computing materials properties at finite temperatures, particularly in regimes where traditional phonon-based perturbation theories fail or cannot be extended beyond the harmonic approximation. These approaches offer accuracy comparable to ab initio molecular dynamics (MD), but at a fraction of the computational cost. However, their reliability critically depends on the quality and representativeness of the training data. In particular, static training datasets often lead to failure when the force field encounters previously unseen atomic configurations during MD simulations. In this work, we present a framework for dynamically training ML force fields and demonstrate its effectiveness across materials with varying degrees of anharmonicity, including cubic boron arsenide (c-BAs), silicon (Si), and tin selenide (SnSe). Our method builds on the conventional lattice dynamics expansion of total energy and incorporates Bayesian error estimation to guide adaptive data acquisition during simulation. Specifically, we show that trajectory-averaged Bayesian errors enable efficient and targeted exploration of the configuration space, significantly enhancing the robustness and transferability of the resulting force fields. We further demonstrate how Bayesian error estimation can be applied to determine the convergence of the dynamic training without requiring additional ab initio data. This proposed framework offers a practical and easily implementable scheme to improve the training process, which is the most critical step in developing reliable ML force fields.

</details>


### [43] [Model density approach to Ewald summations](https://arxiv.org/abs/2601.21776)
*Chiara Ribaldone,Jacques Kontak Desmarais*

Main category: cond-mat.mtrl-sci

TL;DR: A method using model charge densities to cancel multipole moments for faster Ewald summation convergence in electrostatic potential calculations for condensed phase systems.


<details>
  <summary>Details</summary>
Motivation: The electrostatic potential evaluation is fundamental for studying condensed phase systems, but traditional Ewald summations can converge slowly. There's a need for more efficient techniques to accelerate convergence in bulk system calculations.

Method: Introduces a model charge density that cancels multipole moments of the crystalline charge distribution up to a desired order. This accelerates convergence of Ewald sums. The method works with arbitrary unit cells, classical or quantum contexts, and arbitrary basis functions for charge density representation.

Result: The approach provides faster convergence for electrostatic potential calculations in condensed phase systems and clarifies a decades-old implementation in the CRYSTAL code.

Conclusion: The model charge density method offers an efficient approach for accelerating Ewald summation convergence in electrostatic potential calculations, applicable to diverse systems and clarifying existing implementations.

Abstract: The evaluation of the electrostatic potential is fundamental to the study of condensed phase systems. We discuss the calculation of the relevant lattice summations by Ewald-type techniques. A model charge density is introduced, that cancels multipole moments of the crystalline charge distribution up to a desired order, for accelerating convergence of the Ewald sums. The method is applicable to calculations of bulk systems, employing arbitrary unit cells in a classical or quantum context, and with arbitrary basis functions to represent the charge density. The approach clarifies a decades-old implementation in the CRYSTAL code.

</details>


### [44] [MEIDNet: Multimodal generative AI framework for inverse materials design](https://arxiv.org/abs/2601.22009)
*Anand Babu,Rog√©rio Almeida Gouv√™a,Pierre Vandergheynst,Gian-Marco Rignanese*

Main category: cond-mat.mtrl-sci

TL;DR: MEIDNet is a multimodal equivariant inverse design network that combines contrastive learning with EGNNs to accelerate materials discovery by aligning structural and property information across modalities.


<details>
  <summary>Details</summary>
Motivation: To accelerate exploration of chemical-structural space and facilitate discovery of materials meeting predefined property targets by combining generative inverse design with multimodal learning.

Method: Uses Multimodal Equivariant Inverse Design Network (MEIDNet) with contrastive learning and equivariant graph neural networks (EGNN) to encode structures, implements curriculum learning strategies, and fuses three modalities through cross-modal learning.

Result: Achieves strong latent-space alignment (cosine similarity 0.96), ~60x higher learning efficiency than conventional training, and generates low-bandgap perovskite structures at 13.6% SUN (stable, unique, novel) rate validated by ab initio methods.

Conclusion: MEIDNet demonstrates scalability and adaptability for universal learning of chemical space across diverse modalities, paving the way for accelerated materials discovery through multimodal inverse design.

Abstract: In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [45] [Description of electromagnetic fields in inhomogeneous accelerating sections. IV couplers](https://arxiv.org/abs/2601.21809)
*M. I. Ayzatsky*

Main category: physics.acc-ph

TL;DR: A new approach to incorporate coupling elements into generalized coupled mode theory using loop couplers for RF power coupling in accelerating sections, reducing complex integro-differential equations to simpler ordinary differential equation systems.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient analytical model for incorporating coupling elements (loops and transmission lines) into generalized coupled mode theory for RF power coupling in accelerating structures, simplifying the complex mathematical treatment of such systems.

Method: Used a simple model of coupling structured waveguide with external RF power source/load through loops and transmission lines. Reduced the resulting Barbashin-type integro-differential system with degenerate kernels to three independent systems of ordinary differential equations: two for field distribution excited by loops at input/output, and one for field excited by electron beam without loops.

Result: Developed computer code for matching loop couplers for uniform X-band accelerating sections. Applied results to simulate non-uniform section, achieving input reflection coefficient of 8E-3 without additional matching.

Conclusion: The approach successfully simplifies complex coupling element analysis in generalized coupled mode theory, enabling practical design of loop couplers for accelerating sections with good matching performance.

Abstract: A new approach to incorporating coupling elements into a generalized coupled mode theory is presented. The simplest model of coupling of a structured waveguide with an external RF power source and load through loops and transmission lines was used. Even such a simple model significantly complicated the system of coupled equations. It turned into a coupled integro-differential system of the Barbashin type with degenerate kernels. Since the integral kernels are degenerate, this system is reduced to three independent systems of differential equations. Instead of solving a system of coupled integro-differential equations, we need to find solutions to three systems of ordinary differential equations. Two systems describe the distribution of the field excited by one loop and the specified value of the excitation current in it. In the first system the loop is located at the section's input, and in the second, at the section's output. The third system does not depend on the loop parameters at all. It describes the distribution of the field excited by an electron beam in a section without loops. Based on the developed analytical model, the computer code was developed for matching the loop couplers for the uniform accelerating sections of X-band. The calculation results were used to simulate the non-uniform section. Without additional matching, we obtained an input reflection coefficient of 8E-3.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [46] [Multiple binding modes of AKT on PIP$_3$-containing membranes](https://arxiv.org/abs/2601.21216)
*Yuki Nakagaki,Eiji Yamamoto*

Main category: physics.bio-ph

TL;DR: Molecular dynamics simulations reveal AKT adopts multiple membrane-binding modes with PH and kinase domains cooperating via PIP3 interactions, with mode populations dependent on PIP3 concentration.


<details>
  <summary>Details</summary>
Motivation: The PI3K/AKT signaling pathway is crucial for cellular processes, but how AKT's PH and kinase domains cooperate for membrane activation remains unclear at molecular level.

Method: Used molecular dynamics simulations of full-length AKT on PIP3-containing lipid bilayers to study membrane interactions and domain orientations.

Result: Identified four distinct membrane-binding modes with different PH/kinase domain orientations. Found PIP3 interactions with both PH domain (canonical and secondary sites) and kinase domain basic residues. Most stable mode exposes activation-loop phosphorylation site. Binding mode populations depend on PIP3 concentration.

Conclusion: PH and kinase domains cooperatively shape AKT's membrane-bound conformations through specific PIP3 interactions, with PIP3 concentration influencing preferred orientations and potentially regulating activation.

Abstract: The PI3K/AKT signaling pathway is triggered by recruitment of AKT to cellular membranes. Although AKT is a multidomain serine/threonine kinase composed of an N-terminal pleckstrin homology (PH) domain and a C-terminal kinase domain, how these domains cooperate to regulate AKT activation on membranes remains unclear at the molecular level. Here, using molecular dynamics simulations of full-length AKT on PIP$_3$-containing lipid bilayers, we identify four distinct membrane-binding modes that differ in the orientations and membrane contacts of the PH and kinase domains. In addition to PIP$_3$ binding to the PH domain, we observe specific PIP$_3$ interactions with basic residues in the kinase domain. In the most stable mode, PIP$_3$ interacts with both the canonical and a secondary binding site in the PH domain, while the kinase domain adopts an orientation in which the activation-loop phosphorylation site is exposed to the solvent. Interestingly, the populations of these binding modes depend on the PIP$_3$ concentration in the membrane, leading to changes in the preferred orientation of AKT. These findings shed light on how lipid recognition by the PH domain and the kinase domain of AKT cooperatively shape its membrane-bound conformations.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [47] [Navier-Stokes with a fractional transport noise as a limit of multi-scale dynamics](https://arxiv.org/abs/2601.21762)
*Xue-Mei Li,Szymon Sobczak*

Main category: math.PR

TL;DR: The paper defines a bona fide rough path solution for Navier-Stokes equations with rough transport, shows existence for fractional Brownian motion on 3D torus via slow/fast system limits, and proves equivalence to incremental rough driver solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous rough path solution framework for nonlinear SPDEs like Navier-Stokes with rough transport terms, addressing the need for proper mathematical foundations for such equations driven by rough signals like fractional Brownian motion.

Method: Defines bona fide rough path solutions for Navier-Stokes with rough transport, constructs solutions via effective limits of slow/fast systems, and establishes equivalence between this formulation and the incremental rough driver approach.

Result: Proves existence of rough path solutions for Navier-Stokes on 3D torus driven by fractional Brownian motion, shows these solutions arise as limits of slow/fast systems, and demonstrates equivalence to incremental rough driver solutions.

Conclusion: The paper establishes a rigorous rough path solution framework for nonlinear SPDEs with rough transport, providing both existence results and showing broader applicability through equivalence with established incremental solution concepts.

Abstract: We define a bona fide rough path solution for the Navier-Stokes equation with an additional rough transport term, and show that the SPDE on the three-dimensional torus driven by a fractional Brownian motion on $H^œÉ$ has solutions characterised as the effective limits of a slow/fast system. We further show that this rough path solution is equivalent to the widely used incremental notion of solution (the unbounded rough driver formulation), demonstrating broader applicability to other nonlinear SPDEs.

</details>


### [48] [Ergodicity for SPDEs driven by divergence-free transport noise](https://arxiv.org/abs/2601.22056)
*Benjamin Gess,Rishabh S. Gvalani,Adrian Martini*

Main category: math.PR

TL;DR: Strong transport noise can enforce uniqueness of invariant measures in McKean-Vlasov equations, overcoming deterministic multiplicity.


<details>
  <summary>Details</summary>
Motivation: To understand how noise affects ergodic behavior in McKean-Vlasov equations, particularly whether strong transport noise can overcome deterministic non-uniqueness of steady states.

Method: Study McKean-Vlasov equations with common, divergence-free transport noise in dimensions d‚â•2, analyzing ergodic behavior and invariant probability measures.

Result: In dimensions d‚â•2, sufficiently strong mixing transport noise can enforce uniqueness of invariant probability measures, even when the deterministic part has multiple steady states.

Conclusion: Transport noise acts as a regularizing mechanism that can restore uniqueness in ergodic systems where deterministic dynamics alone would exhibit multiple equilibria.

Abstract: We study the ergodic behaviour of the McKean-Vlasov equations driven by common, divergence-free transport noise. In particular, we show that in dimension $d\geq 2$, if the noise is mixing and sufficiently strong it can enforce the uniqueness of invariant probability measures, even if the deterministic part of equation has multiple steady states.

</details>


### [49] [Superdiffusion and anomalous regularization in self-similar random incompressible flows](https://arxiv.org/abs/2601.22142)
*Scott Armstrong,Ahmed Bou-Rabee,Tuomo Kuusi*

Main category: math.PR

TL;DR: The paper proves quenched power-law superdiffusion for particles in random incompressible flows with multiscale self-similar structure, showing displacement variance grows like t^{2/(2-Œ≥)} for small Hurst exponent Œ≥.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of particles in random incompressible flows with multiscale self-similar structure, particularly establishing rigorous mathematical results for superdiffusive scaling predicted by renormalization group heuristics.

Method: Implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator ‚àá¬∑(ŒΩI_d + k)‚àá, using self-similar induction across scales. The approach shows coarse-grained generators are approximated by constant-coefficient Laplacians with effective diffusivity growing like r^Œ≥.

Result: Proves quenched power-law superdiffusion: for typical realizations, displacement variance grows like t^{2/(2-Œ≥)} in perturbative regime Œ≥‚â™1. Identifies leading prefactor up to random relative error of order Œ≥^{¬Ω}|log Œ≥|¬≥. Also proves anomalous regularization: solutions are H√∂lder continuous with exponent 1-CŒ≥^{¬Ω}.

Conclusion: The paper provides rigorous mathematical foundation for superdiffusive scaling in multiscale random flows, confirming renormalization group predictions and establishing precise error bounds. The scale-local approximation reflects the multifractal nature of the environment.

Abstract: We study the long-time behavior of a particle in $\mathbb{R}^d$, $d \geq 2$, subject to molecular diffusion and advection by a random incompressible flow. The velocity field is the divergence of a stationary random stream matrix $\mathbf{k} $ with positive Hurst exponent $Œ≥> 0$, so the resulting random environment is multiscale and self-similar. In the perturbative regime $Œ≥\ll 1$, we prove quenched power-law superdiffusion: for a typical realization of the environment, the displacement variance at time $t$ grows like $t^{2/(2-Œ≥)}$, the scaling predicted by renormalization group heuristics. We also identify the leading prefactor up to a random (quenched) relative error of order $Œ≥^{\frac12}\left| \log Œ≥\right|^3$. The proof implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator $\nabla \cdot (ŒΩI_d + \mathbf{k} ) \nabla$, based on a self-similar induction across scales. We demonstrate that the coarse-grained generator is well-approximated, at each scale $r$, by a constant-coefficient Laplacian with effective diffusivity growing like $r^Œ≥$. This approximation is inherently scale-local: reflecting the multifractal nature of the environment, the relative error does not decay with the scale, but remains of order $Œ≥^{\frac12}\left| \log Œ≥\right|^2$. We also prove anomalous regularization under the quenched law: for almost every realization of the drift, solutions of the associated elliptic equation are H√∂lder continuous with exponent $1 - CŒ≥^{\frac12}$ and satisfy estimates which are uniform in the molecular diffusivity $ŒΩ$ and the scale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [Conditional Denoising Model as a Physical Surrogate Model](https://arxiv.org/abs/2601.21021)
*Jos√© Afonso,Pedro Viegas,Rodrigo Ventura,Vasco Guerra*

Main category: cs.LG

TL;DR: CDM is a generative model that learns physical manifold geometry through denoising, achieving better physical consistency than physics-constrained baselines without explicit physics loss.


<details>
  <summary>Details</summary>
Motivation: Current surrogate modeling faces trade-off between accuracy and physical consistency. Physics-consistent approaches either use soft constraints that don't guarantee strict adherence or post-processing corrections that don't learn underlying solution geometry.

Method: Introduces Conditional Denoising Model (CDM) that learns physical manifold geometry by training network to restore clean states from noisy ones, learning a vector field pointing toward valid solution subspace. Uses time-independent formulation with deterministic fixed-point iteration for inference.

Result: CDM achieves higher parameter and data efficiency than physics-consistent baselines on low-temperature plasma physics benchmark. Denoising objective acts as powerful implicit regularizer - model adheres to physical constraints more strictly than baselines trained with explicit physics losses, despite never seeing governing equations during training.

Conclusion: CDM provides a novel approach to surrogate modeling that learns physical manifold geometry through denoising, achieving superior physical consistency without explicit physics constraints, offering better parameter/data efficiency and strict adherence to physical laws.

Abstract: Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.

</details>


### [51] [LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models](https://arxiv.org/abs/2601.21623)
*Stanislav Budzinskiy,Marian Gloser,Tolunay Yilmaz,Ying Hong Tham,Yuanyi Lin,Wenyi Fang,Fan Wu,Philipp Petersen*

Main category: cs.LG

TL;DR: Adaptive mixed-precision strategy for transformer inference that selectively recomputes critical components with higher precision while using lower precision for most computations, achieving up to 100x accuracy improvements with minimal recomputation.


<details>
  <summary>Details</summary>
Motivation: Mixed-precision computations are essential for efficient AI deployment, especially for large language models. The paper addresses the challenge of maintaining accuracy while using lower precision for transformer inference by developing an adaptive strategy that identifies and selectively recomputes only the most critical components.

Method: Based on rounding error analysis of function compositions f(g(x)), the method provides an adaptive strategy that selects a small subset of g(x) components to compute with higher accuracy while all other computations use lower precision. The strategy is applied to different compositions within transformer architectures.

Result: Numerical studies on GPT-2 models demonstrate that very low recomputation rates (minimal additional computation) can achieve improvements of up to two orders of magnitude (100x) in accuracy compared to uniform low-precision computation.

Conclusion: The adaptive mixed-precision strategy enables efficient transformer inference with significantly improved accuracy by intelligently identifying and selectively recomputing only the most error-sensitive components, making it practical for local deployment of large language models.

Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.

</details>


### [52] [PILD: Physics-Informed Learning via Diffusion](https://arxiv.org/abs/2601.21284)
*Tianyi Zeng,Tianyi Wang,Jiaru Zhang,Zimo Zeng,Feiyang Zhang,Yiming Xu,Sikai Chen,Yajie Zou,Yangyang Wang,Junfeng Jiao,Christian Claudel,Xinbo Chen*

Main category: cs.LG

TL;DR: PILD integrates physics constraints into diffusion models using virtual residual observations and conditional embedding to improve accuracy and generalization in scientific/engineering tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are powerful generative tools but lack physical constraints needed for practical engineering and scientific applications where physical laws must be followed.

Method: Proposes Physics-Informed Learning via Diffusion (PILD) with two key components: 1) virtual residual observation from Laplace distribution to supervise generation during training, and 2) conditional embedding module to inject physical information into denoising network at multiple layers.

Result: Extensive experiments on engineering/scientific tasks (vehicle trajectories, tire forces, Darcy flow, plasma dynamics) show PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.

Conclusion: PILD provides a concise, modular framework that unifies diffusion modeling with physical constraints, broadly applicable to problems governed by ODEs, PDEs, algebraic equations, and inequality constraints.

Abstract: Diffusion models have emerged as powerful generative tools for modeling complex data distributions, yet their purely data-driven nature limits applicability in practical engineering and scientific problems where physical laws need to be followed. This paper proposes Physics-Informed Learning via Diffusion (PILD), a framework that unifies diffusion modeling and first-principles physical constraints by introducing a virtual residual observation sampled from a Laplace distribution to supervise generation during training. To further integrate physical laws, a conditional embedding module is incorporated to inject physical information into the denoising network at multiple layers, ensuring consistent guidance throughout the diffusion process. The proposed PILD framework is concise, modular, and broadly applicable to problems governed by ordinary differential equations, partial differential equations, as well as algebraic equations or inequality constraints. Extensive experiments across engineering and scientific tasks including estimating vehicle trajectories, tire forces, Darcy flow and plasma dynamics, demonstrate that our PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.

</details>


### [53] [PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training](https://arxiv.org/abs/2601.22137)
*Shenghao Yang,Zhichao Wang,Oleg Balabanov,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: PRISM framework accelerates matrix function computations (like square roots and orthogonalization) for neural network training by combining adaptive polynomial approximation with randomized sketching, requiring no explicit spectral bounds.


<details>
  <summary>Details</summary>
Motivation: Matrix functions are crucial for preconditioned gradient methods in neural network training, but existing iterative algorithms need acceleration to be more efficient on modern GPU accelerators.

Method: PRISM combines adaptive polynomial approximation with randomized sketching - at each iteration, it fits a polynomial surrogate to the current spectrum via sketched least-squares problems, adapting automatically to the evolving spectrum without needing explicit spectral bounds.

Result: PRISM accelerates Newton-Schulz-like iterations for matrix square roots and orthogonalization, and empirically accelerates training when integrated into Shampoo and Muon optimizers.

Conclusion: PRISM provides an effective general framework for accelerating matrix function computations in machine learning that adapts automatically to instance-specific spectral properties without manual parameter tuning.

Abstract: Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [54] [Jellyfish exist](https://arxiv.org/abs/2601.21227)
*Ben Andrews,Glen Wheeler*

Main category: math.DG

TL;DR: The paper proves existence of infinitely many geometrically distinct solutions for three different geometric flows: homothetic expanders for elastic flow, epicyclic shrinkers for curve diffusion flow, and epicyclic expanders for ideal flow.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of multiple distinct geometric solutions for various geometric flows, expanding the known solution space for these important PDEs in geometric analysis.

Method: The paper likely uses geometric analysis techniques, possibly variational methods or PDE analysis, to prove existence of infinitely many geometrically distinct solutions for each flow type.

Result: Proves existence of infinitely many geometrically distinct homothetic expanders (jellyfish) for elastic flow, epicyclic shrinkers for curve diffusion flow, and epicyclic expanders for ideal flow.

Conclusion: These results demonstrate rich solution structures for geometric flows, with infinitely many distinct geometric configurations existing for each flow type studied.

Abstract: We show the existence of infinitely many geometrically distinct homothetic expanders (jellyfish) for the elastic flow, epicyclic shrinkers for the curve diffusion flow, and epicyclic expanders for the ideal flow.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [55] [Scattering laws for interfaces in self-gravitating matter flows](https://arxiv.org/abs/2601.21773)
*Bruno Le Floch,Philippe G. LeFloch*

Main category: gr-qc

TL;DR: The paper develops a framework connecting phase transition dynamics with bouncing cosmology, using scattering maps on singular and discontinuity hypersurfaces to evolve self-gravitating matter fields across interfaces while maintaining uniqueness and constraint propagation.


<details>
  <summary>Details</summary>
Motivation: To understand how self-gravitating matter fields evolve through phase transitions and connect phase transition dynamics with bouncing cosmology concepts, particularly addressing how to evolve across gravitational singularities and fluid discontinuities while maintaining physical consistency.

Method: Introduces scattering maps on two classes of hypersurfaces: gravitational singularity hypersurfaces and fluid-discontinuity hypersurfaces. Analyzes causal structures from light cones and acoustic cones to formulate local evolution problems for the Einstein-Euler system with interfaces. Uses scattering relations to supplement field equations for uniqueness.

Result: Develops a framework yielding rigid universal relations plus model-dependent parameters. Under physical requirements (general covariance, causality, constraint compatibility, ultra-locality), aims to classify admissible scattering relations that characterize fluid dynamics coupled to Einstein gravity at macroscopic level.

Conclusion: The approach provides a complete macroscopic description of evolution across interfaces in self-gravitating matter fields, building on previous work with quiescent singularities and extending to fluid systems with phase transitions, with scattering maps encoding the passage across singular/discontinuity hypersurfaces.

Abstract: We consider the evolution of self-gravitating matter fields that may undergo phase transitions, and we connect ideas from phase transition dynamics with concepts from bouncing cosmology. Our framework introduces scattering maps prescribed on two classes of hypersurfaces: a gravitational singularity hypersurface and a fluid-discontinuity hypersurface. By analyzing the causal structures induced by the light cone and the acoustic cone, we formulate a local evolution problem for the Einstein-Euler system in the presence of such interfaces. We explain how suitable scattering relations must supplement the field equations in order to ensure uniqueness and thus yield a complete macroscopic description of the evolution. This viewpoint builds on a theory developed in collaboration with G. Veneziano for quiescent (velocity-dominated) singularities in solutions of the Einstein equations coupled to a scalar field, where the passage across the singular hypersurface is encoded by a singularity scattering map. The guiding question is to identify junction prescriptions that are compatible with the Einstein and Euler equations, in particular with the propagation of constraints. The outcome is a rigid set of universal relations, together with a family of model-dependent parameters. Under physically motivated requirements (general covariance, causality, constraint compatibility, and ultra-locality), we aim to classify admissible scattering relations arising from microscopic physics and characterizing, at the macroscopic level, the dynamics of a fluid coupled to Einstein gravity.

</details>
