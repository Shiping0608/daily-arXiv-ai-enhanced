<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 14]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [math.DG](#math.DG) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [gr-qc](#gr-qc) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Solution of Advection Equation with Discontinuous Initial and Boundary Conditions via Physics-Informed Neural Networks](https://arxiv.org/abs/2601.20978)
*Omid Khosravi,Mehdi Tatari*

Main category: math.NA

TL;DR: PINNs with Fourier features, two-stage training, adaptive weighting, median filtering, bounded mapping, and upwind-inspired loss for 1D advection with discontinuities.


<details>
  <summary>Details</summary>
Motivation: Address challenges in modeling 1D advection equations with discontinuous initial/boundary conditions using PINNs, particularly overcoming spectral bias and excessive smoothing of discontinuities in neural network approximations.

Method: Fourier feature mapping for input representation, two-stage sequential optimization of Fourier parameters and network weights, adaptive loss weighting, spatial median filtering, bounded linear mapping for predictions, and modified upwind-inspired loss for nonlinear problems.

Result: The techniques mitigate spectral bias and reduce excessive smoothing of discontinuous solutions, improving approximation accuracy for 1D advection problems with discontinuities.

Conclusion: Combined techniques enhance PINN performance for discontinuous advection problems, with Fourier features and upwind-inspired loss being particularly effective for handling discontinuities and spectral bias.

Abstract: In this paper, we investigate several techniques for modeling the one-dimensional advection equation for a specific class of problems with discontinuous initial and boundary conditions using physics-informed neural networks (PINNs). To mitigate the spectral bias phenomenon, we employ a Fourier feature mapping layer as the input representation, adopt a two-stage training strategy in which the Fourier feature parameters and the neural network weights are optimized sequentially, and incorporate adaptive loss weighting. To further enhance the approximation accuracy, a median filter is applied to the spatial data, and the predicted solution is constrained through a bounded linear mapping. Moreover, for certain nonlinear problems, we introduce a modified loss function inspired by the upwind numerical scheme to alleviate the excessive smoothing of discontinuous solutions typically observed in neural network approximations.

</details>


### [2] [Identification of space-dependent coefficients in two competing terms of a nonlinear subdiffusion equation](https://arxiv.org/abs/2601.21018)
*Barbara Kaltenbacher,William Rundell*

Main category: math.NA

TL;DR: Reconstruction of spatially varying coefficients in nonlinear diffusion equations from interior observations using fixed point scheme.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inverse problem of reconstructing spatially varying coefficients (p and q) in nonlinear diffusion equations from interior measurements. This is important for applications where physical parameters vary spatially and need to be identified from limited observations.

Method: Devises a fixed point scheme for reconstructing coefficients from interior observations in two scenarios: 1) final time observations under two different excitations, and 2) observations at two different time instances under a single excitation.

Result: Proves convergence of the reconstruction scheme and local uniqueness of the coefficients. Numerical experiments demonstrate the performance of the reconstruction method.

Conclusion: The proposed fixed point scheme successfully reconstructs spatially varying coefficients in nonlinear diffusion equations from limited interior observations, with proven convergence and uniqueness properties.

Abstract: We consider a (sub)diffusion equation with a nonlinearity of the form $pf(u)-qu$, where $p$ and $q$ are space dependent functions. Prominent examples are the Fisher-KPP, the Frank-Kamenetskii-Zeldovich and the Allen-Cahn equations. We devise a fixed point scheme for reconstructing the spatially varying coefficients from interior observations a) at final time under two different excitations b) at two different time instances under a single excitation. Convergence of the scheme as well as local uniqueness of these coefficients is proven. Numerical experiments illustrate the performance of the reconstruction scheme.

</details>


### [3] [Parametric Hyperbolic Conservation Laws: A Unified Framework for Conservation, Entropy Stability, and Hyperbolicity](https://arxiv.org/abs/2601.21080)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: SymCLaw is a parametric hyperbolic conservation law that learns hyperbolic systems from data while guaranteeing conservation, entropy stability, and hyperbolicity by design through parameterized flux functions with real eigenvalues and complete eigenvectors.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for learning hyperbolic systems from data typically enforce only conservation or rely on prior knowledge of governing equations, lacking guarantees for hyperbolicity and entropy stability which are essential for physically meaningful solutions.

Method: Parameterizes flux functions to guarantee real eigenvalues and complete eigenvectors of flux Jacobian (preserving hyperbolicity), jointly learns convex entropy function and associated flux potential for entropy dissipation, and provides entropy-stable numerical flux scheme compatible with standard discretizations.

Result: Numerical experiments on Burgers, shallow water, Euler, and KPP equations show SymCLaw generalizes to unseen initial conditions, maintains stability under noisy training data, and achieves accurate long-time predictions.

Conclusion: SymCLaw provides a principled foundation for data-driven modeling of hyperbolic conservation laws by ensuring conservation, entropy stability, and hyperbolicity by design, making it suitable for integration into classical solvers.

Abstract: We propose a parametric hyperbolic conservation law (SymCLaw) for learning hyperbolic systems directly from data while ensuring conservation, entropy stability, and hyperbolicity by design. Unlike existing approaches that typically enforce only conservation or rely on prior knowledge of the governing equations, our method parameterizes the flux functions in a form that guarantees real eigenvalues and complete eigenvectors of the flux Jacobian, thereby preserving hyperbolicity. At the same time, we embed entropy-stable design principles by jointly learning a convex entropy function and its associated flux potential, ensuring entropy dissipation and the selection of physically admissible weak solutions. A corresponding entropy-stable numerical flux scheme provides compatibility with standard discretizations, allowing seamless integration into classical solvers. Numerical experiments on benchmark problems, including Burgers, shallow water, Euler, and KPP equations, demonstrate that SymCLaw generalizes to unseen initial conditions, maintains stability under noisy training data, and achieves accurate long-time predictions, highlighting its potential as a principled foundation for data-driven modeling of hyperbolic conservation laws.

</details>


### [4] [An efficient implicit scheme for the multimaterial Euler equations in Lagrangian coordinates](https://arxiv.org/abs/2601.21241)
*Simone Chiocchetti,Giovanni Russo*

Main category: math.NA

TL;DR: Implicit Lagrangian method for stratified fluid flows with high density/stiffness ratios to overcome time step restrictions while avoiding interface smearing.


<details>
  <summary>Details</summary>
Motivation: Stratified fluids (fluid metamaterials) with alternating layers present computational challenges: explicit Lagrangian schemes have severe time step restrictions, while Eulerian methods suffer from artificial interface smearing. High density/stiffness ratio flows (water-air, air-granular media) are particularly problematic.

Method: Implicit numerical method for multimaterial Euler equations in Lagrangian coordinates. Uses the structure of Lagrangian equations to formulate a single implicit discrete wave equation for pressure field, yielding symmetric positive definite system for efficiency. Includes filtering strategies to counteract pressure/density oscillations.

Result: Method provides robust, accurate, and efficient solution for stratified media with high density/stiffness ratios. Overcomes prohibitive time step restrictions while maintaining sharp interfaces without artificial smearing.

Conclusion: The implicit Lagrangian approach enables efficient simulation of stratified fluid metamaterials with extreme material property contrasts, addressing key computational challenges in multimaterial flow modeling.

Abstract: Stratified fluids composed of a sequence of alternate layers show interesting macroscopic properties, which may be quite different from those of the individual constituent fluids. On a macroscopic scale, such systems can be considered a sort of fluid metamaterial. In many cases each fluid layer can be described by Euler equations following the stiffened gas equation of state. The computation of detailed numerical solutions of such stratified material poses several challenges, first and foremost the issue of artificial smearing of material parameters across interface boundaries. Lagrangian schemes completely eliminate this issue, but at the cost of rather stringent time step restrictions. In this work we introduce an implicit numerical method for the multimaterial Euler equations in Lagrangian coordinates. The implicit discretization is aimed at bypassing the prohibitive time step restrictions present in flows with stratified media, where one of the materials is particularly dense, or rigid (or both). This is the case for flows of water-air mixtures, air-granular media, or similar high density ratio systems. We will present the novel discretisation approach, which makes extensive use of the remarkable structure of the governing equations in Lagrangian coordinates to find the solution by means of a single implicit discrete wave equation for the pressure field, yielding a symmetric positive definite structure and thus a particularly efficient algorithm. Additionally, we will introduce simple filtering strategies for counteracting the emergence of pressure or density oscillations typically encountered in multimaterial flows, and will present results concerning the robustness, accuracy, and performance of the proposed method, including applications to stratified media with high density and stiffness ratios.

</details>


### [5] [Natural superconvergence points for splines](https://arxiv.org/abs/2601.21368)
*Peng Yang,Zhimin Zhang*

Main category: math.NA

TL;DR: The paper develops a unified theory showing that local symmetric centers in partitions are natural superconvergence points for spline approximations to elliptic problems, with superconvergence occurring when polynomial degree and derivative order share parity.


<details>
  <summary>Details</summary>
Motivation: To establish a systematic understanding of superconvergence phenomena in spline approximations for elliptic problems, identifying natural superconvergence points and their distribution patterns.

Method: Develops theoretical framework starting from 1D case, showing local symmetric centers yield superconvergence when k and s share parity. Extends to higher dimensions on simplicial and tensor-product meshes, generalizing to mixed derivatives. Uses asymptotic expansion analysis for B-spline solutions.

Result: Establishes that superconvergence occurs at local symmetric centers when polynomial degree and derivative order share parity. Shows superconvergence persists in localized symmetric regions, revealing systematic distribution patterns and attainability of superconvergence points.

Conclusion: Superconvergence points are naturally occurring at local symmetric centers, follow systematic patterns, and are readily attainable. The unified theory provides comprehensive characterization of superconvergence phenomena across dimensions and mesh types.

Abstract: This paper develops a unified theory of natural superconvergence points for polynomial spline approximations to second-order elliptic problems. Beginning with the one-dimensional case, we establish that when a point $x_0$ is a local symmetric center of the partition, the numerical error $(u-u_h)^{(s)}(x_0)$ exhibits superconvergence whenever the polynomial degree $k$ and the derivative order $s$ share the same parity. In particular, for the smoothest spline (B-spline) solution, the abundance of superconvergence points allows us to construct asymptotic expansion of the error within the element that fully characterize all superconvergence points, for both function values and derivatives. The theoretical framework is then extended to higher-dimensional settings on simplicial and tensor-product meshes, and the essential conclusions are preserved, with one-dimensional derivatives generalized to mixed derivatives. Numerical experiments demonstrate that superconvergence persists even in extremely localized symmetric regions, revealing that superconvergence points are both readily attainable and follow systematic distribution patterns.

</details>


### [6] [Higher-Order Finite Difference Methods for the Tempered Fractional Laplacian](https://arxiv.org/abs/2601.21388)
*Mingyi Wang,Dongling Wang*

Main category: math.NA

TL;DR: High-order finite difference schemes for tempered fractional Laplacian using new generating functions achieve 4th, 6th, 8th-order convergence with efficient Toeplitz matrix computations.


<details>
  <summary>Details</summary>
Motivation: To develop efficient high-order numerical methods for tempered fractional Laplacian equations, which are important in modeling anomalous diffusion with exponential tempering but lack efficient high-order discretization schemes.

Method: Proposes a general framework of high-order finite difference schemes based on new generating functions derived from discrete symbols. The method constructs discrete operators that yield Toeplitz stiffness matrices, enabling efficient matrix-vector multiplications via fast algorithms.

Result: The discretizations achieve high-order convergence (p=4, 6, 8) for sufficiently smooth functions. Numerical simulations confirm the methods' effectiveness and show excellent agreement with theoretical predictions.

Conclusion: The proposed high-order finite difference framework provides efficient and accurate numerical methods for tempered fractional Laplacian equations, with rigorous stability and convergence analysis validated by numerical experiments.

Abstract: This paper presents a general framework of high-order finite difference (HFD) schemes for the tempered fractional Laplacian (TFL) based on new generating functions obtained from the discrete symbols. Specifically, for sufficiently smooth functions, the resulting discretizations achieve high-order convergence with orders $p=4, 6, 8$. The discrete operators lead to Toeplitz stiffness matrices, allowing efficient matrix-vector multiplications via fast algorithms. Building on these approximations, HFD methods are formulated for solving TFL equations, and their stability and convergence are rigorously analyzed. Numerical simulations confirm the effectiveness of the proposed methods, showing excellent agreement with the theoretical predictions.

</details>


### [7] [Numerical Methods for Dynamical Low-Rank Approximations of Stochastic Differential Equations -- Part I: Time discretization](https://arxiv.org/abs/2601.21428)
*Yoshihito Kazashi,Fabio Nobile,Fabio Zoccolan*

Main category: math.NA

TL;DR: Analysis of three time-discretization methods for Dynamical Low-Rank Approximation of high-dimensional SDEs, comparing standard forward discretization with two staggered algorithms that offer better stability without time-step restrictions.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze numerical time-integration strategies for Dynamical Low-Rank Approximation (DLRA) of high-dimensional stochastic differential equations, addressing stability issues and time-step restrictions in existing methods.

Method: Three time-discretization procedures for the Dynamically Orthogonal (DO) method: 1) Standard forward discretization of both deterministic and stochastic components, 2-3) Two staggered algorithms that alternately update deterministic and stochastic modes in half steps.

Result: Standard forward discretization converges with time-step restriction dependent on smallest singular value of stochastic modes' Gram matrix. Staggered algorithms are more stable and converge without time-step restrictions. Computational experiments support theoretical findings.

Conclusion: Staggered time-discretization algorithms provide superior stability compared to standard forward discretization for DLRA of SDEs, eliminating restrictive time-step requirements while maintaining convergence. Part II will address discretization in probability.

Abstract: In this work (Part I), we study three time-discretization procedures of the Dynamical Low-Rank Approximation (DLRA) of high-dimensional stochastic differential equations (SDEs). Specifically, we consider the Dynamically Orthogonal (DO) method for DLRA proposed and analyzed in arXiv:2308.11581v4, which consists of a linear combination of products between deterministic orthonormal modes and stochastic modes, both time-dependent. The first strategy we consider for numerical time-integration is very standard, consisting in a forward discretization in time of both deterministic and stochastic components. Its convergence is proven subject to a time-step restriction dependent on the smallest singular value of the Gram matrix associated to the stochastic modes. Under the same condition on the time-step, this smallest singular value is shown to be always positive, provided that the SDE under study is driven by a non-degenerate noise. The second and the third algorithms, on the other hand, are staggered ones, in which we alternately update the deterministic and the stochastic modes in half steps. These approaches are shown to be more stable than the first one and allow us to obtain convergence results without the aforementioned restriction on the time-step. Computational experiments support theoretical results. In this work we do not consider the discretization in probability, which will be the topic of Part II.

</details>


### [8] [A Hybrid semi-Lagrangian Flow Mapping Approach for Vlasov Systems: Combining Iterative and Compositional Flow Maps](https://arxiv.org/abs/2601.21668)
*Philipp Krah,Zetao Lin,R. -Paul Wilhelm,Fabio Bacchini,Jean-Christophe Nave,Virginie Grandgirard,Kai Schneider*

Main category: math.NA

TL;DR: Hybrid semi-Lagrangian scheme combining Numerical Flow Iteration (NuFI) and Characteristic Mapping Method (CMM) for Vlasov-Poisson equation, balancing accuracy, conservation, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational limitations of existing methods: NuFI preserves symplectic structure and conserves invariants but scales quadratically with time, while CMM has low computational cost but different trade-offs. Need a method that combines their strengths for efficient, accurate Vlasov-Poisson simulations.

Method: Hybrid approach merging NuFI and CMM: uses NuFI for accurate, conservative local time stepping (preserves symplectic structure), and CMM for efficient solution propagation through submap composition. Reduces storage requirements while maintaining accuracy and structural properties.

Result: Numerical experiments demonstrate effectiveness, showing trade-offs between memory usage and computational cost. Benchmarked against semi-Lagrangian predictor-corrector scheme used in modern gyrokinetic codes, evaluating accuracy and conservation properties.

Conclusion: The hybrid scheme successfully combines strengths of both methods, reducing storage requirements while maintaining accuracy and improving structural properties for Vlasov-Poisson simulations.

Abstract: We propose a hybrid semi-Lagrangian scheme for the Vlasov--Poisson equation that combines the Numerical Flow Iteration (NuFI) method with the Characteristic Mapping Method (CMM). Both approaches exploit the semi-group property of the underlying diffeomorphic flow, enabling the reconstruction of solutions through flow maps that trace characteristics back to their initial positions. NuFI builds this flow map iteratively, preserving symplectic structure and conserving invariants, but its computational cost scales quadratically with time. Its advantage lies in a compact, low-dimensional representation depending only on the electric field. In contrast, CMM achieves low computational costs when remapping by composing the global flow map from explicitly stored submaps. The proposed hybrid method merges these strengths: NuFi is employed for accurate and conservative local time stepping, while CMM efficiently propagates the solution through submap composition. This approach reduces storage requirements, maintains accuracy, and improves structural properties. Numerical experiments demonstrate the effectiveness of the scheme and highlight the trade-offs between memory usage and computational cost. We benchmark against a semi-Lagrangian predictor-corrector scheme used in modern gyrokinetic codes, evaluating accuracy and conservation properties.

</details>


### [9] [Numerical analysis of a locking-free primal hybrid method for linear elasticity with $H(\mathrm{div})$-conforming stress recovery](https://arxiv.org/abs/2601.21635)
*Giovanni Taraschi,Maicon Ribeiro Correa*

Main category: math.NA

TL;DR: A hybrid finite element method for linear elasticity with displacement, pressure, and traction multiplier, featuring stable approximation spaces, optimal convergence, locking-free behavior for near-incompressibility, and H(div)-conforming stress recovery.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for linear elasticity problems that avoids locking phenomena, particularly for nearly incompressible materials, while providing stable and accurate approximations.

Method: Primal hybrid finite element method using displacement, auxiliary pressure field, and Lagrange multiplier for traction. General analysis for discrete solution existence/uniqueness, construction of stable approximation spaces on triangular/quadrilateral meshes, and element-wise stress recovery strategy.

Result: Method achieves optimal convergence orders, is locking-free for nearly incompressible problems, and produces H(div)-conforming, locally equilibrated, weakly symmetric stress approximations robust to locking.

Conclusion: The proposed hybrid finite element method provides a comprehensive solution for linear elasticity with robust performance for near-incompressible materials, combining stable discretization, optimal convergence, and high-quality stress recovery.

Abstract: In this work, we study a primal hybrid finite element method for the approximation of linear elasticity problems, posed in terms of displacement, an auxiliary pressure field, and a Lagrange multiplier related to the traction. We develop a general analysis for the existence and uniqueness of the solution for the discrete problem, which is applied to the construction of stable approximation spaces on triangular and quadrilateral meshes. The use of these spaces lead to optimal convergence orders, resulting in a locking-free method capable of providing robust approximations for nearly incompressible problems. Finally, we propose a strategy for recovering the stress field from the hybrid solution by solving element-wise sub-problems. The resulting stress approximation is $H(\mathrm{div})$-conforming, locally equilibrated, weakly symmetric, and robust to locking.

</details>


### [10] [Adaptive Kernel Methods](https://arxiv.org/abs/2601.21707)
*Tam√°s D√≥zsa,Andrea Angino,Zolt√°n Szab√≥,J√≥zsef Bokor,Matthias Voigt*

Main category: math.NA

TL;DR: The paper introduces adaptive kernel methods with learnable solution spaces that generalize traditional kernel methods, enabling efficient large-scale applications without infinite-dimensional RKHS approximations.


<details>
  <summary>Details</summary>
Motivation: Traditional kernel methods use fixed RKHS solution spaces determined solely by the kernel and dataset, limiting flexibility and efficiency for large-scale problems. The authors aim to create more adaptive kernel methods with learnable parameters to overcome these limitations.

Method: Proposes kernel methods with solution spaces that depend on learnable parameters independent of the dataset, creating variable projection operators. Two main contributions: 1) efficient approximation of kernels from infinite-dimensional RKHSs for large datasets, and 2) construction of fixed-dimensional, parameter-dependent solution spaces for large-scale problems without infinite-dimensional RKHS approximations.

Result: Develops a novel family of adaptive kernel methods that generalize earlier approaches like Random Fourier Features, demonstrating effectiveness through numerical experiments.

Conclusion: The proposed adaptive kernel methods with learnable solution spaces offer improved flexibility and efficiency for large-scale problems compared to traditional fixed RKHS approaches, providing a more general framework for kernel-based learning.

Abstract: Kernel methods approximate nonlinear maps in a data-driven manner by projecting the target map onto a finite-dimensional Hilbert space called the solution space. Traditionally, this space is a subspace of a fixed ambient reproducing kernel Hilbert space (RKHS), determined solely by the chosen kernel and the dataset, whose elements identify the basis elements. Consequently, the projection operator underlying the kernel method depends on the loss function, the dataset, and the choice of ambient RKHS. In this study, we consider kernel methods whose solution spaces also depend on learnable parameters that are independent of the dataset. The resulting methods can be viewed as variable projection operators that depend on the loss function, the dataset, and the new learnable parameters instead of a fixed RKHS. This work has two main contributions. First, we propose an efficient approximation of kernels associated with infinite-dimensional RKHSs, commonly used to reduce the solution-space dimension for large datasets. Second, we construct fixed-dimensional, parameter-dependent solution spaces that enable highly efficient kernel models suitable for large-scale problems without the need to approximate kernels of infinite-dimensional RKHSs. Our novel family of adaptive kernel methods generalizes earlier approaches, including Random Fourier Features, and we demonstrate their effectiveness through several numerical experiments.

</details>


### [11] [A reduced basis method for parabolic PDEs based on a space-time least squares formulation](https://arxiv.org/abs/2601.21736)
*Michael Hinze,Christian Kahle,Michael Stahl*

Main category: math.NA

TL;DR: POD-greedy reduced basis method for parameter-dependent parabolic PDEs using least squares space-time formulation with minimal regularity assumptions.


<details>
  <summary>Details</summary>
Motivation: To extend the least squares space-time approach for parabolic equations to parameter-dependent cases and develop an efficient reduced basis method with certification.

Method: Extends the least squares space-time formulation to parameter-dependent parabolic PDEs, applies reduced basis method with POD-greedy approach, provides offline-online decomposition, and develops absolute/relative error bounds.

Result: Developed a certified reduced basis method for parameter-dependent parabolic PDEs with performance demonstrated through numerical examples.

Conclusion: The method successfully extends reduced basis techniques to parameter-dependent parabolic PDEs using least squares space-time formulation with certification capabilities.

Abstract: In this work, we present a POD-greedy reduced basis method for parabolic partial differential equations (PDEs), based on the least squares space-time formulation proposed in [Hinze, Kahle, Stahl, A least-squares space-time approach for parabolic equations, 2023, arXiv:2305.03402] that assumes only minimal regularity. We extend this approach to the parameter-dependent case. The corresponding variational formulation then is based on a parameter-dependent, symmetric, uniformly coercive, and continuous bilinear form. We apply the reduced basis method to this formulation, following the well-developed techniques for parameterized coercive problems, as seen e.g. in reduced basis methods for parameterized elliptic PDEs. We present an offline-online decomposition and provide certification with absolute and relative error bounds. The performance of the method is demonstrated using selected numerical examples.

</details>


### [12] [Solving Hamilton-Jacobi equations by minimizing residuals of monotone discretizations](https://arxiv.org/abs/2601.21764)
*Olivier Bokanowski,Carlos Esteve-Yag√ºe,Richard Tsai*

Main category: math.NA

TL;DR: Residual minimization for monotone finite-difference discretizations yields well-posed discrete solutions for nonlinear equations, enabling neural network solutions for high-dimensional Hamilton-Jacobi equations.


<details>
  <summary>Details</summary>
Motivation: To solve fully nonlinear Hamilton-Jacobi equations in high dimensions using neural networks trained by minimizing residuals from monotone discretizations, addressing analytical challenges of solvability and uniqueness of local minima that don't follow from monotonicity alone.

Method: Derive sufficient conditions for well-posedness of residual minimization for nonlinear equations defined by monotone finite-difference discretizations, establishing framework for optimization-based solvers.

Result: Established well-posedness of optimization-based solvers, enabling adaptation of Level Set Methods to high-dimensional settings for applications like segmentation and interface tracking, with extensions to degenerate elliptic/parabolic PDEs on graphs with monotone graph Laplacians.

Conclusion: The analysis provides theoretical foundation for using residual minimization with monotone discretizations to solve high-dimensional PDEs via neural networks, overcoming analytical hurdles and enabling new high-dimensional computational capabilities.

Abstract: We derive sufficient conditions under which residual minimization yields well-posed discrete solutions for nonlinear equations defined by monotone finite--difference discretizations. Our analysis is motivated by the challenge of solving fully nonlinear Hamilton--Jacobi (HJ) equations in high dimensions by means of a Neural Network, which is trained by minimizing residuals arising from monotone discretizations of the Hamiltonian. While classical theory ensures that consistency and monotonicity imply convergence to the viscosity solution, treating these discrete systems as optimization problems introduces new analytical hurdles: solvability and the uniqueness of local minima do not follow from monotonicity alone.
  By establishing the well--posedness of these optimization--based solvers, our framework enables the adaptation of Level Set Methods to high--dimensional settings, unlocking new capabilities in applications such as high--dimensional segmentation and interface tracking. Finally, we observe that these arguments extend almost directly to degenerate elliptic or parabolic PDEs on graphs equipped with monotone graph Laplacians.

</details>


### [13] [A novel Krylov subspace method for approximating Fr√©chet derivatives of large-scale matrix functions](https://arxiv.org/abs/2601.21799)
*Daniel Kressner,Peter Oehme*

Main category: math.NA

TL;DR: Novel Krylov subspace method for approximating Fr√©chet derivative matrix-vector products with improved convergence properties compared to standard approaches.


<details>
  <summary>Details</summary>
Motivation: Need to efficiently compute L_f(A,E)b (Fr√©chet derivative of matrix function f(A) in direction E) for sensitivity analysis of network centrality measures and gradient-based optimization problems involving matrix functions. Standard approach using block triangular matrix has poor spectral properties that impede convergence.

Method: Proposes a novel modification of the Arnoldi algorithm that better preserves the block triangular structure of the problem, allowing convergence to be bounded by best polynomial approximation of f' on the numerical range of A.

Result: The modified method avoids convergence difficulties of standard approach and allows theoretical convergence analysis based on approximation of f'. Numerical experiments demonstrate the effectiveness of the approach.

Conclusion: The proposed Krylov subspace method provides an efficient and theoretically sound approach for approximating Fr√©chet derivative matrix-vector products, overcoming limitations of standard methods while maintaining practical applicability.

Abstract: We present a novel Krylov subspace method for approximating $L_f(A, E) \vc{b}$, the matrix-vector product of the Fr√©chet derivative $L_f(A, E)$ of a large-scale matrix function $f(A)$ in direction $E$, a task that arises naturally in the sensitivity analysis of quantities involving matrix functions, such as centrality measures for networks. It also arises in the context of gradient-based methods for optimization problems that feature matrix functions, e.g., when fitting an evolution equation to an observed solution trajectory. In principle, the well-known identity \[
  f\left( \begin{bmatrix}
  A & E \\ 0 & A
  \end{bmatrix} \right) \begin{bmatrix}
  0 \\ \vc{b}
  \end{bmatrix} = \begin{bmatrix}
  L_f(A, E) \vc{b} \\ f(A) \vc{b}
  \end{bmatrix}, \] allows one to directly apply any standard Krylov subspace method, such as the Arnoldi algorithm, to address this task. However, this comes with the major disadvantage that the involved block triangular matrix has unfavorable spectral properties, which impede the convergence analysis and, to a certain extent, also the observed convergence. To avoid these difficulties, we propose a novel modification of the Arnoldi algorithm that aims at better preserving the block triangular structure. In turn, this allows one to bound the convergence of the modified method by the best polynomial approximation of the derivative $f^\prime$ on the numerical range of $A$. Several numerical experiments illustrate our findings.

</details>


### [14] [Quotient geometry of tensor ring decomposition](https://arxiv.org/abs/2601.21874)
*Bin Gao,Renfeng Peng,Ya-xiang Yuan*

Main category: math.NA

TL;DR: The paper establishes the quotient geometry of tensor ring (TR) decomposition by addressing gauge invariance and full-rank conditions, extending results to uniform TR decomposition, with validation through tensor completion experiments.


<details>
  <summary>Details</summary>
Motivation: Tensor ring decomposition has shown practical success but its intrinsic geometry remains poorly understood due to the underlying ring structure and nontrivial gauge invariance, creating a gap in theoretical foundations.

Method: Develops quotient geometry for TR decomposition by imposing full-rank conditions on all unfolding matrices of core tensors and capturing gauge invariance. Extends approach to uniform TR decomposition where all core tensors are identical.

Result: Establishes the mathematical framework for TR decomposition geometry, providing theoretical foundations that enable more efficient numerical methods. Numerical experiments validate the developed geometries through tensor ring completion tasks.

Conclusion: The paper successfully bridges the theoretical gap in understanding TR decomposition geometry, providing a rigorous mathematical framework that supports practical applications and enables more efficient numerical methods for tensor computations.

Abstract: Differential geometries derived from tensor decompositions have been extensively studied and provided the foundations for a variety of efficient numerical methods. Despite the practical success of the tensor ring (TR) decomposition, its intrinsic geometry remains less understood, primarily due to the underlying ring structure and the resulting nontrivial gauge invariance. We establish the quotient geometry of TR decomposition by imposing full-rank conditions on all unfolding matrices of the core tensors and capturing the gauge invariance. Additionally, the results can be extended to the uniform TR decomposition, where all core tensors are identical. Numerical experiments validate the developed geometries via tensor ring completion tasks.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Global oscillatory solutions for the Yang-Mills heat flow](https://arxiv.org/abs/2601.21017)
*Yannick Sire,Juncheng Wei,Youquan Zheng,Yifu Zhou*

Main category: math.AP

TL;DR: The paper analyzes long-time dynamics of SO(4)-equivariant Yang-Mills heat flow in 4D with SU(2) structure group, showing solutions can exhibit blow-up, blow-down, or oscillatory asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time asymptotic behavior of Yang-Mills heat flow solutions in 4D with specific symmetry, particularly investigating whether oscillatory behavior can occur as t‚Üí‚àû.

Method: Study SO(4)-equivariant Yang-Mills heat flow with SU(2) structure group in 4D, focusing on initial data with specific decay at spatial infinity, and analyze long-time dynamics through unified description.

Result: Proved that global solutions can exhibit three types of asymptotic behavior: blow-up, blow-down, and oscillatory behavior at time infinity. This appears to be the first example of Yang-Mills heat flows showing oscillatory behavior as t‚Üí‚àû.

Conclusion: The long-time dynamics of Yang-Mills heat flow in this setting are rich and can be described by initial data in a unified manner, revealing previously unknown oscillatory asymptotic behavior.

Abstract: We investigate the long-time dynamics for the global solution of the $SO(4)$-equivariant Yang-Mills heat flow (YMHF) with structure group $SU(2)$ in space dimension $4$. For a class of initial data with specific decay at spatial infinity, we prove that the long-time dynamics of YMHF can be described by the initial data in a unified manner. As a consequence, the global solutions can exhibit blow-up, blow-down, and more exotically, {\it oscillatory} asymptotic behavior at time infinity. This seems to be the first example of Yang-Mills heat flows with oscillatory behavior as $t\to \infty$.

</details>


### [16] [Decay rates to equilibrium in a nonlinear subdiffusion equation with two counteracting terms](https://arxiv.org/abs/2601.21038)
*Barbara Kaltenbacher*

Main category: math.AP

TL;DR: Proves convergence to steady state for subdiffusion equations with exponential or power law rates under mild coefficient conditions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous convergence results for subdiffusion equations with nonlinear terms, which are important for modeling anomalous diffusion processes in various physical and biological systems.

Method: Analyzes solutions to the subdiffusion equation with fractional time derivatives (‚àÇ‚Çú·µÖu) using analytical techniques to prove convergence to steady state as t‚Üí‚àû.

Result: Proves convergence to a steady state for both exponential (Œ±=1) and power law (Œ±‚àà[0,1)) cases under mild conditions on coefficients p, q, nonlinearity f, source r, and elliptic operator ùïÉ.

Conclusion: The paper establishes rigorous convergence results for subdiffusion equations, providing mathematical foundation for understanding long-term behavior of anomalous diffusion processes with nonlinear interactions.

Abstract: In this paper we prove convergence to a steady state as $t\to\infty$ for solutions to the subdiffusion equation \[ \partial_t^Œ±u - \mathbb{L} u = q(x)u - p(x)f(u) + r \] with the exponential ($Œ±=1$) or power law ($Œ±\in[0,1)$) rates under mild conditions on the coefficients $p$, $q$, the nonlinearity $f$, the source $r$, and the elliptic operator $\mathbb{L}$.

</details>


### [17] [Classical solutions to the Boltzmann equations for gas mixture with unequal molecular masses](https://arxiv.org/abs/2601.21213)
*Gaofeng Wang,Weike Wang,Tianfang Wu*

Main category: math.AP

TL;DR: Global existence of classical solutions for multi-component Boltzmann equations with unequal molecular masses and soft potentials near Maxwellians in periodic domains.


<details>
  <summary>Details</summary>
Motivation: Most existing research focuses on single-species Boltzmann equations, while gas mixtures with unequal molecular masses (like Earth's atmosphere with N‚ÇÇ:O‚ÇÇ mass ratio 7:8) have broader applications but limited studies.

Method: Analyzes Boltzmann equations for mixtures with unequal masses (m·¥¨‚â†m·¥Æ), focusing on detailed characterization of linear collision operator structure and establishing estimates for nonlinear terms under unequal mass conditions.

Result: Establishes global in time existence of classical solutions near Maxwellians for soft potentials (-3<Œ≥<0) in periodic spatial domains, applicable to arbitrary molecular mass ratios.

Conclusion: Results provide foundation for advancing spectral analysis for soft potentials and L¬≤,L^‚àû frameworks in future multi-component Boltzmann equation studies.

Abstract: The Boltzmann equation is essential for gas thermodynamics,as it models how the molecular density distribution $F(t,x,v)$ changes over time. However, existing research primarily focuses on the single species Boltzmann equation, while investigations into gas mixtures with unequal molecular masses remain relatively limited. Notably, mixed gas studies have broader applications exemplified by Earth's atmosphere, composed of 78\% nitrogen, 21\% oxygen, and 1\% trace gases, where the $N_2$ to $O_2$ molecular mass ratio is 28:32 (simplified as 7:8). This work addresses the Boltzmann equations for such mixtures with unequal molecular masses $(m^A\neq m^B)$, establishing the global in time existence of classical solutions near Maxwellians for soft potentials ($-3<Œ≥<0$) in a periodic spatial domain. Our analysis encompasses arbitrary molecular mass ratios. Our analysis encompasses arbitrary molecular mass ratios. The main contribution of this paper lies in the detailed characterization of the linear collision operator's structure and establishing estimates for the nonlinear terms under unequal mass conditions. Consequently, these results may help advance spectral analysis for soft potentials as well as $L^2,L^{\infty}$ frameworks in future studies of multi-component Boltzmann equations.

</details>


### [18] [Hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixture](https://arxiv.org/abs/2601.21245)
*Yeping Li,Gaofeng Wang,Tianfang Wu*

Main category: math.AP

TL;DR: The paper establishes the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixtures using Hilbert expansion, deriving convergence to Euler-Poisson two-fluid system with validity time scaling with Knudsen number.


<details>
  <summary>Details</summary>
Motivation: To rigorously justify the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixtures with arbitrary particle masses and charges, which introduces asymmetric collision effects and prevents equation decoupling, making analysis challenging.

Method: Uses Hilbert expansion method with truncation after 2k-1 terms (k‚â•6), constructs new weight functions for remainder estimation, employs L¬≤-W¬π,‚àû interplay framework, analyzes velocity decay rates of operators, and uses vector-valued functions to handle asymmetric collision effects.

Result: Derives bi-Maxwellian distribution determined by Euler-Poisson two-fluid system, establishes solution validity time of O(Œµ‚Åª ∏) where y depends on potential range Œ≥: y = -(2k-3)/[2(2k-1)] for -1‚â§Œ≥‚â§1, and y = -(2k-3)/[(1-Œ≥)(2k-1)] for -3<Œ≥<-1.

Conclusion: The rigorous hydrodynamic limit is established for gas mixtures with arbitrary masses/charges, providing physically realistic results applicable to analyzing gas flow dynamics in Earth's daytime ionosphere at high altitudes.

Abstract: In this paper, we study the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for a gas mixture in the whole space $(x \in \mathbb{R}^3)$ with the potential range of $Œ≥\in\left(-3, 1\right]$. Using the method of Hilbert expansion, we first derive a bi-Maxwellian determined by the Euler-Poisson system of two fluids. To justify the convergence of the solution rigorously as the Knudsen number tends to zero, we sequentially calculate the first $2k-1$ terms of the expansion series $(k \geq 6)$, and then truncate it, and express the solution as the sum of these first $2k-1$ terms and a remainder term. Within the framework of the $L_{x,v}^2-W_{x,v}^{1,\infty}$ interplay established by Guo and Jang \cite{[ininp]Guo2010CMP}, we construct a new weight function to estimate the remainder term in four different cases regarding the potential $Œ≥$. Here, the particle masses $m^A, m^B > 0$ and their charges $e^A, e^B$ can be given arbitrarily. This causes the collision operator to exhibit asymmetric effects ($m^A \neq m^B$), rendering the system of equations impossible to decouple. So, it adds difficulties to both $L^2$, $L^{\infty}$ estimates for the remainder. Therefore, we adopt the framework of vector-valued functions and analyze the velocity decay rate of the operator $K_{M,2,w}^{Œ±,c}$ to eliminate the singularity induced by small parameters in characteristic line iterations. Our results show that the validity time of the solution is $O(\varepsilon^{-y})$, where $y$ is $-\frac{2k-3}{2(2k-1)}$ when $-1 \leq Œ≥\leq 1$, and it becomes $-\frac{2k-3}{(1-Œ≥)(2k-1)}$, when $-3 < Œ≥< -1$. These results possess strong physical realism and can be applied to analyze gas flow dynamics in the daytime ionosphere at high altitudes above the Earth.

</details>


### [19] [Blow-up phemomenon for the Geng-Xue system and related models](https://arxiv.org/abs/2601.21295)
*Song Liu,Zhaoyang Yin*

Main category: math.AP

TL;DR: The paper studies blow-up criteria and phenomena for the Geng-Xue system with cubic nonlinearity, extending results to a broader b-family of two-component systems.


<details>
  <summary>Details</summary>
Motivation: To analyze the Cauchy problem for the Geng-Xue system with cubic nonlinearity, establishing blow-up criteria and phenomena without relying on conservation laws, and extending these findings to a more general class of systems.

Method: First proves blow-up criteria in low Besov spaces, then demonstrates blow-up phenomena using methods that don't require conservation laws, and finally extends these results to the b-family of two-component systems with cubic nonlinearity.

Result: Establishes blow-up criteria in low Besov spaces, proves blow-up phenomena without conservation law requirements, and successfully extends these results to the broader b-family of two-component systems with cubic nonlinearity.

Conclusion: The paper successfully develops blow-up criteria and phenomena for the Geng-Xue system with cubic nonlinearity, and demonstrates the applicability of these results to a wider class of two-component systems, providing new analytical tools for studying such nonlinear PDEs.

Abstract: In this paper, we consider the Cauchy problem of the Geng-Xue system with cubic nonlinearity. Firstly, we prove a blow-up criteria in the low besov space. Secondly, we prove the blow-up phenomenon by using the method which does not require any conservation law. Finally, we extend our results to the b-family of two-component system with cubic nonlinearity.

</details>


### [20] [Wellposedness and dynamics of two types of reaction--nonlocal diffusion systems under the inhomogeneous spectral fractional Laplacian](https://arxiv.org/abs/2601.21422)
*Pu Yuan,Paul A. Zegeling*

Main category: math.AP

TL;DR: This paper studies semilinear reaction-nonlocal diffusion equations with fractional Laplacian operators, establishing local wellposedness, maximum principles, and analyzing two prototype systems (bistable RNDE and nonlocal Gray-Scott) with numerical simulations of pattern formation.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous analytical framework for reaction-nonlocal diffusion equations that model anomalous diffusion beyond Brownian motion, with applications to pattern formation in complex systems.

Method: Uses harmonic lifting to handle boundary conditions, analytic contraction semigroup theory in C‚ÇÄ(Œ©) to establish wellposedness via Duhamel formula, and applies fractional weak maximum principles to derive invariant-range properties and positivity preservation.

Result: Established local wellposedness with blow-up alternative, L‚àû-contractivity, positivity preservation, maximum principles; for bistable RNDE derived energy dissipation and invariant-range property; for Gray-Scott system proved positivity preservation and explicit L‚àû invariant set; numerical simulations show fractional order impact on patterns.

Conclusion: The semigroup approach provides a robust framework for analyzing reaction-nonlocal diffusion equations, yielding fundamental properties like wellposedness, maximum principles, and invariant sets, with applications to pattern formation systems where fractional diffusion orders significantly influence dynamics.

Abstract: Reactio-nonlocal diffusion equations model nonlocal transport and anomalous diffusion by replacing the Laplacian with a fractional power, capturing diffusion mechanisms beyond Brownian motion. We primarily study the semilinear problem \[ \partial_t u + Œµ^2(-Œî)_g^Œ±u = \mathcal{N}(u) \] allowing constant inhomogeneous Dirichlet boundary condition $u|_{\partialŒ©}=g$. To handle the boundary constraint, we use a harmonic lifting to reformulate the problem as an equivalent homogeneous system with a shifted nonlinearity. Working in \(C_0(Œ©)\), analytic contraction semigroup theory yields the Duhamel formula and quantitative smoothing, implying local wellposedness for locally Lipschitz reactions and a blow-up alternative. The semigroup viewpoint also provides $L^\infty$-contractivity and positivity preservation, which drive pointwise maximum principles and stability bounds. Furthermore, we analyze two prototypes. For the bistable RNDE, we derive an energy dissipation identity and, using a fractional weak maximum principle, obtain an invariant-range property that confines solutions between the two stable steady states. For the nonlocal Gray-Scott system with possibly different fractional diffusion orders, we prove that solutions preserve positivity. Moreover, we identify an explicit \(L^\infty\) invariant set ensuring global boundedness, and derive an eigenfunction-weighted interior \(L^2\) bound. Finally, we perform numerical simulations using a sine pseudospectral discretization and ETDRK4 time-stepping, which the impact of fractional orders on pattern formation, consistent with our analytical results.

</details>


### [21] [Multistatic anisotropic travel-time as a tensor tomography problem](https://arxiv.org/abs/2601.21640)
*Naeem Desai,Oliver Graham,William R. B. Lionheart*

Main category: math.AP

TL;DR: The paper analyzes travel-time imaging problems for anisotropic reflectivity reconstruction using multistatic measurements, relating them to tensor ray transforms and discussing implications of null-spaces.


<details>
  <summary>Details</summary>
Motivation: To understand how to reconstruct anisotropic reflectivity in travel-time imaging problems using multistatic measurements, where reflectivity depends on both incoming and outgoing directions, and to relate these problems to established mathematical transforms.

Method: Formulates travel-time imaging as generalized Radon transforms over isochrones (ellipses in planar case, spheroids in volumetric case). Relates simplified far-field case to tensor ray transforms, specifically Sharafutdinov's longitudinal ray transform, and connects volumetric case to normal Radon transform of tensor fields.

Result: Establishes mathematical connections between travel-time imaging problems and tensor transforms, enabling analysis of reconstruction properties including discussion of known null-spaces in these transforms.

Conclusion: Travel-time imaging for anisotropic reflectivity can be mathematically formulated using tensor transforms, with important implications from the null-space properties of these transforms affecting reconstruction capabilities.

Abstract: Travel-time imaging problems seek to reconstruct an image of reflectivity of a scene by measuring travel time (and amplitude, phase) of electromagnetic or acoustic signals, such as radar and sonar. Multistatic, in this context, means that the transmitters and receivers need not be co-located. The reflectivity is anisotropic if it depends on direction, and in the multistatic case this means incoming and outgoing direction. Travel-time problems can be formulated as generalized Radon transforms of integrals over isochrones, in the planar case ellipses with transmitter and receivers at foci. In a simplified case where transmitters and receivers are distant from the scene, isochrones can be approximated by straight lines. We relate this to tensor ray transforms, specifically the longitudinal ray transform of Sharafutdinov, and discuss the implication of its known null-space. In the volumetric case isochrones are spheroids and we relate the problem to the normal Radon transform of tensor fields.

</details>


### [22] [The Kolmogorov forward equation for a distributed model of regime-switching diffusions](https://arxiv.org/abs/2601.21659)
*Alexander S. Bratus,Olga S. Rozanova*

Main category: math.AP

TL;DR: The paper proposes integro-differential equations for regime-switching diffusion processes with/without advection, develops a constructive algorithm for solving the Cauchy problem, finds explicit solutions for some initial distributions, and shows how discrete hidden state models can be approximated by continuous state distributions.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical models for regime-switching diffusion processes that can handle both continuous state distributions and discrete hidden state approximations, providing analytical tools for solving complex stochastic systems.

Method: Proposes integro-differential equations for regime-switching diffusion processes with and without advection terms, develops a constructive algorithm for solving the Cauchy problem, and establishes connections between discrete and continuous state models.

Result: Demonstrates existence of constructive algorithm for solving Cauchy problem, finds explicit solutions for certain initial state distributions, and shows how discrete hidden state models can be approximated by continuous state distributions.

Conclusion: The framework provides a unified approach for analyzing regime-switching diffusion processes with both discrete and continuous state representations, offering explicit solutions for certain cases and approximation methods for others.

Abstract: For the regime-switching diffusion process with and without advection term we propose an integro-differential equation describing the densities of states continuously distributed over a segment. We demonstrate that there exists a constructive algorithm for solving the Cauchy problem. We then show that for some initial distributions of states, the solution can be found explicitly. We also discuss how a model with a discrete number of hidden states can be approximated by a model with continuously distributed states.

</details>


### [23] [Nonhomogeneous boundary condition for spectral non-local operators](https://arxiv.org/abs/2601.21674)
*Ivan Bioƒçiƒá,Vanja Wagner*

Main category: math.AP

TL;DR: Study of semilinear non-local elliptic problems with spectral-type operators in bounded domains with nonhomogeneous boundary conditions, extending interpolated fractional Laplacian theory.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for non-local elliptic problems that covers and extends the theory of the interpolated fractional Laplacian, particularly addressing the challenge of nonhomogeneous boundary conditions in non-local settings.

Method: Combines stochastic process techniques, potential theory, and spectral analysis. Analyzes problems driven by spectral-type operators œà(-L|D) where œà is a Bernstein function and L|D is the generator of a killed L√©vy process. Introduces weak L¬π trace-like boundary operator and uses renewal functions and distance to boundary for boundary behavior analysis.

Result: Establishes sharp boundary estimates for Green and Poisson potentials, introduces a weak L¬π trace-like boundary operator, and provides existence results for solutions under general nonlinearities including sign-changing and non-monotone cases.

Conclusion: The paper develops a unified treatment of semilinear boundary problems in non-local settings, expressing boundary behavior in terms of renewal functions and distance to boundary, with potential applications to various non-local elliptic problems.

Abstract: We study semilinear non-local elliptic problems driven by spectral-type operators of the form $œà(-L_{|D})$ in a bounded $C^{1,1}$ domain $D\subset \mathbb{R}^d$ with a nonhomogeneous boundary condition. Here $œà$ is a Bernstein function satisfying a weak scaling condition at infinity, and $L_{|D}$ is the generator of a killed L√©vy process. This general framework covers and extends the theory of the interpolated fractional Laplacian. A key novelty in this setting is the analysis of the nonhomogeneous boundary condition formulated in terms of the Poisson potential with respect to the $d-1$ Hausdorff measure on $\partial D$. We establish sharp boundary estimates for Green and Poisson potentials, introduce a weak $L^1$ trace-like boundary operator, and provide existence results for solutions under quite general nonlinearities, including sign-changing and non-monotone cases. The methodology combines stochastic process techniques, potential theory, and spectral analysis, and expresses the boundary behavior of the solution in terms of the renewal function and the distance to the boundary, suggesting a possible unified treatment of semilinear boundary problems in non-local settings.

</details>


### [24] [Localized Big Bang Stability of Spacetime Dimensions $n\geq4$](https://arxiv.org/abs/2601.21677)
*Weihang Zheng*

Main category: math.AP

TL;DR: Proves past nonlinear stability of sub-critical Kasner-scalar field solutions in Einstein-scalar field equations for dimensions n‚â•4, showing perturbed solutions are asymptotically Kasner and terminate at crushing singularities.


<details>
  <summary>Details</summary>
Motivation: To generalize the stability result of Beyer-Oliynyk-Zheng (arXiv:2502.09210v2) to all higher dimensional spacetimes, extending the understanding of Kasner-scalar field solutions in Einstein-scalar field equations.

Method: Analysis of Einstein-scalar field equations on truncated cone domains in spacetime dimensions n‚â•4, studying perturbed solutions around sub-critical Kasner-scalar field configurations.

Result: Perturbed solutions are asymptotically pointwise Kasner, geodesically incomplete in contracting direction, and terminate at quiescent and crushing singularities with blow-up of curvature invariants.

Conclusion: Successfully generalizes the stability result to all higher dimensions, establishing the past nonlinear stability of sub-critical Kasner-scalar field solutions in Einstein-scalar field equations for n‚â•4.

Abstract: We prove the past nonlinear stability of the sub-critical Kasner-scalar field solutions to the Einstein-scalar field equations on a truncated cone domain in spacetime dimensions $n\geq4$. Our analysis demonstrates that the perturbed solutions are asymptotically pointwise Kasner, geodesically incomplete in the contracting direction and terminate at quiescent and crushing singularities characterized by the blow-up of curvature invariants. This work generalizes the result of Beyer-Oliynyk-Zheng in [arXiv:2502.09210v2] to all higher dimensional spacetimes.

</details>


### [25] [Unique Continuation Property for Stochastic Wave Equations](https://arxiv.org/abs/2601.21854)
*Qi L√º,Zhonghua Liao*

Main category: math.AP

TL;DR: Stochastic wave equations restore unique continuation across characteristic surfaces, unlike deterministic waves where it fails.


<details>
  <summary>Details</summary>
Motivation: To investigate whether stochastic wave equations exhibit unique continuation properties across characteristic hypersurfaces, which is known to fail in deterministic wave equations due to H√∂rmander-type counterexamples.

Method: Develop novel stochastic Carleman estimates that leverage the It√¥ diffusion term's positive energy contribution, which is absent in deterministic models. Extend analysis to equations with non-homogeneous stochastic sources.

Result: Proved that if a solution to a linear stochastic wave equation vanishes on one side of a characteristic surface, it must vanish in a full neighborhood of any point on that surface, provided the stochastic diffusion coefficient is non-degenerate. Also established global unique continuation from interior of arbitrarily narrow characteristic cones.

Conclusion: Stochastic wave equations exhibit fundamentally different behavior from deterministic ones, restoring unique continuation properties across characteristic surfaces. This reveals qualitative differences between deterministic and stochastic hyperbolic dynamics and opens new possibilities for control theory and inverse problems in stochastic settings.

Abstract: This paper establishes a fundamental and surprising phenomenon in the theory of stochastic wave equations: the restoration of the unique continuation property (UCP) across characteristic hypersurfaces, a property that is known to fail generically in the deterministic setting. We prove that if a solution to a linear stochastic wave equation vanishes on one side of a characteristic surface $Œì$, then it must vanish in a full neighborhood of any point on $Œì$, provided the stochastic diffusion coefficient is non-degenerate. This result stands in sharp contrast to the classical H√∂rmander-type counterexamples for deterministic waves.
  Furthermore, we extend the UCP to equations with non-homogeneous stochastic sources and establish a global unique continuation result from the interior of an arbitrarily narrow characteristic cone. Our proofs rely on a novel stochastic Carleman estimate, where the It√¥ diffusion term introduces a crucial positive energy contribution that is absent in deterministic models.
  These findings demonstrate a qualitative difference between deterministic and stochastic hyperbolic dynamics and open new avenues for control theory and inverse problems in stochastic setting.

</details>


### [26] [Probabilistically Strong Solutions to Stochastic Euler Equations](https://arxiv.org/abs/2601.22073)
*Benjamin Gess,Robert Lasarzik*

Main category: math.AP

TL;DR: Existence of probabilistically strong, measure-valued solutions for stochastic Navier-Stokes equations and their convergence to stochastic Euler solutions in vanishing viscosity limit.


<details>
  <summary>Details</summary>
Motivation: To solve the open problem of constructing probabilistically strong solutions for stochastic Euler equations that satisfy energy inequality for general L¬≤ initial data.

Method: Introduce concept of energy-variational solutions in stochastic context to treat nonlinearities without changing probability space; extend to fluids driven by transport noise.

Result: Establish existence of probabilistically strong, measure-valued solutions for stochastic Navier-Stokes equations and prove their convergence to probabilistically strong solutions for stochastic Euler equations in vanishing viscosity limit.

Conclusion: Solves the open problem of constructing probabilistically strong solutions for stochastic Euler equations with energy inequality for general L¬≤ initial data, with extensions to transport noise.

Abstract: In this paper, we establish the existence of probabilistically strong, measure-valued solutions for the stochastic incompressible Navier--Stokes equations and prove their convergence, in the vanishing viscosity limit, to probabilistically strong solutions for the stochastic incompressible Euler equations. In particular, this solves the open problem of constructing probabilistically strong solutions for the stochastic Euler equations that satisfy the energy inequality for general $L^2$ initial data. We introduce the concept of energy-variational solutions in the stochastic context in order to treat the nonlinearities without changing the probability space. Furthermore, we extend these results to fluids driven by transport noise.

</details>


### [27] [On Global Weak Solutions for the Magnetic Two-Component Hunter-Saxton System](https://arxiv.org/abs/2601.22088)
*Levin Maier*

Main category: math.AP

TL;DR: The paper provides analytical foundations for the magnetic two-component Hunter-Saxton system (M2HS), deriving explicit solution formulas and constructing global conservative weak solutions.


<details>
  <summary>Details</summary>
Motivation: The M2HS system was recently derived as a magnetic geodesic equation on an infinite-dimensional configuration space, but the geometric framework and global weak flow were only outlined. This paper aims to provide the analytical foundations from the PDE perspective.

Method: 1. Derive explicit solution formula in Lagrangian variables via Riccati reduction. 2. Rigorously construct global conservative weak solutions by developing analytic theory of the relaxed configuration space and associated weak magnetic geodesic flow.

Result: Obtained explicit solution formula and blow-up criterion with explicit expression for blow-up time. Successfully constructed global conservative weak solutions, realizing the geometric program proposed in the earlier work.

Conclusion: The paper establishes the analytical foundations for the M2HS system, providing both explicit solution formulas and rigorous construction of global weak solutions, completing the geometric program outlined in previous work.

Abstract: We study the magnetic two-component Hunter-Saxton system (M2HS), which was recently derived in \cite{M24} as a magnetic geodesic equation on an infinite-dimensional configuration space. While the geometric framework and the global weak flow were outlined there, the present paper provides the analytical foundations of this construction from the PDE perspective.
  First, we derive an explicit solution formula in Lagrangian variables via a Riccati reduction, yielding an alternative proof of the blow-up criterion together with an explicit expression for the blow-up time. Second, we rigorously construct global conservative weak solutions by developing the analytic theory of the relaxed configuration space and the associated weak magnetic geodesic flow, thereby realizing the geometric program proposed in \cite{M24}.

</details>


### [28] [Microlocal maximal hypoellipticity from the geometric viewpoint: I](https://arxiv.org/abs/2601.22122)
*Omar Mohsen*

Main category: math.AP

TL;DR: Develops a bi-graded pseudo-differential calculus for H√∂rmander vector fields that unifies classical and sub-Riemannian calculi, using geometric methods and operator algebras.


<details>
  <summary>Details</summary>
Motivation: To create a unified pseudo-differential calculus that works for both classical Riemannian settings and sub-Riemannian structures induced by H√∂rmander vector fields, addressing limitations of existing approaches.

Method: Uses geometric constructions (resolution of singularities) combined with operator algebra methods to develop a comprehensive bi-graded calculus including Sobolev spaces, wavefront sets, and principal symbols.

Result: Establishes that invertibility of the principal symbol implies microlocal maximal hypoellipticity, resolving affirmatively the microlocal version of Helffer and Nourrigat's conjecture.

Conclusion: Provides a powerful unified framework for analyzing operators in sub-Riemannian geometry with applications to hypoellipticity problems, solving important open conjectures.

Abstract: Given some vector fields on a smooth manifold satisfying H√∂rmander's condition, we define a bi-graded pseudo-differential calculus which contains the classical pseudo-differential calculus and a pseudo-differential calculus adapted to the sub-Riemannian structure induced by the vector fields.
  Our approach is based on geometric constructions (resolution of singularities) together with methods from operators algebras. We develop this calculus in full generality, including Sobolev spaces, the wavefront set, and the principal symbol, etc.
  In particular, using this calculus, we prove that invertibility of the principal symbol implies microlocal maximal hypoellipticity. This allows us to resolve affirmatively the microlocal version of a conjecture of Helffer and Nourrigat.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Semi-implicit Lax-Wendroff kinetic scheme for hydrodynamic phonon transport](https://arxiv.org/abs/2601.21161)
*Shijie Li,Hong Liang,Songze Chen,Chuang Zhang*

Main category: physics.comp-ph

TL;DR: A semi-implicit Lax-Wendroff kinetic scheme for hydrodynamic phonon transport using double relaxation time approximation, enabling larger time steps and cell sizes than phonon mean free path/relaxation time.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for multi-scale thermal conduction in solid materials that can handle both normal and resistive phonon scattering processes while allowing larger computational time steps and cell sizes than traditional methods.

Method: Semi-implicit Lax-Wendroff kinetic scheme based on Boltzmann transport equation with double relaxation time approximation. Uses trapezoidal rule for scattering terms and midpoint rule for migration terms in finite volume framework. Solves kinetic equation for interfacial flux reconstruction instead of interpolation, coupling phonon migration and scattering within time steps. Uses second-order upwind/central schemes for distribution function reconstruction.

Result: The method allows cell size and time step to be larger than phonon mean free path and relaxation time at small Knudsen numbers. Numerical tests show accurate capture of multi-scale thermal conduction phenomena across different normal/resistive scattering rates.

Conclusion: The developed semi-implicit kinetic scheme successfully handles multi-scale phonon transport with efficient computational performance, accurately modeling thermal conduction across different scattering regimes while relaxing strict time step and spatial resolution constraints.

Abstract: A semi-implicit Lax-Wendroff kinetic scheme is developed for hydrodynamic phonon transport in solid materials based on the Boltzmann transport equation under the double relaxation time approximation, in which both the normal and resistive scattering processes are accounted. The trapezoidal and midpoint rules are adopted for the temporal integration of the scattering and migration terms under the framework of finite volume method, respectively. Instead of direct numerical interpolation, the kinetic equation is solved again when reconstructing the interfacial flux, in order to realize the coupling of phonon migration and scattering within a numerical time step. Specifically, the finite difference scheme is introduced and the second-order upwind or central schemes are used for the reconstruction of the interfacial distribution function and its spatial gradient. Consequently, the cell size and time step of the present method could be larger than the phonon mean free path and relaxation time in the limit of small Knudsen numbers. Numerical tests demonstrate that the present method can accurately capture multi-scale thermal conduction phenomena within different normal or resistive scattering rates.

</details>


### [30] [Acquiring Human-Like Mechanics Intuition from Scarce Observations via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21881)
*Jingruo Peng,Shuze Zhu*

Main category: physics.comp-ph

TL;DR: A reinforcement learning framework using episodic switching across related physical observations enables agents to develop mechanics intuition from just 2-3 observations, generalizing accurately beyond training data.


<details>
  <summary>Details</summary>
Motivation: To understand how humans can infer accurate mechanical outcomes from few observations (mechanics intuition) and develop artificial systems with similar data-efficient learning capabilities.

Method: Reinforcement learning framework where agents encode continuous physical observation parameters into state and are trained via episodic switching across closely related observations.

Result: Agents acquire robust mechanics intuition with only 2-3 observations, generalizing accurately over wide parameter ranges beyond training data, demonstrated on brachistochrone and large-deformation elastic plate problems.

Conclusion: Episodic switching provides a principled route to artificial mechanics intuition, with theoretical explanation showing generalization emerges when learned value function enforces Bellman consistency across neighboring task parameters, capturing low-dimensional solution manifolds.

Abstract: Humans can infer accurate mechanical outcomes from only a few observations, a capability known as mechanics intuition. The mechanisms behind such data-efficient learning remain unclear. Here, we propose a reinforcement learning framework in which an agent encodes continuous physical observation parameters into its state and is trained via episodic switching across closely related observations. With merely two or three observations, the agent acquires robust mechanics intuition that generalizes accurately over wide parameter ranges, substantially beyond the training data, as demonstrated on the brachistochrone and a large-deformation elastic plate. We explain this generalization through a unified theoretical view: it emerges when the learned value function enforces Bellman consistency across neighboring task parameters, rendering the Bellman residual stationary with respect to physical variations. This induces a smooth policy that captures a low-dimensional solution manifold underlying the continuum of tasks. Our work establishes episodic switching as a principled route to artificial mechanics intuition and offers a theoretical link to similar generalization abilities in biological learners.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [A joint diffusion approach to multi-modal inference in inertial confinement fusion](https://arxiv.org/abs/2601.21006)
*Michael S. Jones,Justin Kunimune,Daniel Casey,Bogdan Kustowski,Eugene Kur,Kelli Humbird*

Main category: physics.plasm-ph

TL;DR: JointDiff is a generative framework using joint diffusion to predict conditional simulation input/output distributions from partial multi-modal observations in inertial confinement fusion, unifying forward modeling, inverse inference, and output imputation.


<details>
  <summary>Details</summary>
Motivation: In inertial confinement fusion (ICF), simulation and experiment produce mixed scalar and image outputs, but only a subset of simulated data are available experimentally. There's a need to bridge this gap and enable predictions from partial observations.

Method: JointDiff uses joint diffusion to unify forward surrogate modeling, inverse inference, and output imputation into one architecture. It's trained on large ensembles of 3D Multi-Rocket Piston simulations.

Result: The model demonstrates high accuracy, statistical robustness, and transferability to experiments performed at the National Ignition Facility (NIF).

Conclusion: JointDiff establishes a flexible generative surrogate for multi-modal scientific tasks, with implications for understanding diagnostic constraints, aligning simulation to experiment, and accelerating ICF design.

Abstract: A combination of physics-based simulation and experiments has been critical to achieving ignition in inertial confinement fusion (ICF). Simulation and experiment both produce a mixture of scalar and images outputs, however only a subset of simulated data are available experimentally. We introduce a generative framework, called JointDiff, which enables predictions of conditional simulation input and output distributions from partial, multi-modal observations. The model leverages joint diffusion to unify forward surrogate modeling, inverse inference, and output imputation into one architecture. We train our model on a large ensemble of three-dimensional Multi-Rocket Piston simulations and demonstrate high accuracy, statistical robustness, and transferability to experiments performed at the National Ignition Facility (NIF). This work establishes JointDiff as a flexible generative surrogate for multi-modal scientific tasks, with implications for understanding diagnostic constraints, aligning simulation to experiment, and accelerating ICF design.

</details>


### [32] [Initial observations in X-point target divertor discharges on MAST-U](https://arxiv.org/abs/2601.21840)
*N. Lonigro,K. Verhaegh,J. Harrison,B. Lipschultz,C. Bowman,F. Federici,J. Flanagan,D. Greenhouse,D. Moulton,P. Ryan,R. Scannell,S. Silburn,T. Wijkamp,D. Brida,C. Theiler,the EUROfusion Tokamak Exploitation Team,the MAST Upgrade Team*

Main category: physics.plasm-ph

TL;DR: MAST-U experiments demonstrate that combining Super-X divertor geometry with an additional X-point (XPT configuration) improves plasma exhaust by enhancing plasma-neutral interactions, reducing target temperatures/heat fluxes, and potentially improving ELM buffering.


<details>
  <summary>Details</summary>
Motivation: Future fusion reactors face challenging exhaust conditions requiring improved momentum, power, and particle losses. The study explores how multiple alternative divertor configurations can be combined to address these challenges.

Method: Performed first high-power (>3 MW) H-mode experiments on MAST-U using a double-null X-point-target (XPT) divertor configuration, combining large strike point radius (similar to Super-X divertor) with an additional X-point near separatrix in baffled outer divertor chambers.

Result: XPT configuration shows additional exhaust benefits over Super-X divertor: broader electron density profile near secondary X-point leads to enhanced plasma-neutral interactions, broader hydrogenic emission, larger power/ion sinks, lower target electron temperatures, and reduced heat fluxes. Preliminary evidence suggests improved ELM buffering.

Conclusion: Combining multiple alternative divertor configuration strategies (XPT) can significantly improve momentum, power, and particle losses, potentially addressing the challenging exhaust conditions of future fusion reactors.

Abstract: The first high-power (> 3 MW) H-mode experiments using a double-null X-point-target (XPT) divertor configuration have been performed on MAST-U. The XPT geometry is obtained by combining a large strike point radius, similar to the Super-X divertor (SXD), with an additional X-point near the separatrix in the baffled outer divertor chambers and leads to additional exhaust benefits over the SXD. The broader electron density profile near the secondary X-point leads to additional plasma-neutral interactions, evidenced by a broader hydrogenic emission profile, and resulting in larger power and ion sinks. The increase in plasma-neutral interactions also leads to lower target electron temperatures and heat fluxes. These benefits appear to extend to transients, and preliminary evidence of improved ELM buffering in the XPT is presented. These results showcase how multiple alternative divertor configuration strategies can be combined to improve momentum, power, and particle losses, which may be required for the challenging exhaust conditions of future reactors.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [33] [Jellyfish exist](https://arxiv.org/abs/2601.21227)
*Ben Andrews,Glen Wheeler*

Main category: math.DG

TL;DR: The paper proves existence of infinitely many distinct geometric solutions for three different geometric flows: homothetic expanders for elastic flow, epicyclic shrinkers for curve diffusion flow, and epicyclic expanders for ideal flow.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of multiple geometric solutions for various geometric flows, demonstrating the richness of solution spaces beyond simple or trivial solutions.

Method: The abstract doesn't specify the exact method, but likely involves geometric analysis, variational methods, or PDE techniques to construct and prove existence of these special solutions.

Result: Proves existence of infinitely many geometrically distinct: 1) homothetic expanders (jellyfish) for elastic flow, 2) epicyclic shrinkers for curve diffusion flow, and 3) epicyclic expanders for ideal flow.

Conclusion: The solution spaces for these geometric flows contain rich families of special solutions, including expanders and shrinkers with various geometric properties.

Abstract: We show the existence of infinitely many geometrically distinct homothetic expanders (jellyfish) for the elastic flow, epicyclic shrinkers for the curve diffusion flow, and epicyclic expanders for the ideal flow.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [34] [Description of electromagnetic fields in inhomogeneous accelerating sections. IV couplers](https://arxiv.org/abs/2601.21809)
*M. I. Ayzatsky*

Main category: physics.acc-ph

TL;DR: A new method simplifies coupling analysis in waveguide systems by reducing complex integro-differential equations to three simpler ordinary differential equation systems.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient approach for analyzing coupling elements in waveguide systems, particularly for matching loop couplers in accelerating sections, which traditionally involve complex integro-differential equations.

Method: Used a simple coupling model with structured waveguide, RF power source, and load through loops/transmission lines. Reduced the Barbashin-type integro-differential system with degenerate kernels to three independent systems of ordinary differential equations - two for loop-excited fields and one for beam-excited fields.

Result: Successfully developed computer code for matching loop couplers in X-band accelerating sections. Applied to non-uniform sections achieving input reflection coefficient of 8E-3 without additional matching.

Conclusion: The proposed approach effectively simplifies complex coupling analysis, enabling practical design of waveguide coupling systems with good performance results.

Abstract: A new approach to incorporating coupling elements into a generalized coupled mode theory is presented. The simplest model of coupling of a structured waveguide with an external RF power source and load through loops and transmission lines was used. Even such a simple model significantly complicated the system of coupled equations. It turned into a coupled integro-differential system of the Barbashin type with degenerate kernels. Since the integral kernels are degenerate, this system is reduced to three independent systems of differential equations. Instead of solving a system of coupled integro-differential equations, we need to find solutions to three systems of ordinary differential equations. Two systems describe the distribution of the field excited by one loop and the specified value of the excitation current in it. In the first system the loop is located at the section's input, and in the second, at the section's output. The third system does not depend on the loop parameters at all. It describes the distribution of the field excited by an electron beam in a section without loops. Based on the developed analytical model, the computer code was developed for matching the loop couplers for the uniform accelerating sections of X-band. The calculation results were used to simulate the non-uniform section. Without additional matching, we obtained an input reflection coefficient of 8E-3.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [35] [Multiple binding modes of AKT on PIP$_3$-containing membranes](https://arxiv.org/abs/2601.21216)
*Yuki Nakagaki,Eiji Yamamoto*

Main category: physics.bio-ph

TL;DR: Molecular dynamics simulations reveal four distinct membrane-binding modes of full-length AKT kinase on PIP3-containing membranes, showing cooperative lipid recognition by both PH and kinase domains that depends on PIP3 concentration.


<details>
  <summary>Details</summary>
Motivation: Despite AKT's importance in PI3K/AKT signaling pathway, the molecular mechanism of how its PH domain and kinase domain cooperate for membrane recruitment and activation remains unclear.

Method: Used molecular dynamics simulations of full-length AKT on PIP3-containing lipid bilayers to study membrane binding at atomic resolution.

Result: Identified four distinct membrane-binding modes with different PH and kinase domain orientations. Found PIP3 interactions with both canonical and secondary sites in PH domain, plus specific PIP3 interactions with basic residues in kinase domain. Most stable mode exposes activation-loop phosphorylation site. Binding mode populations depend on PIP3 concentration.

Conclusion: Lipid recognition by both PH and kinase domains cooperatively shapes AKT's membrane-bound conformations, providing molecular insights into AKT activation mechanism on membranes.

Abstract: The PI3K/AKT signaling pathway is triggered by recruitment of AKT to cellular membranes. Although AKT is a multidomain serine/threonine kinase composed of an N-terminal pleckstrin homology (PH) domain and a C-terminal kinase domain, how these domains cooperate to regulate AKT activation on membranes remains unclear at the molecular level. Here, using molecular dynamics simulations of full-length AKT on PIP$_3$-containing lipid bilayers, we identify four distinct membrane-binding modes that differ in the orientations and membrane contacts of the PH and kinase domains. In addition to PIP$_3$ binding to the PH domain, we observe specific PIP$_3$ interactions with basic residues in the kinase domain. In the most stable mode, PIP$_3$ interacts with both the canonical and a secondary binding site in the PH domain, while the kinase domain adopts an orientation in which the activation-loop phosphorylation site is exposed to the solvent. Interestingly, the populations of these binding modes depend on the PIP$_3$ concentration in the membrane, leading to changes in the preferred orientation of AKT. These findings shed light on how lipid recognition by the PH domain and the kinase domain of AKT cooperatively shape its membrane-bound conformations.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [36] [Towards regularized learning from functional data with covariate shift](https://arxiv.org/abs/2601.21019)
*Markus Holzleitner,Sergiy Pereverzyev,Sergei V. Pereverzyev,Vaibhav Silmana,S. Sivananthan*

Main category: math.ST

TL;DR: A regularization framework for unsupervised domain adaptation in vector-valued regression under covariate shift using vector-valued RKHS, with theoretical convergence rates and an aggregation approach for parameter selection.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of covariate shift in unsupervised domain adaptation where input distributions differ between training and test data, making reliable learning difficult for vector-valued regression tasks.

Method: Develop a regularization framework using vector-valued reproducing kernel Hilbert spaces (vRKHS), restrict hypothesis space, create operator learning algorithm for functional outputs, and propose aggregation-based approach combining estimators with different regularization parameters and kernels.

Result: Established optimal convergence rates under general source conditions, provided theoretical justification for aggregation approach effectiveness, and demonstrated robustness on real-world face image dataset in mitigating distributional discrepancies.

Conclusion: The proposed framework provides a theoretically sound and practical solution for unsupervised domain adaptation in vector-valued regression under covariate shift, with effective parameter selection through aggregation and demonstrated real-world applicability.

Abstract: This paper investigates a general regularization framework for unsupervised domain adaptation in vector-valued regression under the covariate shift assumption, utilizing vector-valued reproducing kernel Hilbert spaces (vRKHS). Covariate shift occurs when the input distributions of the training and test data differ, introducing significant challenges for reliable learning. By restricting the hypothesis space, we develop a practical operator learning algorithm capable of handling functional outputs. We establish optimal convergence rates for the proposed framework under a general source condition, providing a theoretical foundation for regularized learning in this setting. We also propose an aggregation-based approach that forms a linear combination of estimators corresponding to different regularization parameters and different kernels. The proposed approach addresses the challenge of selecting appropriate tuning parameters, which is crucial for constructing a good estimator, and we provide a theoretical justification for its effectiveness. Furthermore, we illustrate the proposed method on a real-world face image dataset, demonstrating robustness and effectiveness in mitigating distributional discrepancies under covariate shift.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [37] [Rapid estimation of global sea surface temperatures from sparse streaming in situ observations](https://arxiv.org/abs/2601.21913)
*Cassidy All,Kevin Ho,Maya Magnuski,Christopher Nicolaides,Louisa B. Ebby,Mohammad Farazmand*

Main category: physics.ao-ph

TL;DR: S-DEIM combines empirical interpolation with RNNs to reconstruct high-resolution sea surface temperatures from sparse measurements, achieving 40% better accuracy than previous methods with real-time computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate high-resolution sea surface temperature (SST) reconstruction from sparse measurements is crucial for weather forecasting and climate projections, but existing methods produce inaccurate results when measurements are sparse.

Method: Sparse Discrete Empirical Interpolation Method (S-DEIM) combines two components: empirical interpolation from instantaneous in situ observations, and a learned component from historical time series using recurrent neural networks (RNNs). The method uses NOAA's weekly high-resolution SST data (1989-2021) for training.

Result: S-DEIM achieves 40% higher accuracy than DEIM and Q-DEIM, with 91% of estimates within ¬±1¬∞C of true SST. It's robust to sensor placement (only 1-2% error increase with random distribution) and computationally efficient (1-minute offline training, <1-second reconstruction).

Conclusion: S-DEIM enables accurate, real-time high-resolution SST reconstruction from sparse streaming data (0.2% of grid points) without requiring a physical model, making it suitable for operational weather forecasting and climate applications.

Abstract: Reconstructing high-resolution sea surface temperatures (SST) from staggered SST measurements is essential for weather forecasting and climate projections. However, when SST measurements are sparse, the resulting inferred SST fields are rather inaccurate. Here, we demonstrate the ability of Sparse Discrete Empirical Interpolation Method (S-DEIM) to reconstruct the high-resolution SST field from sparse in situ observations, without using a model. The S-DEIM estimate consists of two terms, one computed from instantaneous in situ observations using empirical interpolation, and the other learned from the historical time series of observations using recurrent neural networks (RNNs). We train the RNNs using the National Oceanic and Atmospheric Administration's weekly high-resolution SST dataset spanning the years 1989-2021 which constitutes the training data. Subsequently, we examine the performance of S-DEIM on the test data, comprising January 2022 to January 2023. For this test data, S-DEIM infers the high-resolution SST from 100 in situ observations, constituting only 0.2% of the high-resolution spatial grid. We show that the resulting S-DEIM reconstructions are about 40% more accurate than earlier empirical interpolation methods, such as DEIM and Q-DEIM. Furthermore, 91% of S-DEIM estimates fall within $\pm 1^\circ$C of the true SST. We also demonstrate that S-DEIM is robust with respect to sensor placement: even when the sensors are distributed randomly, S-DEIM reconstruction error deteriorates only by 1-2%. S-DEIM is also computationally efficient. Training the RNN, which is performed only once offline, takes approximately one minute. Once trained, the S-DEIM reconstructions are computed in less than a second. As such, S-DEIM can be used for rapid SST reconstruction from sparse streaming observational data in real time.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [38] [Navier-Stokes with a fractional transport noise as a limit of multi-scale dynamics](https://arxiv.org/abs/2601.21762)
*Xue-Mei Li,Szymon Sobczak*

Main category: math.PR

TL;DR: Rough path solutions for Navier-Stokes equations with rough transport, connecting to slow/fast systems and showing equivalence with incremental solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous mathematical framework for solving Navier-Stokes equations with rough transport terms, particularly relevant for SPDEs driven by fractional Brownian motion, and to establish connections between different solution concepts.

Method: Define bona fide rough path solutions for Navier-Stokes with rough transport, analyze SPDE on 3D torus driven by fractional Brownian motion, show solutions as effective limits of slow/fast systems, and prove equivalence with incremental solutions (unbounded rough driver formulation).

Result: Established existence of rough path solutions for Navier-Stokes with rough transport, demonstrated these solutions arise as limits of slow/fast systems, and proved equivalence between rough path solutions and incremental solutions, showing broader applicability to nonlinear SPDEs.

Conclusion: The paper provides a unified framework for rough path solutions to Navier-Stokes equations with rough transport, connecting multiple solution concepts and demonstrating applicability to a wide class of nonlinear SPDEs driven by rough noise.

Abstract: We define a bona fide rough path solution for the Navier-Stokes equation with an additional rough transport term, and show that the SPDE on the three-dimensional torus driven by a fractional Brownian motion on $H^œÉ$ has solutions characterised as the effective limits of a slow/fast system. We further show that this rough path solution is equivalent to the widely used incremental notion of solution (the unbounded rough driver formulation), demonstrating broader applicability to other nonlinear SPDEs.

</details>


### [39] [Ergodicity for SPDEs driven by divergence-free transport noise](https://arxiv.org/abs/2601.22056)
*Benjamin Gess,Rishabh S. Gvalani,Adrian Martini*

Main category: math.PR

TL;DR: Strong mixing noise enforces unique invariant measures in McKean-Vlasov equations, overcoming multiple deterministic steady states.


<details>
  <summary>Details</summary>
Motivation: To understand how stochastic perturbations, specifically common divergence-free transport noise, affect the ergodic behavior and invariant measures of McKean-Vlasov equations, particularly when deterministic dynamics have multiple steady states.

Method: Study McKean-Vlasov equations with common divergence-free transport noise, analyze ergodic behavior in dimensions d‚â•2, examine conditions where sufficiently strong mixing noise can enforce uniqueness of invariant probability measures.

Result: In dimension d‚â•2, sufficiently strong mixing noise can enforce uniqueness of invariant probability measures, even when the deterministic part of the equation has multiple steady states.

Conclusion: Strong mixing transport noise serves as a regularization mechanism that can overcome non-uniqueness issues in deterministic McKean-Vlasov equations, ensuring unique ergodic behavior.

Abstract: We study the ergodic behaviour of the McKean-Vlasov equations driven by common, divergence-free transport noise. In particular, we show that in dimension $d\geq 2$, if the noise is mixing and sufficiently strong it can enforce the uniqueness of invariant probability measures, even if the deterministic part of equation has multiple steady states.

</details>


### [40] [Superdiffusion and anomalous regularization in self-similar random incompressible flows](https://arxiv.org/abs/2601.22142)
*Scott Armstrong,Ahmed Bou-Rabee,Tuomo Kuusi*

Main category: math.PR

TL;DR: The paper proves quenched power-law superdiffusion for particles in random incompressible flows with multiscale self-similar structure, showing displacement variance grows as t^{2/(2-Œ≥)} and identifying the leading prefactor with controlled error.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of particles subject to molecular diffusion and advection by random incompressible flows, particularly in multiscale self-similar environments with positive Hurst exponent Œ≥, where renormalization group heuristics predict anomalous superdiffusive scaling.

Method: Implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator ‚àá¬∑(ŒΩI_d + k)‚àá, using self-similar induction across scales. The approach demonstrates that coarse-grained generators are well-approximated by constant-coefficient Laplacians with effective diffusivity growing like r^Œ≥, with scale-local error control.

Result: Proves quenched power-law superdiffusion: for typical realizations, displacement variance grows as t^{2/(2-Œ≥)} with leading prefactor identified up to random relative error of order Œ≥^{¬Ω}|log Œ≥|¬≥. Also proves anomalous regularization: solutions of associated elliptic equations are H√∂lder continuous with exponent 1 - CŒ≥^{¬Ω} with uniform estimates.

Conclusion: The work establishes rigorous mathematical foundations for anomalous superdiffusion in multiscale random flows, confirming renormalization group predictions and providing precise error bounds. The results reveal the multifractal nature of the environment and demonstrate scale-local approximation properties of the renormalization scheme.

Abstract: We study the long-time behavior of a particle in $\mathbb{R}^d$, $d \geq 2$, subject to molecular diffusion and advection by a random incompressible flow. The velocity field is the divergence of a stationary random stream matrix $\mathbf{k} $ with positive Hurst exponent $Œ≥> 0$, so the resulting random environment is multiscale and self-similar. In the perturbative regime $Œ≥\ll 1$, we prove quenched power-law superdiffusion: for a typical realization of the environment, the displacement variance at time $t$ grows like $t^{2/(2-Œ≥)}$, the scaling predicted by renormalization group heuristics. We also identify the leading prefactor up to a random (quenched) relative error of order $Œ≥^{\frac12}\left| \log Œ≥\right|^3$. The proof implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator $\nabla \cdot (ŒΩI_d + \mathbf{k} ) \nabla$, based on a self-similar induction across scales. We demonstrate that the coarse-grained generator is well-approximated, at each scale $r$, by a constant-coefficient Laplacian with effective diffusivity growing like $r^Œ≥$. This approximation is inherently scale-local: reflecting the multifractal nature of the environment, the relative error does not decay with the scale, but remains of order $Œ≥^{\frac12}\left| \log Œ≥\right|^2$. We also prove anomalous regularization under the quenched law: for almost every realization of the drift, solutions of the associated elliptic equation are H√∂lder continuous with exponent $1 - CŒ≥^{\frac12}$ and satisfy estimates which are uniform in the molecular diffusivity $ŒΩ$ and the scale.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [41] [Scattering laws for interfaces in self-gravitating matter flows](https://arxiv.org/abs/2601.21773)
*Bruno Le Floch,Philippe G. LeFloch*

Main category: gr-qc

TL;DR: The paper develops a framework connecting phase transition dynamics with bouncing cosmology using scattering maps on singularity and discontinuity hypersurfaces to evolve self-gravitating matter fields through phase transitions.


<details>
  <summary>Details</summary>
Motivation: To understand how self-gravitating matter fields evolve through phase transitions in cosmology, connecting phase transition dynamics with bouncing cosmology concepts, and developing a complete macroscopic description of such evolution.

Method: Introduces scattering maps on two classes of hypersurfaces: gravitational singularity hypersurface and fluid-discontinuity hypersurface. Analyzes causal structures from light and acoustic cones to formulate local evolution for Einstein-Euler system with interfaces. Uses scattering relations to supplement field equations for uniqueness.

Result: Develops a framework yielding rigid universal relations plus model-dependent parameters. Under requirements of general covariance, causality, constraint compatibility, and ultra-locality, aims to classify admissible scattering relations from microscopic physics.

Conclusion: The approach extends previous work on quiescent singularities to fluid systems, providing a systematic way to describe evolution through phase transitions in self-gravitating matter using scattering maps compatible with Einstein-Euler equations.

Abstract: We consider the evolution of self-gravitating matter fields that may undergo phase transitions, and we connect ideas from phase transition dynamics with concepts from bouncing cosmology. Our framework introduces scattering maps prescribed on two classes of hypersurfaces: a gravitational singularity hypersurface and a fluid-discontinuity hypersurface. By analyzing the causal structures induced by the light cone and the acoustic cone, we formulate a local evolution problem for the Einstein-Euler system in the presence of such interfaces. We explain how suitable scattering relations must supplement the field equations in order to ensure uniqueness and thus yield a complete macroscopic description of the evolution. This viewpoint builds on a theory developed in collaboration with G. Veneziano for quiescent (velocity-dominated) singularities in solutions of the Einstein equations coupled to a scalar field, where the passage across the singular hypersurface is encoded by a singularity scattering map. The guiding question is to identify junction prescriptions that are compatible with the Einstein and Euler equations, in particular with the propagation of constraints. The outcome is a rigid set of universal relations, together with a family of model-dependent parameters. Under physically motivated requirements (general covariance, causality, constraint compatibility, and ultra-locality), we aim to classify admissible scattering relations arising from microscopic physics and characterizing, at the macroscopic level, the dynamics of a fluid coupled to Einstein gravity.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [42] [Phase analysis of Ising machines and their implications on optimization](https://arxiv.org/abs/2507.08533)
*Shu Zhou,K. Y. Michael Wong,Juntao Wang,David Shui Wing Hui,Daniel Ebler,Jie Sun*

Main category: cond-mat.stat-mech

TL;DR: Analysis shows optimal Ising machine design requires binary spin distribution phase coexisting with gapless phase, achieved through digitization operations like digCIM algorithm.


<details>
  <summary>Details</summary>
Motivation: Ising machines offer computational advantages for combinatorial optimization but suffer from heuristic parameter tuning and lack systematic design principles, limiting solution quality.

Method: Analyze phase diagrams of spin distributions in Sherrington-Kirkpatrick model to identify optimal operating conditions, then design digitization operations to expand coexistence phase region.

Result: Ground states achieved in binary spin distribution phase; optimal solutions produced where binary phase and gapless phase coexist; digitization operations expand coexistence region.

Conclusion: Systematic Ising machine design possible by targeting binary-gapless phase coexistence, demonstrated by digCIM algorithm that outperforms heuristic approaches.

Abstract: Ising machines, which are dynamical systems designed to operate in a parallel and iterative manner, have emerged as a new paradigm for solving combinatorial optimization problems. Despite computational advantages, the quality of solutions depends heavily on the form of dynamics and tuning of parameters, which are in general set heuristically due to the lack of systematic insights. Here, we focus on optimal Ising machine design by analyzing phase diagrams of spin distributions in the Sherrington-Kirkpatrick model. We find that that the ground state can be achieved in the phase where the spin distribution becomes binary, and optimal solutions are produced where the binary phase and gapless phase coexist. Our analysis shows that such coexistence phase region can be expanded by carefully placing a digitization operation, giving rise to a family of superior Ising machines, as illustrated by the proposed algorithm digCIM.

</details>


### [43] [High-precision Dynamic Monte Carlo Study of Rigidity Percolation](https://arxiv.org/abs/2601.21399)
*Mingzhong Lu,Yufeng Song,Qiyuan Shi,Ming Li,Youjin Deng*

Main category: cond-mat.stat-mech

TL;DR: Dynamic rigidity percolation on triangular lattice reveals temporal self-similarity and cascade events, with improved critical exponents.


<details>
  <summary>Details</summary>
Motivation: While static properties of rigidity percolation on triangular lattices are well-studied, the dynamics of the rigidity transition remain less explored, particularly how rigid clusters emerge and evolve during bond addition.

Method: Developed a dynamic pebble game algorithm that monitors rigid cluster evolution as bonds are sequentially added to an empty lattice, with computational efficiency comparable to standard static pebble game. Used event-based ensemble approach for high-precision measurements.

Result: Discovered temporal self-similarity in cluster dynamics, identified large-scale cascade events where single bond addition triggers merger of extensive number of clusters scaling with system size. Obtained high-precision critical parameters: p_c = 0.6602778(10), 1/ŒΩ = 0.850(3), d_f = 1.850(2).

Conclusion: The dynamic approach reveals previously overlooked temporal self-similarity in rigidity percolation and provides substantially improved estimates of critical exponents, advancing understanding of mechanical stability onset in disordered materials.

Abstract: Rigidity percolation provides an important basis for understanding the onset of mechanical stability in disordered materials. While most studies on the triangular lattice have focused on static properties at fixed bond~(site) occupation probabilities, the dynamics of the rigidity transition remain less explored. In this work, we formulate a dynamic pebble game algorithm that monitors how rigid clusters emerge and evolve as bonds are added sequentially to an empty lattice, with computational efficiency comparable to the standard static pebble game. We uncover a previously overlooked temporal self-similarity exhibited in multiple quantities, including the cluster size changes and merged cluster sizes during bond addition, as well as the number of simultaneously merging clusters. We identify large-scale cascade events in which a single bond addition triggers the merger of an extensive number of clusters that scales with system size with inverse correlation-length exponent. Using an event-based ensemble approach, we obtain high-precision estimates of the critical point $p_c = 0.660\,277\,8(10)$, the inverse correlation-length exponent $1/ŒΩ= 0.850(3)$, and the fractal dimension $d_f = 1.850(2)$, representing substantial improvements over existing values.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [44] [Solving the Offline and Online Min-Max Problem of Non-smooth Submodular-Concave Functions: A Zeroth-Order Approach](https://arxiv.org/abs/2601.21243)
*Amir Ali Farzin,Yuen-Man Pun,Philipp Braun,Tyler Summers,Iman Shames*

Main category: math.OC

TL;DR: A zeroth-order method for solving max-min and min-max problems with non-smooth submodular-concave objectives, using Lov√°sz extension subgradients and Gaussian smoothing, with convergence guarantees to Œµ-saddle points.


<details>
  <summary>Details</summary>
Motivation: To develop efficient algorithms for solving complex optimization problems where the objective function is non-smooth, submodular with respect to the minimizer, and concave with respect to the maximizer, which arise in various applications like robust optimization and game theory.

Method: A zeroth-order method combining two techniques: 1) using subgradients of the Lov√°sz extension for the submodular part (minimizer), and 2) Gaussian smoothing to estimate gradients for the concave part (maximizer). The algorithm handles both offline and online settings.

Result: Theoretically proven convergence to Œµ-saddle points in expectation for offline case, and O(‚àö(N¬∑PÃÑ_N)) online duality gap in expectation, where N is iterations and PÃÑ_N is path length of optimal decisions. Complexity analysis and hyperparameter selection provided, with numerical validation.

Conclusion: The proposed zeroth-order method effectively solves challenging max-min/min-max problems with non-smooth submodular-concave objectives, providing theoretical guarantees for both offline and online settings, with practical implementation guidance and numerical verification.

Abstract: We consider max-min and min-max problems with objective functions that are possibly non-smooth, submodular with respect to the minimiser and concave with respect to the maximiser. We investigate the performance of a zeroth-order method applied to this problem. The method is based on the subgradient of the Lov√°sz extension of the objective function with respect to the minimiser and based on Gaussian smoothing to estimate the smoothed function gradient with respect to the maximiser. In expectation sense, we prove the convergence of the algorithm to an $Œµ$-saddle point in the offline case. Moreover, we show that, in the expectation sense, in the online setting, the algorithm achieves $O(\sqrt{N\bar{P}_N})$ online duality gap, where $N$ is the number of iterations and $\bar{P}_N$ is the path length of the sequence of optimal decisions. The complexity analysis and hyperparameter selection are presented for all the cases. The theoretical results are illustrated via numerical examples.

</details>


### [45] [On Approximate Computation of Critical Points](https://arxiv.org/abs/2601.21917)
*Amir Ali Ahmadi,Georgina Hall*

Main category: math.OC

TL;DR: Computing even very coarse approximations of critical points is NP-hard for simple nonconvex polynomial functions, challenging the belief that approximate critical point computation is tractable in nonconvex optimization.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the commonly-held belief in nonconvex optimization that approximate computation of critical points is a tractable task, aiming to establish theoretical hardness results for this fundamental problem.

Method: The authors prove computational hardness results through complexity theory arguments, showing that if a polynomial-time algorithm could compute coarse approximations of critical points for constant-degree polynomials, then P=NP. They also establish hardness results under additional structural assumptions including guaranteed existence/uniqueness of critical points, lower-bounded functions, and distance-based approximation measures.

Result: The main result shows that computing approximations with gradient norm at most 2^n for polynomials of degree as low as three is NP-hard. Even with additional favorable conditions (existence/uniqueness guarantees, lower-bounded functions), approximate critical point computation remains computationally intractable.

Conclusion: Approximate computation of critical points for nonconvex functions is fundamentally hard, contradicting the widespread assumption in optimization that this task is tractable. These results establish theoretical limits on what can be efficiently computed in nonconvex optimization.

Abstract: We show that computing even very coarse approximations of critical points is intractable for simple classes of nonconvex functions. More concretely, we prove that if there exists a polynomial-time algorithm that takes as input a polynomial in $n$ variables of constant degree (as low as three) and outputs a point whose gradient has Euclidean norm at most $2^n$ whenever the polynomial has a critical point, then P=NP. The algorithm is permitted to return an arbitrary point when no critical point exists. We also prove hardness results for approximate computation of critical points under additional structural assumptions, including settings in which existence and uniqueness of a critical point are guaranteed, the function is lower bounded, and approximation is measured in terms of distance to a critical point. Overall, our results stand in contrast to the commonly-held belief that, in nonconvex optimization, approximate computation of critical points is a tractable task.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [46] [Accelerated Inorganic Electrides Discovery by Generative Models and Hierarchical Screening](https://arxiv.org/abs/2601.21077)
*Shuo Tao,Qiang Zhu*

Main category: cond-mat.mtrl-sci

TL;DR: Accelerated discovery framework identifies 264 new electron-rich compounds including 13 stable electrides from systematic exploration of binary/ternary chemical spaces.


<details>
  <summary>Details</summary>
Motivation: Electrides have exceptional properties but are challenging to discover due to difficulty in achieving energetically favorable electron localization in crystal cavities.

Method: Combines physical principles, diffusion-based materials generation with hierarchical thermodynamic and electronic structure screening. Systematically explored 1,510 binary and 6,654 ternary compositions containing excess valence electrons, filtered with high-throughput validation on stability and electronic structure.

Result: Identified 264 new electron-rich compounds within 0.05 eV/atom above convex hull at DFT level, including 13 thermodynamically stable electrides.

Conclusion: Demonstrates a generalizable strategy for targeted materials discovery in vast chemical spaces.

Abstract: Electrides are exotic compounds in which excess electrons occupy interstitial regions of the crystal lattice and serve as anions, exhibiting exceptional properties such as low work function, high electron mobility, and strong catalytic activity. Although they show promise for diverse applications, identifying new electrides remains challenging due to the difficulty of achieving energetically favorable electron localization in crystal cavities. Here, we present an accelerated materials discovery framework that combines physical principles, diffusion-based materials generation with hierarchical thermodynamic and electronic structure screening. Using this workflow, we systematically explored 1,510 binary and 6,654 ternary chemical compositions containing excess valence electrons from electropositive alkaline, alkaline-earth, and early transition metals, and then filtered them with a high throughput validation on both thermodynamical stability and electronic structure analysis. As a result, we have identified 264 new electron rich compounds within 0.05 eV/atom above the convex hull at the density functional theory (DFT) level, including 13 thermodynamically stable electrides. Our approach demonstrates a generalizable strategy for targeted materials discovery in a vast chemical space.

</details>


### [47] [Dynamically training machine-learning-based force fields for strongly anharmonic materials](https://arxiv.org/abs/2601.21311)
*Martin Callsen,Tai-Ting Lee,Mei-Yin Chou*

Main category: cond-mat.mtrl-sci

TL;DR: Dynamic training framework for ML force fields using Bayesian error estimation to guide adaptive data acquisition during simulations, improving robustness and transferability across materials with varying anharmonicity.


<details>
  <summary>Details</summary>
Motivation: ML force fields offer accuracy comparable to ab initio MD at lower cost, but their reliability depends on training data quality. Static datasets often fail when encountering unseen configurations during MD simulations, necessitating more robust training approaches.

Method: Dynamic training framework building on conventional lattice dynamics expansion of total energy, incorporating Bayesian error estimation to guide adaptive data acquisition during simulation. Uses trajectory-averaged Bayesian errors for efficient configuration space exploration.

Result: Demonstrated effectiveness across materials with varying anharmonicity (c-BAs, Si, SnSe). Bayesian error estimation enables targeted exploration of configuration space, enhancing force field robustness and transferability. Also allows convergence determination without additional ab initio data.

Conclusion: Proposed framework offers practical, easily implementable scheme to improve ML force field training process - the most critical step in developing reliable force fields for materials simulations.

Abstract: Machine learning (ML) force fields have emerged as a powerful tool for computing materials properties at finite temperatures, particularly in regimes where traditional phonon-based perturbation theories fail or cannot be extended beyond the harmonic approximation. These approaches offer accuracy comparable to ab initio molecular dynamics (MD), but at a fraction of the computational cost. However, their reliability critically depends on the quality and representativeness of the training data. In particular, static training datasets often lead to failure when the force field encounters previously unseen atomic configurations during MD simulations. In this work, we present a framework for dynamically training ML force fields and demonstrate its effectiveness across materials with varying degrees of anharmonicity, including cubic boron arsenide (c-BAs), silicon (Si), and tin selenide (SnSe). Our method builds on the conventional lattice dynamics expansion of total energy and incorporates Bayesian error estimation to guide adaptive data acquisition during simulation. Specifically, we show that trajectory-averaged Bayesian errors enable efficient and targeted exploration of the configuration space, significantly enhancing the robustness and transferability of the resulting force fields. We further demonstrate how Bayesian error estimation can be applied to determine the convergence of the dynamic training without requiring additional ab initio data. This proposed framework offers a practical and easily implementable scheme to improve the training process, which is the most critical step in developing reliable ML force fields.

</details>


### [48] [Model density approach to Ewald summations](https://arxiv.org/abs/2601.21776)
*Chiara Ribaldone,Jacques Kontak Desmarais*

Main category: cond-mat.mtrl-sci

TL;DR: A method using model charge densities to cancel multipole moments for accelerating Ewald sum convergence in electrostatic potential calculations for condensed phase systems.


<details>
  <summary>Details</summary>
Motivation: The electrostatic potential evaluation is fundamental for studying condensed phase systems, but traditional Ewald summation techniques can have slow convergence. The paper aims to develop a more efficient approach to accelerate these calculations.

Method: Introduces a model charge density that cancels multipole moments of the crystalline charge distribution up to a desired order, which accelerates convergence of Ewald sums. The method is applicable to bulk systems with arbitrary unit cells, works in both classical and quantum contexts, and can use arbitrary basis functions for charge density representation.

Result: The approach provides an efficient technique for accelerating Ewald sum convergence and clarifies a decades-old implementation in the CRYSTAL code, making it applicable to a wide range of condensed phase systems.

Conclusion: The method offers a general and efficient approach for electrostatic potential calculations in condensed phase systems by accelerating Ewald sum convergence through multipole moment cancellation, with broad applicability across different computational contexts.

Abstract: The evaluation of the electrostatic potential is fundamental to the study of condensed phase systems. We discuss the calculation of the relevant lattice summations by Ewald-type techniques. A model charge density is introduced, that cancels multipole moments of the crystalline charge distribution up to a desired order, for accelerating convergence of the Ewald sums. The method is applicable to calculations of bulk systems, employing arbitrary unit cells in a classical or quantum context, and with arbitrary basis functions to represent the charge density. The approach clarifies a decades-old implementation in the CRYSTAL code.

</details>


### [49] [MEIDNet: Multimodal generative AI framework for inverse materials design](https://arxiv.org/abs/2601.22009)
*Anand Babu,Rog√©rio Almeida Gouv√™a,Pierre Vandergheynst,Gian-Marco Rignanese*

Main category: cond-mat.mtrl-sci

TL;DR: MEIDNet is a multimodal framework that uses contrastive learning and equivariant graph neural networks to jointly learn structural information and materials properties for accelerated inverse design of materials.


<details>
  <summary>Details</summary>
Motivation: To accelerate the exploration of chemical-structural space and facilitate discovery of materials that satisfy predefined property targets by combining generative inverse design with multimodal learning.

Method: Uses Multimodal Equivariant Inverse Design Network (MEIDNet) with contrastive learning, equivariant graph neural networks (EGNN) for structure encoding, cross-modal learning to fuse three modalities, and curriculum learning strategies.

Result: Achieves strong latent-space alignment (cosine similarity 0.96), ~60x higher learning efficiency than conventional training, and generates low-bandgap perovskite structures at 13.6% SUN (stable, unique, novel) rate validated by ab initio methods.

Conclusion: MEIDNet demonstrates scalability and adaptability, paving the way for universal learning of chemical space across diverse modalities through multimodal inverse design.

Abstract: In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [50] [Wave generation via oscillatory reconnection at a three-dimensional magnetic null point](https://arxiv.org/abs/2601.21520)
*Luiz A. C. A. Schiavo,Gert J. J. Botha,James A. McLaughlin*

Main category: astro-ph.SR

TL;DR: 3D MHD simulation shows oscillatory reconnection at a magnetic null point generates both slow magnetoacoustic waves propagating in all directions and Alfv√©n waves propagating perpendicular to spine motion in the fan plane.


<details>
  <summary>Details</summary>
Motivation: To investigate wave generation and time-dependent reconnection around a 3D magnetic null point, particularly understanding the wave behavior resulting from oscillatory reconnection and its implications for coronal seismology.

Method: Three-dimensional nonlinear magnetohydrodynamic (MHD) simulation with non-periodic perturbation in the xz-plane triggering oscillatory reconnection at a 3D null point. Used three distinct wave proxies (compressible parallel, compressible transverse, incompressible parallel) and Spectral Proper Orthogonal Decomposition to analyze MHD wave behavior.

Result: Oscillatory reconnection generates a slow magnetoacoustic wave with period P that propagates outward in all directions along both the spine and fan plane. Also generates a propagating Alfv√©n wave with period P exclusively along the y-axis in the fan plane (perpendicular to spine motion).

Conclusion: The findings provide new insights into waves generated from 3D null points, revealing distinct propagation patterns for different wave modes, which has important implications for coronal seismology and understanding wave behavior in magnetic null point configurations.

Abstract: This work conducts a three-dimensional (3D), nonlinear magnetohydrodynamic (MHD) simulation to investigate wave generating, time-dependent reconnection around a magnetic null point. A non-periodic perturbation (in the $xz$-plane) triggers oscillatory reconnection (OR) at the 3D null, resulting in a self-sustained oscillation with a constant period $P$. We investigate the response of the system using three distinct wave proxies (compressible parallel, compressible transverse and incompressible parallel) as well as Spectral Proper Orthogonal Decomposition for decoupling and analyzing the resultant MHD wave behavior. We find that OR generates a slow magnetoacoustic wave of period $P$ that propagates outwards in all directions along the spine and fan plane of the 3D null point. We also find the generation of a propagating Alfv√©n wave of period $P$, exclusively along the $y$-axis in the fan plane, i.e. in the direction perpendicular to the spine motion. These findings provide new insights into waves generated from a 3D null point and their implications for coronal seismology.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [51] [Weighted Sobolev Spaces and Distributional Spectral Theory for Generalized Aging Operators via Transmutation Methods](https://arxiv.org/abs/2601.21497)
*Gustavo Dorrego*

Main category: math.FA

TL;DR: Develops a distributional theory for Weighted Weyl-Sonine operators using transmutation, constructing weighted Schwartz spaces and tempered distributions to analyze spectral properties in heterogeneous/aging media.


<details>
  <summary>Details</summary>
Motivation: Spectral analysis in heterogeneous and aging media requires functional frameworks beyond standard Hilbert spaces. Need rigorous distributional theory for non-local operators to handle complex media dynamics.

Method: Uses structure-preserving transmutation method to construct Weighted Schwartz Space S_{œà,œâ} and its dual S'_{œà,œâ} with Fr√©chet topology consistent with aging dynamics generator. Extends Weighted Fourier Transform, characterizes weighted Dirac delta, introduces Weighted Sobolev Spaces H^{s}_{œà,œâ} via spectral multipliers.

Result: Derives sharp embedding theorem: |u(t)| ‚â§ C œâ(t)^{-1} ||u||_{H^s_{œà,œâ}}, connecting spectral energy to pointwise decay. Provides unified geometric characterization of fractional regimes (Hadamard, Riemann-Liouville) within single operator-theoretic framework.

Conclusion: Establishes rigorous distributional theory for Weighted Weyl-Sonine operators, creating functional framework for spectral analysis in heterogeneous/aging media with applications to various fractional calculus regimes.

Abstract: The spectral analysis of operators in heterogeneous and aging media typically requires a functional framework that extends beyond the standard Hilbertian setting. In this paper, we establish a rigorous distributional theory for a class of non-local operators, termed Weighted Weyl-Sonine operators, by employing a structure-preserving transmutation method. We construct the Weighted Schwartz Space $\mathcal{S}_{œà,œâ}$ and its topological dual, the space of Weighted Tempered Distributions $\mathcal{S}'_{œà,œâ}$, ensuring that the underlying Fr√©chet topology is consistent with the infinitesimal generator of the aging dynamics. This topological foundation allows us to: (i) extend the Weighted Fourier Transform to generalized functions as a unitary isomorphism; (ii) provide an explicit spectral characterization of the weighted Dirac delta $Œ¥_{œà,œâ}$ and its scaling laws under geometric dilations; and (iii) introduce a scale of Weighted Sobolev Spaces $H^{s}_{œà,œâ}$ defined via spectral multipliers. A central result is the derivation of a sharp embedding theorem, $|u(t)| \le C œâ(t)^{-1} \|u\|_{H^s_{œà,œâ}}$, which rigorously connects abstract spectral energy to the pointwise decay induced by the weight $œâ$. This framework provides a unified geometric characterization of several fractional regimes, including the Hadamard and Riemann-Liouville cases, within a single operator-theoretic architecture.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Conditional Denoising Model as a Physical Surrogate Model](https://arxiv.org/abs/2601.21021)
*Jos√© Afonso,Pedro Viegas,Rodrigo Ventura,Vasco Guerra*

Main category: cs.LG

TL;DR: CDM is a generative model that learns physical manifold geometry through denoising, achieving better physics adherence than soft-constraint approaches without explicit physics loss.


<details>
  <summary>Details</summary>
Motivation: Current surrogate modeling faces trade-off between accuracy and physical consistency. Physics-consistent approaches either use soft constraints that don't guarantee strict adherence or rely on post-processing that doesn't learn solution geometry.

Method: Introduces Conditional Denoising Model (CDM) that learns physical manifold geometry by training network to restore clean states from noisy ones. Uses time-independent formulation with deterministic fixed-point iteration for inference, projecting noisy approximations onto equilibrium manifold.

Result: CDM achieves higher parameter and data efficiency than physics-consistent baselines on low-temperature plasma physics benchmark. Denoising objective acts as powerful implicit regularizer - despite never seeing governing equations, model adheres to physical constraints more strictly than baselines with explicit physics losses.

Conclusion: CDM provides a novel approach to surrogate modeling that learns physical manifold geometry through denoising, achieving better physics adherence without explicit physics constraints, offering improved efficiency and regularization.

Abstract: Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.

</details>


### [53] [LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models](https://arxiv.org/abs/2601.21623)
*Stanislav Budzinskiy,Marian Gloser,Tolunay Yilmaz,Ying Hong Tham,Yuanyi Lin,Wenyi Fang,Fan Wu,Philipp Petersen*

Main category: cs.LG

TL;DR: Adaptive mixed-precision strategy for transformer inference that selectively recomputes critical components with higher precision while keeping most computations at lower precision, achieving up to 100x accuracy improvements with minimal recomputation.


<details>
  <summary>Details</summary>
Motivation: Mixed-precision computations are essential for efficient AI deployment, especially for large language models. The paper addresses the challenge of maintaining accuracy while using lower-precision computations for transformer inference, focusing on how to strategically allocate higher precision to critical components.

Method: Based on rounding error analysis of function compositions f(g(x)), the method provides an adaptive strategy that identifies and selects a small subset of components in g(x) to compute with higher accuracy, while allowing all other computations to use lower precision. This strategy is applied to different compositions within transformer architectures.

Result: Numerical studies on GPT-2 models show that very low recomputation rates (selecting only a small subset of components for higher precision) can achieve improvements of up to two orders of magnitude (100x) in accuracy compared to uniform lower-precision computation.

Conclusion: The adaptive mixed-precision strategy enables efficient transformer inference with minimal computational overhead while dramatically improving accuracy, making it a practical solution for deploying large language models with reduced precision computations.

Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.

</details>


### [54] [PILD: Physics-Informed Learning via Diffusion](https://arxiv.org/abs/2601.21284)
*Tianyi Zeng,Tianyi Wang,Jiaru Zhang,Zimo Zeng,Feiyang Zhang,Yiming Xu,Sikai Chen,Yajie Zou,Yangyang Wang,Junfeng Jiao,Christian Claudel,Xinbo Chen*

Main category: cs.LG

TL;DR: PILD integrates physics constraints into diffusion models via virtual residual observations and conditional embedding, improving accuracy and generalization across engineering/scientific tasks.


<details>
  <summary>Details</summary>
Motivation: Pure data-driven diffusion models lack physical consistency needed for engineering/scientific applications where physical laws must be followed.

Method: Introduces virtual residual observations from Laplace distribution to supervise generation, plus conditional embedding module to inject physical information into denoising network at multiple layers.

Result: PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines across vehicle trajectories, tire forces, Darcy flow, and plasma dynamics tasks.

Conclusion: PILD provides a concise, modular framework that successfully unifies diffusion modeling with first-principles physical constraints for diverse engineering/scientific problems.

Abstract: Diffusion models have emerged as powerful generative tools for modeling complex data distributions, yet their purely data-driven nature limits applicability in practical engineering and scientific problems where physical laws need to be followed. This paper proposes Physics-Informed Learning via Diffusion (PILD), a framework that unifies diffusion modeling and first-principles physical constraints by introducing a virtual residual observation sampled from a Laplace distribution to supervise generation during training. To further integrate physical laws, a conditional embedding module is incorporated to inject physical information into the denoising network at multiple layers, ensuring consistent guidance throughout the diffusion process. The proposed PILD framework is concise, modular, and broadly applicable to problems governed by ordinary differential equations, partial differential equations, as well as algebraic equations or inequality constraints. Extensive experiments across engineering and scientific tasks including estimating vehicle trajectories, tire forces, Darcy flow and plasma dynamics, demonstrate that our PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.

</details>


### [55] [PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training](https://arxiv.org/abs/2601.22137)
*Shenghao Yang,Zhichao Wang,Oleg Balabanov,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: PRISM is a framework that accelerates iterative matrix function computations by combining adaptive polynomial approximation with randomized sketching, requiring no explicit spectral bounds and automatically adapting to evolving spectra.


<details>
  <summary>Details</summary>
Motivation: Matrix functions like square root, inverse roots, and orthogonalization are crucial for preconditioned gradient methods in neural network training, but existing iterative algorithms need acceleration and better adaptation to modern GPU architectures.

Method: PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via sketched least-squares problems, adapting to the instance with minimal overhead and without requiring explicit spectral bounds.

Result: PRISM accelerates Newton-Schulz-like iterations for matrix square roots and orthogonalization, and empirically accelerates training when integrated into Shampoo and Muon optimizers.

Conclusion: PRISM provides a general framework for accelerating iterative matrix function computations that is well-suited for modern GPU accelerators, requires no explicit spectral bounds, and automatically adapts to evolving spectra, making it practical for neural network training.

Abstract: Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.

</details>
