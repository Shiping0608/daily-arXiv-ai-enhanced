<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 14]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [math.FA](#math.FA) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Solution of Advection Equation with Discontinuous Initial and Boundary Conditions via Physics-Informed Neural Networks](https://arxiv.org/abs/2601.20978)
*Omid Khosravi,Mehdi Tatari*

Main category: math.NA

TL;DR: PINNs with Fourier features, two-stage training, adaptive weighting, median filtering, bounded mapping, and upwind-inspired loss for 1D advection with discontinuities.


<details>
  <summary>Details</summary>
Motivation: Address challenges in using physics-informed neural networks (PINNs) for 1D advection equations with discontinuous initial/boundary conditions, particularly spectral bias and excessive smoothing of discontinuities.

Method: Fourier feature mapping to mitigate spectral bias; two-stage training (Fourier parameters then network weights); adaptive loss weighting; median filtering of spatial data; bounded linear mapping for solution constraints; modified upwind-inspired loss for nonlinear problems.

Result: Improved modeling of discontinuous solutions in 1D advection problems with reduced spectral bias and better preservation of discontinuities compared to standard PINN approaches.

Conclusion: The proposed techniques effectively enhance PINN performance for advection problems with discontinuities by addressing spectral bias, improving training stability, and preserving sharp solution features.

Abstract: In this paper, we investigate several techniques for modeling the one-dimensional advection equation for a specific class of problems with discontinuous initial and boundary conditions using physics-informed neural networks (PINNs). To mitigate the spectral bias phenomenon, we employ a Fourier feature mapping layer as the input representation, adopt a two-stage training strategy in which the Fourier feature parameters and the neural network weights are optimized sequentially, and incorporate adaptive loss weighting. To further enhance the approximation accuracy, a median filter is applied to the spatial data, and the predicted solution is constrained through a bounded linear mapping. Moreover, for certain nonlinear problems, we introduce a modified loss function inspired by the upwind numerical scheme to alleviate the excessive smoothing of discontinuous solutions typically observed in neural network approximations.

</details>


### [2] [Identification of space-dependent coefficients in two competing terms of a nonlinear subdiffusion equation](https://arxiv.org/abs/2601.21018)
*Barbara Kaltenbacher,William Rundell*

Main category: math.NA

TL;DR: Reconstruction of spatially varying coefficients in nonlinear diffusion equations from interior observations using fixed point scheme.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inverse problem of reconstructing spatially varying coefficients (p and q) in nonlinear diffusion equations from limited interior observations. This is important for applications involving Fisher-KPP, Frank-Kamenetskii-Zeldovich, and Allen-Cahn equations where coefficient identification is crucial but challenging with incomplete data.

Method: Devises a fixed point scheme for coefficient reconstruction from interior observations in two scenarios: 1) final time observations under two different excitations, and 2) two different time instances under a single excitation. The method handles nonlinearities of the form pf(u)-qu.

Result: Proves convergence of the fixed point scheme and establishes local uniqueness of the reconstructed coefficients. Numerical experiments demonstrate the practical performance of the reconstruction scheme.

Conclusion: The proposed fixed point scheme successfully reconstructs spatially varying coefficients in nonlinear diffusion equations from limited interior observations, with proven convergence and local uniqueness properties, making it applicable to important physical models like Fisher-KPP and Allen-Cahn equations.

Abstract: We consider a (sub)diffusion equation with a nonlinearity of the form $pf(u)-qu$, where $p$ and $q$ are space dependent functions. Prominent examples are the Fisher-KPP, the Frank-Kamenetskii-Zeldovich and the Allen-Cahn equations. We devise a fixed point scheme for reconstructing the spatially varying coefficients from interior observations a) at final time under two different excitations b) at two different time instances under a single excitation. Convergence of the scheme as well as local uniqueness of these coefficients is proven. Numerical experiments illustrate the performance of the reconstruction scheme.

</details>


### [3] [Parametric Hyperbolic Conservation Laws: A Unified Framework for Conservation, Entropy Stability, and Hyperbolicity](https://arxiv.org/abs/2601.21080)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: SymCLaw is a parametric hyperbolic conservation law framework that learns hyperbolic systems from data while guaranteeing conservation, entropy stability, and hyperbolicity by design.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for learning hyperbolic systems from data typically enforce only conservation or require prior knowledge of governing equations, lacking guarantees for hyperbolicity and entropy stability which are essential for physical consistency and numerical stability.

Method: Parameterizes flux functions to guarantee real eigenvalues and complete eigenvectors of flux Jacobian (preserving hyperbolicity). Jointly learns convex entropy function and associated flux potential to ensure entropy dissipation. Provides entropy-stable numerical flux scheme compatible with standard discretizations.

Result: Demonstrates generalization to unseen initial conditions, maintains stability under noisy training data, and achieves accurate long-time predictions on benchmark problems including Burgers, shallow water, Euler, and KPP equations.

Conclusion: SymCLaw provides a principled foundation for data-driven modeling of hyperbolic conservation laws by ensuring physical consistency through built-in conservation, hyperbolicity, and entropy stability guarantees.

Abstract: We propose a parametric hyperbolic conservation law (SymCLaw) for learning hyperbolic systems directly from data while ensuring conservation, entropy stability, and hyperbolicity by design. Unlike existing approaches that typically enforce only conservation or rely on prior knowledge of the governing equations, our method parameterizes the flux functions in a form that guarantees real eigenvalues and complete eigenvectors of the flux Jacobian, thereby preserving hyperbolicity. At the same time, we embed entropy-stable design principles by jointly learning a convex entropy function and its associated flux potential, ensuring entropy dissipation and the selection of physically admissible weak solutions. A corresponding entropy-stable numerical flux scheme provides compatibility with standard discretizations, allowing seamless integration into classical solvers. Numerical experiments on benchmark problems, including Burgers, shallow water, Euler, and KPP equations, demonstrate that SymCLaw generalizes to unseen initial conditions, maintains stability under noisy training data, and achieves accurate long-time predictions, highlighting its potential as a principled foundation for data-driven modeling of hyperbolic conservation laws.

</details>


### [4] [An efficient implicit scheme for the multimaterial Euler equations in Lagrangian coordinates](https://arxiv.org/abs/2601.21241)
*Simone Chiocchetti,Giovanni Russo*

Main category: math.NA

TL;DR: Implicit Lagrangian method for stratified fluid flows with high density/stiffness ratios to overcome time step restrictions while avoiding interface smearing.


<details>
  <summary>Details</summary>
Motivation: Stratified fluids (fluid metamaterials) with alternating layers have unique macroscopic properties, but numerical simulation faces challenges: explicit Lagrangian schemes have severe time step restrictions, while Eulerian methods suffer from artificial interface smearing, especially problematic for high density/stiffness ratio systems like water-air or air-granular media.

Method: Implicit numerical method for multimaterial Euler equations in Lagrangian coordinates. Uses the structure of Lagrangian equations to formulate a single implicit discrete wave equation for pressure field, yielding symmetric positive definite system for efficiency. Includes filtering strategies to suppress pressure/density oscillations common in multimaterial flows.

Result: Method enables simulation of stratified media with high density and stiffness ratios while bypassing prohibitive time step restrictions. Demonstrates robustness, accuracy, and performance improvements for challenging multimaterial flow problems.

Conclusion: The implicit Lagrangian approach provides an efficient solution for stratified fluid simulations, combining the interface-preserving advantages of Lagrangian methods with the computational efficiency of implicit time integration, making it suitable for practical applications with extreme material property contrasts.

Abstract: Stratified fluids composed of a sequence of alternate layers show interesting macroscopic properties, which may be quite different from those of the individual constituent fluids. On a macroscopic scale, such systems can be considered a sort of fluid metamaterial. In many cases each fluid layer can be described by Euler equations following the stiffened gas equation of state. The computation of detailed numerical solutions of such stratified material poses several challenges, first and foremost the issue of artificial smearing of material parameters across interface boundaries. Lagrangian schemes completely eliminate this issue, but at the cost of rather stringent time step restrictions. In this work we introduce an implicit numerical method for the multimaterial Euler equations in Lagrangian coordinates. The implicit discretization is aimed at bypassing the prohibitive time step restrictions present in flows with stratified media, where one of the materials is particularly dense, or rigid (or both). This is the case for flows of water-air mixtures, air-granular media, or similar high density ratio systems. We will present the novel discretisation approach, which makes extensive use of the remarkable structure of the governing equations in Lagrangian coordinates to find the solution by means of a single implicit discrete wave equation for the pressure field, yielding a symmetric positive definite structure and thus a particularly efficient algorithm. Additionally, we will introduce simple filtering strategies for counteracting the emergence of pressure or density oscillations typically encountered in multimaterial flows, and will present results concerning the robustness, accuracy, and performance of the proposed method, including applications to stratified media with high density and stiffness ratios.

</details>


### [5] [Natural superconvergence points for splines](https://arxiv.org/abs/2601.21368)
*Peng Yang,Zhimin Zhang*

Main category: math.NA

TL;DR: The paper develops a unified theory showing that symmetric points in mesh partitions yield superconvergence for spline approximations of elliptic problems, with patterns depending on polynomial degree and derivative parity.


<details>
  <summary>Details</summary>
Motivation: To systematically identify and characterize superconvergence points in polynomial spline approximations for second-order elliptic problems, moving beyond isolated observations to a comprehensive theoretical framework.

Method: Develops theory starting from 1D case showing superconvergence at symmetric partition centers when polynomial degree k and derivative order s share parity. Uses B-spline solutions to construct asymptotic error expansions. Extends framework to higher dimensions on simplicial and tensor-product meshes, generalizing to mixed derivatives.

Result: Establishes systematic distribution patterns of superconvergence points that are readily attainable even in localized symmetric regions. Numerical experiments confirm persistence of superconvergence and validate theoretical predictions.

Conclusion: Provides a unified theory revealing that superconvergence points follow systematic patterns based on symmetry and parity conditions, offering practical guidance for error estimation and adaptive refinement in spline-based numerical methods.

Abstract: This paper develops a unified theory of natural superconvergence points for polynomial spline approximations to second-order elliptic problems. Beginning with the one-dimensional case, we establish that when a point $x_0$ is a local symmetric center of the partition, the numerical error $(u-u_h)^{(s)}(x_0)$ exhibits superconvergence whenever the polynomial degree $k$ and the derivative order $s$ share the same parity. In particular, for the smoothest spline (B-spline) solution, the abundance of superconvergence points allows us to construct asymptotic expansion of the error within the element that fully characterize all superconvergence points, for both function values and derivatives. The theoretical framework is then extended to higher-dimensional settings on simplicial and tensor-product meshes, and the essential conclusions are preserved, with one-dimensional derivatives generalized to mixed derivatives. Numerical experiments demonstrate that superconvergence persists even in extremely localized symmetric regions, revealing that superconvergence points are both readily attainable and follow systematic distribution patterns.

</details>


### [6] [Higher-Order Finite Difference Methods for the Tempered Fractional Laplacian](https://arxiv.org/abs/2601.21388)
*Mingyi Wang,Dongling Wang*

Main category: math.NA

TL;DR: High-order finite difference schemes for tempered fractional Laplacian using new generating functions achieve 4th, 6th, and 8th-order convergence with efficient Toeplitz matrix computations.


<details>
  <summary>Details</summary>
Motivation: Need efficient high-order numerical methods for tempered fractional Laplacian equations, which arise in various applications but pose computational challenges due to non-local nature and stiffness matrices.

Method: Develop general framework of high-order finite difference schemes using new generating functions from discrete symbols; construct Toeplitz stiffness matrices enabling fast matrix-vector multiplication via fast algorithms.

Result: Achieve high-order convergence (p=4,6,8) for sufficiently smooth functions; numerical simulations confirm effectiveness and match theoretical predictions.

Conclusion: Proposed HFD methods provide efficient, high-order accurate numerical solutions for tempered fractional Laplacian equations with rigorous stability and convergence guarantees.

Abstract: This paper presents a general framework of high-order finite difference (HFD) schemes for the tempered fractional Laplacian (TFL) based on new generating functions obtained from the discrete symbols. Specifically, for sufficiently smooth functions, the resulting discretizations achieve high-order convergence with orders $p=4, 6, 8$. The discrete operators lead to Toeplitz stiffness matrices, allowing efficient matrix-vector multiplications via fast algorithms. Building on these approximations, HFD methods are formulated for solving TFL equations, and their stability and convergence are rigorously analyzed. Numerical simulations confirm the effectiveness of the proposed methods, showing excellent agreement with the theoretical predictions.

</details>


### [7] [Numerical Methods for Dynamical Low-Rank Approximations of Stochastic Differential Equations -- Part I: Time discretization](https://arxiv.org/abs/2601.21428)
*Yoshihito Kazashi,Fabio Nobile,Fabio Zoccolan*

Main category: math.NA

TL;DR: This paper analyzes three time-discretization methods for Dynamical Low-Rank Approximation of high-dimensional SDEs, focusing on stability and convergence properties.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop and analyze numerical time-integration strategies for the Dynamically Orthogonal (DO) method in DLRA of high-dimensional stochastic differential equations, addressing stability issues and time-step restrictions that arise in standard discretization approaches.

Method: The paper studies three time-discretization procedures: 1) Standard forward discretization of both deterministic and stochastic components, 2-3) Two staggered algorithms that alternately update deterministic and stochastic modes in half steps. The analysis focuses on the DO method for DLRA, which uses linear combinations of products between deterministic orthonormal modes and stochastic modes.

Result: The standard forward discretization converges under a time-step restriction dependent on the smallest singular value of the Gram matrix of stochastic modes. The staggered algorithms are more stable and achieve convergence without this time-step restriction. Computational experiments support these theoretical findings.

Conclusion: Staggered time-discretization algorithms provide superior stability compared to standard forward discretization for DLRA of SDEs, eliminating restrictive time-step conditions while maintaining convergence. This work focuses only on time discretization, leaving probability discretization for Part II.

Abstract: In this work (Part I), we study three time-discretization procedures of the Dynamical Low-Rank Approximation (DLRA) of high-dimensional stochastic differential equations (SDEs). Specifically, we consider the Dynamically Orthogonal (DO) method for DLRA proposed and analyzed in arXiv:2308.11581v4, which consists of a linear combination of products between deterministic orthonormal modes and stochastic modes, both time-dependent. The first strategy we consider for numerical time-integration is very standard, consisting in a forward discretization in time of both deterministic and stochastic components. Its convergence is proven subject to a time-step restriction dependent on the smallest singular value of the Gram matrix associated to the stochastic modes. Under the same condition on the time-step, this smallest singular value is shown to be always positive, provided that the SDE under study is driven by a non-degenerate noise. The second and the third algorithms, on the other hand, are staggered ones, in which we alternately update the deterministic and the stochastic modes in half steps. These approaches are shown to be more stable than the first one and allow us to obtain convergence results without the aforementioned restriction on the time-step. Computational experiments support theoretical results. In this work we do not consider the discretization in probability, which will be the topic of Part II.

</details>


### [8] [Numerical analysis of a locking-free primal hybrid method for linear elasticity with $H(\mathrm{div})$-conforming stress recovery](https://arxiv.org/abs/2601.21635)
*Giovanni Taraschi,Maicon Ribeiro Correa*

Main category: math.NA

TL;DR: A hybrid finite element method for linear elasticity using displacement, pressure, and traction Lagrange multiplier, with stable spaces on triangles/quadrilaterals, locking-free for nearly incompressible materials, plus H(div)-conforming stress recovery.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for linear elasticity problems that avoids locking phenomena in nearly incompressible materials, while providing stable discrete approximations and accurate stress recovery.

Method: Primal hybrid finite element method with three-field formulation (displacement, auxiliary pressure, traction Lagrange multiplier). General analysis for discrete solution existence/uniqueness. Construction of stable approximation spaces on triangular and quadrilateral meshes. Element-wise stress recovery strategy.

Result: Method achieves optimal convergence orders, is locking-free for nearly incompressible problems. Stress recovery yields H(div)-conforming, locally equilibrated, weakly symmetric stress field that is robust to locking.

Conclusion: The proposed hybrid finite element method provides a robust, locking-free approach for linear elasticity with optimal convergence and accurate stress recovery, particularly effective for nearly incompressible materials.

Abstract: In this work, we study a primal hybrid finite element method for the approximation of linear elasticity problems, posed in terms of displacement, an auxiliary pressure field, and a Lagrange multiplier related to the traction. We develop a general analysis for the existence and uniqueness of the solution for the discrete problem, which is applied to the construction of stable approximation spaces on triangular and quadrilateral meshes. The use of these spaces lead to optimal convergence orders, resulting in a locking-free method capable of providing robust approximations for nearly incompressible problems. Finally, we propose a strategy for recovering the stress field from the hybrid solution by solving element-wise sub-problems. The resulting stress approximation is $H(\mathrm{div})$-conforming, locally equilibrated, weakly symmetric, and robust to locking.

</details>


### [9] [A Hybrid semi-Lagrangian Flow Mapping Approach for Vlasov Systems: Combining Iterative and Compositional Flow Maps](https://arxiv.org/abs/2601.21668)
*Philipp Krah,Zetao Lin,R. -Paul Wilhelm,Fabio Bacchini,Jean-Christophe Nave,Virginie Grandgirard,Kai Schneider*

Main category: math.NA

TL;DR: Hybrid semi-Lagrangian scheme combining Numerical Flow Iteration (NuFI) and Characteristic Mapping Method (CMM) for Vlasov-Poisson equations, balancing accuracy, conservation, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient Vlasov-Poisson solver that combines the strengths of two existing methods: NuFI's accurate, conservative local time stepping and CMM's efficient global propagation through submap composition, addressing the quadratic time scaling limitation of NuFI while maintaining structural properties.

Method: Hybrid approach using NuFI for accurate and conservative local time stepping, and CMM for efficient propagation through composition of explicitly stored submaps. The method exploits the semi-group property of the underlying diffeomorphic flow to reconstruct solutions through flow maps tracing characteristics back to initial positions.

Result: The hybrid scheme reduces storage requirements, maintains accuracy, and improves structural properties. Numerical experiments demonstrate effectiveness and highlight trade-offs between memory usage and computational cost. Benchmarking against a semi-Lagrangian predictor-corrector scheme shows competitive accuracy and conservation properties.

Conclusion: The proposed hybrid method successfully merges the strengths of NuFI and CMM, offering an efficient Vlasov-Poisson solver with reduced storage, maintained accuracy, and improved structural properties, while providing insights into memory-computation trade-offs.

Abstract: We propose a hybrid semi-Lagrangian scheme for the Vlasov--Poisson equation that combines the Numerical Flow Iteration (NuFI) method with the Characteristic Mapping Method (CMM). Both approaches exploit the semi-group property of the underlying diffeomorphic flow, enabling the reconstruction of solutions through flow maps that trace characteristics back to their initial positions. NuFI builds this flow map iteratively, preserving symplectic structure and conserving invariants, but its computational cost scales quadratically with time. Its advantage lies in a compact, low-dimensional representation depending only on the electric field. In contrast, CMM achieves low computational costs when remapping by composing the global flow map from explicitly stored submaps. The proposed hybrid method merges these strengths: NuFi is employed for accurate and conservative local time stepping, while CMM efficiently propagates the solution through submap composition. This approach reduces storage requirements, maintains accuracy, and improves structural properties. Numerical experiments demonstrate the effectiveness of the scheme and highlight the trade-offs between memory usage and computational cost. We benchmark against a semi-Lagrangian predictor-corrector scheme used in modern gyrokinetic codes, evaluating accuracy and conservation properties.

</details>


### [10] [Adaptive Kernel Methods](https://arxiv.org/abs/2601.21707)
*Tam√°s D√≥zsa,Andrea Angino,Zolt√°n Szab√≥,J√≥zsef Bokor,Matthias Voigt*

Main category: math.NA

TL;DR: Proposes adaptive kernel methods with learnable solution spaces that generalize traditional kernel approaches, enabling efficient large-scale modeling without infinite-dimensional RKHS approximations.


<details>
  <summary>Details</summary>
Motivation: Traditional kernel methods use fixed RKHS solution spaces determined solely by kernel choice and dataset. This limits flexibility and efficiency, especially for large-scale problems where infinite-dimensional RKHS approximations are needed.

Method: Introduces kernel methods with parameter-dependent solution spaces where learnable parameters are independent of dataset. Proposes two approaches: 1) efficient approximation of infinite-dimensional RKHS kernels, and 2) fixed-dimensional parameter-dependent solution spaces that avoid infinite-dimensional approximations entirely.

Result: Develops a novel family of adaptive kernel methods that generalize earlier approaches like Random Fourier Features. Demonstrates effectiveness through numerical experiments, showing suitability for large-scale problems.

Conclusion: The proposed adaptive kernel methods with learnable solution spaces provide more flexible and efficient alternatives to traditional kernel methods, enabling effective large-scale modeling without the computational burden of infinite-dimensional RKHS approximations.

Abstract: Kernel methods approximate nonlinear maps in a data-driven manner by projecting the target map onto a finite-dimensional Hilbert space called the solution space. Traditionally, this space is a subspace of a fixed ambient reproducing kernel Hilbert space (RKHS), determined solely by the chosen kernel and the dataset, whose elements identify the basis elements. Consequently, the projection operator underlying the kernel method depends on the loss function, the dataset, and the choice of ambient RKHS. In this study, we consider kernel methods whose solution spaces also depend on learnable parameters that are independent of the dataset. The resulting methods can be viewed as variable projection operators that depend on the loss function, the dataset, and the new learnable parameters instead of a fixed RKHS. This work has two main contributions. First, we propose an efficient approximation of kernels associated with infinite-dimensional RKHSs, commonly used to reduce the solution-space dimension for large datasets. Second, we construct fixed-dimensional, parameter-dependent solution spaces that enable highly efficient kernel models suitable for large-scale problems without the need to approximate kernels of infinite-dimensional RKHSs. Our novel family of adaptive kernel methods generalizes earlier approaches, including Random Fourier Features, and we demonstrate their effectiveness through several numerical experiments.

</details>


### [11] [A reduced basis method for parabolic PDEs based on a space-time least squares formulation](https://arxiv.org/abs/2601.21736)
*Michael Hinze,Christian Kahle,Michael Stahl*

Main category: math.NA

TL;DR: A POD-greedy reduced basis method for parameter-dependent parabolic PDEs using a least squares space-time formulation with minimal regularity assumptions.


<details>
  <summary>Details</summary>
Motivation: To extend the least squares space-time approach for parabolic PDEs (which assumes minimal regularity) to parameter-dependent cases, enabling efficient reduced-order modeling for parameterized parabolic problems.

Method: Extends the least squares space-time formulation to parameter-dependent parabolic PDEs, resulting in a symmetric, uniformly coercive, and continuous bilinear form. Applies reduced basis method with POD-greedy approach, following techniques for parameterized coercive problems. Includes offline-online decomposition and provides certification with absolute/relative error bounds.

Result: The method demonstrates performance through selected numerical examples, showing it can effectively handle parameter-dependent parabolic PDEs with the proposed reduced basis approach.

Conclusion: The paper successfully extends the least squares space-time formulation to parameter-dependent parabolic PDEs and develops an efficient POD-greedy reduced basis method with error certification, providing a robust framework for reduced-order modeling of such problems.

Abstract: In this work, we present a POD-greedy reduced basis method for parabolic partial differential equations (PDEs), based on the least squares space-time formulation proposed in [Hinze, Kahle, Stahl, A least-squares space-time approach for parabolic equations, 2023, arXiv:2305.03402] that assumes only minimal regularity. We extend this approach to the parameter-dependent case. The corresponding variational formulation then is based on a parameter-dependent, symmetric, uniformly coercive, and continuous bilinear form. We apply the reduced basis method to this formulation, following the well-developed techniques for parameterized coercive problems, as seen e.g. in reduced basis methods for parameterized elliptic PDEs. We present an offline-online decomposition and provide certification with absolute and relative error bounds. The performance of the method is demonstrated using selected numerical examples.

</details>


### [12] [Solving Hamilton-Jacobi equations by minimizing residuals of monotone discretizations](https://arxiv.org/abs/2601.21764)
*Olivier Bokanowski,Carlos Esteve-Yag√ºe,Richard Tsai*

Main category: math.NA

TL;DR: Residual minimization for monotone discretizations of nonlinear PDEs yields well-posed discrete solutions, enabling neural network-based solutions for high-dimensional Hamilton-Jacobi equations.


<details>
  <summary>Details</summary>
Motivation: To solve fully nonlinear Hamilton-Jacobi equations in high dimensions using neural networks trained by minimizing residuals from monotone discretizations, addressing analytical challenges of solvability and uniqueness of local minima.

Method: Derive sufficient conditions for well-posedness of residual minimization with monotone finite-difference discretizations, establishing framework for optimization-based solvers.

Result: Established well-posedness of optimization-based solvers, enabling adaptation of Level Set Methods to high-dimensional settings for applications like segmentation and interface tracking.

Conclusion: The analysis extends to degenerate elliptic/parabolic PDEs on graphs with monotone graph Laplacians, providing theoretical foundation for neural network-based PDE solutions in high dimensions.

Abstract: We derive sufficient conditions under which residual minimization yields well-posed discrete solutions for nonlinear equations defined by monotone finite--difference discretizations. Our analysis is motivated by the challenge of solving fully nonlinear Hamilton--Jacobi (HJ) equations in high dimensions by means of a Neural Network, which is trained by minimizing residuals arising from monotone discretizations of the Hamiltonian. While classical theory ensures that consistency and monotonicity imply convergence to the viscosity solution, treating these discrete systems as optimization problems introduces new analytical hurdles: solvability and the uniqueness of local minima do not follow from monotonicity alone.
  By establishing the well--posedness of these optimization--based solvers, our framework enables the adaptation of Level Set Methods to high--dimensional settings, unlocking new capabilities in applications such as high--dimensional segmentation and interface tracking. Finally, we observe that these arguments extend almost directly to degenerate elliptic or parabolic PDEs on graphs equipped with monotone graph Laplacians.

</details>


### [13] [A novel Krylov subspace method for approximating Fr√©chet derivatives of large-scale matrix functions](https://arxiv.org/abs/2601.21799)
*Daniel Kressner,Peter Oehme*

Main category: math.NA

TL;DR: A novel Krylov subspace method for approximating Fr√©chet derivative matrix-vector products of large-scale matrix functions, avoiding spectral issues of standard approaches.


<details>
  <summary>Details</summary>
Motivation: Need to compute L_f(A,E)b for sensitivity analysis of matrix functions (e.g., network centrality measures) and gradient-based optimization involving matrix functions. Standard approach using block triangular matrix has unfavorable spectral properties.

Method: Modified Arnoldi algorithm that better preserves block triangular structure, allowing convergence analysis bounded by best polynomial approximation of f' on numerical range of A.

Result: Proposed method avoids spectral issues of standard approach, provides theoretical convergence bounds, and is validated through numerical experiments.

Conclusion: The novel Krylov subspace method effectively approximates Fr√©chet derivative matrix-vector products with improved convergence properties compared to standard approaches.

Abstract: We present a novel Krylov subspace method for approximating $L_f(A, E) \vc{b}$, the matrix-vector product of the Fr√©chet derivative $L_f(A, E)$ of a large-scale matrix function $f(A)$ in direction $E$, a task that arises naturally in the sensitivity analysis of quantities involving matrix functions, such as centrality measures for networks. It also arises in the context of gradient-based methods for optimization problems that feature matrix functions, e.g., when fitting an evolution equation to an observed solution trajectory. In principle, the well-known identity \[
  f\left( \begin{bmatrix}
  A & E \\ 0 & A
  \end{bmatrix} \right) \begin{bmatrix}
  0 \\ \vc{b}
  \end{bmatrix} = \begin{bmatrix}
  L_f(A, E) \vc{b} \\ f(A) \vc{b}
  \end{bmatrix}, \] allows one to directly apply any standard Krylov subspace method, such as the Arnoldi algorithm, to address this task. However, this comes with the major disadvantage that the involved block triangular matrix has unfavorable spectral properties, which impede the convergence analysis and, to a certain extent, also the observed convergence. To avoid these difficulties, we propose a novel modification of the Arnoldi algorithm that aims at better preserving the block triangular structure. In turn, this allows one to bound the convergence of the modified method by the best polynomial approximation of the derivative $f^\prime$ on the numerical range of $A$. Several numerical experiments illustrate our findings.

</details>


### [14] [Quotient geometry of tensor ring decomposition](https://arxiv.org/abs/2601.21874)
*Bin Gao,Renfeng Peng,Ya-xiang Yuan*

Main category: math.NA

TL;DR: The paper establishes the quotient geometry of tensor ring decomposition by addressing gauge invariance and full-rank conditions, extending results to uniform TR decomposition.


<details>
  <summary>Details</summary>
Motivation: Tensor ring decomposition has practical success but its intrinsic geometry remains poorly understood due to ring structure and nontrivial gauge invariance, limiting theoretical foundations for numerical methods.

Method: Establishes quotient geometry of TR decomposition by imposing full-rank conditions on all unfolding matrices of core tensors and capturing gauge invariance. Extends results to uniform TR decomposition where all core tensors are identical.

Result: Developed geometries are validated through numerical experiments on tensor ring completion tasks, demonstrating practical applicability.

Conclusion: The paper provides foundational geometric understanding of tensor ring decomposition, addressing gauge invariance challenges and establishing theoretical framework for efficient numerical methods.

Abstract: Differential geometries derived from tensor decompositions have been extensively studied and provided the foundations for a variety of efficient numerical methods. Despite the practical success of the tensor ring (TR) decomposition, its intrinsic geometry remains less understood, primarily due to the underlying ring structure and the resulting nontrivial gauge invariance. We establish the quotient geometry of TR decomposition by imposing full-rank conditions on all unfolding matrices of the core tensors and capturing the gauge invariance. Additionally, the results can be extended to the uniform TR decomposition, where all core tensors are identical. Numerical experiments validate the developed geometries via tensor ring completion tasks.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Global oscillatory solutions for the Yang-Mills heat flow](https://arxiv.org/abs/2601.21017)
*Yannick Sire,Juncheng Wei,Youquan Zheng,Yifu Zhou*

Main category: math.AP

TL;DR: The paper analyzes long-time dynamics of SO(4)-equivariant Yang-Mills heat flow in 4D with SU(2) structure group, showing solutions can exhibit blow-up, blow-down, and oscillatory asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time asymptotic behavior of Yang-Mills heat flow solutions, particularly investigating whether exotic behaviors like oscillations can occur at time infinity, which hasn't been observed before in this context.

Method: Study SO(4)-equivariant Yang-Mills heat flow with SU(2) structure group in 4D space, focusing on initial data with specific decay conditions at spatial infinity, using analytical techniques to prove unified description of long-time dynamics.

Result: Proved that long-time dynamics can be described by initial data in unified manner, leading to three types of asymptotic behavior: blow-up, blow-down, and oscillatory behavior as t‚Üí‚àû - the first example of Yang-Mills heat flows with oscillatory asymptotic behavior.

Conclusion: Yang-Mills heat flows can exhibit rich asymptotic behaviors including exotic oscillatory patterns at time infinity, expanding understanding of long-time dynamics in geometric evolution equations.

Abstract: We investigate the long-time dynamics for the global solution of the $SO(4)$-equivariant Yang-Mills heat flow (YMHF) with structure group $SU(2)$ in space dimension $4$. For a class of initial data with specific decay at spatial infinity, we prove that the long-time dynamics of YMHF can be described by the initial data in a unified manner. As a consequence, the global solutions can exhibit blow-up, blow-down, and more exotically, {\it oscillatory} asymptotic behavior at time infinity. This seems to be the first example of Yang-Mills heat flows with oscillatory behavior as $t\to \infty$.

</details>


### [16] [Decay rates to equilibrium in a nonlinear subdiffusion equation with two counteracting terms](https://arxiv.org/abs/2601.21038)
*Barbara Kaltenbacher*

Main category: math.AP

TL;DR: The paper proves convergence to steady states for subdiffusion equations with exponential or power law rates under mild conditions on coefficients and operators.


<details>
  <summary>Details</summary>
Motivation: To establish long-time behavior (convergence to steady states) for subdiffusion equations with nonlinear reaction terms, which is important for understanding the asymptotic dynamics of such systems in various applications.

Method: Analyzes solutions to the subdiffusion equation with fractional time derivatives (‚àÇ‚Çú·µÖu) using operator theory and functional analysis, considering both exponential (Œ±=1) and power law (Œ±‚àà[0,1)) decay rates.

Result: Proves convergence to steady states as t‚Üí‚àû under mild conditions on coefficients p, q, nonlinearity f, source r, and elliptic operator ùïÉ.

Conclusion: The subdiffusion equation with nonlinear reaction terms converges to equilibrium states for both exponential and power law decay regimes under appropriate conditions.

Abstract: In this paper we prove convergence to a steady state as $t\to\infty$ for solutions to the subdiffusion equation \[ \partial_t^Œ±u - \mathbb{L} u = q(x)u - p(x)f(u) + r \] with the exponential ($Œ±=1$) or power law ($Œ±\in[0,1)$) rates under mild conditions on the coefficients $p$, $q$, the nonlinearity $f$, the source $r$, and the elliptic operator $\mathbb{L}$.

</details>


### [17] [Classical solutions to the Boltzmann equations for gas mixture with unequal molecular masses](https://arxiv.org/abs/2601.21213)
*Gaofeng Wang,Weike Wang,Tianfang Wu*

Main category: math.AP

TL;DR: Global existence of classical solutions for multi-component Boltzmann equations with unequal molecular masses and soft potentials near Maxwellians in periodic domains.


<details>
  <summary>Details</summary>
Motivation: Most research focuses on single-species Boltzmann equations, while gas mixtures with unequal molecular masses (like Earth's atmosphere with N‚ÇÇ:O‚ÇÇ mass ratio 7:8) have limited study despite broader applications.

Method: Analyzes Boltzmann equations for gas mixtures with arbitrary molecular mass ratios (m·¥¨‚â†m·¥Æ), focusing on detailed characterization of linear collision operator structure and establishing estimates for nonlinear terms under unequal mass conditions.

Result: Establishes global in time existence of classical solutions near Maxwellians for soft potentials (-3<Œ≥<0) in periodic spatial domains, applicable to arbitrary molecular mass ratios.

Conclusion: The analysis advances spectral analysis for soft potentials and L¬≤,L^‚àû frameworks for future multi-component Boltzmann equation studies, providing foundation for unequal mass mixture research.

Abstract: The Boltzmann equation is essential for gas thermodynamics,as it models how the molecular density distribution $F(t,x,v)$ changes over time. However, existing research primarily focuses on the single species Boltzmann equation, while investigations into gas mixtures with unequal molecular masses remain relatively limited. Notably, mixed gas studies have broader applications exemplified by Earth's atmosphere, composed of 78\% nitrogen, 21\% oxygen, and 1\% trace gases, where the $N_2$ to $O_2$ molecular mass ratio is 28:32 (simplified as 7:8). This work addresses the Boltzmann equations for such mixtures with unequal molecular masses $(m^A\neq m^B)$, establishing the global in time existence of classical solutions near Maxwellians for soft potentials ($-3<Œ≥<0$) in a periodic spatial domain. Our analysis encompasses arbitrary molecular mass ratios. Our analysis encompasses arbitrary molecular mass ratios. The main contribution of this paper lies in the detailed characterization of the linear collision operator's structure and establishing estimates for the nonlinear terms under unequal mass conditions. Consequently, these results may help advance spectral analysis for soft potentials as well as $L^2,L^{\infty}$ frameworks in future studies of multi-component Boltzmann equations.

</details>


### [18] [Hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixture](https://arxiv.org/abs/2601.21245)
*Yeping Li,Gaofeng Wang,Tianfang Wu*

Main category: math.AP

TL;DR: The paper studies the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixtures using Hilbert expansion, deriving bi-Maxwellian distributions and establishing convergence as Knudsen number tends to zero with validity time estimates depending on potential range.


<details>
  <summary>Details</summary>
Motivation: To rigorously justify the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for gas mixtures with arbitrary particle masses and charges, which introduces asymmetric effects making the system non-decoupled and more challenging to analyze.

Method: Uses Hilbert expansion method, calculates first 2k-1 terms (k‚â•6) of expansion series, constructs new weight function within L¬≤-W¬π,‚àû framework, employs vector-valued functions to handle asymmetric collision operators, and analyzes velocity decay rates to eliminate singularity from small parameters.

Result: Derives bi-Maxwellian distribution determined by Euler-Poisson system of two fluids. Shows solution validity time is O(Œµ‚Åª ∏) where y = -(2k-3)/[2(2k-1)] for -1‚â§Œ≥‚â§1, and y = -(2k-3)/[(1-Œ≥)(2k-1)] for -3<Œ≥<-1.

Conclusion: The results provide rigorous justification of hydrodynamic limit for gas mixtures with strong physical realism, applicable to analyzing gas flow dynamics in the daytime ionosphere at high altitudes above Earth.

Abstract: In this paper, we study the hydrodynamic limit of the Vlasov-Poisson-Boltzmann system for a gas mixture in the whole space $(x \in \mathbb{R}^3)$ with the potential range of $Œ≥\in\left(-3, 1\right]$. Using the method of Hilbert expansion, we first derive a bi-Maxwellian determined by the Euler-Poisson system of two fluids. To justify the convergence of the solution rigorously as the Knudsen number tends to zero, we sequentially calculate the first $2k-1$ terms of the expansion series $(k \geq 6)$, and then truncate it, and express the solution as the sum of these first $2k-1$ terms and a remainder term. Within the framework of the $L_{x,v}^2-W_{x,v}^{1,\infty}$ interplay established by Guo and Jang \cite{[ininp]Guo2010CMP}, we construct a new weight function to estimate the remainder term in four different cases regarding the potential $Œ≥$. Here, the particle masses $m^A, m^B > 0$ and their charges $e^A, e^B$ can be given arbitrarily. This causes the collision operator to exhibit asymmetric effects ($m^A \neq m^B$), rendering the system of equations impossible to decouple. So, it adds difficulties to both $L^2$, $L^{\infty}$ estimates for the remainder. Therefore, we adopt the framework of vector-valued functions and analyze the velocity decay rate of the operator $K_{M,2,w}^{Œ±,c}$ to eliminate the singularity induced by small parameters in characteristic line iterations. Our results show that the validity time of the solution is $O(\varepsilon^{-y})$, where $y$ is $-\frac{2k-3}{2(2k-1)}$ when $-1 \leq Œ≥\leq 1$, and it becomes $-\frac{2k-3}{(1-Œ≥)(2k-1)}$, when $-3 < Œ≥< -1$. These results possess strong physical realism and can be applied to analyze gas flow dynamics in the daytime ionosphere at high altitudes above the Earth.

</details>


### [19] [Blow-up phemomenon for the Geng-Xue system and related models](https://arxiv.org/abs/2601.21295)
*Song Liu,Zhaoyang Yin*

Main category: math.AP

TL;DR: The paper analyzes blow-up criteria and phenomena for the Geng-Xue system with cubic nonlinearity, extending results to a b-family of two-component systems.


<details>
  <summary>Details</summary>
Motivation: To study the Cauchy problem for the Geng-Xue system with cubic nonlinearity, particularly focusing on blow-up behavior and extending analysis to related systems.

Method: First proves blow-up criteria in low Besov spaces, then demonstrates blow-up phenomena using methods that don't require conservation laws, and finally extends results to b-family systems.

Result: Establishes blow-up criteria for the Geng-Xue system, proves blow-up phenomena exist, and shows these results extend to the b-family of two-component systems with cubic nonlinearity.

Conclusion: The Geng-Xue system with cubic nonlinearity exhibits blow-up behavior, and similar blow-up phenomena occur in the broader b-family of two-component systems, with analysis possible without relying on conservation laws.

Abstract: In this paper, we consider the Cauchy problem of the Geng-Xue system with cubic nonlinearity. Firstly, we prove a blow-up criteria in the low besov space. Secondly, we prove the blow-up phenomenon by using the method which does not require any conservation law. Finally, we extend our results to the b-family of two-component system with cubic nonlinearity.

</details>


### [20] [Wellposedness and dynamics of two types of reaction--nonlocal diffusion systems under the inhomogeneous spectral fractional Laplacian](https://arxiv.org/abs/2601.21422)
*Pu Yuan,Paul A. Zegeling*

Main category: math.AP

TL;DR: Study of reaction-nonlocal diffusion equations with fractional Laplacian, analyzing wellposedness, maximum principles, and pattern formation for bistable and Gray-Scott systems.


<details>
  <summary>Details</summary>
Motivation: To understand nonlocal transport and anomalous diffusion beyond Brownian motion, and to develop analytical tools for reaction-nonlocal diffusion equations with boundary constraints.

Method: Use harmonic lifting to handle boundary conditions, analytic contraction semigroup theory for wellposedness, fractional weak maximum principle for invariant-range properties, and sine pseudospectral discretization with ETDRK4 time-stepping for numerical simulations.

Result: Proved local wellposedness, L‚àû-contractivity, positivity preservation, invariant-range property for bistable systems, global boundedness for Gray-Scott systems, and demonstrated fractional order impact on pattern formation.

Conclusion: The semigroup framework provides robust analytical tools for reaction-nonlocal diffusion equations, enabling rigorous analysis of wellposedness, stability, and pattern formation in fractional diffusion systems.

Abstract: Reactio-nonlocal diffusion equations model nonlocal transport and anomalous diffusion by replacing the Laplacian with a fractional power, capturing diffusion mechanisms beyond Brownian motion. We primarily study the semilinear problem \[ \partial_t u + Œµ^2(-Œî)_g^Œ±u = \mathcal{N}(u) \] allowing constant inhomogeneous Dirichlet boundary condition $u|_{\partialŒ©}=g$. To handle the boundary constraint, we use a harmonic lifting to reformulate the problem as an equivalent homogeneous system with a shifted nonlinearity. Working in \(C_0(Œ©)\), analytic contraction semigroup theory yields the Duhamel formula and quantitative smoothing, implying local wellposedness for locally Lipschitz reactions and a blow-up alternative. The semigroup viewpoint also provides $L^\infty$-contractivity and positivity preservation, which drive pointwise maximum principles and stability bounds. Furthermore, we analyze two prototypes. For the bistable RNDE, we derive an energy dissipation identity and, using a fractional weak maximum principle, obtain an invariant-range property that confines solutions between the two stable steady states. For the nonlocal Gray-Scott system with possibly different fractional diffusion orders, we prove that solutions preserve positivity. Moreover, we identify an explicit \(L^\infty\) invariant set ensuring global boundedness, and derive an eigenfunction-weighted interior \(L^2\) bound. Finally, we perform numerical simulations using a sine pseudospectral discretization and ETDRK4 time-stepping, which the impact of fractional orders on pattern formation, consistent with our analytical results.

</details>


### [21] [Multistatic anisotropic travel-time as a tensor tomography problem](https://arxiv.org/abs/2601.21640)
*Naeem Desai,Oliver Graham,William R. B. Lionheart*

Main category: math.AP

TL;DR: The paper analyzes travel-time imaging problems for anisotropic reflectivity reconstruction using multistatic measurements, relating them to tensor ray transforms and discussing implications of null-space properties.


<details>
  <summary>Details</summary>
Motivation: To understand how to reconstruct anisotropic reflectivity images from multistatic travel-time measurements (radar/sonar) where reflectivity depends on direction, and to connect these problems to established mathematical transforms.

Method: Formulates travel-time imaging as generalized Radon transforms over isochrones (ellipses/spheroids), relates simplified far-field case to tensor ray transforms (specifically Sharafutdinov's longitudinal ray transform), and connects volumetric case to normal Radon transform of tensor fields.

Result: Establishes mathematical connections between multistatic travel-time imaging problems and tensor ray transforms, enabling analysis of reconstruction properties including null-space implications for anisotropic reflectivity recovery.

Conclusion: Travel-time imaging for anisotropic reflectivity can be mathematically framed using tensor transforms, with the longitudinal ray transform's null-space having important implications for what can and cannot be reconstructed from multistatic measurements.

Abstract: Travel-time imaging problems seek to reconstruct an image of reflectivity of a scene by measuring travel time (and amplitude, phase) of electromagnetic or acoustic signals, such as radar and sonar. Multistatic, in this context, means that the transmitters and receivers need not be co-located. The reflectivity is anisotropic if it depends on direction, and in the multistatic case this means incoming and outgoing direction. Travel-time problems can be formulated as generalized Radon transforms of integrals over isochrones, in the planar case ellipses with transmitter and receivers at foci. In a simplified case where transmitters and receivers are distant from the scene, isochrones can be approximated by straight lines. We relate this to tensor ray transforms, specifically the longitudinal ray transform of Sharafutdinov, and discuss the implication of its known null-space. In the volumetric case isochrones are spheroids and we relate the problem to the normal Radon transform of tensor fields.

</details>


### [22] [The Kolmogorov forward equation for a distributed model of regime-switching diffusions](https://arxiv.org/abs/2601.21659)
*Alexander S. Bratus,Olga S. Rozanova*

Main category: math.AP

TL;DR: The paper proposes integro-differential equations for regime-switching diffusion processes, develops a constructive solution algorithm for the Cauchy problem, finds explicit solutions for certain initial distributions, and shows how discrete hidden state models can be approximated by continuous state models.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical tools for analyzing regime-switching diffusion processes (with and without advection) by describing state densities through integro-differential equations, enabling explicit solutions and bridging discrete and continuous state models.

Method: Proposes integro-differential equations for densities of continuously distributed states over a segment, develops a constructive algorithm for solving the Cauchy problem, finds explicit solutions for specific initial distributions, and demonstrates approximation of discrete hidden state models by continuous state models.

Result: Existence of a constructive algorithm for solving the Cauchy problem, explicit solutions for certain initial state distributions, and a method to approximate discrete hidden state models using continuously distributed state models.

Conclusion: The framework provides effective mathematical tools for analyzing regime-switching diffusion processes, connecting discrete and continuous state representations through integro-differential equations with practical solution methods.

Abstract: For the regime-switching diffusion process with and without advection term we propose an integro-differential equation describing the densities of states continuously distributed over a segment. We demonstrate that there exists a constructive algorithm for solving the Cauchy problem. We then show that for some initial distributions of states, the solution can be found explicitly. We also discuss how a model with a discrete number of hidden states can be approximated by a model with continuously distributed states.

</details>


### [23] [Nonhomogeneous boundary condition for spectral non-local operators](https://arxiv.org/abs/2601.21674)
*Ivan Bioƒçiƒá,Vanja Wagner*

Main category: math.AP

TL;DR: Study of semilinear non-local elliptic problems with spectral-type operators in bounded domains, focusing on nonhomogeneous boundary conditions and establishing existence results for general nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of non-local elliptic problems beyond the fractional Laplacian framework, particularly addressing the challenge of formulating and analyzing nonhomogeneous boundary conditions for spectral-type operators in bounded domains.

Method: Combines stochastic process techniques (killed L√©vy processes), potential theory, and spectral analysis. Introduces weak L^1 trace-like boundary operator and analyzes boundary behavior using renewal functions and distance to boundary.

Result: Establishes sharp boundary estimates for Green and Poisson potentials, introduces boundary trace operator, and provides existence results for solutions under general nonlinearities including sign-changing and non-monotone cases.

Conclusion: The framework provides a unified treatment of semilinear boundary problems in non-local settings, extending beyond fractional Laplacian theory and offering new tools for analyzing boundary behavior of solutions.

Abstract: We study semilinear non-local elliptic problems driven by spectral-type operators of the form $œà(-L_{|D})$ in a bounded $C^{1,1}$ domain $D\subset \mathbb{R}^d$ with a nonhomogeneous boundary condition. Here $œà$ is a Bernstein function satisfying a weak scaling condition at infinity, and $L_{|D}$ is the generator of a killed L√©vy process. This general framework covers and extends the theory of the interpolated fractional Laplacian. A key novelty in this setting is the analysis of the nonhomogeneous boundary condition formulated in terms of the Poisson potential with respect to the $d-1$ Hausdorff measure on $\partial D$. We establish sharp boundary estimates for Green and Poisson potentials, introduce a weak $L^1$ trace-like boundary operator, and provide existence results for solutions under quite general nonlinearities, including sign-changing and non-monotone cases. The methodology combines stochastic process techniques, potential theory, and spectral analysis, and expresses the boundary behavior of the solution in terms of the renewal function and the distance to the boundary, suggesting a possible unified treatment of semilinear boundary problems in non-local settings.

</details>


### [24] [Localized Big Bang Stability of Spacetime Dimensions $n\geq4$](https://arxiv.org/abs/2601.21677)
*Weihang Zheng*

Main category: math.AP

TL;DR: The paper proves nonlinear stability of sub-critical Kasner-scalar field solutions in higher dimensions (n‚â•4), showing perturbed solutions remain asymptotically Kasner and terminate at crushing singularities.


<details>
  <summary>Details</summary>
Motivation: To extend previous stability results for Kasner-scalar field solutions from specific cases to all higher dimensional spacetimes, addressing the behavior of perturbed solutions near singularities.

Method: Mathematical analysis of Einstein-scalar field equations on truncated cone domains, generalizing previous work by Beyer-Oliynyk-Zheng to dimensions n‚â•4.

Result: Proves past nonlinear stability of sub-critical Kasner-scalar field solutions, showing perturbed solutions are asymptotically pointwise Kasner, geodesically incomplete, and terminate at quiescent crushing singularities with curvature blow-up.

Conclusion: Successfully generalizes stability results to all higher dimensions, confirming that perturbed Kasner-scalar field solutions maintain asymptotic Kasner behavior and end in crushing singularities in n‚â•4 spacetime dimensions.

Abstract: We prove the past nonlinear stability of the sub-critical Kasner-scalar field solutions to the Einstein-scalar field equations on a truncated cone domain in spacetime dimensions $n\geq4$. Our analysis demonstrates that the perturbed solutions are asymptotically pointwise Kasner, geodesically incomplete in the contracting direction and terminate at quiescent and crushing singularities characterized by the blow-up of curvature invariants. This work generalizes the result of Beyer-Oliynyk-Zheng in [arXiv:2502.09210v2] to all higher dimensional spacetimes.

</details>


### [25] [Unique Continuation Property for Stochastic Wave Equations](https://arxiv.org/abs/2601.21854)
*Qi L√º,Zhonghua Liao*

Main category: math.AP

TL;DR: Stochastic wave equations restore unique continuation across characteristic surfaces, unlike deterministic equations where it fails.


<details>
  <summary>Details</summary>
Motivation: To investigate whether stochasticity can restore the unique continuation property (UCP) for wave equations across characteristic hypersurfaces, a property known to fail in deterministic settings.

Method: Develop novel stochastic Carleman estimates that leverage the It√¥ diffusion term's positive energy contribution, which is absent in deterministic models.

Result: Proved that solutions to linear stochastic wave equations vanishing on one side of characteristic surfaces must vanish in full neighborhoods when diffusion coefficients are non-degenerate. Extended results to non-homogeneous stochastic sources and global continuation from narrow characteristic cones.

Conclusion: Stochasticity fundamentally changes hyperbolic dynamics by restoring UCP, demonstrating qualitative differences from deterministic models and opening new avenues for stochastic control theory and inverse problems.

Abstract: This paper establishes a fundamental and surprising phenomenon in the theory of stochastic wave equations: the restoration of the unique continuation property (UCP) across characteristic hypersurfaces, a property that is known to fail generically in the deterministic setting. We prove that if a solution to a linear stochastic wave equation vanishes on one side of a characteristic surface $Œì$, then it must vanish in a full neighborhood of any point on $Œì$, provided the stochastic diffusion coefficient is non-degenerate. This result stands in sharp contrast to the classical H√∂rmander-type counterexamples for deterministic waves.
  Furthermore, we extend the UCP to equations with non-homogeneous stochastic sources and establish a global unique continuation result from the interior of an arbitrarily narrow characteristic cone. Our proofs rely on a novel stochastic Carleman estimate, where the It√¥ diffusion term introduces a crucial positive energy contribution that is absent in deterministic models.
  These findings demonstrate a qualitative difference between deterministic and stochastic hyperbolic dynamics and open new avenues for control theory and inverse problems in stochastic setting.

</details>


### [26] [Probabilistically Strong Solutions to Stochastic Euler Equations](https://arxiv.org/abs/2601.22073)
*Benjamin Gess,Robert Lasarzik*

Main category: math.AP

TL;DR: Existence of probabilistically strong measure-valued solutions for stochastic Navier-Stokes equations and their convergence to stochastic Euler solutions in vanishing viscosity limit, solving open problem for general L¬≤ initial data.


<details>
  <summary>Details</summary>
Motivation: To solve the open problem of constructing probabilistically strong solutions for stochastic Euler equations that satisfy energy inequality for general L¬≤ initial data, and to extend results to fluids driven by transport noise.

Method: Introduce concept of energy-variational solutions in stochastic context to treat nonlinearities without changing probability space; establish existence of measure-valued solutions for stochastic Navier-Stokes and prove convergence to Euler solutions via vanishing viscosity limit.

Result: Successfully construct probabilistically strong solutions for stochastic Euler equations satisfying energy inequality for general L¬≤ initial data; extend results to transport noise-driven fluids.

Conclusion: The paper solves a significant open problem in stochastic fluid dynamics by developing energy-variational solution framework and establishing convergence from Navier-Stokes to Euler equations in vanishing viscosity limit.

Abstract: In this paper, we establish the existence of probabilistically strong, measure-valued solutions for the stochastic incompressible Navier--Stokes equations and prove their convergence, in the vanishing viscosity limit, to probabilistically strong solutions for the stochastic incompressible Euler equations. In particular, this solves the open problem of constructing probabilistically strong solutions for the stochastic Euler equations that satisfy the energy inequality for general $L^2$ initial data. We introduce the concept of energy-variational solutions in the stochastic context in order to treat the nonlinearities without changing the probability space. Furthermore, we extend these results to fluids driven by transport noise.

</details>


### [27] [On Global Weak Solutions for the Magnetic Two-Component Hunter-Saxton System](https://arxiv.org/abs/2601.22088)
*Levin Maier*

Main category: math.AP

TL;DR: The paper provides analytical foundations for the magnetic two-component Hunter-Saxton system (M2HS), deriving explicit solution formulas and constructing global conservative weak solutions.


<details>
  <summary>Details</summary>
Motivation: The M2HS was recently derived as a magnetic geodesic equation on an infinite-dimensional configuration space. While the geometric framework and global weak flow were outlined in previous work, this paper aims to provide the analytical foundations from the PDE perspective.

Method: 1. Derive explicit solution formula in Lagrangian variables via Riccati reduction. 2. Rigorously construct global conservative weak solutions by developing analytic theory of the relaxed configuration space and associated weak magnetic geodesic flow.

Result: 1. Obtained explicit solution formula yielding alternative proof of blow-up criterion with explicit expression for blow-up time. 2. Successfully constructed global conservative weak solutions, realizing the geometric program proposed in previous work.

Conclusion: The paper establishes the analytical foundations for the M2HS system, providing both explicit solution formulas and rigorous construction of global weak solutions, thereby completing the geometric program outlined in previous research.

Abstract: We study the magnetic two-component Hunter-Saxton system (M2HS), which was recently derived in \cite{M24} as a magnetic geodesic equation on an infinite-dimensional configuration space. While the geometric framework and the global weak flow were outlined there, the present paper provides the analytical foundations of this construction from the PDE perspective.
  First, we derive an explicit solution formula in Lagrangian variables via a Riccati reduction, yielding an alternative proof of the blow-up criterion together with an explicit expression for the blow-up time. Second, we rigorously construct global conservative weak solutions by developing the analytic theory of the relaxed configuration space and the associated weak magnetic geodesic flow, thereby realizing the geometric program proposed in \cite{M24}.

</details>


### [28] [Microlocal maximal hypoellipticity from the geometric viewpoint: I](https://arxiv.org/abs/2601.22122)
*Omar Mohsen*

Main category: math.AP

TL;DR: Develops a bi-graded pseudo-differential calculus for H√∂rmander vector fields, combining classical and sub-Riemannian approaches, and proves microlocal maximal hypoellipticity from principal symbol invertibility.


<details>
  <summary>Details</summary>
Motivation: To create a unified pseudo-differential calculus that works for both classical settings and sub-Riemannian structures induced by H√∂rmander vector fields, addressing limitations of existing approaches.

Method: Uses geometric constructions (resolution of singularities) and operator algebra methods to develop a bi-graded calculus including Sobolev spaces, wavefront sets, and principal symbols.

Result: Establishes that invertibility of the principal symbol implies microlocal maximal hypoellipticity, resolving affirmatively the microlocal version of Helffer and Nourrigat's conjecture.

Conclusion: Provides a comprehensive pseudo-differential framework for sub-Riemannian analysis with applications to hypoellipticity problems, advancing the understanding of PDEs in non-elliptic settings.

Abstract: Given some vector fields on a smooth manifold satisfying H√∂rmander's condition, we define a bi-graded pseudo-differential calculus which contains the classical pseudo-differential calculus and a pseudo-differential calculus adapted to the sub-Riemannian structure induced by the vector fields.
  Our approach is based on geometric constructions (resolution of singularities) together with methods from operators algebras. We develop this calculus in full generality, including Sobolev spaces, the wavefront set, and the principal symbol, etc.
  In particular, using this calculus, we prove that invertibility of the principal symbol implies microlocal maximal hypoellipticity. This allows us to resolve affirmatively the microlocal version of a conjecture of Helffer and Nourrigat.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Semi-implicit Lax-Wendroff kinetic scheme for hydrodynamic phonon transport](https://arxiv.org/abs/2601.21161)
*Shijie Li,Hong Liang,Songze Chen,Chuang Zhang*

Main category: physics.comp-ph

TL;DR: A semi-implicit Lax-Wendroff kinetic scheme for hydrodynamic phonon transport using double relaxation time approximation with trapezoidal and midpoint rules for temporal integration.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for multi-scale thermal conduction in solid materials that can handle both normal and resistive phonon scattering processes, allowing larger cell sizes and time steps than traditional methods.

Method: Semi-implicit Lax-Wendroff kinetic scheme based on Boltzmann transport equation with double relaxation time approximation. Uses trapezoidal rule for scattering terms and midpoint rule for migration terms in finite volume framework. Solves kinetic equation for interfacial flux reconstruction instead of direct interpolation, coupling phonon migration and scattering within each time step.

Result: The method enables cell sizes and time steps larger than phonon mean free path and relaxation time at small Knudsen numbers. Numerical tests show accurate capture of multi-scale thermal conduction phenomena across different normal and resistive scattering rates.

Conclusion: The developed scheme successfully handles hydrodynamic phonon transport with both scattering processes, providing an efficient approach for multi-scale thermal conduction simulations in solid materials.

Abstract: A semi-implicit Lax-Wendroff kinetic scheme is developed for hydrodynamic phonon transport in solid materials based on the Boltzmann transport equation under the double relaxation time approximation, in which both the normal and resistive scattering processes are accounted. The trapezoidal and midpoint rules are adopted for the temporal integration of the scattering and migration terms under the framework of finite volume method, respectively. Instead of direct numerical interpolation, the kinetic equation is solved again when reconstructing the interfacial flux, in order to realize the coupling of phonon migration and scattering within a numerical time step. Specifically, the finite difference scheme is introduced and the second-order upwind or central schemes are used for the reconstruction of the interfacial distribution function and its spatial gradient. Consequently, the cell size and time step of the present method could be larger than the phonon mean free path and relaxation time in the limit of small Knudsen numbers. Numerical tests demonstrate that the present method can accurately capture multi-scale thermal conduction phenomena within different normal or resistive scattering rates.

</details>


### [30] [Acquiring Human-Like Mechanics Intuition from Scarce Observations via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21881)
*Jingruo Peng,Shuze Zhu*

Main category: physics.comp-ph

TL;DR: A reinforcement learning framework using episodic switching across related physical observations enables agents to develop accurate mechanics intuition from just 2-3 observations, generalizing far beyond training data.


<details>
  <summary>Details</summary>
Motivation: Humans can infer mechanical outcomes from few observations (mechanics intuition), but the mechanisms behind such data-efficient learning remain unclear. The paper aims to understand and replicate this capability in artificial agents.

Method: Proposes a reinforcement learning framework where agents encode continuous physical observation parameters into their state and are trained via episodic switching across closely related observations. The agent learns from only 2-3 observations.

Result: The agent acquires robust mechanics intuition that generalizes accurately over wide parameter ranges, substantially beyond training data, demonstrated on brachistochrone and large-deformation elastic plate problems. Generalization emerges when learned value function enforces Bellman consistency across neighboring task parameters.

Conclusion: Episodic switching provides a principled route to artificial mechanics intuition, with theoretical explanation linking to similar generalization abilities in biological learners through smooth policies capturing low-dimensional solution manifolds.

Abstract: Humans can infer accurate mechanical outcomes from only a few observations, a capability known as mechanics intuition. The mechanisms behind such data-efficient learning remain unclear. Here, we propose a reinforcement learning framework in which an agent encodes continuous physical observation parameters into its state and is trained via episodic switching across closely related observations. With merely two or three observations, the agent acquires robust mechanics intuition that generalizes accurately over wide parameter ranges, substantially beyond the training data, as demonstrated on the brachistochrone and a large-deformation elastic plate. We explain this generalization through a unified theoretical view: it emerges when the learned value function enforces Bellman consistency across neighboring task parameters, rendering the Bellman residual stationary with respect to physical variations. This induces a smooth policy that captures a low-dimensional solution manifold underlying the continuum of tasks. Our work establishes episodic switching as a principled route to artificial mechanics intuition and offers a theoretical link to similar generalization abilities in biological learners.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [A joint diffusion approach to multi-modal inference in inertial confinement fusion](https://arxiv.org/abs/2601.21006)
*Michael S. Jones,Justin Kunimune,Daniel Casey,Bogdan Kustowski,Eugene Kur,Kelli Humbird*

Main category: physics.plasm-ph

TL;DR: JointDiff is a generative framework using joint diffusion to predict conditional simulation input/output distributions from partial multi-modal observations in inertial confinement fusion, unifying forward surrogate modeling, inverse inference, and output imputation.


<details>
  <summary>Details</summary>
Motivation: In inertial confinement fusion (ICF), only a subset of simulated data is available experimentally, creating a gap between physics-based simulations and experimental observations. There's a need to bridge this gap by predicting complete simulation distributions from partial multi-modal experimental data.

Method: JointDiff uses joint diffusion to create a unified architecture that handles forward surrogate modeling, inverse inference, and output imputation. It's trained on large ensembles of 3D Multi-Rocket Piston simulations and validated on National Ignition Facility experiments.

Result: The model demonstrates high accuracy, statistical robustness, and transferability to real NIF experiments. It successfully predicts conditional simulation input and output distributions from partial multi-modal observations.

Conclusion: JointDiff establishes itself as a flexible generative surrogate for multi-modal scientific tasks in ICF, with potential applications for understanding diagnostic constraints, aligning simulation to experiment, and accelerating ICF design.

Abstract: A combination of physics-based simulation and experiments has been critical to achieving ignition in inertial confinement fusion (ICF). Simulation and experiment both produce a mixture of scalar and images outputs, however only a subset of simulated data are available experimentally. We introduce a generative framework, called JointDiff, which enables predictions of conditional simulation input and output distributions from partial, multi-modal observations. The model leverages joint diffusion to unify forward surrogate modeling, inverse inference, and output imputation into one architecture. We train our model on a large ensemble of three-dimensional Multi-Rocket Piston simulations and demonstrate high accuracy, statistical robustness, and transferability to experiments performed at the National Ignition Facility (NIF). This work establishes JointDiff as a flexible generative surrogate for multi-modal scientific tasks, with implications for understanding diagnostic constraints, aligning simulation to experiment, and accelerating ICF design.

</details>


### [32] [Initial observations in X-point target divertor discharges on MAST-U](https://arxiv.org/abs/2601.21840)
*N. Lonigro,K. Verhaegh,J. Harrison,B. Lipschultz,C. Bowman,F. Federici,J. Flanagan,D. Greenhouse,D. Moulton,P. Ryan,R. Scannell,S. Silburn,T. Wijkamp,D. Brida,C. Theiler,the EUROfusion Tokamak Exploitation Team,the MAST Upgrade Team*

Main category: physics.plasm-ph

TL;DR: MAST-U experiments show double-null X-point-target (XPT) divertor configuration improves exhaust performance over Super-X divertor by enhancing plasma-neutral interactions, reducing target temperatures and heat fluxes.


<details>
  <summary>Details</summary>
Motivation: Future fusion reactors face challenging exhaust conditions requiring improved momentum, power, and particle losses. The study explores combining multiple alternative divertor configurations to address these challenges.

Method: Performed high-power (>3 MW) H-mode experiments on MAST-U using a double-null X-point-target (XPT) divertor configuration, combining large strike point radius (similar to Super-X) with additional X-point near separatrix in baffled outer divertor chambers.

Result: XPT configuration provides additional exhaust benefits over Super-X divertor: broader electron density profile near secondary X-point enhances plasma-neutral interactions, evidenced by broader hydrogenic emission profile, resulting in larger power and ion sinks, lower target electron temperatures, and reduced heat fluxes. Preliminary evidence shows improved ELM buffering.

Conclusion: Combining multiple alternative divertor configuration strategies (XPT) improves momentum, power, and particle losses, demonstrating a promising approach for addressing exhaust challenges in future fusion reactors.

Abstract: The first high-power (> 3 MW) H-mode experiments using a double-null X-point-target (XPT) divertor configuration have been performed on MAST-U. The XPT geometry is obtained by combining a large strike point radius, similar to the Super-X divertor (SXD), with an additional X-point near the separatrix in the baffled outer divertor chambers and leads to additional exhaust benefits over the SXD. The broader electron density profile near the secondary X-point leads to additional plasma-neutral interactions, evidenced by a broader hydrogenic emission profile, and resulting in larger power and ion sinks. The increase in plasma-neutral interactions also leads to lower target electron temperatures and heat fluxes. These benefits appear to extend to transients, and preliminary evidence of improved ELM buffering in the XPT is presented. These results showcase how multiple alternative divertor configuration strategies can be combined to improve momentum, power, and particle losses, which may be required for the challenging exhaust conditions of future reactors.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [33] [Rapid estimation of global sea surface temperatures from sparse streaming in situ observations](https://arxiv.org/abs/2601.21913)
*Cassidy All,Kevin Ho,Maya Magnuski,Christopher Nicolaides,Louisa B. Ebby,Mohammad Farazmand*

Main category: physics.ao-ph

TL;DR: S-DEIM combines empirical interpolation with RNNs to reconstruct high-resolution sea surface temperatures from sparse observations, achieving 40% better accuracy than previous methods with real-time computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate reconstruction of high-resolution sea surface temperatures from sparse measurements is crucial for weather forecasting and climate projections, but existing methods produce inaccurate results when data is sparse.

Method: S-DEIM combines two components: 1) empirical interpolation from instantaneous in situ observations, and 2) RNNs trained on historical SST data (1989-2021 NOAA dataset). The method reconstructs SST from only 0.2% of grid points (100 observations).

Result: S-DEIM achieves 40% better accuracy than DEIM/Q-DEIM, with 91% of estimates within ¬±1¬∞C of true SST. It's robust to sensor placement (only 1-2% degradation with random placement) and computationally efficient (1-minute offline training, <1-second reconstruction).

Conclusion: S-DEIM enables accurate, real-time high-resolution SST reconstruction from sparse streaming data without requiring physical models, making it suitable for operational weather forecasting and climate monitoring applications.

Abstract: Reconstructing high-resolution sea surface temperatures (SST) from staggered SST measurements is essential for weather forecasting and climate projections. However, when SST measurements are sparse, the resulting inferred SST fields are rather inaccurate. Here, we demonstrate the ability of Sparse Discrete Empirical Interpolation Method (S-DEIM) to reconstruct the high-resolution SST field from sparse in situ observations, without using a model. The S-DEIM estimate consists of two terms, one computed from instantaneous in situ observations using empirical interpolation, and the other learned from the historical time series of observations using recurrent neural networks (RNNs). We train the RNNs using the National Oceanic and Atmospheric Administration's weekly high-resolution SST dataset spanning the years 1989-2021 which constitutes the training data. Subsequently, we examine the performance of S-DEIM on the test data, comprising January 2022 to January 2023. For this test data, S-DEIM infers the high-resolution SST from 100 in situ observations, constituting only 0.2% of the high-resolution spatial grid. We show that the resulting S-DEIM reconstructions are about 40% more accurate than earlier empirical interpolation methods, such as DEIM and Q-DEIM. Furthermore, 91% of S-DEIM estimates fall within $\pm 1^\circ$C of the true SST. We also demonstrate that S-DEIM is robust with respect to sensor placement: even when the sensors are distributed randomly, S-DEIM reconstruction error deteriorates only by 1-2%. S-DEIM is also computationally efficient. Training the RNN, which is performed only once offline, takes approximately one minute. Once trained, the S-DEIM reconstructions are computed in less than a second. As such, S-DEIM can be used for rapid SST reconstruction from sparse streaming observational data in real time.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [34] [Accelerated Inorganic Electrides Discovery by Generative Models and Hierarchical Screening](https://arxiv.org/abs/2601.21077)
*Shuo Tao,Qiang Zhu*

Main category: cond-mat.mtrl-sci

TL;DR: AI-driven discovery framework identifies 264 new electron-rich compounds including 13 stable electrides through systematic screening of binary/ternary compositions.


<details>
  <summary>Details</summary>
Motivation: Electrides have exceptional properties (low work function, high electron mobility, strong catalytic activity) but are challenging to discover due to difficulty achieving energetically favorable electron localization in crystal cavities.

Method: Combined physical principles with diffusion-based materials generation and hierarchical thermodynamic/electronic structure screening. Systematically explored 1,510 binary and 6,654 ternary compositions with excess valence electrons, filtered through high-throughput validation of thermodynamical stability and electronic structure analysis.

Result: Identified 264 new electron-rich compounds within 0.05 eV/atom above convex hull at DFT level, including 13 thermodynamically stable electrides.

Conclusion: The approach demonstrates a generalizable strategy for targeted materials discovery in vast chemical spaces, enabling systematic identification of novel electride materials.

Abstract: Electrides are exotic compounds in which excess electrons occupy interstitial regions of the crystal lattice and serve as anions, exhibiting exceptional properties such as low work function, high electron mobility, and strong catalytic activity. Although they show promise for diverse applications, identifying new electrides remains challenging due to the difficulty of achieving energetically favorable electron localization in crystal cavities. Here, we present an accelerated materials discovery framework that combines physical principles, diffusion-based materials generation with hierarchical thermodynamic and electronic structure screening. Using this workflow, we systematically explored 1,510 binary and 6,654 ternary chemical compositions containing excess valence electrons from electropositive alkaline, alkaline-earth, and early transition metals, and then filtered them with a high throughput validation on both thermodynamical stability and electronic structure analysis. As a result, we have identified 264 new electron rich compounds within 0.05 eV/atom above the convex hull at the density functional theory (DFT) level, including 13 thermodynamically stable electrides. Our approach demonstrates a generalizable strategy for targeted materials discovery in a vast chemical space.

</details>


### [35] [Dynamically training machine-learning-based force fields for strongly anharmonic materials](https://arxiv.org/abs/2601.21311)
*Martin Callsen,Tai-Ting Lee,Mei-Yin Chou*

Main category: cond-mat.mtrl-sci

TL;DR: Dynamic training framework for ML force fields using Bayesian error estimation to guide adaptive data acquisition during simulations, improving robustness and transferability across materials with varying anharmonicity.


<details>
  <summary>Details</summary>
Motivation: ML force fields offer computational efficiency comparable to ab initio MD but suffer from reliability issues when trained on static datasets that fail to generalize to unseen atomic configurations encountered during simulations.

Method: Dynamic training framework building on conventional lattice dynamics expansion with Bayesian error estimation to guide adaptive data acquisition during simulation. Uses trajectory-averaged Bayesian errors for efficient configuration space exploration.

Result: Demonstrated effectiveness across materials with varying anharmonicity (c-BAs, Si, SnSe). Bayesian error estimation enables targeted exploration and determines training convergence without additional ab initio data.

Conclusion: The framework provides a practical, easily implementable scheme to improve ML force field training, addressing the critical reliability issue of static training datasets through dynamic, adaptive learning.

Abstract: Machine learning (ML) force fields have emerged as a powerful tool for computing materials properties at finite temperatures, particularly in regimes where traditional phonon-based perturbation theories fail or cannot be extended beyond the harmonic approximation. These approaches offer accuracy comparable to ab initio molecular dynamics (MD), but at a fraction of the computational cost. However, their reliability critically depends on the quality and representativeness of the training data. In particular, static training datasets often lead to failure when the force field encounters previously unseen atomic configurations during MD simulations. In this work, we present a framework for dynamically training ML force fields and demonstrate its effectiveness across materials with varying degrees of anharmonicity, including cubic boron arsenide (c-BAs), silicon (Si), and tin selenide (SnSe). Our method builds on the conventional lattice dynamics expansion of total energy and incorporates Bayesian error estimation to guide adaptive data acquisition during simulation. Specifically, we show that trajectory-averaged Bayesian errors enable efficient and targeted exploration of the configuration space, significantly enhancing the robustness and transferability of the resulting force fields. We further demonstrate how Bayesian error estimation can be applied to determine the convergence of the dynamic training without requiring additional ab initio data. This proposed framework offers a practical and easily implementable scheme to improve the training process, which is the most critical step in developing reliable ML force fields.

</details>


### [36] [Model density approach to Ewald summations](https://arxiv.org/abs/2601.21776)
*Chiara Ribaldone,Jacques Kontak Desmarais*

Main category: cond-mat.mtrl-sci

TL;DR: A method using model charge densities to cancel multipole moments for faster convergence of Ewald sums in electrostatic potential calculations for condensed phase systems.


<details>
  <summary>Details</summary>
Motivation: The electrostatic potential evaluation is fundamental for studying condensed phase systems, but traditional Ewald summation techniques can have slow convergence. There's a need for more efficient methods to accelerate convergence of these lattice summations.

Method: Introduces a model charge density that cancels multipole moments of the crystalline charge distribution up to a desired order. This accelerates convergence of Ewald sums. The method works with arbitrary unit cells, in classical or quantum contexts, and with arbitrary basis functions for charge density representation.

Result: The approach provides an efficient technique for calculating electrostatic potentials in condensed phase systems with improved convergence properties. It also clarifies a decades-old implementation in the CRYSTAL code.

Conclusion: The method offers a general approach for accelerating Ewald summation convergence by canceling multipole moments, applicable to various computational contexts and providing insight into existing implementations like CRYSTAL.

Abstract: The evaluation of the electrostatic potential is fundamental to the study of condensed phase systems. We discuss the calculation of the relevant lattice summations by Ewald-type techniques. A model charge density is introduced, that cancels multipole moments of the crystalline charge distribution up to a desired order, for accelerating convergence of the Ewald sums. The method is applicable to calculations of bulk systems, employing arbitrary unit cells in a classical or quantum context, and with arbitrary basis functions to represent the charge density. The approach clarifies a decades-old implementation in the CRYSTAL code.

</details>


### [37] [MEIDNet: Multimodal generative AI framework for inverse materials design](https://arxiv.org/abs/2601.22009)
*Anand Babu,Rog√©rio Almeida Gouv√™a,Pierre Vandergheynst,Gian-Marco Rignanese*

Main category: cond-mat.mtrl-sci

TL;DR: MEIDNet is a multimodal equivariant graph neural network framework that combines contrastive learning with generative inverse design to accelerate materials discovery by aligning structural and property information across multiple modalities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to accelerate materials discovery by developing a framework that can efficiently explore the chemical-structural space and discover materials that meet specific property targets, addressing the challenge of navigating complex materials design spaces.

Method: The method combines multimodal learning with equivariant graph neural networks (EGNN) through contrastive learning. It uses cross-modal learning to fuse three modalities and implements curriculum learning strategies to improve training efficiency.

Result: MEIDNet achieves strong latent-space alignment with cosine similarity 0.96, ~60x higher learning efficiency than conventional training, and generates low-bandgap perovskite structures at a 13.6% stable, unique, and novel (SUN) rate validated by ab initio methods.

Conclusion: The framework demonstrates scalability and adaptability for universal learning of chemical space across diverse modalities, paving the way for accelerated materials discovery through multimodal inverse design.

Abstract: In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [38] [Weighted Sobolev Spaces and Distributional Spectral Theory for Generalized Aging Operators via Transmutation Methods](https://arxiv.org/abs/2601.21497)
*Gustavo Dorrego*

Main category: math.FA

TL;DR: Develops a distributional theory for Weighted Weyl-Sonine operators using transmutation, constructing weighted Schwartz spaces and tempered distributions to analyze spectral properties in heterogeneous/aging media.


<details>
  <summary>Details</summary>
Motivation: Standard Hilbertian frameworks are insufficient for spectral analysis in heterogeneous and aging media, requiring extended functional frameworks to handle non-local operators.

Method: Uses structure-preserving transmutation to construct Weighted Schwartz Space and its dual (Weighted Tempered Distributions), establishes Fr√©chet topology consistent with aging dynamics generator, extends Weighted Fourier Transform, and introduces Weighted Sobolev Spaces via spectral multipliers.

Result: Derives sharp embedding theorem connecting spectral energy to pointwise decay, provides spectral characterization of weighted Dirac delta, and unifies fractional regimes (Hadamard and Riemann-Liouville) within single operator-theoretic architecture.

Conclusion: The framework provides rigorous distributional theory for non-local operators in heterogeneous/aging media, enabling unified geometric characterization of fractional regimes through weighted functional spaces and spectral analysis.

Abstract: The spectral analysis of operators in heterogeneous and aging media typically requires a functional framework that extends beyond the standard Hilbertian setting. In this paper, we establish a rigorous distributional theory for a class of non-local operators, termed Weighted Weyl-Sonine operators, by employing a structure-preserving transmutation method. We construct the Weighted Schwartz Space $\mathcal{S}_{œà,œâ}$ and its topological dual, the space of Weighted Tempered Distributions $\mathcal{S}'_{œà,œâ}$, ensuring that the underlying Fr√©chet topology is consistent with the infinitesimal generator of the aging dynamics. This topological foundation allows us to: (i) extend the Weighted Fourier Transform to generalized functions as a unitary isomorphism; (ii) provide an explicit spectral characterization of the weighted Dirac delta $Œ¥_{œà,œâ}$ and its scaling laws under geometric dilations; and (iii) introduce a scale of Weighted Sobolev Spaces $H^{s}_{œà,œâ}$ defined via spectral multipliers. A central result is the derivation of a sharp embedding theorem, $|u(t)| \le C œâ(t)^{-1} \|u\|_{H^s_{œà,œâ}}$, which rigorously connects abstract spectral energy to the pointwise decay induced by the weight $œâ$. This framework provides a unified geometric characterization of several fractional regimes, including the Hadamard and Riemann-Liouville cases, within a single operator-theoretic architecture.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [39] [Navier-Stokes with a fractional transport noise as a limit of multi-scale dynamics](https://arxiv.org/abs/2601.21762)
*Xue-Mei Li,Szymon Sobczak*

Main category: math.PR

TL;DR: Rough path solutions for Navier-Stokes with rough transport term, equivalence to incremental solutions, applications to SPDEs driven by fractional Brownian motion.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous rough path solution framework for Navier-Stokes equations with rough transport terms, connecting to slow/fast system limits and establishing equivalence with existing solution concepts.

Method: Define bona fide rough path solutions for Navier-Stokes with rough transport, show existence as effective limits of slow/fast systems, prove equivalence to incremental solutions (unbounded rough driver formulation).

Result: Existence of rough path solutions for 3D torus Navier-Stokes driven by fractional Brownian motion, characterization as slow/fast system limits, equivalence between rough path and incremental solution concepts.

Conclusion: Established rigorous rough path solution theory for nonlinear SPDEs with rough transport, demonstrating broader applicability beyond Navier-Stokes to other nonlinear SPDEs.

Abstract: We define a bona fide rough path solution for the Navier-Stokes equation with an additional rough transport term, and show that the SPDE on the three-dimensional torus driven by a fractional Brownian motion on $H^œÉ$ has solutions characterised as the effective limits of a slow/fast system. We further show that this rough path solution is equivalent to the widely used incremental notion of solution (the unbounded rough driver formulation), demonstrating broader applicability to other nonlinear SPDEs.

</details>


### [40] [Ergodicity for SPDEs driven by divergence-free transport noise](https://arxiv.org/abs/2601.22056)
*Benjamin Gess,Rishabh S. Gvalani,Adrian Martini*

Main category: math.PR

TL;DR: Strong mixing transport noise enforces unique invariant measures in McKean-Vlasov equations, overcoming deterministic multiple steady states.


<details>
  <summary>Details</summary>
Motivation: To understand how stochastic perturbations, specifically common divergence-free transport noise, can regularize the ergodic behavior of McKean-Vlasov equations and enforce uniqueness of invariant measures despite deterministic non-uniqueness.

Method: Study McKean-Vlasov equations with common transport noise, focusing on mixing properties and noise strength. Analyze ergodic behavior in dimension d‚â•2, examining how sufficiently strong mixing noise can overcome deterministic multiplicity of steady states.

Result: In dimensions d‚â•2, sufficiently strong mixing transport noise can enforce uniqueness of invariant probability measures, even when the deterministic part of the equation has multiple steady states.

Conclusion: Transport noise serves as a regularization mechanism that can enforce ergodic uniqueness in McKean-Vlasov systems, demonstrating the stabilizing effect of stochastic perturbations on collective behavior.

Abstract: We study the ergodic behaviour of the McKean-Vlasov equations driven by common, divergence-free transport noise. In particular, we show that in dimension $d\geq 2$, if the noise is mixing and sufficiently strong it can enforce the uniqueness of invariant probability measures, even if the deterministic part of equation has multiple steady states.

</details>


### [41] [Superdiffusion and anomalous regularization in self-similar random incompressible flows](https://arxiv.org/abs/2601.22142)
*Scott Armstrong,Ahmed Bou-Rabee,Tuomo Kuusi*

Main category: math.PR

TL;DR: The paper proves quenched power-law superdiffusion for particles in random incompressible flows with multiscale self-similar structure, showing displacement variance grows as t^{2/(2-Œ≥)} and identifying the leading prefactor with controlled error.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of particles in random incompressible flows, particularly in multiscale self-similar environments, and to rigorously establish the superdiffusive scaling predicted by renormalization group heuristics.

Method: Implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator ‚àá¬∑(ŒΩI_d + k)‚àá, using self-similar induction across scales. The approach shows that coarse-grained generators are approximated by constant-coefficient Laplacians with effective diffusivity growing like r^Œ≥.

Result: Proves quenched power-law superdiffusion: for typical realizations, displacement variance grows as t^{2/(2-Œ≥)}. Identifies leading prefactor up to random relative error of order Œ≥^{1/2}|log Œ≥|^3. Also proves anomalous regularization: solutions are H√∂lder continuous with exponent 1 - CŒ≥^{1/2}.

Conclusion: The paper provides rigorous mathematical justification for renormalization group predictions of superdiffusion in multiscale random flows, demonstrating scale-local approximations and anomalous regularization properties in the perturbative regime Œ≥‚â™1.

Abstract: We study the long-time behavior of a particle in $\mathbb{R}^d$, $d \geq 2$, subject to molecular diffusion and advection by a random incompressible flow. The velocity field is the divergence of a stationary random stream matrix $\mathbf{k} $ with positive Hurst exponent $Œ≥> 0$, so the resulting random environment is multiscale and self-similar. In the perturbative regime $Œ≥\ll 1$, we prove quenched power-law superdiffusion: for a typical realization of the environment, the displacement variance at time $t$ grows like $t^{2/(2-Œ≥)}$, the scaling predicted by renormalization group heuristics. We also identify the leading prefactor up to a random (quenched) relative error of order $Œ≥^{\frac12}\left| \log Œ≥\right|^3$. The proof implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator $\nabla \cdot (ŒΩI_d + \mathbf{k} ) \nabla$, based on a self-similar induction across scales. We demonstrate that the coarse-grained generator is well-approximated, at each scale $r$, by a constant-coefficient Laplacian with effective diffusivity growing like $r^Œ≥$. This approximation is inherently scale-local: reflecting the multifractal nature of the environment, the relative error does not decay with the scale, but remains of order $Œ≥^{\frac12}\left| \log Œ≥\right|^2$. We also prove anomalous regularization under the quenched law: for almost every realization of the drift, solutions of the associated elliptic equation are H√∂lder continuous with exponent $1 - CŒ≥^{\frac12}$ and satisfy estimates which are uniform in the molecular diffusivity $ŒΩ$ and the scale.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [42] [Scattering laws for interfaces in self-gravitating matter flows](https://arxiv.org/abs/2601.21773)
*Bruno Le Floch,Philippe G. LeFloch*

Main category: gr-qc

TL;DR: The paper develops a framework connecting phase transition dynamics with bouncing cosmology by introducing scattering maps on gravitational singularity and fluid-discontinuity hypersurfaces to evolve self-gravitating matter fields through interfaces.


<details>
  <summary>Details</summary>
Motivation: To understand how self-gravitating matter fields evolve through phase transitions and connect phase transition dynamics with bouncing cosmology concepts, particularly addressing the challenge of evolving across gravitational singularities and fluid discontinuities.

Method: Introduces scattering maps on two classes of hypersurfaces: gravitational singularity hypersurfaces and fluid-discontinuity hypersurfaces. Analyzes causal structures from light and acoustic cones to formulate local evolution problems for Einstein-Euler systems with interfaces. Builds on previous work with G. Veneziano for quiescent singularities.

Result: Develops a framework requiring suitable scattering relations to supplement field equations for uniqueness and complete macroscopic evolution description. Identifies junction prescriptions compatible with Einstein and Euler equations, particularly constraint propagation, yielding rigid universal relations plus model-dependent parameters.

Conclusion: Under physical requirements (general covariance, causality, constraint compatibility, ultra-locality), aims to classify admissible scattering relations from microscopic physics that characterize macroscopic dynamics of fluids coupled to Einstein gravity through interfaces.

Abstract: We consider the evolution of self-gravitating matter fields that may undergo phase transitions, and we connect ideas from phase transition dynamics with concepts from bouncing cosmology. Our framework introduces scattering maps prescribed on two classes of hypersurfaces: a gravitational singularity hypersurface and a fluid-discontinuity hypersurface. By analyzing the causal structures induced by the light cone and the acoustic cone, we formulate a local evolution problem for the Einstein-Euler system in the presence of such interfaces. We explain how suitable scattering relations must supplement the field equations in order to ensure uniqueness and thus yield a complete macroscopic description of the evolution. This viewpoint builds on a theory developed in collaboration with G. Veneziano for quiescent (velocity-dominated) singularities in solutions of the Einstein equations coupled to a scalar field, where the passage across the singular hypersurface is encoded by a singularity scattering map. The guiding question is to identify junction prescriptions that are compatible with the Einstein and Euler equations, in particular with the propagation of constraints. The outcome is a rigid set of universal relations, together with a family of model-dependent parameters. Under physically motivated requirements (general covariance, causality, constraint compatibility, and ultra-locality), we aim to classify admissible scattering relations arising from microscopic physics and characterizing, at the macroscopic level, the dynamics of a fluid coupled to Einstein gravity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Conditional Denoising Model as a Physical Surrogate Model](https://arxiv.org/abs/2601.21021)
*Jos√© Afonso,Pedro Viegas,Rodrigo Ventura,Vasco Guerra*

Main category: cs.LG

TL;DR: CDM is a generative model that learns physical manifold geometry through denoising, achieving better physics adherence than soft-constraint methods without explicit equation training.


<details>
  <summary>Details</summary>
Motivation: Current surrogate modeling faces trade-off between accuracy and physics consistency. Soft constraints often fail to guarantee strict adherence to governing equations, while post-processing corrections don't learn underlying solution geometry.

Method: Introduces Conditional Denoising Model (CDM) that learns physical manifold geometry by training network to restore clean states from noisy ones. Uses time-independent formulation with deterministic fixed-point iteration for inference, projecting noisy approximations onto equilibrium manifold.

Result: CDM achieves higher parameter and data efficiency than physics-consistent baselines. Denoising objective acts as powerful implicit regularizer - despite never seeing governing equations during training, model adheres to physical constraints more strictly than baselines trained with explicit physics losses.

Conclusion: CDM provides novel approach to physics-consistent surrogate modeling by learning solution manifold geometry through denoising, offering better physical adherence without explicit equation knowledge during training.

Abstract: Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.

</details>


### [44] [LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models](https://arxiv.org/abs/2601.21623)
*Stanislav Budzinskiy,Marian Gloser,Tolunay Yilmaz,Ying Hong Tham,Yuanyi Lin,Wenyi Fang,Fan Wu,Philipp Petersen*

Main category: cs.LG

TL;DR: Adaptive mixed-precision strategy for transformer inference that selectively recomputes critical components with higher precision while using lower precision for most computations, achieving up to 100x accuracy improvements with minimal recomputation overhead.


<details>
  <summary>Details</summary>
Motivation: Mixed-precision computations are essential for efficient AI deployment, especially for large language models. The paper addresses the challenge of maintaining accuracy while using lower precision for transformer inference by focusing on compositionally-rich functions where rounding errors accumulate through function compositions.

Method: Based on rounding error analysis of function compositions f(g(x)), the method provides an adaptive strategy that identifies and selectively recomputes a small subset of critical components of g(x) with higher precision, while allowing all other computations to use lower precision. This strategy is specifically applied to different compositions within transformer architectures.

Result: Numerical experiments on GPT-2 models demonstrate that very low recomputation rates (minimal overhead) can achieve accuracy improvements of up to two orders of magnitude (up to 100x improvement) compared to uniform low-precision computation.

Conclusion: The adaptive mixed-precision strategy enables efficient transformer inference with significantly improved accuracy by intelligently allocating higher precision only to the most error-sensitive components, making it practical for local deployment of large language models while maintaining computational efficiency.

Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.

</details>


### [45] [PILD: Physics-Informed Learning via Diffusion](https://arxiv.org/abs/2601.21284)
*Tianyi Zeng,Tianyi Wang,Jiaru Zhang,Zimo Zeng,Feiyang Zhang,Yiming Xu,Sikai Chen,Yajie Zou,Yangyang Wang,Junfeng Jiao,Christian Claudel,Xinbo Chen*

Main category: cs.LG

TL;DR: PILD integrates diffusion models with physical constraints using virtual residual observations and conditional embedding to ensure physics compliance in generative tasks.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models are purely data-driven and don't incorporate physical laws, limiting their applicability in engineering and scientific problems where physical constraints must be followed.

Method: Proposes Physics-Informed Learning via Diffusion (PILD) with two key components: 1) virtual residual observation sampled from Laplace distribution to supervise generation during training, and 2) conditional embedding module to inject physical information into denoising network at multiple layers.

Result: Extensive experiments across vehicle trajectories, tire forces, Darcy flow, and plasma dynamics show PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.

Conclusion: PILD provides a concise, modular framework that successfully unifies diffusion modeling with first-principles physical constraints, making it broadly applicable to problems governed by ODEs, PDEs, and algebraic equations/inequality constraints.

Abstract: Diffusion models have emerged as powerful generative tools for modeling complex data distributions, yet their purely data-driven nature limits applicability in practical engineering and scientific problems where physical laws need to be followed. This paper proposes Physics-Informed Learning via Diffusion (PILD), a framework that unifies diffusion modeling and first-principles physical constraints by introducing a virtual residual observation sampled from a Laplace distribution to supervise generation during training. To further integrate physical laws, a conditional embedding module is incorporated to inject physical information into the denoising network at multiple layers, ensuring consistent guidance throughout the diffusion process. The proposed PILD framework is concise, modular, and broadly applicable to problems governed by ordinary differential equations, partial differential equations, as well as algebraic equations or inequality constraints. Extensive experiments across engineering and scientific tasks including estimating vehicle trajectories, tire forces, Darcy flow and plasma dynamics, demonstrate that our PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.

</details>


### [46] [PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training](https://arxiv.org/abs/2601.22137)
*Shenghao Yang,Zhichao Wang,Oleg Balabanov,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: PRISM is a framework that accelerates iterative matrix function computations (like square roots and orthogonalization) by combining adaptive polynomial approximation with randomized sketching, requiring no explicit spectral bounds and automatically adapting to evolving spectra.


<details>
  <summary>Details</summary>
Motivation: Matrix functions (square root, inverse roots, orthogonalization) are crucial for preconditioned gradient methods in neural network training, but existing iterative algorithms need acceleration to be more efficient on modern GPU accelerators.

Method: PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance with minimal overhead. It's applied to accelerate Newton-Schulz-like iterations for matrix functions.

Result: PRISM accelerates training when integrated into Shampoo and Muon optimizers, requiring no explicit spectral bounds or singular value estimates, and adapts automatically to the evolving spectrum.

Conclusion: PRISM provides an effective general framework for accelerating iterative matrix function computations in machine learning, offering adaptive polynomial approximation with randomized sketching that outperforms prior methods.

Abstract: Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [47] [Phase analysis of Ising machines and their implications on optimization](https://arxiv.org/abs/2507.08533)
*Shu Zhou,K. Y. Michael Wong,Juntao Wang,David Shui Wing Hui,Daniel Ebler,Jie Sun*

Main category: cond-mat.stat-mech

TL;DR: Analysis shows optimal Ising machine design requires binary spin distributions and coexistence of binary/gapless phases, achieved via digitization operations like digCIM algorithm.


<details>
  <summary>Details</summary>
Motivation: Ising machines offer computational advantages for combinatorial optimization but suffer from heuristic parameter tuning and lack systematic design principles, limiting solution quality.

Method: Analyze phase diagrams of spin distributions in Sherrington-Kirkpatrick model to identify optimal operating conditions, then design digitization operations to expand coexistence phase region.

Result: Ground state achieved in binary spin distribution phase; optimal solutions produced where binary and gapless phases coexist; digitization operations expand coexistence region enabling superior Ising machines.

Conclusion: Systematic Ising machine design requires targeting binary/gapless phase coexistence, achievable through digitization operations like digCIM algorithm, providing superior optimization performance.

Abstract: Ising machines, which are dynamical systems designed to operate in a parallel and iterative manner, have emerged as a new paradigm for solving combinatorial optimization problems. Despite computational advantages, the quality of solutions depends heavily on the form of dynamics and tuning of parameters, which are in general set heuristically due to the lack of systematic insights. Here, we focus on optimal Ising machine design by analyzing phase diagrams of spin distributions in the Sherrington-Kirkpatrick model. We find that that the ground state can be achieved in the phase where the spin distribution becomes binary, and optimal solutions are produced where the binary phase and gapless phase coexist. Our analysis shows that such coexistence phase region can be expanded by carefully placing a digitization operation, giving rise to a family of superior Ising machines, as illustrated by the proposed algorithm digCIM.

</details>


### [48] [High-precision Dynamic Monte Carlo Study of Rigidity Percolation](https://arxiv.org/abs/2601.21399)
*Mingzhong Lu,Yufeng Song,Qiyuan Shi,Ming Li,Youjin Deng*

Main category: cond-mat.stat-mech

TL;DR: Dynamic pebble game algorithm reveals temporal self-similarity and cascade events in rigidity percolation on triangular lattice, providing improved critical exponents.


<details>
  <summary>Details</summary>
Motivation: Most studies on rigidity percolation focus on static properties, while dynamics of the rigidity transition remain less explored. The authors aim to investigate how rigid clusters emerge and evolve dynamically during bond addition.

Method: Developed a dynamic pebble game algorithm that monitors rigid cluster evolution as bonds are sequentially added to an empty lattice. Used event-based ensemble approach to analyze cascade events where single bond additions trigger extensive cluster mergers.

Result: Discovered temporal self-similarity in cluster dynamics, identified large-scale cascade events scaling with system size, and obtained high-precision critical parameters: p_c = 0.6602778(10), 1/ŒΩ = 0.850(3), and fractal dimension d_f = 1.850(2).

Conclusion: The dynamic approach reveals previously overlooked temporal self-similarity in rigidity percolation and provides substantially improved estimates of critical exponents, advancing understanding of mechanical stability onset in disordered materials.

Abstract: Rigidity percolation provides an important basis for understanding the onset of mechanical stability in disordered materials. While most studies on the triangular lattice have focused on static properties at fixed bond~(site) occupation probabilities, the dynamics of the rigidity transition remain less explored. In this work, we formulate a dynamic pebble game algorithm that monitors how rigid clusters emerge and evolve as bonds are added sequentially to an empty lattice, with computational efficiency comparable to the standard static pebble game. We uncover a previously overlooked temporal self-similarity exhibited in multiple quantities, including the cluster size changes and merged cluster sizes during bond addition, as well as the number of simultaneously merging clusters. We identify large-scale cascade events in which a single bond addition triggers the merger of an extensive number of clusters that scales with system size with inverse correlation-length exponent. Using an event-based ensemble approach, we obtain high-precision estimates of the critical point $p_c = 0.660\,277\,8(10)$, the inverse correlation-length exponent $1/ŒΩ= 0.850(3)$, and the fractal dimension $d_f = 1.850(2)$, representing substantial improvements over existing values.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [49] [Towards regularized learning from functional data with covariate shift](https://arxiv.org/abs/2601.21019)
*Markus Holzleitner,Sergiy Pereverzyev,Sergei V. Pereverzyev,Vaibhav Silmana,S. Sivananthan*

Main category: math.ST

TL;DR: A general regularization framework for unsupervised domain adaptation in vector-valued regression under covariate shift using vector-valued RKHS, with theoretical convergence rates and aggregation-based parameter selection.


<details>
  <summary>Details</summary>
Motivation: Address challenges of covariate shift in domain adaptation where input distributions differ between training and test data, making reliable learning difficult for vector-valued regression tasks.

Method: Uses vector-valued reproducing kernel Hilbert spaces (vRKHS) with restricted hypothesis space for operator learning with functional outputs. Proposes aggregation-based approach combining estimators from different regularization parameters and kernels.

Result: Establishes optimal convergence rates under general source conditions, provides theoretical justification for aggregation method, and demonstrates robustness on real-world face image dataset.

Conclusion: The framework effectively handles covariate shift in vector-valued regression, with theoretical guarantees and practical aggregation approach for parameter selection, showing promise for real-world applications.

Abstract: This paper investigates a general regularization framework for unsupervised domain adaptation in vector-valued regression under the covariate shift assumption, utilizing vector-valued reproducing kernel Hilbert spaces (vRKHS). Covariate shift occurs when the input distributions of the training and test data differ, introducing significant challenges for reliable learning. By restricting the hypothesis space, we develop a practical operator learning algorithm capable of handling functional outputs. We establish optimal convergence rates for the proposed framework under a general source condition, providing a theoretical foundation for regularized learning in this setting. We also propose an aggregation-based approach that forms a linear combination of estimators corresponding to different regularization parameters and different kernels. The proposed approach addresses the challenge of selecting appropriate tuning parameters, which is crucial for constructing a good estimator, and we provide a theoretical justification for its effectiveness. Furthermore, we illustrate the proposed method on a real-world face image dataset, demonstrating robustness and effectiveness in mitigating distributional discrepancies under covariate shift.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [50] [Solving the Offline and Online Min-Max Problem of Non-smooth Submodular-Concave Functions: A Zeroth-Order Approach](https://arxiv.org/abs/2601.21243)
*Amir Ali Farzin,Yuen-Man Pun,Philipp Braun,Tyler Summers,Iman Shames*

Main category: math.OC

TL;DR: Zeroth-order method for max-min problems with non-smooth submodular-concave objectives, using Lov√°sz extension and Gaussian smoothing, with convergence guarantees to Œµ-saddle points.


<details>
  <summary>Details</summary>
Motivation: Address max-min and min-max optimization problems with challenging objective functions that are non-smooth, submodular with respect to the minimizer, and concave with respect to the maximizer. These problems arise in various applications but are difficult to solve due to the complex structure of the objective functions.

Method: A zeroth-order method that combines: 1) using the subgradient of the Lov√°sz extension of the objective function with respect to the minimizer, and 2) applying Gaussian smoothing to estimate the smoothed function gradient with respect to the maximizer. The method handles both offline and online settings.

Result: Theoretically proven convergence to an Œµ-saddle point in expectation for the offline case. For the online setting, the algorithm achieves O(‚àö(N¬∑PÃÑ_N)) online duality gap in expectation, where N is iterations and PÃÑ_N is the path length of optimal decisions. Complexity analysis and hyperparameter selection are provided.

Conclusion: The proposed zeroth-order method effectively solves challenging max-min problems with non-smooth submodular-concave objectives, with theoretical convergence guarantees in both offline and online settings, supported by numerical validation.

Abstract: We consider max-min and min-max problems with objective functions that are possibly non-smooth, submodular with respect to the minimiser and concave with respect to the maximiser. We investigate the performance of a zeroth-order method applied to this problem. The method is based on the subgradient of the Lov√°sz extension of the objective function with respect to the minimiser and based on Gaussian smoothing to estimate the smoothed function gradient with respect to the maximiser. In expectation sense, we prove the convergence of the algorithm to an $Œµ$-saddle point in the offline case. Moreover, we show that, in the expectation sense, in the online setting, the algorithm achieves $O(\sqrt{N\bar{P}_N})$ online duality gap, where $N$ is the number of iterations and $\bar{P}_N$ is the path length of the sequence of optimal decisions. The complexity analysis and hyperparameter selection are presented for all the cases. The theoretical results are illustrated via numerical examples.

</details>


### [51] [On Approximate Computation of Critical Points](https://arxiv.org/abs/2601.21917)
*Amir Ali Ahmadi,Georgina Hall*

Main category: math.OC

TL;DR: Computing even very coarse approximations of critical points is NP-hard for simple nonconvex polynomial functions, challenging the common belief that approximate critical point computation is tractable in nonconvex optimization.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the commonly-held belief in optimization that approximate computation of critical points is a tractable task for nonconvex functions. There's a need to establish computational hardness results for this fundamental problem in nonconvex optimization.

Method: The authors prove computational hardness results through complexity theory arguments. They show that if a polynomial-time algorithm could compute coarse approximations of critical points (with gradient norm ‚â§ 2^n) for constant-degree polynomials, then P=NP. They also extend these hardness results to various structural settings including guaranteed existence/uniqueness of critical points, lower-bounded functions, and distance-based approximation measures.

Result: The main result proves that computing even very coarse approximations of critical points (with gradient norm bounded by 2^n) for simple classes of nonconvex functions (polynomials of constant degree as low as three) is NP-hard. Additional hardness results are established under various structural assumptions, showing that approximate critical point computation remains intractable even in more constrained settings.

Conclusion: The paper demonstrates that approximate computation of critical points is fundamentally intractable for simple nonconvex functions, contradicting the widespread belief in optimization that this task is computationally feasible. These results establish important computational barriers in nonconvex optimization theory.

Abstract: We show that computing even very coarse approximations of critical points is intractable for simple classes of nonconvex functions. More concretely, we prove that if there exists a polynomial-time algorithm that takes as input a polynomial in $n$ variables of constant degree (as low as three) and outputs a point whose gradient has Euclidean norm at most $2^n$ whenever the polynomial has a critical point, then P=NP. The algorithm is permitted to return an arbitrary point when no critical point exists. We also prove hardness results for approximate computation of critical points under additional structural assumptions, including settings in which existence and uniqueness of a critical point are guaranteed, the function is lower bounded, and approximation is measured in terms of distance to a critical point. Overall, our results stand in contrast to the commonly-held belief that, in nonconvex optimization, approximate computation of critical points is a tractable task.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [52] [Description of electromagnetic fields in inhomogeneous accelerating sections. IV couplers](https://arxiv.org/abs/2601.21809)
*M. I. Ayzatsky*

Main category: physics.acc-ph

TL;DR: A new method simplifies coupled mode theory for waveguide-RF coupling using loops, reducing integro-differential equations to three independent ODE systems for easier analysis and simulation.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient approach for incorporating coupling elements (loops and transmission lines) into generalized coupled mode theory for waveguide-RF systems, simplifying the complex mathematical modeling required for such coupled systems.

Method: Used a simple model of waveguide coupling with external RF power through loops and transmission lines. The resulting Barbashin-type integro-differential system with degenerate kernels was reduced to three independent systems of ordinary differential equations: two describing field distribution from loops at input/output sections, and one describing field from electron beam without loops.

Result: Developed analytical model and computer code for matching loop couplers in X-band accelerating sections. Applied to simulate non-uniform sections, achieving low input reflection coefficient of 8E-3 without additional matching.

Conclusion: The proposed approach successfully simplifies complex coupled mode theory problems, enabling practical simulation and design of waveguide-RF coupling systems with good performance results.

Abstract: A new approach to incorporating coupling elements into a generalized coupled mode theory is presented. The simplest model of coupling of a structured waveguide with an external RF power source and load through loops and transmission lines was used. Even such a simple model significantly complicated the system of coupled equations. It turned into a coupled integro-differential system of the Barbashin type with degenerate kernels. Since the integral kernels are degenerate, this system is reduced to three independent systems of differential equations. Instead of solving a system of coupled integro-differential equations, we need to find solutions to three systems of ordinary differential equations. Two systems describe the distribution of the field excited by one loop and the specified value of the excitation current in it. In the first system the loop is located at the section's input, and in the second, at the section's output. The third system does not depend on the loop parameters at all. It describes the distribution of the field excited by an electron beam in a section without loops. Based on the developed analytical model, the computer code was developed for matching the loop couplers for the uniform accelerating sections of X-band. The calculation results were used to simulate the non-uniform section. Without additional matching, we obtained an input reflection coefficient of 8E-3.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [53] [Jellyfish exist](https://arxiv.org/abs/2601.21227)
*Ben Andrews,Glen Wheeler*

Main category: math.DG

TL;DR: The paper proves existence of infinitely many geometrically distinct solutions for three different geometric flows: homothetic expanders for elastic flow, epicyclic shrinkers for curve diffusion flow, and epicyclic expanders for ideal flow.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of multiple distinct geometric solutions for various geometric flows, expanding the known solution space and understanding the rich structure of these PDEs.

Method: The authors use geometric analysis techniques to prove existence of infinitely many geometrically distinct solutions, likely employing variational methods, PDE analysis, and geometric measure theory.

Result: Proved existence of infinitely many geometrically distinct homothetic expanders for elastic flow, epicyclic shrinkers for curve diffusion flow, and epicyclic expanders for ideal flow.

Conclusion: These results demonstrate the rich solution structure of geometric flows and provide new families of special solutions that could serve as models for understanding singularity formation and long-time behavior.

Abstract: We show the existence of infinitely many geometrically distinct homothetic expanders (jellyfish) for the elastic flow, epicyclic shrinkers for the curve diffusion flow, and epicyclic expanders for the ideal flow.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [54] [Wave generation via oscillatory reconnection at a three-dimensional magnetic null point](https://arxiv.org/abs/2601.21520)
*Luiz A. C. A. Schiavo,Gert J. J. Botha,James A. McLaughlin*

Main category: astro-ph.SR

TL;DR: 3D MHD simulation shows oscillatory reconnection at a magnetic null point generates both slow magnetoacoustic waves (propagating in all directions) and Alfv√©n waves (propagating perpendicular to spine motion in fan plane), with implications for coronal seismology.


<details>
  <summary>Details</summary>
Motivation: To investigate wave generation and time-dependent reconnection around a 3D magnetic null point, particularly understanding the types of waves produced and their propagation characteristics, which has implications for understanding wave phenomena in solar corona and coronal seismology.

Method: Three-dimensional nonlinear magnetohydrodynamic (MHD) simulation with non-periodic perturbation in xz-plane triggering oscillatory reconnection at 3D null point. Used three distinct wave proxies (compressible parallel, compressible transverse, incompressible parallel) and Spectral Proper Orthogonal Decomposition to analyze resultant MHD wave behavior.

Result: Oscillatory reconnection generates: 1) Slow magnetoacoustic wave of period P propagating outwards in all directions along spine and fan plane; 2) Propagating Alfv√©n wave of period P exclusively along y-axis in fan plane (perpendicular to spine motion). The system exhibits self-sustained oscillation with constant period P.

Conclusion: The study provides new insights into wave generation from 3D null points, revealing distinct propagation patterns for different wave modes, which advances understanding of wave phenomena in magnetic null configurations and has significant implications for coronal seismology applications.

Abstract: This work conducts a three-dimensional (3D), nonlinear magnetohydrodynamic (MHD) simulation to investigate wave generating, time-dependent reconnection around a magnetic null point. A non-periodic perturbation (in the $xz$-plane) triggers oscillatory reconnection (OR) at the 3D null, resulting in a self-sustained oscillation with a constant period $P$. We investigate the response of the system using three distinct wave proxies (compressible parallel, compressible transverse and incompressible parallel) as well as Spectral Proper Orthogonal Decomposition for decoupling and analyzing the resultant MHD wave behavior. We find that OR generates a slow magnetoacoustic wave of period $P$ that propagates outwards in all directions along the spine and fan plane of the 3D null point. We also find the generation of a propagating Alfv√©n wave of period $P$, exclusively along the $y$-axis in the fan plane, i.e. in the direction perpendicular to the spine motion. These findings provide new insights into waves generated from a 3D null point and their implications for coronal seismology.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [55] [Multiple binding modes of AKT on PIP$_3$-containing membranes](https://arxiv.org/abs/2601.21216)
*Yuki Nakagaki,Eiji Yamamoto*

Main category: physics.bio-ph

TL;DR: Molecular dynamics simulations reveal four distinct membrane-binding modes of full-length AKT on PIP3-containing bilayers, showing cooperative lipid recognition by both PH and kinase domains that depends on PIP3 concentration.


<details>
  <summary>Details</summary>
Motivation: The PI3K/AKT pathway is crucial for cellular signaling, but the molecular mechanism of how AKT's PH and kinase domains cooperate during membrane recruitment and activation remains unclear.

Method: Used molecular dynamics simulations of full-length AKT on PIP3-containing lipid bilayers to study membrane-binding conformations at the molecular level.

Result: Identified four distinct membrane-binding modes with different orientations and membrane contacts. Found specific PIP3 interactions with basic residues in the kinase domain, not just the PH domain. The most stable mode involves PIP3 binding to both canonical and secondary sites in the PH domain while exposing the activation-loop phosphorylation site. Binding mode populations depend on PIP3 concentration.

Conclusion: Lipid recognition by both PH and kinase domains cooperatively shapes AKT's membrane-bound conformations, providing molecular insights into AKT activation mechanisms on membranes.

Abstract: The PI3K/AKT signaling pathway is triggered by recruitment of AKT to cellular membranes. Although AKT is a multidomain serine/threonine kinase composed of an N-terminal pleckstrin homology (PH) domain and a C-terminal kinase domain, how these domains cooperate to regulate AKT activation on membranes remains unclear at the molecular level. Here, using molecular dynamics simulations of full-length AKT on PIP$_3$-containing lipid bilayers, we identify four distinct membrane-binding modes that differ in the orientations and membrane contacts of the PH and kinase domains. In addition to PIP$_3$ binding to the PH domain, we observe specific PIP$_3$ interactions with basic residues in the kinase domain. In the most stable mode, PIP$_3$ interacts with both the canonical and a secondary binding site in the PH domain, while the kinase domain adopts an orientation in which the activation-loop phosphorylation site is exposed to the solvent. Interestingly, the populations of these binding modes depend on the PIP$_3$ concentration in the membrane, leading to changes in the preferred orientation of AKT. These findings shed light on how lipid recognition by the PH domain and the kinase domain of AKT cooperatively shape its membrane-bound conformations.

</details>
