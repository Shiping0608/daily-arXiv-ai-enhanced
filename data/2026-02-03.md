<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 74]
- [math.AP](#math.AP) [Total: 67]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 12]
- [math.PR](#math.PR) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cond-mat.quant-gas](#cond-mat.quant-gas) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [math.SP](#math.SP) [Total: 3]
- [stat.ML](#stat.ML) [Total: 2]
- [math.DG](#math.DG) [Total: 6]
- [physics.optics](#physics.optics) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 2]
- [cs.LG](#cs.LG) [Total: 12]
- [math.OC](#math.OC) [Total: 2]
- [gr-qc](#gr-qc) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Large Language Models: A Mathematical Formulation](https://arxiv.org/abs/2601.22170)
*Ricardo Baptista,Andrew Stuart,Son Tran*

Main category: math.NA

TL;DR: A mathematical framework for understanding large language models (LLMs) that explains token encoding, next-token prediction architecture, learning processes, and deployment for various tasks using accessible mathematical concepts.


<details>
  <summary>Details</summary>
Motivation: To provide a clear mathematical foundation for understanding how LLMs work, despite their empirical success, there's a need for a structured framework that explains their underlying mechanisms using accessible mathematics.

Method: Develops a mathematical framework describing: 1) text encoding into token sequences, 2) next-token prediction model architecture, 3) learning from data processes, and 4) deployment for various tasks. Uses concepts from information theory, probability, and optimization.

Result: Establishes a platform for analyzing LLM accuracy, efficiency, and robustness, and suggests directions for developing modified and new methodologies. The framework makes complex LLM algorithms accessible through straightforward mathematical concepts.

Conclusion: The mathematical framework provides a foundation for understanding, analyzing, and improving LLMs, enabling systematic investigation of their properties and guiding future methodological developments in language modeling.

Abstract: Large language models (LLMs) process and predict sequences containing text to answer questions, and address tasks including document summarization, providing recommendations, writing software and solving quantitative problems. We provide a mathematical framework for LLMs by describing the encoding of text sequences into sequences of tokens, defining the architecture for next-token prediction models, explaining how these models are learned from data, and demonstrating how they are deployed to address a variety of tasks. The mathematical sophistication required to understand this material is not high, and relies on straightforward ideas from information theory, probability and optimization. Nonetheless, the combination of ideas resting on these different components from the mathematical sciences yields a complex algorithmic structure; and this algorithmic structure has demonstrated remarkable empirical successes. The mathematical framework established here provides a platform from which it is possible to formulate and address questions concerning the accuracy, efficiency and robustness of the algorithms that constitute LLMs. The framework also suggests directions for development of modified and new methodologies.

</details>


### [2] [On the $L^p$-Convergence and Denoising Performance of Durrmeyer-Type Max-Min Neural Network Operators](https://arxiv.org/abs/2601.22174)
*Berke Şahin,İsmail Aslan*

Main category: math.NA

TL;DR: Durrmeyer-type max-min neural network operators are introduced, proven to converge in L^p norm, with quantitative convergence rates and demonstrated superior smoothing and filtering performance compared to existing operators.


<details>
  <summary>Details</summary>
Motivation: To develop improved neural network operators that combine Durrmeyer-type generalizations with maximum-minimum operations for better approximation properties and signal processing capabilities.

Method: Generalize maximum-minimum neural network operators using Durrmeyer-type modifications, analyze sigmoidal functions and max-min operations, establish convergence properties in various norms, and derive quantitative convergence rates.

Result: Proved convergence in pointwise, supremum, and L^p norms for functions in L^p([a,b],[0,1]), derived quantitative convergence rates, and demonstrated superior smoothing and filtering performance compared to Kantorovich-type and standard max-min operators.

Conclusion: Durrmeyer-type max-min neural network operators provide effective approximation tools with proven convergence properties, smoother approximations, and superior filtering capabilities for both approximation theory and signal processing applications.

Abstract: In this paper, we investigate Durrmeyer-type generalizations of maximum-minimum neural network operators. The primary objective of this study is to establish the convergence of these operators in the $L^{p}$ norm for functions $f\in L^{p}([a,b],[0,1])$ with $1\leq p<\infty$. To this end, we analyze the properties of sigmoidal functions and maximum-minimum operations, subsequently establishing the convergence of the proposed operator in pointwise, supremum, and $L^{p}$ norms. Furthermore, we derive quantitative estimates for the rates of convergence. In the applications section, numerical and graphical examples demonstrate that the proposed Durrmeyer-type operators provide smoother approximations compared to Kantorovich-type and standard max-min operators. Finally, we highlight the superior filtering performance of these operators in signal analysis, validating their effectiveness in both approximation and data processing tasks.

</details>


### [3] [Convergence Analysis of the Discrete Constrained Saddle Dynamics and Their Momentum Variants](https://arxiv.org/abs/2601.22341)
*Qiang Du,Baoming Shi*

Main category: math.NA

TL;DR: Constrained saddle dynamics with momentum acceleration for finding saddle points on manifolds, with linear convergence rates and reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Need efficient methods to locate saddle points on manifolds for various applications, with challenges in convergence rates and computational cost.

Method: Discrete constrained saddle dynamics and momentum variants on manifolds, with theoretical analysis of convergence rates and eigenvector updates.

Result: Proved local linear convergence with rate depending on Hessian condition number, momentum accelerates convergence, single-step eigenvector update sufficient.

Conclusion: Momentum-based constrained saddle dynamics effectively accelerate convergence, especially for ill-conditioned problems, with reduced computational requirements.

Abstract: We study the discrete constrained saddle dynamics and their momentum variants for locating saddle points on manifolds. Under the assumption of exact unstable eigenvectors, we establish a local linear convergence of the discrete constrained saddle dynamics and show that the convergence rate depends on the condition number of the Riemannian Hessian. To mitigate this dependence, we introduce a momentum-based constrained saddle dynamics and prove local convergence of the continuous-time dynamics as well as the corresponding discrete scheme, which further demonstrates that momentum accelerates convergence, particularly in ill-conditioned settings. In addition, we show that a single-step eigenvector update is sufficient to guarantee local convergence; thus, the assumption of exact unstable eigenvectors is not necessary, which substantially reduces the computational cost. Finally, numerical experiments, including applications to the Thomson problem, the Rayleigh quotient on the Stiefel manifold, and the energy functional of Bose-Einstein condensates, are presented to complement the theoretical analysis.

</details>


### [4] [Low-Rank Approximation by Randomly Pivoted LU](https://arxiv.org/abs/2601.22344)
*Marc Aurèle Gilles,Heather Wilber*

Main category: math.NA

TL;DR: RPLU (Randomly Pivoted LU) provides efficient low-rank approximation with geometric convergence for matrices with rapidly decaying singular values, offering memory-efficient implementation and exploiting matrix structure.


<details>
  <summary>Details</summary>
Motivation: To develop a low-rank approximation algorithm that is both memory-efficient and can exploit matrix structure, addressing limitations of existing methods in constrained memory environments and for structured matrices.

Method: Randomly Pivoted LU (RPLU) - a Gaussian elimination variant where pivots are sampled proportional to squared entries of the Schur complement, enabling geometric convergence for matrices with rapidly decaying singular values.

Result: RPLU achieves geometric convergence in expectation, requires only O(k² + m + n) storage and O(k(m+n) + kM(A) + k³) operations for rank-k approximation, and outperforms existing methods for memory-constrained settings and structured matrices like Cauchy-like matrices.

Conclusion: RPLU is an effective low-rank approximation algorithm that combines memory efficiency with the ability to exploit matrix structure, making it suitable for applications in rational approximation and GPU-based linear system solving.

Abstract: The low-rank approximation properties of Randomly Pivoted LU (RPLU), a variant of Gaussian elimination where pivots are sampled proportional to the squared entries of the Schur complement, are analyzed. It is shown that the RPLU iterates converge geometrically in expectation for matrices with rapidly decaying singular values. RPLU outperforms existing low-rank approximation algorithms in two settings: first, when memory is limited, RPLU can be implemented with $\mathcal{O}(k^2 + m + n)$ storage and $\mathcal{O}( k(m + n)+ k\mathcal{M}(\mat{A}) + k^3)$ operations, where $\mathcal{M}(\mat{A})$ is the cost of a matvec with $\mat{A}\in\mathbb{C}^{n\times m}$ or its adjoint, for a rank-$k$ approximation. Second, when the matrix and its Schur complements share exploitable structure, such as for Cauchy-like matrices. The efficacy of RPLU is illustrated with several examples, including applications in rational approximation and solving large linear systems on GPUs.

</details>


### [5] [Forward-KL Convergence of Time-Inhomogeneous Langevin Diffusions](https://arxiv.org/abs/2601.22349)
*Andreas Habring,Martin Zach*

Main category: math.NA

TL;DR: Non-asymptotic analysis of time-dependent Langevin diffusions and their discretizations for annealing/tempering samplers, with convergence bounds in forward-KL divergence.


<details>
  <summary>Details</summary>
Motivation: Many practical samplers use time-dependent drifts (from annealing/tempering schedules) to improve exploration and stability, but lack unified non-asymptotic analysis of these Langevin diffusions and their discretizations.

Method: Provides unified non-asymptotic convergence analysis for continuous-time Langevin diffusions with time-dependent drifts and their Euler-Maruyama discretizations under abstract conditions on the drift, covering various annealing schemes.

Result: Derives convergence bounds in forward-Kullback-Leibler divergence for both continuous-time diffusion and discretized versions, applicable to practical annealing schemes like geometric tempering and annealed Langevin sampling.

Conclusion: The analysis provides theoretical foundation for time-dependent samplers, with numerical experiments comparing annealing schemes in low- and high-dimensional settings to validate the theory.

Abstract: Many practical samplers rely on time-dependent drifts -- often induced by annealing or tempering schedules -- to improve exploration and stability. This motivates a unified non-asymptotic analysis of the corresponding Langevin diffusions and their discretizations. We provide a convergence analysis that includes non-asymptotic bounds for the continuous-time diffusion and its Euler--Maruyama discretization in the forward-Kullback--Leibler divergence under a single set of abstract conditions on the time-dependent drift. The results apply to many practically-relevant annealing schemes, including geometric tempering and annealed Langevin sampling. In addition, we provide numerical experiments comparing the annealing schemes covered by our theory in low- and high-dimensional settings.

</details>


### [6] [Inverse acoustic scattering for random obstacles with multi-frequency data](https://arxiv.org/abs/2601.22560)
*Zhiqi Sun,Xiang Xu,Yiwen Lin*

Main category: math.NA

TL;DR: Two-stage inversion method for random obstacle scattering using Gaussian process modeling with KL expansion to recover both baseline shape and statistical characteristics from multi-frequency data.


<details>
  <summary>Details</summary>
Motivation: To develop an inverse scattering method for random obstacles where the scatterer boundary is modeled as a random process, requiring simultaneous recovery of both geometric shape and statistical properties of boundary fluctuations.

Method: Two-stage inversion: first stage reconstructs baseline shape using multi-frequency data; second stage estimates statistical characteristics (KL eigenvalues and covariance hyperparameters) of Gaussian process boundary fluctuations with modified covariance function.

Result: Theoretical justifications provided for model well-definedness, convergence of two-stage procedure, and uniqueness discussion. Numerical experiments show stable recovery of both geometric and statistical information for simple and complex obstacle shapes.

Conclusion: Proposed method successfully addresses inverse random obstacle scattering by combining Gaussian process modeling with two-stage inversion, enabling simultaneous recovery of deterministic baseline shape and statistical characteristics of random boundary fluctuations.

Abstract: We study an inverse random obstacle scattering problems in $\mathbb{R}^2$ where the scatterer is formulated by a Gaussian process defined on the angular parameter domain. Equipped with a modified covariance function which is mathematically well-defined and physically consistent, the Gaussian process admits a parameterization via Karhunen--Loève (KL) expansion. Based on observed multi-frequency data, we develop a two-stage inversion method: the first stage reconstructs the baseline shape of the random scatterer and the second stage estimates the statistical characteristics of the boundary fluctuations, including KL eigenvalues and covariance hyperparameters. We further provide theoretical justifications for the modeling and inversion pipeline, covering well-definedness of the Gaussian-process model, convergence for the two-stage procedure and a brief discussion on uniqueness. Numerical experiments demonstrate stable recovery of both geometric and statistical information for obstacles with simple and more complex shapes.

</details>


### [7] [An ultra-weak three-field finite element formulation for the biharmonic and extended Fisher--Kolmogorov equations](https://arxiv.org/abs/2601.22587)
*Rekha Khot,Bishnu P. Lamichhane,Ricardo Ruiz-Baier*

Main category: math.NA

TL;DR: Ultra-weak three-field formulation for biharmonic problem with solution, gradient, and Lagrange multiplier as unknowns; well-posedness established using saddle-point theory; conforming FEM with Raviart-Thomas discretization; extended to time-dependent semilinear extended Fisher-Kolmogorov equation.


<details>
  <summary>Details</summary>
Motivation: To develop a novel ultra-weak three-field formulation for the biharmonic problem that introduces the solution, its gradient, and an additional Lagrange multiplier as separate unknowns, allowing for more flexible discretization approaches.

Method: 1) Ultra-weak three-field formulation with solution, gradient, and Lagrange multiplier unknowns; 2) Well-posedness analysis using abstract saddle-point theory; 3) Conforming finite element scheme with Raviart-Thomas discretizations for auxiliary variables; 4) Discrete inf-sup condition for stability; 5) Extension to time-dependent semilinear extended Fisher-Kolmogorov equation.

Result: 1) Established well-posedness of continuous and discrete formulations; 2) Derived a priori error estimates; 3) Developed stable finite element scheme; 4) Successfully extended analysis to time-dependent semilinear case; 5) Demonstrated performance through numerical examples.

Conclusion: The proposed ultra-weak three-field formulation provides a robust framework for solving biharmonic problems with well-posed continuous and discrete formulations, stable finite element discretization, and extensibility to time-dependent semilinear equations, validated by numerical experiments.

Abstract: This paper discusses a so-called ultra-weak three-field formulation of the biharmonic problem where the solution, its gradient, and an additional Lagrange multiplier are the three unknowns. We establish the well-posedness of the problem using the abstract theory for saddle-point problems, and develop a conforming finite element scheme based on Raviart--Thomas discretisations of the two auxiliary variables. The well-posedness of the discrete formulation and the corresponding a priori error estimate are proved using a discrete inf-sup condition. We further extend the analysis to the time-dependent semilinear equation, namely extended Fisher--Kolmogorov equation. We present a few numerical examples to demonstrate the performance of our approach.

</details>


### [8] [An inertial minimal-deformation-rate framework for shape optimization](https://arxiv.org/abs/2601.22605)
*Falai Chen,Buyang Li,Jiajie Li,Rong Tang*

Main category: math.NA

TL;DR: A robust numerical framework combining inertial flow with mesh preservation for PDE-constrained shape optimization and Willmore-driven surface hole filling, achieving faster convergence and better mesh quality without remeshing.


<details>
  <summary>Details</summary>
Motivation: Address two key challenges in geometric evolution problems: (1) slow progress in flat energy landscapes leading to premature stagnation at suboptimal configurations, and (2) mesh deterioration during geometric evolution requiring frequent remeshing.

Method: Couples second-order inertial flow with minimal-deformation-rate (MDR) mesh motion strategy to accelerate convergence while preserving mesh quality. Incorporates surface-diffusion regularization within Barrett-Garcke-Nürberg (BGN) framework for robustness with non-smooth/non-convex initial geometries. Extends inertial MDR methodology to Willmore-type surface hole filling.

Result: Numerical experiments show markedly faster convergence to lower original objective values, with consistently superior mesh preservation throughout the evolution. Enables high-order smooth reconstructions even from incompatible initial data.

Conclusion: The proposed framework provides a robust solution for PDE-constrained shape optimization and surface hole filling problems, overcoming stagnation issues and mesh deterioration while eliminating the need for remeshing.

Abstract: We propose a robust numerical framework for PDE-constrained shape optimization and Willmore-driven surface hole filling. To address two central challenges -- slow progress in flat energy landscapes, which can trigger premature stagnation at suboptimal configurations, and mesh deterioration during geometric evolution -- we couple a second-order inertial flow with a minimal-deformation-rate (MDR) mesh motion strategy. This coupling accelerates convergence while preserving mesh quality and thus avoids remeshing. To further enhance robustness for non-smooth or non-convex initial geometries, we incorporate surface-diffusion regularization within the Barrett-Garcke-N"urnberg (BGN) framework. Moreover, we extend the inertial MDR methodology to Willmore-type surface hole filling, enabling high-order smooth reconstructions even from incompatible initial data. Numerical experiments demonstrate markedly faster convergence to lower original objective values, together with consistently superior mesh preservation throughout the evolution.

</details>


### [9] [A Mathematical Analysis of a Smooth-Convex-Concave Splitting Scheme for the Swift--Hohenberg Equation](https://arxiv.org/abs/2601.22687)
*Yuki Yonekura,Daiki Iwade,Shun Sato,Takayasu Matsuo*

Main category: math.NA

TL;DR: First linearly implicit finite difference scheme for 3D Swift-Hohenberg equation that preserves energy-dissipation law while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Existing structure-preserving schemes for the Swift-Hohenberg equation are fully implicit and computationally expensive, creating a need for more efficient methods that still preserve key physical properties.

Method: Introduces a design principle for dissipation-preserving finite difference schemes using discrete inequalities for the underlying energy, assuming Lipschitz continuous gradient and convexity/strong convexity of relevant terms. Results in a linearly implicit scheme for 3D Swift-Hohenberg equation.

Result: The proposed method preserves original energy-dissipation law, guarantees unique solvability, ensures boundedness of numerical solutions, and admits an a priori error estimate with sufficiently small time steps.

Conclusion: This is the first linearly implicit finite difference scheme for the Swift-Hohenberg equation that simultaneously achieves computational efficiency while preserving all key mathematical and physical properties.

Abstract: The Swift--Hohenberg equation is a widely studied fourth-order model, originally proposed to describe hydrodynamic fluctuations. It admits an energy-dissipation law and, under suitable assumptions, bounded solutions. Many structure-preserving numerical schemes have been proposed to retain such properties; however, existing approaches are often fully implicit and therefore computationally expensive. We introduce a simple design principle for constructing dissipation-preserving finite difference schemes and apply it to the Swift--Hohenberg equation in three spatial dimensions. Our analysis relies on discrete inequalities for the underlying energy, assuming a Lipschitz continuous gradient and either convexity or $μ$-strong convexity of the relevant terms. The resulting method is linearly implicit, yet it preserves the original energy-dissipation law, guarantees unique solvability, ensures boundedness of numerical solutions, and admits an a priori error estimate, provided that the time step is sufficiently small. To the best of our knowledge, this is the first linearly implicit finite difference scheme for the Swift--Hohenberg equation for which all of these properties are established.

</details>


### [10] [Numerical Differentiation of Functions of Two Variables Using Chebyshev Polynomials](https://arxiv.org/abs/2601.22762)
*Maksym Kyselov,Sergiy G. Solodky*

Main category: math.NA

TL;DR: A new truncation method using Chebyshev polynomial expansions and hyperbolic cross for numerical differentiation of bivariate functions from weighted Wiener classes.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for numerical differentiation of bivariate functions from weighted Wiener classes, addressing the challenge of reconstructing partial derivatives of arbitrary order with controlled error estimates.

Method: A new truncation method based on Chebyshev polynomial expansions and hyperbolic cross approach, exploiting Chebyshev polynomials' approximation properties and their connection to weighted spaces through the Chebyshev weight function.

Result: Derived a choice rule for truncation parameter as function of noise level, smoothness parameters, and differentiation order, enabling explicit error estimates in both weighted integral norms and uniform metric.

Conclusion: The proposed method provides a systematic approach for numerical differentiation of bivariate functions with rigorous error control in different metrics, leveraging Chebyshev polynomial properties and hyperbolic cross techniques.

Abstract: We investigate the problem of numerical differentiation of bivariate functions from weighted Wiener classes using Chebyshev polynomial expansions. We develop and analyze a new version of the truncation method based on Chebyshev polynomials and the idea of hyperbolic cross to reconstruct partial derivatives of arbitrary order. The method exploits the approximation properties of Chebyshev polynomials and their natural connection to weighted spaces through the Chebyshev weight function. We derive a choice rule for the truncation parameter as a function of the noise level, smoothness parameters of the function class, and the order of differentiation. This approach allows us to establish explicit error estimates in both weighted integral norms and uniform metric.

</details>


### [11] [Approximation of PDE solution manifolds: Sparse-grid interpolation and quadrature](https://arxiv.org/abs/2601.22825)
*Dinh Dũng,Van Kien Nguyen,Duong Thanh Pham,Christoph Schwab*

Main category: math.NA

TL;DR: The paper develops fully-discrete approximations and quadratures for infinite-variate functions in Bochner spaces using sparse-grid tensor-product polynomial interpolation based on Chebyshev points, with applications to parametric PDEs and holomorphic maps.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of approximating infinite-variate functions in high-dimensional parameter spaces, particularly for parametric PDEs and holomorphic maps, while avoiding the curse of dimensionality through efficient sparse-grid constructions.

Method: Uses sparse-grid tensor-product polynomial interpolation based on univariate Chebyshev points, with Jacobi generalized polynomial chaos expansions. Develops fully-discrete approximations via stable discretizations of Hilbert spaces and sparse-grid tensor-product projectors.

Result: Obtains convergence rates for approximations and quadratures free from the curse of dimensionality, verified for linear elliptic diffusion equations with affine-parametric coefficients and abstract holomorphic maps between Hilbert spaces.

Conclusion: The framework provides dimension-independent convergence rates for sparse-grid approximations and quadratures in infinite-variate settings, with applications to parametric PDEs and holomorphic maps, and lays groundwork for neural network extensions.

Abstract: We study fully-discrete approximations and quadratures of infinite-variate functions in abstract Bochner spaces associated with a Hilbert space $X$ and an infinite-tensor-product Jacobi measure. For target infinite-variate functions taking values in $X$ which admit absolutely convergent Jacobi generalized polynomial chaos expansions, with suitable weighted summability conditions for the coefficient sequences, we generalize and improve prior results on construction of sequences of finite sparse-grid tensor-product polynomial interpolation approximations and quadratures, based on the univariate Chebyshev points. For a generic stable discretization of $X$ in terms of a dense sequence $(V_m)_{m \in \mathbb{N}_0}$ of finite-dimensional subspaces, we obtain fully-discrete, linear approximations in terms of so-called sparse-grid tensor-product projectors, with convergence rates of approximations as well as of sparse-grid tensor-product quadratures of the target functions.
  We verify the abstract assumptions in two fundamental application settings: first, a linear elliptic diffusion equation with affine-parametric coefficients and second, abstract holomorphic maps between separable Hilbert spaces with affine-parametric input data encoding. For these settings, as in [37,20], cancellation of anti-symmetric terms in ultra-spherical Jacobi generalized polynomial chaos expansion coefficients implies crucially improved convergence rates of sparse-grid tensor-product quadrature with respect to the infinite-tensor-product Jacobi weight, free from the ``curse-of-dimension".
  Largely self-contained proofs of all results are developed. Approximation convergence rate results in the present setting which are based on construction of neural network surrogates, for unbounded parameter ranges with Gaussian measures, will be developed in extensions of the present work.

</details>


### [12] [On the convergence and efficiency of splitting schemes for the Cahn-Hilliard-Biot model](https://arxiv.org/abs/2601.22854)
*Cedric Riethmüller,Erlend Storvik*

Main category: math.NA

TL;DR: Novel solution strategy for Cahn-Hilliard-Biot model using semi-implicit time discretization and alternating minimization with proven convergence.


<details>
  <summary>Details</summary>
Motivation: The Cahn-Hilliard-Biot model presents significant challenges due to its coupled, nonlinear, and non-convex nature, requiring a consistent and efficient solution strategy for this three-way coupled system of solid phase separation, fluid dynamics, and elastic deformations in porous media.

Method: Introduces a semi-implicit time discretization that transforms the system into a convex minimization problem, then uses abstract convex theory to prove convergence of an alternating minimization method with flexible spatial discretization (requiring standard inverse inequalities).

Result: Numerical experiments demonstrate the promise of the proposed solution strategy in terms of both efficiency and robustness for solving the challenging Cahn-Hilliard-Biot system.

Conclusion: The paper presents a novel, theoretically sound solution strategy with proven convergence properties that effectively addresses the computational challenges of the coupled Cahn-Hilliard-Biot model.

Abstract: In this paper, we present a novel solution strategy for the Cahn-Hilliard-Biot model, a three-way coupled system that features the interplay of solid phase separation, fluid dynamics, and elastic deformations in porous media. It is a phase-field model that combines the Cahn-Hilliard regularized interface equation and Biot's equations of poroelasticity. Solving the system poses significant challenges due to its coupled, nonlinear, and non-convex nature. The main goal of this work is to provide a consistent and efficient solution strategy. With this in mind, we introduce a semi-implicit time discretization such that the resulting discrete system is equivalent to a convex minimization problem. Then, using abstract theory for convex problems, we prove the convergence of an alternating minimization method to the time-discrete system. The solution strategy is relatively flexible in terms of spatial discretization, although we require standard inverse inequalities for the guaranteed convergence of the alternating minimization method. Finally, we perform some numerical experiments that show the promise of the proposed solution strategy, both in terms of efficiency and robustness.

</details>


### [13] [Bayesian Interpolating Neural Network (B-INN): a scalable and reliable Bayesian model for large-scale physical systems](https://arxiv.org/abs/2601.22860)
*Chanwook Park,Brian Kim,Jiachen Guo,Wing Kam Liu*

Main category: math.NA

TL;DR: B-INN: A scalable Bayesian surrogate model combining interpolation theory with tensor decomposition for efficient uncertainty quantification in large-scale industrial simulations.


<details>
  <summary>Details</summary>
Motivation: Current neural networks and machine learning models for uncertainty quantification have limited scalability and poor reliability, making them impractical for industry-scale active learning where simulations take days/weeks and produce gigabytes of data.

Method: Combines high-order interpolation theory with tensor decomposition and alternating direction algorithm for effective dimensionality reduction while maintaining predictive accuracy. The model has linear complexity O(N) with respect to training samples.

Result: B-INNs are 20 to 10,000 times faster than Bayesian neural networks and Gaussian processes while providing robust uncertainty estimation. The function space is a subset of Gaussian processes.

Conclusion: B-INN provides a practical foundation for uncertainty-driven active learning in large-scale industrial simulations, offering computational efficiency and robust uncertainty calibration essential for industrial applications.

Abstract: Neural networks and machine learning models for uncertainty quantification suffer from limited scalability and poor reliability compared to their deterministic counterparts. In industry-scale active learning settings, where generating a single high-fidelity simulation may require days or weeks of computation and produce data volumes on the order of gigabytes, they quickly become impractical. This paper proposes a scalable and reliable Bayesian surrogate model, termed the Bayesian Interpolating Neural Network (B-INN). The B-INN combines high-order interpolation theory with tensor decomposition and alternating direction algorithm to enable effective dimensionality reduction without compromising predictive accuracy. We theoretically show that the function space of a B-INN is a subset of that of Gaussian processes, while its Bayesian inference exhibits linear complexity, $\mathcal{O}(N)$, with respect to the number of training samples. Numerical experiments demonstrate that B-INNs can be from 20 times to 10,000 times faster with a robust uncertainty estimation compared to Bayesian neural networks and Gaussian processes. These capabilities make B-INN a practical foundation for uncertainty-driven active learning in large-scale industrial simulations, where computational efficiency and robust uncertainty calibration are paramount.

</details>


### [14] [Randomized Methods for Kernelized DMD](https://arxiv.org/abs/2601.22867)
*Peter Oehme*

Main category: math.NA

TL;DR: The paper proposes using RPCholesky algorithm for kernelized DMD to accelerate processing of large-scale datasets through randomized low-rank approximations with better stability guarantees.


<details>
  <summary>Details</summary>
Motivation: Dynamic Mode Decomposition (DMD) needs acceleration for large-scale datasets, and existing randomized techniques for kernelized DMD lack stability guarantees and optimal tradeoffs between exploration and exploitation of data information.

Method: Apply RPCholesky algorithm to kernelized DMD (KDMD) using adaptive randomized sampling to approximate positive semidefinite kernel matrices, providing better stability than previous randomized methods.

Result: The proposed method demonstrates efficacy on established DMD benchmark problems with increasing dimensions, showing improved numerical stability and better tradeoff between exploration and exploitation.

Conclusion: RPCholesky algorithm combined with KDMD provides an effective randomized technique for accelerating DMD on large-scale datasets with enhanced stability guarantees compared to existing methods.

Abstract: Dynamic Mode Decomposition (DMD) is a data-driven method related to Koopman operator theory that extracts information about dominant dynamics from data snapshots. In this paper we examine techniques to accelerate the application of DMD to large-scale data sets with an eye on randomized techniques. Randomized techniques exploit low-rank matrix approximations at a much smaller computational cost, therefore permitting the use of increased data set sizes. In particular, we propose the application of the RPCholesky algorithm in the setting of kernelized DMD (KDMD). This algorithm relies on adaptive randomized sampling to approximate positive semidefinite kernel matrices and provides better stability guarantees than previously implemented randomized methods for KDMD. Differences between existing competitive randomized techniques and our proposed implementation are discussed with a focus on numerical stability and tradeoff between exploration and exploitation of information obtained from data. The efficacy of this new combination of algorithms is demonstrated on well-established benchmark problems from DMD literature increasing in problem dimension.

</details>


### [15] [FNWoS: Fractional Neural Walk-on-Spheres Methods for High-Dimensional PDEs Driven by $α$-stable Lévy Process on Irregular Domains](https://arxiv.org/abs/2601.22942)
*Ling Guo,Mingxin Qin,Changtao Sheng,Hao Wu,Fanhai Zeng*

Main category: math.NA

TL;DR: FNWoS is a derivative-free neural-enhanced Monte Carlo method for solving high-dimensional fractional Poisson equations on irregular domains, combining simplified walk-on-spheres with neural surrogates and buffered training.


<details>
  <summary>Details</summary>
Motivation: Solving high-dimensional fractional Poisson equations on irregular domains is computationally challenging due to the curse of dimensionality and complex boundary conditions. Traditional methods struggle with both high dimensions and irregular geometries.

Method: 1) Simplified fractional walk-on-spheres (FWoS) with constant weights to reduce per-trajectory cost; 2) Neural network surrogate integration (FNWoS) to amortize sampling; 3) Truncated path strategy for α near 2; 4) Buffered supervision (BFNWoS) with cached training pairs and progressive Monte Carlo target refinement.

Result: The method demonstrates accuracy, scalability, and computational efficiency in extensive numerical experiments, including tests on irregular domains and problems with dimensions up to 1000. It achieves more accurate evaluation with dramatically fewer trajectories than classical FWoS.

Conclusion: The proposed FNWoS and BFNWoS methods provide effective solutions for high-dimensional fractional Poisson equations on irregular domains, overcoming limitations of traditional Monte Carlo methods through neural network integration and buffered training strategies.

Abstract: In this paper, we develop a highly parallel and derivative-free fractional neural walk-on-spheres method (FNWoS) for solving high-dimensional fractional Poisson equations on irregular domains. We first propose a simplified fractional walk-on-spheres (FWoS) scheme that replaces the high-dimensional normalized weight integral with a constant weight and adopts a correspondingly simpler sampling density, substantially reducing per-trajectory cost. To mitigate the slow convergence of standard Monte Carlo sampling, FNWoS is then proposed via integrating this simplified FWoS estimator, derived from the Feynman-Kac representation, with a neural network surrogate. By amortizing sampling effort over the entire domain during training, FNWoS achieves more accurate evaluation at arbitrary query points with dramatically fewer trajectories than classical FWoS. To further enhance efficiency in regimes where the fractional order $α$ is close to 2 and trajectories become excessively long, we introduce a truncated path strategy with a prescribed maximum step count. Building on this, we propose a buffered supervision mechanism that caches training pairs and progressively refines their Monte Carlo targets during training, removing the need to precompute a highly accurate training set and yielding the buffered fractional neural walk-on-spheres method (BFNWoS). Extensive numerical experiments, including tests on irregular domains and problems with dimensions up to $1000$, demonstrate the accuracy, scalability, and computational efficiency of the proposed methods.

</details>


### [16] [Preconditioning and Numerical Stability in Neural Network Training for Parametric PDEs](https://arxiv.org/abs/2601.23185)
*Markus Bachmayr,Wolfgang Dahmen,Chenguang Duan,Mathias Oster*

Main category: math.NA

TL;DR: Preconditioning via well-conditioned frame representations improves neural network training for PDE approximations, with stable representations enabling low-precision computations without precision loss.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of neural network-based approximations for parameter-dependent PDE solutions by addressing numerical stability issues in preconditioning methods.

Method: Using well-conditioned frame representations of operators for preconditioning, and developing stable representations that enable computations with single- and half-precision floating point numbers.

Result: Significant improvement in training performance compared to standard methods, with stable representations allowing low-precision computations without loss of numerical precision.

Conclusion: Preconditioning via well-conditioned frame representations enhances neural network training for PDE approximations, and the proposed stable representations enable efficient low-precision computations while maintaining numerical stability.

Abstract: In the context of training neural network-based approximations of solutions of parameter-dependent PDEs, we investigate the effect of preconditioning via well-conditioned frame representations of operators and demonstrate a significant improvement on the performance of standard training methods. We also observe that standard representations of preconditioned matrices are insufficient for obtaining numerical stability and propose a generally applicable form of stable representations that enables computations with single- and half-precision floating point numbers without loss of precision.

</details>


### [17] [Applications of QR-based Vector-Valued Rational Approximation](https://arxiv.org/abs/2601.23237)
*Simon Dirckx*

Main category: math.NA

TL;DR: QR-AAA algorithm applied to multiple computational problems showing flexibility and effectiveness


<details>
  <summary>Details</summary>
Motivation: To demonstrate the practical utility and versatility of the QR-AAA algorithm across diverse computational applications

Method: QR-AAA algorithm - a greedy scheme for vector-valued rational approximation applied to various computational settings

Result: Successful applications in Stokes flow computation, multivariate rational approximation, function extension, novel quadrature methods, and near-field BEM approximation

Conclusion: QR-AAA algorithm proves to be flexible and practically effective across multiple computational domains

Abstract: Several applications of the QR-AAA algorithm, a greedy scheme for vector-valued rational approximation, are presented. The focus is on demonstrating the flexibility and practical effectiveness of QR-AAA in a variety of computational settings, including Stokes flow computation, multivariate rational approximation, function extension, the development of novel quadrature methods and near-field approximation in the boundary element method.

</details>


### [18] [A Primal-Dual Level Set Method for Computing Geodesic Distances](https://arxiv.org/abs/2601.23244)
*Hailiang Liu,Laura Zinnel*

Main category: math.NA

TL;DR: A primal-dual level set method for computing geodesic distances on surfaces using implicit surface representation and constraint minimization.


<details>
  <summary>Details</summary>
Motivation: Geodesic distance computation on surfaces is more complex than Euclidean distance due to surface geometry influence, with wide applications in various fields.

Method: Primal-dual level set method that implicitly represents surfaces as zero level sets, formulates constraint minimization problems, and uses regularization and acceleration techniques.

Result: The method is robust, efficient, easy to implement, with established convergence results for high-resolution PDE systems, and numerical evidence suggests convergence to geodesics in refinement limit.

Conclusion: The primal-dual level set approach provides an effective solution for geodesic distance computation on surfaces with theoretical convergence guarantees and practical implementation advantages.

Abstract: The numerical computation of shortest paths or geodesics on surfaces, along with the associated geodesic distance, has a wide range of applications. Compared to Euclidean distance computation, these tasks are more complex due to the influence of surface geometry on the behavior of shortest paths. This paper introduces a primal-dual level set method for computing geodesic distances. A key insight is that the underlying surface can be implicitly represented as a zero level set, allowing us to formulate a constraint minimization problem. We employ the primal-dual methodology, along with regularization and acceleration techniques, to develop our algorithm. This approach is robust, efficient, and easy to implement. We establish a convergence result for the high-resolution PDE system, and numerical evidence suggests that the method converges to a geodesic in the limit of refinement.

</details>


### [19] [Rank Reduction AutoEncoders for Mechanical Design: Advancing Novel and Efficient Data-Driven Topology Optimization](https://arxiv.org/abs/2601.23269)
*Ismael Ben-Yelun,Mohammed El Fallaki Idrissi,Jad Mounayer,Sebastian Rodriguez,Francisco Chinesta*

Main category: math.NA

TL;DR: A data-driven framework combining Rank Reduction Autoencoders (RRAEs) with neural networks for fast forward/inverse topology optimization, enabling efficient surrogate models for mechanical design.


<details>
  <summary>Details</summary>
Motivation: To address the computational expense of traditional topology optimization by developing fast surrogate models that can efficiently approximate relationships between optimized geometries and mechanical responses.

Method: Uses Rank Reduction Autoencoders (RRAEs) based on SVD to compress high-dimensional TO data into low-rank latent representations. Separate RRAEs are trained for geometry and different QoIs (scalar metrics, 1D stress fields, 2D von Mises distributions). Multilayer perceptrons then map between latent spaces for forward (geometry→response) and inverse (response→geometry) problems.

Result: The framework achieves accurate and computationally efficient surrogate models on a half MBB beam benchmark. Performance improves with richer QoIs, showing increasing robustness and fidelity. Enables generative mechanical design through latent-space exploration.

Conclusion: The proposed RRAE-based framework successfully enables fast forward/inverse analysis in topology optimization, providing a foundation for efficient surrogate modeling and generative mechanical design with potential applications in computational mechanics.

Abstract: This work presents a data-driven framework for fast forward and inverse analysis in topology optimization (TO) by combining Rank Reduction Autoencoders (RRAEs) with neural latent-space mappings. The methodology targets the efficient approximation of the relationship between optimized geometries and their corresponding mechanical responses or Quantity of Interest (QoI), with a particular focus on compliance-minimized linear elastic structures. High-dimensional TO results are first compressed using RRAEs, which encode the data into a low-rank approximation via Singular Value Decomposition (SVD), obtained in this sense the most important features that approximate the data. Separate RRAE models are trained for geometry and for different types of QoIs, including scalar metrics, one-dimensional stress fields, and full two-dimensional von Mises stress distributions. The resulting low-dimensional latent coefficients of the latent space are then related through multilayer perceptrons to address both direct problems -- predicting structural responses from geometry -- and inverse problems -- recovering geometries from prescribed performance targets. The proposed approach is demonstrated on a benchmark TO problem based on a half MBB beam, using datasets generated via density-based Solid Isotropic Material with Penalization (SIMP) optimization. Numerical results show that the framework enables accurate and computationally efficient surrogate models, with increasing robustness and fidelity as richer QoIs are considered. The methodology also provides a foundation for generative mechanical design by enabling the synthesis of new geometries and responses through latent-space exploration.

</details>


### [20] [Large Language Models: A Mathematical Formulation](https://arxiv.org/abs/2601.22170)
*Ricardo Baptista,Andrew Stuart,Son Tran*

Main category: math.NA

TL;DR: A mathematical framework for understanding large language models (LLMs) that covers token encoding, next-token prediction architecture, learning processes, and deployment for various tasks using basic mathematical concepts.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic mathematical foundation for understanding how LLMs work, despite their empirical success, by establishing a formal framework that explains their components and operations.

Method: Develops a mathematical framework describing: 1) text encoding into token sequences, 2) next-token prediction model architecture, 3) learning processes from data, and 4) deployment mechanisms for various tasks using concepts from information theory, probability, and optimization.

Result: Establishes a comprehensive mathematical platform that enables formulation and analysis of questions about LLM accuracy, efficiency, and robustness, while suggesting directions for new methodologies.

Conclusion: The paper provides an accessible mathematical framework for LLMs that explains their complex algorithmic structure and empirical success, offering a foundation for future research and methodological improvements.

Abstract: Large language models (LLMs) process and predict sequences containing text to answer questions, and address tasks including document summarization, providing recommendations, writing software and solving quantitative problems. We provide a mathematical framework for LLMs by describing the encoding of text sequences into sequences of tokens, defining the architecture for next-token prediction models, explaining how these models are learned from data, and demonstrating how they are deployed to address a variety of tasks. The mathematical sophistication required to understand this material is not high, and relies on straightforward ideas from information theory, probability and optimization. Nonetheless, the combination of ideas resting on these different components from the mathematical sciences yields a complex algorithmic structure; and this algorithmic structure has demonstrated remarkable empirical successes. The mathematical framework established here provides a platform from which it is possible to formulate and address questions concerning the accuracy, efficiency and robustness of the algorithms that constitute LLMs. The framework also suggests directions for development of modified and new methodologies.

</details>


### [21] [On the $L^p$-Convergence and Denoising Performance of Durrmeyer-Type Max-Min Neural Network Operators](https://arxiv.org/abs/2601.22174)
*Berke Şahin,İsmail Aslan*

Main category: math.NA

TL;DR: Durrmeyer-type generalizations of max-min neural network operators are proposed, showing L^p convergence with quantitative rates, and demonstrating smoother approximations and better filtering than Kantorovich-type operators.


<details>
  <summary>Details</summary>
Motivation: To develop improved neural network operators that provide smoother approximations and better filtering performance than existing Kantorovich-type and standard max-min operators, particularly for functions in L^p spaces.

Method: Durrmeyer-type generalizations of maximum-minimum neural network operators, analyzing properties of sigmoidal functions and max-min operations, establishing convergence in pointwise, supremum, and L^p norms with quantitative estimates.

Result: Proved convergence of operators in L^p norm for functions in L^p([a,b],[0,1]), derived quantitative convergence rates, demonstrated smoother approximations than Kantorovich-type operators, and showed superior filtering performance in signal analysis.

Conclusion: Durrmeyer-type max-min neural network operators provide effective approximation tools with smoother results and better filtering capabilities than existing alternatives, making them valuable for both approximation theory and signal processing applications.

Abstract: In this paper, we investigate Durrmeyer-type generalizations of maximum-minimum neural network operators. The primary objective of this study is to establish the convergence of these operators in the $L^{p}$ norm for functions $f\in L^{p}([a,b],[0,1])$ with $1\leq p<\infty$. To this end, we analyze the properties of sigmoidal functions and maximum-minimum operations, subsequently establishing the convergence of the proposed operator in pointwise, supremum, and $L^{p}$ norms. Furthermore, we derive quantitative estimates for the rates of convergence. In the applications section, numerical and graphical examples demonstrate that the proposed Durrmeyer-type operators provide smoother approximations compared to Kantorovich-type and standard max-min operators. Finally, we highlight the superior filtering performance of these operators in signal analysis, validating their effectiveness in both approximation and data processing tasks.

</details>


### [22] [Convergence Analysis of the Discrete Constrained Saddle Dynamics and Their Momentum Variants](https://arxiv.org/abs/2601.22341)
*Qiang Du,Baoming Shi*

Main category: math.NA

TL;DR: Constrained saddle dynamics with momentum acceleration for finding saddle points on manifolds, with proven linear convergence and reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Need efficient methods for locating saddle points on manifolds in optimization problems, particularly addressing slow convergence in ill-conditioned settings and high computational cost of eigenvector updates.

Method: Develop discrete constrained saddle dynamics and momentum variants for manifolds; prove convergence under exact unstable eigenvectors assumption; introduce momentum-based acceleration; show single-step eigenvector update suffices.

Result: Established local linear convergence with rate depending on Riemannian Hessian condition number; momentum accelerates convergence especially in ill-conditioned settings; single-step eigenvector update maintains convergence while reducing computational cost.

Conclusion: Momentum-based constrained saddle dynamics provide accelerated convergence for saddle point problems on manifolds, with practical efficiency improvements through reduced eigenvector computation requirements.

Abstract: We study the discrete constrained saddle dynamics and their momentum variants for locating saddle points on manifolds. Under the assumption of exact unstable eigenvectors, we establish a local linear convergence of the discrete constrained saddle dynamics and show that the convergence rate depends on the condition number of the Riemannian Hessian. To mitigate this dependence, we introduce a momentum-based constrained saddle dynamics and prove local convergence of the continuous-time dynamics as well as the corresponding discrete scheme, which further demonstrates that momentum accelerates convergence, particularly in ill-conditioned settings. In addition, we show that a single-step eigenvector update is sufficient to guarantee local convergence; thus, the assumption of exact unstable eigenvectors is not necessary, which substantially reduces the computational cost. Finally, numerical experiments, including applications to the Thomson problem, the Rayleigh quotient on the Stiefel manifold, and the energy functional of Bose-Einstein condensates, are presented to complement the theoretical analysis.

</details>


### [23] [Low-Rank Approximation by Randomly Pivoted LU](https://arxiv.org/abs/2601.22344)
*Marc Aurèle Gilles,Heather Wilber*

Main category: math.NA

TL;DR: RPLU (Randomly Pivoted LU) provides efficient low-rank approximation with geometric convergence for matrices with rapidly decaying singular values, offering advantages in memory-limited settings and for structured matrices like Cauchy-like matrices.


<details>
  <summary>Details</summary>
Motivation: The paper aims to analyze the low-rank approximation properties of RPLU, a variant of Gaussian elimination with randomized pivoting, to address computational efficiency challenges in memory-limited settings and for matrices with exploitable structure.

Method: RPLU uses Gaussian elimination where pivots are sampled proportional to the squared entries of the Schur complement. The algorithm is analyzed theoretically for convergence properties and computational complexity.

Result: RPLU converges geometrically in expectation for matrices with rapidly decaying singular values. It achieves O(k² + m + n) storage and O(k(m+n) + kM(A) + k³) operations for rank-k approximation, outperforming existing methods in memory-limited settings and for structured matrices like Cauchy-like matrices.

Conclusion: RPLU is an effective low-rank approximation algorithm that combines theoretical guarantees with practical efficiency, particularly valuable for memory-constrained applications and matrices with exploitable structure, as demonstrated in rational approximation and GPU-based linear system solving.

Abstract: The low-rank approximation properties of Randomly Pivoted LU (RPLU), a variant of Gaussian elimination where pivots are sampled proportional to the squared entries of the Schur complement, are analyzed. It is shown that the RPLU iterates converge geometrically in expectation for matrices with rapidly decaying singular values. RPLU outperforms existing low-rank approximation algorithms in two settings: first, when memory is limited, RPLU can be implemented with $\mathcal{O}(k^2 + m + n)$ storage and $\mathcal{O}( k(m + n)+ k\mathcal{M}(\mat{A}) + k^3)$ operations, where $\mathcal{M}(\mat{A})$ is the cost of a matvec with $\mat{A}\in\mathbb{C}^{n\times m}$ or its adjoint, for a rank-$k$ approximation. Second, when the matrix and its Schur complements share exploitable structure, such as for Cauchy-like matrices. The efficacy of RPLU is illustrated with several examples, including applications in rational approximation and solving large linear systems on GPUs.

</details>


### [24] [Forward-KL Convergence of Time-Inhomogeneous Langevin Diffusions](https://arxiv.org/abs/2601.22349)
*Andreas Habring,Martin Zach*

Main category: math.NA

TL;DR: Non-asymptotic analysis of time-dependent Langevin diffusions and their discretizations for annealing/tempering samplers, with convergence bounds in forward-KL divergence.


<details>
  <summary>Details</summary>
Motivation: Many practical samplers use time-dependent drifts (from annealing/tempering schedules) to improve exploration and stability, but lack unified non-asymptotic analysis of these Langevin diffusions and their discretizations.

Method: Provides non-asymptotic convergence analysis for continuous-time diffusion and its Euler-Maruyama discretization in forward-KL divergence under abstract conditions on time-dependent drift. Applies theory to various annealing schemes.

Result: Developed unified framework covering geometric tempering, annealed Langevin sampling, and other annealing schemes. Includes numerical experiments comparing schemes in low- and high-dimensional settings.

Conclusion: Provides comprehensive non-asymptotic analysis of time-dependent Langevin samplers, enabling better understanding and comparison of annealing schemes for improved sampling performance.

Abstract: Many practical samplers rely on time-dependent drifts -- often induced by annealing or tempering schedules -- to improve exploration and stability. This motivates a unified non-asymptotic analysis of the corresponding Langevin diffusions and their discretizations. We provide a convergence analysis that includes non-asymptotic bounds for the continuous-time diffusion and its Euler--Maruyama discretization in the forward-Kullback--Leibler divergence under a single set of abstract conditions on the time-dependent drift. The results apply to many practically-relevant annealing schemes, including geometric tempering and annealed Langevin sampling. In addition, we provide numerical experiments comparing the annealing schemes covered by our theory in low- and high-dimensional settings.

</details>


### [25] [Inverse acoustic scattering for random obstacles with multi-frequency data](https://arxiv.org/abs/2601.22560)
*Zhiqi Sun,Xiang Xu,Yiwen Lin*

Main category: math.NA

TL;DR: Two-stage inversion method for random obstacle scattering using Gaussian process modeling with KL expansion to recover both baseline shape and statistical characteristics from multi-frequency data.


<details>
  <summary>Details</summary>
Motivation: To solve inverse random obstacle scattering problems where scatterers have random boundary fluctuations, requiring methods to recover both geometric shape and statistical properties of the randomness.

Method: Two-stage inversion: 1) Reconstruct baseline shape using multi-frequency data, 2) Estimate statistical characteristics (KL eigenvalues and covariance hyperparameters) of boundary fluctuations using Gaussian process modeling with modified covariance function and KL expansion.

Result: Numerical experiments show stable recovery of both geometric and statistical information for obstacles with simple and complex shapes, with theoretical justifications for modeling and inversion pipeline.

Conclusion: The proposed Gaussian process model with modified covariance function and two-stage inversion method effectively solves inverse random obstacle scattering problems, recovering both deterministic baseline shapes and statistical characteristics of random fluctuations.

Abstract: We study an inverse random obstacle scattering problems in $\mathbb{R}^2$ where the scatterer is formulated by a Gaussian process defined on the angular parameter domain. Equipped with a modified covariance function which is mathematically well-defined and physically consistent, the Gaussian process admits a parameterization via Karhunen--Loève (KL) expansion. Based on observed multi-frequency data, we develop a two-stage inversion method: the first stage reconstructs the baseline shape of the random scatterer and the second stage estimates the statistical characteristics of the boundary fluctuations, including KL eigenvalues and covariance hyperparameters. We further provide theoretical justifications for the modeling and inversion pipeline, covering well-definedness of the Gaussian-process model, convergence for the two-stage procedure and a brief discussion on uniqueness. Numerical experiments demonstrate stable recovery of both geometric and statistical information for obstacles with simple and more complex shapes.

</details>


### [26] [An ultra-weak three-field finite element formulation for the biharmonic and extended Fisher--Kolmogorov equations](https://arxiv.org/abs/2601.22587)
*Rekha Khot,Bishnu P. Lamichhane,Ricardo Ruiz-Baier*

Main category: math.NA

TL;DR: Ultra-weak three-field formulation for biharmonic problem with solution, gradient, and Lagrange multiplier as unknowns; well-posedness established via saddle-point theory; conforming FEM using Raviart-Thomas elements; extended to time-dependent semilinear extended Fisher-Kolmogorov equation.


<details>
  <summary>Details</summary>
Motivation: To develop a novel ultra-weak three-field formulation for the biharmonic problem that introduces additional unknowns (solution, gradient, and Lagrange multiplier) to enable more flexible discretization approaches and extend the analysis to time-dependent semilinear equations.

Method: 1) Ultra-weak three-field formulation with solution, gradient, and Lagrange multiplier as unknowns; 2) Well-posedness analysis using abstract saddle-point theory; 3) Conforming finite element scheme with Raviart-Thomas discretizations for auxiliary variables; 4) Discrete inf-sup condition for discrete well-posedness; 5) Extension to time-dependent semilinear extended Fisher-Kolmogorov equation.

Result: 1) Established well-posedness of the continuous formulation; 2) Developed conforming finite element scheme with proven discrete well-posedness; 3) Derived a priori error estimates; 4) Successfully extended analysis to time-dependent semilinear equation; 5) Numerical examples demonstrate approach performance.

Conclusion: The ultra-weak three-field formulation provides a robust framework for biharmonic problems, enabling flexible discretization with Raviart-Thomas elements and extending naturally to time-dependent semilinear equations, with numerical results validating the theoretical analysis.

Abstract: This paper discusses a so-called ultra-weak three-field formulation of the biharmonic problem where the solution, its gradient, and an additional Lagrange multiplier are the three unknowns. We establish the well-posedness of the problem using the abstract theory for saddle-point problems, and develop a conforming finite element scheme based on Raviart--Thomas discretisations of the two auxiliary variables. The well-posedness of the discrete formulation and the corresponding a priori error estimate are proved using a discrete inf-sup condition. We further extend the analysis to the time-dependent semilinear equation, namely extended Fisher--Kolmogorov equation. We present a few numerical examples to demonstrate the performance of our approach.

</details>


### [27] [An inertial minimal-deformation-rate framework for shape optimization](https://arxiv.org/abs/2601.22605)
*Falai Chen,Buyang Li,Jiajie Li,Rong Tang*

Main category: math.NA

TL;DR: A robust numerical framework combining inertial flow with minimal-deformation-rate mesh motion for PDE-constrained shape optimization and Willmore-driven surface hole filling, addressing slow convergence and mesh deterioration issues.


<details>
  <summary>Details</summary>
Motivation: Address two key challenges in shape optimization: (1) slow progress in flat energy landscapes leading to premature stagnation at suboptimal configurations, and (2) mesh deterioration during geometric evolution that requires frequent remeshing.

Method: Couples second-order inertial flow with minimal-deformation-rate (MDR) mesh motion strategy to accelerate convergence while preserving mesh quality. Incorporates surface-diffusion regularization within Barrett-Garcke-Nürberg (BGN) framework for robustness with non-smooth/non-convex initial geometries. Extends inertial MDR methodology to Willmore-type surface hole filling.

Result: Numerical experiments show markedly faster convergence to lower original objective values, with consistently superior mesh preservation throughout the evolution. Enables high-order smooth reconstructions even from incompatible initial data.

Conclusion: The proposed framework provides a robust solution for PDE-constrained shape optimization and Willmore-driven surface hole filling, effectively addressing convergence and mesh quality issues without requiring remeshing.

Abstract: We propose a robust numerical framework for PDE-constrained shape optimization and Willmore-driven surface hole filling. To address two central challenges -- slow progress in flat energy landscapes, which can trigger premature stagnation at suboptimal configurations, and mesh deterioration during geometric evolution -- we couple a second-order inertial flow with a minimal-deformation-rate (MDR) mesh motion strategy. This coupling accelerates convergence while preserving mesh quality and thus avoids remeshing. To further enhance robustness for non-smooth or non-convex initial geometries, we incorporate surface-diffusion regularization within the Barrett-Garcke-N"urnberg (BGN) framework. Moreover, we extend the inertial MDR methodology to Willmore-type surface hole filling, enabling high-order smooth reconstructions even from incompatible initial data. Numerical experiments demonstrate markedly faster convergence to lower original objective values, together with consistently superior mesh preservation throughout the evolution.

</details>


### [28] [A Mathematical Analysis of a Smooth-Convex-Concave Splitting Scheme for the Swift--Hohenberg Equation](https://arxiv.org/abs/2601.22687)
*Yuki Yonekura,Daiki Iwade,Shun Sato,Takayasu Matsuo*

Main category: math.NA

TL;DR: A linearly implicit finite difference scheme for the 3D Swift-Hohenberg equation that preserves energy dissipation, ensures bounded solutions, and has unique solvability with a priori error estimates.


<details>
  <summary>Details</summary>
Motivation: Existing structure-preserving schemes for the Swift-Hohenberg equation are often fully implicit and computationally expensive. There's a need for more efficient methods that still preserve the important physical properties of the original equation.

Method: A simple design principle for constructing dissipation-preserving finite difference schemes applied to the 3D Swift-Hohenberg equation. The analysis uses discrete inequalities for the underlying energy, assuming Lipschitz continuous gradient and convexity/strong convexity of relevant terms.

Result: The resulting method is linearly implicit (more efficient than fully implicit), preserves the original energy-dissipation law, guarantees unique solvability, ensures boundedness of numerical solutions, and admits an a priori error estimate with sufficiently small time steps.

Conclusion: This is the first linearly implicit finite difference scheme for the Swift-Hohenberg equation that establishes all these properties simultaneously, offering an efficient alternative to existing fully implicit methods while maintaining structure preservation.

Abstract: The Swift--Hohenberg equation is a widely studied fourth-order model, originally proposed to describe hydrodynamic fluctuations. It admits an energy-dissipation law and, under suitable assumptions, bounded solutions. Many structure-preserving numerical schemes have been proposed to retain such properties; however, existing approaches are often fully implicit and therefore computationally expensive. We introduce a simple design principle for constructing dissipation-preserving finite difference schemes and apply it to the Swift--Hohenberg equation in three spatial dimensions. Our analysis relies on discrete inequalities for the underlying energy, assuming a Lipschitz continuous gradient and either convexity or $μ$-strong convexity of the relevant terms. The resulting method is linearly implicit, yet it preserves the original energy-dissipation law, guarantees unique solvability, ensures boundedness of numerical solutions, and admits an a priori error estimate, provided that the time step is sufficiently small. To the best of our knowledge, this is the first linearly implicit finite difference scheme for the Swift--Hohenberg equation for which all of these properties are established.

</details>


### [29] [Numerical Differentiation of Functions of Two Variables Using Chebyshev Polynomials](https://arxiv.org/abs/2601.22762)
*Maksym Kyselov,Sergiy G. Solodky*

Main category: math.NA

TL;DR: A new truncation method using Chebyshev polynomial expansions and hyperbolic cross for numerical differentiation of bivariate functions from weighted Wiener classes, with explicit error estimates.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of numerical differentiation of bivariate functions from weighted Wiener classes, which is challenging due to noise and the need to reconstruct partial derivatives of arbitrary order accurately.

Method: Develops a new truncation method based on Chebyshev polynomial expansions and hyperbolic cross approach. Uses Chebyshev polynomials' approximation properties and their connection to weighted spaces through Chebyshev weight function. Derives a rule for truncation parameter as function of noise level, smoothness parameters, and differentiation order.

Result: Establishes explicit error estimates in both weighted integral norms and uniform metric. The method allows reconstruction of partial derivatives of arbitrary order from noisy data.

Conclusion: The proposed Chebyshev polynomial-based truncation method with hyperbolic cross provides an effective approach for numerical differentiation of bivariate functions in weighted Wiener classes with rigorous error bounds.

Abstract: We investigate the problem of numerical differentiation of bivariate functions from weighted Wiener classes using Chebyshev polynomial expansions. We develop and analyze a new version of the truncation method based on Chebyshev polynomials and the idea of hyperbolic cross to reconstruct partial derivatives of arbitrary order. The method exploits the approximation properties of Chebyshev polynomials and their natural connection to weighted spaces through the Chebyshev weight function. We derive a choice rule for the truncation parameter as a function of the noise level, smoothness parameters of the function class, and the order of differentiation. This approach allows us to establish explicit error estimates in both weighted integral norms and uniform metric.

</details>


### [30] [Approximation of PDE solution manifolds: Sparse-grid interpolation and quadrature](https://arxiv.org/abs/2601.22825)
*Dinh Dũng,Van Kien Nguyen,Duong Thanh Pham,Christoph Schwab*

Main category: math.NA

TL;DR: The paper develops fully-discrete sparse-grid approximations and quadratures for infinite-variate functions in Bochner spaces, with improved convergence rates free from the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of approximating infinite-variate functions with high-dimensional parameter spaces, particularly for problems where traditional methods suffer from the curse of dimensionality. The work aims to develop efficient numerical methods for parametric PDEs and holomorphic maps.

Method: Uses sparse-grid tensor-product polynomial interpolation based on Chebyshev points, combined with stable discretization of Hilbert spaces. Leverages cancellation properties in Jacobi polynomial chaos expansions to improve convergence rates.

Result: Obtains fully-discrete approximations with convergence rates free from the curse of dimensionality. Demonstrates effectiveness for linear elliptic diffusion with affine-parametric coefficients and holomorphic maps between Hilbert spaces.

Conclusion: The developed sparse-grid framework provides efficient approximation and quadrature methods for infinite-variate problems, with theoretical guarantees and practical applications to parametric PDEs. Future work will extend to neural network surrogates for Gaussian measures.

Abstract: We study fully-discrete approximations and quadratures of infinite-variate functions in abstract Bochner spaces associated with a Hilbert space $X$ and an infinite-tensor-product Jacobi measure. For target infinite-variate functions taking values in $X$ which admit absolutely convergent Jacobi generalized polynomial chaos expansions, with suitable weighted summability conditions for the coefficient sequences, we generalize and improve prior results on construction of sequences of finite sparse-grid tensor-product polynomial interpolation approximations and quadratures, based on the univariate Chebyshev points. For a generic stable discretization of $X$ in terms of a dense sequence $(V_m)_{m \in \mathbb{N}_0}$ of finite-dimensional subspaces, we obtain fully-discrete, linear approximations in terms of so-called sparse-grid tensor-product projectors, with convergence rates of approximations as well as of sparse-grid tensor-product quadratures of the target functions.
  We verify the abstract assumptions in two fundamental application settings: first, a linear elliptic diffusion equation with affine-parametric coefficients and second, abstract holomorphic maps between separable Hilbert spaces with affine-parametric input data encoding. For these settings, as in [37,20], cancellation of anti-symmetric terms in ultra-spherical Jacobi generalized polynomial chaos expansion coefficients implies crucially improved convergence rates of sparse-grid tensor-product quadrature with respect to the infinite-tensor-product Jacobi weight, free from the ``curse-of-dimension".
  Largely self-contained proofs of all results are developed. Approximation convergence rate results in the present setting which are based on construction of neural network surrogates, for unbounded parameter ranges with Gaussian measures, will be developed in extensions of the present work.

</details>


### [31] [On the convergence and efficiency of splitting schemes for the Cahn-Hilliard-Biot model](https://arxiv.org/abs/2601.22854)
*Cedric Riethmüller,Erlend Storvik*

Main category: math.NA

TL;DR: Novel solution strategy for Cahn-Hilliard-Biot model using semi-implicit time discretization and alternating minimization with proven convergence.


<details>
  <summary>Details</summary>
Motivation: The Cahn-Hilliard-Biot model presents significant challenges due to its coupled, nonlinear, and non-convex nature, requiring consistent and efficient solution strategies for phase separation, fluid dynamics, and elastic deformations in porous media.

Method: Semi-implicit time discretization that transforms the discrete system into a convex minimization problem, followed by alternating minimization method with proven convergence using abstract convex theory. Flexible spatial discretization with standard inverse inequalities.

Result: The proposed solution strategy shows promise in numerical experiments, demonstrating both efficiency and robustness in solving the challenging three-way coupled system.

Conclusion: The paper presents a novel, consistent, and efficient solution strategy for the Cahn-Hilliard-Biot model with proven convergence properties and demonstrated practical effectiveness through numerical experiments.

Abstract: In this paper, we present a novel solution strategy for the Cahn-Hilliard-Biot model, a three-way coupled system that features the interplay of solid phase separation, fluid dynamics, and elastic deformations in porous media. It is a phase-field model that combines the Cahn-Hilliard regularized interface equation and Biot's equations of poroelasticity. Solving the system poses significant challenges due to its coupled, nonlinear, and non-convex nature. The main goal of this work is to provide a consistent and efficient solution strategy. With this in mind, we introduce a semi-implicit time discretization such that the resulting discrete system is equivalent to a convex minimization problem. Then, using abstract theory for convex problems, we prove the convergence of an alternating minimization method to the time-discrete system. The solution strategy is relatively flexible in terms of spatial discretization, although we require standard inverse inequalities for the guaranteed convergence of the alternating minimization method. Finally, we perform some numerical experiments that show the promise of the proposed solution strategy, both in terms of efficiency and robustness.

</details>


### [32] [Bayesian Interpolating Neural Network (B-INN): a scalable and reliable Bayesian model for large-scale physical systems](https://arxiv.org/abs/2601.22860)
*Chanwook Park,Brian Kim,Jiachen Guo,Wing Kam Liu*

Main category: math.NA

TL;DR: B-INN: A scalable Bayesian surrogate model combining interpolation theory with tensor decomposition for efficient uncertainty quantification in large-scale simulations, achieving 20-10,000x speedup over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current neural networks and machine learning models for uncertainty quantification have limited scalability and poor reliability, making them impractical for industry-scale active learning where simulations take days/weeks and produce gigabytes of data.

Method: Combines high-order interpolation theory with tensor decomposition and alternating direction algorithm for effective dimensionality reduction while maintaining predictive accuracy. Theoretically shows B-INN's function space is a subset of Gaussian processes with linear complexity O(N) Bayesian inference.

Result: B-INNs achieve 20 to 10,000 times faster performance with robust uncertainty estimation compared to Bayesian neural networks and Gaussian processes, making them practical for large-scale industrial simulations.

Conclusion: B-INN provides a practical foundation for uncertainty-driven active learning in large-scale industrial simulations where computational efficiency and robust uncertainty calibration are critical requirements.

Abstract: Neural networks and machine learning models for uncertainty quantification suffer from limited scalability and poor reliability compared to their deterministic counterparts. In industry-scale active learning settings, where generating a single high-fidelity simulation may require days or weeks of computation and produce data volumes on the order of gigabytes, they quickly become impractical. This paper proposes a scalable and reliable Bayesian surrogate model, termed the Bayesian Interpolating Neural Network (B-INN). The B-INN combines high-order interpolation theory with tensor decomposition and alternating direction algorithm to enable effective dimensionality reduction without compromising predictive accuracy. We theoretically show that the function space of a B-INN is a subset of that of Gaussian processes, while its Bayesian inference exhibits linear complexity, $\mathcal{O}(N)$, with respect to the number of training samples. Numerical experiments demonstrate that B-INNs can be from 20 times to 10,000 times faster with a robust uncertainty estimation compared to Bayesian neural networks and Gaussian processes. These capabilities make B-INN a practical foundation for uncertainty-driven active learning in large-scale industrial simulations, where computational efficiency and robust uncertainty calibration are paramount.

</details>


### [33] [Randomized Methods for Kernelized DMD](https://arxiv.org/abs/2601.22867)
*Peter Oehme*

Main category: math.NA

TL;DR: This paper proposes using RPCholesky algorithm for kernelized DMD to accelerate processing of large-scale datasets through randomized low-rank approximations with better stability guarantees.


<details>
  <summary>Details</summary>
Motivation: Dynamic Mode Decomposition (DMD) needs acceleration for large-scale datasets. Randomized techniques can reduce computational cost while maintaining accuracy, but existing methods have stability issues that need to be addressed.

Method: The authors propose applying the RPCholesky algorithm to kernelized DMD (KDMD). This algorithm uses adaptive randomized sampling to approximate positive semidefinite kernel matrices, providing better stability guarantees than previous randomized methods for KDMD.

Result: The proposed method demonstrates efficacy on established benchmark problems from DMD literature across increasing problem dimensions, showing improved numerical stability and better tradeoff between exploration and exploitation of data information compared to existing competitive randomized techniques.

Conclusion: RPCholesky algorithm combined with KDMD provides an effective approach for accelerating DMD on large-scale datasets with enhanced stability, making it suitable for high-dimensional problems where computational efficiency is critical.

Abstract: Dynamic Mode Decomposition (DMD) is a data-driven method related to Koopman operator theory that extracts information about dominant dynamics from data snapshots. In this paper we examine techniques to accelerate the application of DMD to large-scale data sets with an eye on randomized techniques. Randomized techniques exploit low-rank matrix approximations at a much smaller computational cost, therefore permitting the use of increased data set sizes. In particular, we propose the application of the RPCholesky algorithm in the setting of kernelized DMD (KDMD). This algorithm relies on adaptive randomized sampling to approximate positive semidefinite kernel matrices and provides better stability guarantees than previously implemented randomized methods for KDMD. Differences between existing competitive randomized techniques and our proposed implementation are discussed with a focus on numerical stability and tradeoff between exploration and exploitation of information obtained from data. The efficacy of this new combination of algorithms is demonstrated on well-established benchmark problems from DMD literature increasing in problem dimension.

</details>


### [34] [FNWoS: Fractional Neural Walk-on-Spheres Methods for High-Dimensional PDEs Driven by $α$-stable Lévy Process on Irregular Domains](https://arxiv.org/abs/2601.22942)
*Ling Guo,Mingxin Qin,Changtao Sheng,Hao Wu,Fanhai Zeng*

Main category: math.NA

TL;DR: FNWoS: A neural-enhanced walk-on-spheres method for solving high-dimensional fractional Poisson equations on irregular domains, with buffered supervision for improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Solving high-dimensional fractional Poisson equations on irregular domains is computationally challenging. Traditional walk-on-spheres methods suffer from slow convergence and high per-trajectory costs, especially when the fractional order α approaches 2, leading to excessively long trajectories.

Method: 1) Simplified fractional walk-on-spheres (FWoS) with constant weight and simpler sampling density; 2) Neural network surrogate integration (FNWoS) amortizing sampling over the domain; 3) Truncated path strategy for α near 2; 4) Buffered supervision (BFNWoS) caching training pairs and refining Monte Carlo targets progressively.

Result: The method demonstrates accuracy, scalability, and computational efficiency in extensive numerical experiments, including tests on irregular domains and problems with dimensions up to 1000. BFNWoS achieves accurate evaluation with dramatically fewer trajectories than classical FWoS.

Conclusion: FNWoS and BFNWoS provide effective solutions for high-dimensional fractional Poisson equations on irregular domains, combining neural network surrogates with walk-on-spheres methods to overcome computational bottlenecks while maintaining accuracy and scalability.

Abstract: In this paper, we develop a highly parallel and derivative-free fractional neural walk-on-spheres method (FNWoS) for solving high-dimensional fractional Poisson equations on irregular domains. We first propose a simplified fractional walk-on-spheres (FWoS) scheme that replaces the high-dimensional normalized weight integral with a constant weight and adopts a correspondingly simpler sampling density, substantially reducing per-trajectory cost. To mitigate the slow convergence of standard Monte Carlo sampling, FNWoS is then proposed via integrating this simplified FWoS estimator, derived from the Feynman-Kac representation, with a neural network surrogate. By amortizing sampling effort over the entire domain during training, FNWoS achieves more accurate evaluation at arbitrary query points with dramatically fewer trajectories than classical FWoS. To further enhance efficiency in regimes where the fractional order $α$ is close to 2 and trajectories become excessively long, we introduce a truncated path strategy with a prescribed maximum step count. Building on this, we propose a buffered supervision mechanism that caches training pairs and progressively refines their Monte Carlo targets during training, removing the need to precompute a highly accurate training set and yielding the buffered fractional neural walk-on-spheres method (BFNWoS). Extensive numerical experiments, including tests on irregular domains and problems with dimensions up to $1000$, demonstrate the accuracy, scalability, and computational efficiency of the proposed methods.

</details>


### [35] [Preconditioning and Numerical Stability in Neural Network Training for Parametric PDEs](https://arxiv.org/abs/2601.23185)
*Markus Bachmayr,Wolfgang Dahmen,Chenguang Duan,Mathias Oster*

Main category: math.NA

TL;DR: Preconditioning via well-conditioned frame representations improves neural network training for PDE approximations, with stable representations enabling low-precision computations.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of neural network training for approximating solutions of parameter-dependent PDEs, addressing issues with numerical stability in preconditioned representations.

Method: Using well-conditioned frame representations for preconditioning operators, and developing stable representations that work with single- and half-precision floating point numbers.

Result: Significant improvement in training performance and numerical stability, enabling computations with reduced precision without loss of accuracy.

Conclusion: Preconditioning with well-conditioned frames enhances neural network training for PDE approximations, and stable representations are crucial for maintaining precision in low-precision computations.

Abstract: In the context of training neural network-based approximations of solutions of parameter-dependent PDEs, we investigate the effect of preconditioning via well-conditioned frame representations of operators and demonstrate a significant improvement on the performance of standard training methods. We also observe that standard representations of preconditioned matrices are insufficient for obtaining numerical stability and propose a generally applicable form of stable representations that enables computations with single- and half-precision floating point numbers without loss of precision.

</details>


### [36] [Applications of QR-based Vector-Valued Rational Approximation](https://arxiv.org/abs/2601.23237)
*Simon Dirckx*

Main category: math.NA

TL;DR: QR-AAA algorithm applications in computational mathematics: Stokes flow, multivariate rational approximation, function extension, quadrature methods, and boundary element method near-field approximation.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the flexibility and practical effectiveness of the QR-AAA algorithm across diverse computational mathematics applications, showing its broad utility beyond basic rational approximation.

Method: QR-AAA algorithm - a greedy scheme for vector-valued rational approximation, applied to multiple computational settings including Stokes flow, multivariate rational approximation, function extension, quadrature development, and boundary element method near-field approximation.

Result: Successful demonstration of QR-AAA's effectiveness across various computational applications, showing its practical value in solving real-world computational mathematics problems.

Conclusion: QR-AAA algorithm proves to be a flexible and effective tool for vector-valued rational approximation with broad applicability across multiple computational mathematics domains.

Abstract: Several applications of the QR-AAA algorithm, a greedy scheme for vector-valued rational approximation, are presented. The focus is on demonstrating the flexibility and practical effectiveness of QR-AAA in a variety of computational settings, including Stokes flow computation, multivariate rational approximation, function extension, the development of novel quadrature methods and near-field approximation in the boundary element method.

</details>


### [37] [A Primal-Dual Level Set Method for Computing Geodesic Distances](https://arxiv.org/abs/2601.23244)
*Hailiang Liu,Laura Zinnel*

Main category: math.NA

TL;DR: A primal-dual level set method for computing geodesic distances on surfaces using implicit surface representation and constraint minimization.


<details>
  <summary>Details</summary>
Motivation: Geodesic distance computation on surfaces is more complex than Euclidean distance due to surface geometry influence, with wide applications in various fields.

Method: Implicit surface representation as zero level set, formulation as constraint minimization problem, primal-dual methodology with regularization and acceleration techniques.

Result: Robust, efficient, and easy-to-implement algorithm with convergence proof for high-resolution PDE system and numerical evidence of convergence to geodesics.

Conclusion: The primal-dual level set method provides an effective approach for geodesic distance computation with theoretical convergence guarantees.

Abstract: The numerical computation of shortest paths or geodesics on surfaces, along with the associated geodesic distance, has a wide range of applications. Compared to Euclidean distance computation, these tasks are more complex due to the influence of surface geometry on the behavior of shortest paths. This paper introduces a primal-dual level set method for computing geodesic distances. A key insight is that the underlying surface can be implicitly represented as a zero level set, allowing us to formulate a constraint minimization problem. We employ the primal-dual methodology, along with regularization and acceleration techniques, to develop our algorithm. This approach is robust, efficient, and easy to implement. We establish a convergence result for the high-resolution PDE system, and numerical evidence suggests that the method converges to a geodesic in the limit of refinement.

</details>


### [38] [Rank Reduction AutoEncoders for Mechanical Design: Advancing Novel and Efficient Data-Driven Topology Optimization](https://arxiv.org/abs/2601.23269)
*Ismael Ben-Yelun,Mohammed El Fallaki Idrissi,Jad Mounayer,Sebastian Rodriguez,Francisco Chinesta*

Main category: math.NA

TL;DR: A data-driven framework combining Rank Reduction Autoencoders with neural networks for fast forward/inverse topology optimization, enabling efficient surrogate models for compliance-minimized structures.


<details>
  <summary>Details</summary>
Motivation: To develop efficient surrogate models for topology optimization that can quickly approximate relationships between optimized geometries and mechanical responses, addressing both forward prediction and inverse design problems.

Method: Uses Rank Reduction Autoencoders (RRAEs) based on SVD to compress high-dimensional TO data into low-rank latent representations. Separate RRAEs are trained for geometry and different QoIs (scalar metrics, 1D stress fields, 2D von Mises stress). Multilayer perceptrons then map between latent coefficients for forward and inverse problems.

Result: The framework achieves accurate and computationally efficient surrogate models on a half MBB beam benchmark. Performance improves with richer QoIs, showing increasing robustness and fidelity. Enables generative mechanical design through latent-space exploration.

Conclusion: The proposed RRAE-based framework successfully enables fast forward/inverse analysis in topology optimization, providing a foundation for efficient surrogate modeling and generative mechanical design with potential applications in structural optimization.

Abstract: This work presents a data-driven framework for fast forward and inverse analysis in topology optimization (TO) by combining Rank Reduction Autoencoders (RRAEs) with neural latent-space mappings. The methodology targets the efficient approximation of the relationship between optimized geometries and their corresponding mechanical responses or Quantity of Interest (QoI), with a particular focus on compliance-minimized linear elastic structures. High-dimensional TO results are first compressed using RRAEs, which encode the data into a low-rank approximation via Singular Value Decomposition (SVD), obtained in this sense the most important features that approximate the data. Separate RRAE models are trained for geometry and for different types of QoIs, including scalar metrics, one-dimensional stress fields, and full two-dimensional von Mises stress distributions. The resulting low-dimensional latent coefficients of the latent space are then related through multilayer perceptrons to address both direct problems -- predicting structural responses from geometry -- and inverse problems -- recovering geometries from prescribed performance targets. The proposed approach is demonstrated on a benchmark TO problem based on a half MBB beam, using datasets generated via density-based Solid Isotropic Material with Penalization (SIMP) optimization. Numerical results show that the framework enables accurate and computationally efficient surrogate models, with increasing robustness and fidelity as richer QoIs are considered. The methodology also provides a foundation for generative mechanical design by enabling the synthesis of new geometries and responses through latent-space exploration.

</details>


### [39] [A Bayesian Approach to Feedback Control for Hyperbolic Balance Laws](https://arxiv.org/abs/2602.00244)
*Markus Bambach,Shaoshuai Chu,Michael Herty,Yunong Lin*

Main category: math.NA

TL;DR: Bayesian framework for feedback boundary control in hyperbolic balance laws using Lyapunov decay estimates as likelihood, validated on linear/nonlinear/stochastic systems and extended to practical applications.


<details>
  <summary>Details</summary>
Motivation: To develop a robust, discretization-agnostic method for feedback boundary control selection that works across linear, nonlinear, and stochastic regimes where analytical results are limited or unavailable.

Method: Bayesian framework that propagates probability distributions over feedback parameters using Lyapunov decay estimates as likelihood; validated with first-order and second-order local Lax-Friedrichs discretizations on various hyperbolic systems.

Result: Recovers known stability intervals for linear systems; demonstrates accuracy and robustness for nonlinear/stochastic systems (Saint-Venant, Burgers); extends to practical applications like laser powder bed fusion with multiple control parameters.

Conclusion: The proposed Bayesian framework provides a practical, discretization-agnostic feedback selection procedure that is consistent with theory and applicable to complex nonlinear/stochastic control problems.

Abstract: We propose a Bayesian framework for feedback boundary control for hyperbolic balance laws. The method propagates a probability distribution over feedback parameters by using Lyapunov decay estimates as a likelihood. In the linear setting, the framework recovers the available analytical results, while simultaneously extending them to nonlinear regimes where such results are not readily accessible. We first validate the method using the first-order local Lax-Friedrichs (LLF) discretizations on linear models -- the decoupled wave system and the linearized Saint-Venant equations -- recovering the known stability intervals and mixed boundary couplings reported in the control literature. We then consider nonlinear and stochastic settings, including the nonlinear Saint-Venant system and Burgers equation with random initial data, as well as a nonconservative perturbation with source terms, and demonstrate that the computed stability domains remain accurate and robust with respect to the choice of indicator and the initial prior. We further show that the methodology carries over to a second-order semi-discrete LLF scheme and to a model with two interacting control parameters for the temperature field development in laser powder bed fusion with feedback power regulation. These numerical experiments confirm consistency with available theory on benchmark cases and highlight the practicality of the proposed, discretization-agnostic feedback selection procedure.

</details>


### [40] [Analysis of a numerical scheme for 3-wave kinetic equations](https://arxiv.org/abs/2602.00264)
*Minh-Binh Tran,Bangjie Wang*

Main category: math.NA

TL;DR: This paper provides rigorous numerical analysis for existing 3-wave kinetic equation schemes, establishing well-posedness, stability, positivity propagation, and moment control for discrete equations.


<details>
  <summary>Details</summary>
Motivation: Several accurate and computationally efficient numerical schemes for 3-wave kinetic equations have been proposed, but their rigorous numerical analysis remains open. This paper aims to close this gap by providing comprehensive theoretical foundations for these discrete schemes.

Method: The authors establish a comprehensive well-posedness and qualitative theory for the discrete equation arising from existing numerical schemes. They analyze the discrete equation in ℓ¹(ℕ) space and prove various properties including existence, uniqueness, stability, and moment behavior.

Result: The paper proves: global existence, uniqueness, and Lipschitz stability of nonnegative classical solutions in ℓ¹(ℕ); uniform bounds and decay of moments; exponential energy decay; sharp creation and propagation of positivity characterized by arithmetic structure of initial support; propagation and instantaneous creation of polynomial, Mittag-Leffler, and exponential moments for high energy tail control.

Conclusion: The paper successfully closes the theoretical gap for existing 3-wave kinetic equation schemes by providing rigorous numerical analysis, establishing comprehensive well-posedness and qualitative properties, and validating theoretical findings with numerical results.

Abstract: Several numerical schemes for 3-wave kinetic equations have been proposed in recent work and shown to be accurate and computationally efficient [8,33,34,35]. However, their rigorous numerical analysis remains open. This paper aims to close this gap. We establish a comprehensive well-posedness and qualitative theory for the
  discrete equation arising from those schemes. We prove
  global existence, uniqueness, and Lipschitz stability of nonnegative classical solutions
  in $\ell^1(\mathbb{N})$, together with uniform bounds and decay of moments. We further
  show exponential energy decay and a sharp creation and propagation of positivity
  characterized by the arithmetic structure of the initial support. Finally, we obtain
  the propagation and instantaneous creation of polynomial, Mittag-Leffler, and
  exponential moments, providing quantitative control of high energy tails. We validate the theoretical findings by numerical results.

</details>


### [41] [Generalized Inverses of Matrix Products: From Fundamental Subspaces to Randomized Decompositions](https://arxiv.org/abs/2602.00386)
*Michał P. Karpowicz,Gilbert Strang*

Main category: math.NA

TL;DR: The paper establishes a unifying framework for generalized and randomized matrix inverses by analyzing the Moore-Penrose pseudoinverse of matrix products A=CR, providing geometric interpretations and new formulas for randomized approximations.


<details>
  <summary>Details</summary>
Motivation: To create a unified theoretical framework connecting classical generalized inverses with modern randomized matrix approximations, providing geometric insights into the relationships between fundamental subspaces and enabling rigorous analysis of randomized algorithms.

Method: Analyzes matrix product pseudoinverses from first principles using geometric subspace analysis, establishes reverse order laws and universal formulas, derives new generalized randomized formulas with sketching matrices, and extends framework to generalized inverses and specialized forms.

Result: Established conditions for reverse order law (A⁺ = R⁺C⁺), derived universal formula A⁺ = (C⁺CR)⁺(CRR⁺)⁺, developed new randomized formula A⁺ₚ = (PᵀA)⁺PᵀAQ(AQ)⁺ with rank-preservation equivalence, extended framework to analyze randomized SVD, Nyström approximation, and CUR decomposition, and provided rigorous error analysis for effective resistance estimation.

Conclusion: The geometric framework unifies classical and randomized matrix inverses, provides structural understanding of randomized algorithms, enables rigorous error analysis for applications like sensor placement and resistance estimation, and establishes that the resistance approximation scheme always underestimates true values with bounded error.

Abstract: We investigate the Moore-Penrose pseudoinverse and generalized inverse of a matrix product $A=CR$ to establish a unifying framework for generalized and randomized matrix inverses. This analysis is rooted in first principles, focusing on the geometry of the four fundamental subspaces. We examine:
  (1) the reverse order law, $A^+ = R^+C^+$, which holds when $C$ has independent columns and $R$ has independent rows,
  (2) the universally correct formula, $A^+ = (C^+CR)^+(CRR^+)^+$, providing a geometric interpretation of the mappings between the involved subspaces,
  (3) a new generalized randomized formula, $A^+_p = (P^TA)^+P^TAQ(AQ)^+$, which gives $A^+_p = A^+$ if and only if the sketching matrices $P$ and $Q$ preserve the rank of $A$, i.e., $\mathrm{rank}(P^TA) = \mathrm{rank}(AQ) = \mathrm{rank}(A)$.
  The framework is extended to generalized $\{1,2\}$-inverses and specialized forms, revealing the underlying structure of established randomized linear algebra algorithms, including randomized SVD, the Nyström approximation, and CUR decomposition. We demonstrate applications in sparse sensor placement and effective resistance estimation. For the latter, we provide a rigorous quantitative analysis of an approximation scheme, establishing that it always underestimates the true resistance and deriving a worst-case spectral bound on the error of resistance differences.

</details>


### [42] [A Cayley-free Two-Step Algorithm for Inverse Singular Value Problems](https://arxiv.org/abs/2602.00517)
*Jiechang Fan,Weiping Shen,Yusong Luo,Enping Lou*

Main category: math.NA

TL;DR: Proposed a Cayley-free two-step algorithm for inverse singular value problems that avoids Cayley transformations and reduces computational cost compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Inverse singular value problems (ISVPs) arise in various applications, and existing two-step algorithms require Cayley transformations and solving many linear systems, which is computationally expensive.

Method: Developed a Cayley-free two-step algorithm inspired by inverse eigenvalue problem methodologies. The algorithm eliminates Cayley transformations and avoids solving 2(m+n) linear systems for approximate singular vectors at each outer iteration.

Result: Proved cubic root-convergence rate under the assumption of nonsingular Jacobian matrix at solution. Numerical experiments validated the algorithm's effectiveness.

Conclusion: The proposed Cayley-free algorithm provides an efficient alternative for solving ISVPs with reduced computational cost while maintaining convergence properties.

Abstract: In this paper, we investigate numerical solutions for inverse singular value problems (for short, ISVPs) arising in various applications. Inspired by the methodologies employed for inverse eigenvalue problems, we propose a Cayley-free two-step algorithm for solving the ISVP. Compared to the existing two-step algorithms for the ISVP, our algorithm eliminates the need for Cayley transformations and consequently avoids solving $2(m+n)$ linear systems during the computation of approximate singular vectors at each outer iteration. Under the assumption that the Jacobian matrix at a solution is nonsingular, we present a convergence analysis for the proposed algorithm and prove a cubic root-convergence rate. Numerical experiments are conducted to validate the effectiveness of our algorithm.

</details>


### [43] [Fully discrete approximation of the semilinear stochastic wave equation on the sphere](https://arxiv.org/abs/2602.00556)
*David Cohen,Stefano Di Giovacchino,Annika Lang*

Main category: math.NA

TL;DR: Explicit fully discrete scheme for semilinear stochastic wave equation on sphere using trigonometric integrator in time and spectral Galerkin in space achieves strong and almost sure convergence.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for solving semilinear stochastic wave equations on spherical domains, which are important in geophysical and astrophysical applications, using explicit fully discrete schemes with proven convergence properties.

Method: Combines stochastic trigonometric integrator for temporal discretization with spectral Galerkin approximation in space using spherical harmonic functions, creating an explicit fully discrete numerical scheme.

Result: Proves strong convergence and almost sure convergence of the numerical scheme, with convergence rates confirmed through numerical experiments.

Conclusion: The proposed explicit fully discrete scheme provides a reliable and efficient method for solving semilinear stochastic wave equations on spheres with guaranteed convergence properties.

Abstract: The semilinear stochastic wave equation on the sphere driven by multiplicative Gaussian noise is discretized by a stochastic trigonometric integrator in time and a spectral Galerkin approximation in space based on the spherical harmonic functions. Strong and almost sure convergence of the explicit fully discrete numerical scheme are shown. Furthermore, these rates are confirmed by numerical experiments.

</details>


### [44] [Massively parallel Schwarz methods for the high frequency Helmholtz equation](https://arxiv.org/abs/2602.00735)
*Yan Xie,Shihua Gong,Ivan G. Graham,Euan A. Spence,Chen-Song Zhang*

Main category: math.NA

TL;DR: Parallel overlapping Schwarz method for high-frequency Helmholtz equations with O(k^d) scalability and O(k) iteration counts using RAS-PML with decreasing overlap/PML width.


<details>
  <summary>Details</summary>
Motivation: High-frequency Helmholtz equations produce large, indefinite, ill-conditioned, complex-valued linear systems that are challenging to solve efficiently in parallel. Need scalable domain decomposition methods that maintain good convergence while minimizing communication overhead as frequency increases.

Method: Practical variant of restricted additive Schwarz method with Perfectly Matched Layer transmission conditions (RAS-PML). Overlap width and additional PML layer on each subdomain decrease as O(k^{-1} log(k)) as frequency k→∞. Applied to finite element discretizations of Helmholtz equations with Cartesian domain decomposition.

Result: Method achieves O(k^d) parallel scalability under Cartesian domain decomposition. Exhibits O(k) iteration counts and convergence time for d-dimensional Helmholtz problems (d=2,3) as k increases. Preliminary experiments on 2D problems with constant wave speed show promising performance.

Conclusion: RAS-PML with decreasing overlap/PML width provides effective parallel solution method for high-frequency Helmholtz problems, balancing convergence with communication efficiency. Method shows promising scalability and iteration complexity, though currently limited to constant wave speed 2D problems with extensions to variable wavespeed and 3D planned for future work.

Abstract: We investigate the parallel one-level overlapping Schwarz method for solving finite element discretization of high-frequency Helmholtz equations. The resulting linear systems are large, indefinite, ill-conditioned, and complex-valued. We present a practical variant of the restricted additive Schwarz method with Perfectly Matched Layer transmission conditions (RAS-PML), which was originally analyzed in a theoretical setting in {\tt arXiv:2404.02156}, with some numerical experiments given in {\tt arXiv:2408.16580}. In our algorithm, the width of the overlap and the additional PML layer on each subdomain is allowed to decrease with $\mathcal{O}(k^{-1} \log(k))$, as the frequency $k \rightarrow \infty$, and this is observed to ensure good convergence while avoiding excessive communication. In experiments, the proposed method achieves $\mathcal{O}(k^d)$ parallel scalability under Cartesian domain decomposition and exhibits $\mathcal{O}(k)$ iteration counts and convergence time for $d$-dimensional Helmholtz problems ($d = 2,3$) as $k$ increases. In this preliminary note we restrict to experiments on 2D problems with constant wave speed. Details, analysis and extensions to variable wavespeed and 3D will be given in future work.

</details>


### [45] [Finite Element Eigenfunction Network (FEENet): A Hybrid Framework for Solving PDEs on Complex Geometries](https://arxiv.org/abs/2602.00870)
*Shiyuan Li,Hossein Salahshoor*

Main category: math.NA

TL;DR: FEENet is a hybrid spectral learning framework that combines FEM eigenfunction bases with neural networks to learn PDE solutions on complex geometries with superior accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Neural operators struggle with complex/irregular geometries due to lack of geometry-aware representations. Current approaches don't effectively incorporate intrinsic geometric information into the learning framework.

Method: FEENet uses FEM to compute a one-time eigenfunction basis intrinsic to the geometry, then represents PDE solutions in this geometry-adapted basis and learns to predict spectral coefficients.

Result: FEENet achieves superior accuracy and computational efficiency compared to DeepONet across various parameterized PDEs on complex 2D/3D geometries, with resolution-independent inference and interpretability.

Conclusion: Hybrid approaches combining structure-preserving numerical methods (like FEM) with data-driven learning offer a promising pathway for solving real-world PDE problems on complex geometries.

Abstract: Neural operators aim to learn mappings between infinite-dimensional function spaces, but their performance often degrades on complex or irregular geometries due to the lack of geometry-aware representations. We propose the Finite Element Eigenfunction Network (FEENet), a hybrid spectral learning framework grounded in the eigenfunction theory of differential operators. For a given domain, FEENet leverages the Finite Element Method (FEM)toperformaone-timecomputationofaneigenfunctionbasisintrinsictothegeometry. PDE solutions are subsequently represented in this geometry-adapted basis, and learning is reduced to predicting the corresponding spectral coefficients. Numerical experiments conducted across a range of parameterized PDEs and complex two- and three-dimensional geometries, including benchmarks against the seminal DeepONet framework (1), demonstrate that FEENet consistently achieves superior accuracy and computational efficiency. We further highlight key advantages of the proposed approach, including resolution-independent inference, interpretability, and natural generalization to nonlocal operators defined as functions of differential operators. We envision that hybrid approaches of this form, which combine structure-preserving numerical methods with data-driven learning, offer a promising pathway toward solving real-world PDE problems on complex geometries.

</details>


### [46] [A New Combination of Preconditioned Gradient Descent Methods and Vector Extrapolation Techniques for Nonlinear Least-Squares Problems](https://arxiv.org/abs/2602.00897)
*Abdellatif Mouhssine*

Main category: math.NA

TL;DR: The paper proposes an extrapolation-accelerated framework that combines preconditioned gradient-based methods with vector extrapolation techniques to simultaneously improve convergence rate and solution accuracy for nonlinear least-squares problems.


<details>
  <summary>Details</summary>
Motivation: Classical extrapolation methods for accelerating fixed-point iterative methods often reduce iteration counts or computational cost, but they don't necessarily improve the accuracy of computed approximations. There's a need for acceleration techniques that enhance both convergence speed and solution quality.

Method: The paper integrates vector extrapolation techniques (polynomial-type extrapolation methods and the vector ε-algorithm) with preconditioned gradient descent methods for solving nonlinear least-squares problems. This creates a hybrid framework that combines extrapolation acceleration with gradient-based optimization.

Result: Numerical experiments show that the proposed extrapolation-accelerated framework improves both convergence rate and solution accuracy. The results demonstrate reduced iteration counts, computational times, and relative reconstruction errors compared to standard approaches.

Conclusion: The combination of preconditioned gradient-based methods with vector extrapolation techniques provides an effective framework for accelerating convergence while simultaneously improving approximation accuracy in nonlinear least-squares problems, outperforming classical extrapolation strategies that only focus on speed.

Abstract: Vector extrapolation methods are widely used in large-scale simulation studies, and numerous extrapolation-based acceleration techniques have been developed to enhance the convergence of linear and nonlinear fixed-point iterative methods. While classical extrapolation strategies often reduce the number of iterations or the computational cost, they do not necessarily lead to a significant improvement in the accuracy of the computed approximations. In this paper, we study the combination of preconditioned gradient-based methods with extrapolation strategies and propose an extrapolation-accelerated framework that simultaneously improves convergence and approximation accuracy. The focus is on the solution of nonlinear least-squares problems through the integration of vector extrapolation techniques with preconditioned gradient descent methods. A comprehensive set of numerical experiments is carried out to study the behavior of polynomial-type extrapolation methods and the vector $\varepsilon$-algorithm when coupled with gradient descent schemes, with and without preconditioning. The results demonstrate the impact of extrapolation techniques on both convergence rate and solution accuracy, and report iteration counts, computational times, and relative reconstruction errors. The performance of the proposed hybrid approaches is further assessed through a benchmarking study against Gauss--Newton methods based on generalized Krylov subspaces.

</details>


### [47] [High-order DLM-ALE discretizations with robust operator preconditioning for fluid-rigid-body interaction](https://arxiv.org/abs/2602.01094)
*Qi Xin,Shihua Gong,Lingyue Shen,Pinjing Wen,Yumiao Zhang,Yan Chen,Jiarui Han,Jinchao Xu*

Main category: math.NA

TL;DR: High-order numerical framework for fluid-rigid-body interaction on moving meshes, with applications to deterministic lateral displacement microfluidic devices.


<details>
  <summary>Details</summary>
Motivation: Design of deterministic lateral displacement (DLD) microfluidic devices requires accurate simulation of fluid-rigid-body interactions with high-order accuracy.

Method: Combines distributed Lagrange multiplier (DLM) for rigid-body motion, arbitrary Lagrangian-Eulerian (ALE) mapping for moving fluid domain, isoparametric Taylor-Hood elements for spatial discretization, and high-order partitioned Runge-Kutta for time integration.

Result: Established well-posedness of generalized Stokes formulation, demonstrated high-order convergence for fluid solution and rigid-body dynamics, and showed robust iterative convergence of preconditioners.

Conclusion: Developed a stable, high-order numerical framework for fluid-rigid-body interaction with applications to microfluidic device design, featuring robust preconditioning and accurate particle trajectory prediction.

Abstract: Motivated by the design of deterministic lateral displacement (DLD) microfluidic devices, we develop a high-order numerical framework for fluid-rigid-body interaction on fitted moving meshes. Rigid-body motion is enforced by a distributed Lagrange multiplier (DLM) formulation, while the moving fluid domain is treated by an arbitrary Lagrangian-Eulerian (ALE) mapping. In space, we use isoparametric Taylor-Hood elements to achieve high-order accuracy and to represent curved boundaries and the fluid-particle interface. In time, we employ a high-order partitioned Runge-Kutta strategy in which the mesh motion is advanced explicitly and the coupled physical fields are advanced implicitly, yielding high-order accuracy for the particle trajectory. The fully coupled system is linearized into a generalized Stokes problem subject to distributed constraints of incompressibility and rigid-body motion. We establish well-posedness of this generalized Stokes formulation at both the continuous and discrete levels, providing the stability foundation for operator preconditioning that is robust with respect to key physical and discretization parameters. Numerical experiments on representative benchmarks, including a DLD case, demonstrate high-order convergence for the fluid solution and rigid-body dynamics, as well as robust iterative convergence of the proposed preconditioners.

</details>


### [48] [Surrogate to Poincaré inequalities on manifolds for structured dimension reduction in nonlinear feature spaces](https://arxiv.org/abs/2602.01143)
*Pasco Alexandre,Nouy Anthony*

Main category: math.NA

TL;DR: The paper proposes methods for approximating high-dimensional functions using composition of feature maps and profile functions, focusing on structured nonlinear feature maps that extract features from separate variable groups using gradient-based optimization with Poincaré inequalities.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of approximating high-dimensional continuously differentiable functions by constructing efficient feature maps that extract few relevant features, enabling better function approximation with reduced dimensionality.

Method: Uses gradient-based method leveraging Poincaré inequalities on nonlinear manifolds to minimize a non-convex loss functional. Investigates two settings: 1) collective setting with a feature map for parametrized function families using a new quadratic surrogate loss, and 2) grouped setting with separate feature maps for different input groups.

Result: Introduces a quadratic surrogate for the non-convex loss function in collective setting with theoretical upper bound, and shows grouped setting equivalence to multiple collective settings for separate variable groups.

Conclusion: The proposed structured feature map construction methods provide theoretical foundations for efficient high-dimensional function approximation, with collective and grouped settings offering different approaches to handle the optimization challenges of non-convex loss minimization.

Abstract: This paper is concerned with the approximation of continuously differentiable functions with high-dimensional input by a composition of two functions: a feature map that extracts few features from the input space, and a profile function that approximates the target function taking the features as its low-dimensional input. We focus on the construction of structured nonlinear feature maps, that extract features on separate groups of variables, using a recently introduced gradient-based method that leverages Poincaré inequalities on nonlinear manifolds. This method consists in minimizing a non-convex loss functional, which can be a challenging task, especially for small training samples. We first investigate a collective setting, in which we construct a feature map suitable to a parametrized family of high-dimensional functions. In this setting we introduce a new quadratic surrogate to the non-convex loss function and show an upper bound on the latter. We then investigate a grouped setting, in which we construct separate feature maps for separate groups of inputs, and we show that this setting is almost equivalent to multiple collective settings, one for each group of variables.

</details>


### [49] [Novel linear, decoupled, and energy dissipative schemes for the Navier-Stokes-Darcy model and extension to related two-phase flow](https://arxiv.org/abs/2602.01175)
*Xiaoli Li,Jie Shen,Xinhui Wang*

Main category: math.NA

TL;DR: Efficient energy-dissipative schemes for Navier-Stokes-Darcy model using prediction-correction with relaxation technique to guarantee original energy dissipation and unconditional boundedness.


<details>
  <summary>Details</summary>
Motivation: To develop stable and efficient numerical schemes for the Navier-Stokes-Darcy model and related two-phase flows that preserve the original energy dissipation property while maintaining computational efficiency.

Method: Prediction-correction framework with a novel relaxation technique in the correction step to ensure dissipation of original energy, requiring only linear equations with constant coefficients at each time step.

Result: Schemes guarantee unconditional boundedness of numerical solutions in l∞(L²) and l²(H¹) norms, rigorously proven to dissipate original energy, with error analysis for first-order scheme and demonstrated accuracy/stability in benchmark experiments.

Conclusion: The proposed prediction-correction schemes with relaxation technique provide efficient, stable, and accurate numerical methods for Navier-Stokes-Darcy models with guaranteed original energy dissipation properties.

Abstract: We construct efficient original-energy-dissipative schemes for the Navier-Stokes-Darcy model and related two-phase flows using a prediction-correction framework. A new relaxation technique is incorporated in the correction step to guarantee dissipation of the original energy, thereby ensuring unconditional boundedness of the numerical solutions for velocity and hydraulic head in the $l^{\infty}(L^2)$ and $l^2(H^1)$ norms. At each time step, the schemes require solving only a sequence of linear equations with constant coefficients. We rigorously prove that the schemes dissipate the original energy and, as an example, carry out a rigorous error analysis of the first-order scheme for the Navier-Stokes-Darcy model. Finally, a series of benchmark numerical experiments are conducted to demonstrate the accuracy, stability, and effectiveness of the proposed methods.

</details>


### [50] [A survey of scalar and vector extrapolation](https://arxiv.org/abs/2602.01301)
*Khalide Jbilou*

Main category: math.NA

TL;DR: Comprehensive review of scalar and vector extrapolation methods for accelerating convergence of iterative algorithms and series summations, covering historical development, theoretical foundations, and modern applications.


<details>
  <summary>Details</summary>
Motivation: To provide a unified perspective on extrapolation techniques that improve computational efficiency by accelerating convergence without modifying underlying iterative processes, bridging historical origins with contemporary applications.

Method: Systematic review and analysis of classical scalar methods (Richardson extrapolation, Aitken process, Shanks transformation, Wynn epsilon algorithm) and vector extrapolation methods (polynomial-based and epsilon algorithm generalizations), examining theoretical foundations, error models, convergence properties, and implementation considerations.

Result: Comprehensive synthesis showing how these methods eliminate dominant error components, connect to Pade approximants and rational approximations, and provide frameworks for accelerating convergence in various computational contexts including iterative solvers and Krylov subspace methods.

Conclusion: Extrapolation methods are essential tools in numerical analysis that significantly enhance computational efficiency by constructing transformed sequences with faster convergence, with both scalar and vector variants offering powerful techniques for modern large-scale simulations and iterative algorithms.

Abstract: Scalar extrapolation and convergence acceleration methods are central tools in numerical analysis for improving the efficiency of iterative algorithms and the summation of slowly convergent series. These methods construct transformed sequences that converge more rapidly to the same limit without altering the underlying iterative process, thereby reducing computational cost and enhancing numerical accuracy. Historically, the origins of such techniques can be traced back to classical algebraic methods by AlKhwarizmi and early series acceleration techniques by Newton, while systematic approaches emerged in the 20th century with Aitken process and Richardson extrapolation. Later developments, including the Shanks transformation and Wynn epsilon algorithm, provided general frameworks capable of eliminating multiple dominant error components, with deep connections to Pade approximants and rational approximations of generating functions. This paper presents a comprehensive review of classical scalar extrapolation methods, including Richardson extrapolation, Aitken process, Shanks transformation, Wynn epsilon algorithm, and other algorithms. We examine their theoretical foundations, asymptotic error models, convergence properties, numerical stability, and practical implementation considerations. The second part of this work is dedicated to vector extrapolation methods: polynomial based ones and epsilon algorithm generalizations to vector sequences. Additionally, we highlight modern developments such as their applications to iterative solvers, Krylov subspace methods, and large-scale computational simulations. The aim of this review is to provide a unified perspective on scalar and vector extrapolation techniques, bridging historical origins, theoretical insights, and contemporary computational applications.

</details>


### [51] [Multigrid Poisson Solver for Complex Geometries Using Finite Difference Method](https://arxiv.org/abs/2602.01888)
*Deepak Gautam,Bhooshan Paradkar*

Main category: math.NA

TL;DR: A transformation optics-inspired method for solving Poisson/Laplace equations in complex geometries by mapping to uniform computational domains.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve Poisson/Laplace equations in complex and arbitrarily shaped geometries, overcoming challenges of traditional methods in irregular domains.

Method: Uses coordinate transformations to map physical domain to uniform computational domain, modifying material parameters and source terms accordingly within finite difference framework.

Result: Achieves second-order accuracy, enables use of fast solvers (multigrid) on uniform grids, and handles both homogeneous and inhomogeneous problems efficiently.

Conclusion: The transformation optics approach provides an efficient, accurate method for solving Poisson equations in complex geometries while maintaining simplicity of uniform grid solvers.

Abstract: We present an efficient numerical method, inspired by transformation optics, for solving the Poisson equation in complex and arbitrarily shaped geometries. The approach operates by mapping the physical domain to a uniform computational domain through coordinate transformations, which can be applied either to the entire domain or selectively to specific boundaries inside the domain. This flexibility allows both homogeneous (Laplace equation) and inhomogeneous (Poisson equation) problems to be solved efficiently using iterative or fast direct solvers, with only the material parameters and source terms modified according to the transformation. The method is formulated within a finite difference framework, where the modified material properties are computed from the coordinate transformation equations, either analytically or numerically. This enables accurate treatment of arbitrary geometric shapes while retaining the simplicity of a uniform grid solver. Numerical experiments confirm that the method achieves second-order accuracy , and offers a straightforward pathway to integrate fast solvers such as multigrid methods on the uniform computational grid.

</details>


### [52] [Finite element theta schemes for the viscous Burgers' equation with nonlinear Neumann boundary feedback control](https://arxiv.org/abs/2602.01315)
*Shishu Pal Singh,Sudeep Kundu*

Main category: math.NA

TL;DR: A fully discrete numerical scheme for viscous Burgers equations with nonlinear Neumann boundary feedback control, using θ-scheme in time and finite elements in space, proven to be unconditionally exponentially stable with optimal error estimates.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical scheme that preserves the stabilization properties of continuous viscous Burgers equations with boundary feedback control at the discrete level, ensuring both stability and accuracy.

Method: Combines θ-scheme for temporal discretization (θ ∈ [1/2, 1]) with conforming finite element method for spatial approximation, establishing existence/uniqueness of fully discrete solutions.

Result: Proves unconditional exponential stability of the scheme, optimal error estimates for state variables and boundary controls in both 1D and 2D, validated through numerical experiments.

Conclusion: The proposed numerical scheme successfully retains the stabilization properties of the continuous model while providing accurate discrete approximations, making it effective for viscous Burgers equations with boundary feedback control.

Abstract: In this article, we develop a fully discrete numerical scheme for the one-dimensional (1D) and two-dimensional (2D) viscous Burgers equations with nonlinear Neumann boundary feedback control. The temporal discretization employs a $θ$-scheme, while a conforming finite element method is used for the spatial approximation. The existence and uniqueness of the fully discrete solution are established. We further prove that the scheme is unconditionally exponentially stable for $θ\in [1/2, 1]$, thereby ensuring that the stabilization property of the continuous model is retained at the discrete level. In addition, optimal error estimates are obtained for both the state variable and the boundary control inputs in 1D and 2D frameworks. Finally, several numerical experiments are presented to validate our theoretical findings and to demonstrate the effectiveness of the proposed stabilization strategy under varying model parameters.

</details>


### [53] [Third-Order Geometric-Volume Conservation in Cahn--Hilliard Models](https://arxiv.org/abs/2602.01497)
*Josef Musil*

Main category: math.NA

TL;DR: Improved volume conservation in degenerate Cahn-Hilliard phase-field models using moment-balanced regularization kernels to eliminate artificial interface drift.


<details>
  <summary>Details</summary>
Motivation: Phase-field models for surface-diffusion-driven interface motion suffer from artificial volume drift at finite interface thickness, even when the sharp-interface limit conserves volume. This leads to inaccurate simulations of phenomena like droplet coarsening.

Method: Extends the improved-conservation framework by replacing classical mass conservation with exact conservation of a designed monotone mapping Q(φ). Uses matched-asymptotic analysis, derives inner corrections, and identifies integral-moment cancellation condition. Designs regularization kernels (exponential and Padé-type) to satisfy cancellation condition and achieve higher-order volume conservation.

Result: Proposed moment-balanced kernels achieve formal third-order accuracy in geometric-volume conservation with respect to interface thickness. Numerical benchmarks show virtual elimination of artificial drift and prevention of premature extinction of small droplets in multi-scale coarsening and shape relaxation.

Conclusion: The integral-moment cancellation condition provides a practical design rule for regularization kernels that significantly improves volume conservation in phase-field simulations, enabling more accurate modeling of interface evolution without artificial drift.

Abstract: Degenerate Cahn-Hilliard phase-field models provide a robust approximation of surface-diffusion-driven interface motion without explicit front tracking. In computations, however, the geometric volume enclosed by the interface -- the region where the order parameter $φ$ is positive -- may drift at finite interface thickness, producing artificial shrinkage or growth even when the sharp-interface limit conserves volume. We revisit and extend the improved-conservation framework of Zhou et al., where one replaces classical mass conservation by the exact conservation of a designed monotone mapping $Q(φ)$ that more accurately approximates a step function. Building on this framework, we (i) carry out the matched-asymptotic analysis in the unscaled physical time formulation, (ii) derive a simplified representation of the first-order inner correction to the interface profile, and (iii) identify an integral-moment cancellation condition that controls the leading geometric-volume defect. This mechanism becomes a practical design rule: we select regularization kernels within parameterized families -- including exponential and Pade-type -- to reach effective higher-order behavior and satisfy the cancellation condition at moderate parameter values. As a result, the proposed kernels achieve formal third-order accuracy in geometric-volume conservation with respect to interface thickness. Finally, we describe an unconditional energy-dissipative numerical discretization that exactly preserves the discrete conserved quantity. Numerical benchmarks on multi-scale droplet coarsening and shape relaxation demonstrate that the moment-balanced kernels virtually eliminate artificial drift and prevent premature extinction of small droplets.

</details>


### [54] [Nonlinear model reduction for transport-dominated problems](https://arxiv.org/abs/2602.01397)
*Jan S. Hesthaven,Benjamin Peherstorfer,Benjamin Unger*

Main category: math.NA

TL;DR: Survey of nonlinear model reduction methods for transport-dominated problems where linear approximations fail due to the Kolmogorov barrier.


<details>
  <summary>Details</summary>
Motivation: Linear reduced-space approximations are intrinsically inefficient for transport-dominated problems with wave-like phenomena and moving coherent structures, which face the Kolmogorov barrier limitation.

Method: Organizes nonlinear model reduction techniques around three key elements: nonlinear parametrizations, reduced dynamics, and online solvers. Categorizes approaches into transformation-based methods, online adaptive techniques, and formulations combining generic nonlinear parametrizations with instantaneous residual minimization.

Result: Provides a comprehensive survey and categorization framework for nonlinear model reduction methods that can handle challenging regimes where linear methods fail.

Conclusion: Nonlinear model reduction methods offer effective alternatives to linear approximations for transport-dominated problems, with various approaches organized around key computational elements and categorized into distinct methodological families.

Abstract: This article surveys nonlinear model reduction methods that remain effective in regimes where linear reduced-space approximations are intrinsically inefficient, such as transport-dominated problems with wave-like phenomena and moving coherent structures, which are commonly associated with the Kolmogorov barrier. The article organizes nonlinear model reduction techniques around three key elements -- nonlinear parametrizations, reduced dynamics, and online solvers -- and categorizes existing approaches into transformation-based methods, online adaptive techniques, and formulations that combine generic nonlinear parametrizations with instantaneous residual minimization.

</details>


### [55] [Dimension-Free Multimodal Sampling via Preconditioned Annealed Langevin Dynamics](https://arxiv.org/abs/2602.01449)
*Lorenzo Baldassari,Josselin Garnier,Knut Solna,Maarten V. de Hoop*

Main category: math.NA

TL;DR: The paper provides a dimension-uniform analysis of annealed Langevin dynamics (ALD) for multimodal targets approximated by Gaussian mixtures, establishing stability conditions and robustness to initialization/score approximation errors.


<details>
  <summary>Details</summary>
Motivation: There's an empirical-theoretical gap: ALD works well empirically for multimodal sampling but lacks theoretical guarantees on dimension-uniform stability. The paper aims to bridge this gap by providing rigorous dimension-uniform analysis of ALD for Gaussian mixture targets.

Method: Analyzes continuous-time ALD along an explicit annealing path (progressively removing Gaussian smoothing). Identifies spectral conditions linking smoothing covariance and mixture component covariances. Establishes dimension-robustness to imperfect initialization and score approximation under misspecified-mixture score models.

Result: Derives sufficient conditions under which ALD achieves prescribed accuracy within a single, dimension-uniform time horizon. Shows preconditioning with sufficiently decaying spectrum is necessary to prevent error accumulation across coordinates. Numerical experiments validate the theory.

Conclusion: The paper provides rigorous theoretical foundations for ALD's empirical success on multimodal targets, establishing dimension-uniform stability conditions and highlighting the importance of proper preconditioning to maintain control across high dimensions.

Abstract: Designing algorithms that can explore multimodal target distributions accurately across successive refinements of an underlying high-dimensional problem is a central challenge in sampling. Annealed Langevin dynamics (ALD) is a widely used alternative to classical Langevin since it often yields much faster mixing on multimodal targets, but there is still a gap between this empirical success and existing theory: when, and under which design choices, can ALD be guaranteed to remain stable as dimension increases? In this paper, we help bridge this gap by providing a uniform-in-dimension analysis of continuous-time ALD for multimodal targets that can be well-approximated by Gaussian mixture models. Along an explicit annealing path obtained by progressively removing Gaussian smoothing of the target, we identify sufficient spectral conditions - linking smoothing covariance and the covariances of the Gaussian components of the mixture - under which ALD achieves a prescribed accuracy within a single, dimension-uniform time horizon. We then establish dimension-robustness to imperfect initialization and score approximation: under a misspecified-mixture score model, we derive explicit conditions showing that preconditioning the ALD algorithm with a sufficiently decaying spectrum is necessary to prevent error terms from accumulating across coordinates and destroying dimension-uniform control. Finally, numerical experiments illustrate and validate the theory.

</details>


### [56] [Beyond Taylor: Divergence-Based Functional Expansions and Their Application to Numerical Integration](https://arxiv.org/abs/2602.01467)
*Junping Wang*

Main category: math.NA

TL;DR: A new functional expansion framework based on divergence identities enables high-order quadrature for polytopes by transforming volume integrals to boundary integrals, enhanced by complex-shift techniques for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic alternative to tessellation-based quadrature for arbitrary flat-faced polytopes that avoids the limitations of traditional Taylor series expansions and provides computationally efficient integration methods.

Method: A functional expansion framework using exact differential identities linking functions and derivatives through polynomial weights, applying the Divergence Theorem to transform volume integrals to boundary integrals recursively, and enhancing accuracy with complex-shift techniques using roots of unity.

Result: The framework provides high-order numerical quadrature formulas for multi-dimensional domains, with rigorous geometric analysis for surface integration transformations, offering minimal function evaluations through error term annihilation.

Conclusion: The proposed method offers a robust, systematic, and computationally efficient alternative to tessellation-based quadrature for arbitrary flat-faced polytopes, with applications in numerical integration across various domains.

Abstract: This paper introduces a new functional expansion framework that extends classical ideas beyond the Taylor series. Unlike traditional Taylor expansions based on local polynomial approximations, the proposed approach arises from exact differential identities that link a function and its derivatives through polynomial weight factors. This formulation expresses smooth functions via divergence-based relations connecting derivatives of all orders with systematically scaled polynomial coefficients. This framework provides a natural foundation for constructing high-order numerical quadrature formulas, particularly for multi-dimensional domains. By exploiting the divergence structure, volume integrals are systematically transformed into boundary integrals using the Divergence Theorem, recursively reducing the integration domain from an $n$-dimensional body to its $(n-1)$-dimensional facets, and ultimately to its vertices.
  The article further enhances the framework's accuracy by introducing a complex-shift technique. It is demonstrated that by positioning the expansion center at specific roots of unity in the complex plane, lower-order error terms are annihilated, yielding high-order real-valued quadrature rules with minimal function evaluations. Additionally, a rigorous geometric analysis of the affine transformations required for surface integration is provided, deriving explicit formulas for the transformation of normal vectors and surface measures. The proposed method offers a robust, systematic, and computationally efficient alternative to tessellation-based quadrature for arbitrary flat-faced polytopes.

</details>


### [57] [A Flux-Correction Form of the Third-Order Edge-Based Scheme for a General Numerical Flux Function](https://arxiv.org/abs/2602.01938)
*Hiroaki Nishikawa*

Main category: math.NA

TL;DR: A flux-correction form for third-order edge-based Euler scheme that allows general flux functions while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable direct use of general numerical flux functions (like HLLC, LDFSS) in third-order edge-based schemes for Euler equations without sacrificing accuracy.

Method: Replace arithmetic average of flux extrapolations with general numerical flux evaluated at edge midpoint plus correction term; uses U-MUSCL scheme with κ = 1/2 to compute left/right states for quadratic functions.

Result: The flux-correction form preserves third-order accuracy when general numerical flux is evaluated with properly computed left/right states; verified with HLLC and LDFSS flux functions on irregular tetrahedral grids.

Conclusion: Proposed method successfully enables general flux functions in third-order edge-based schemes while maintaining accuracy, verified through numerical experiments.

Abstract: In this short note, we present a flux-correction form of the third-order edge-based scheme for the Euler equations that enables the direct use of a general flux function. The core idea is to replace, without loss of accuracy, the arithmetic average of the flux extrapolations by a general numerical flux evaluated at the edge midpoint, together with a correction term. We show that the proposed flux-correction form preserves third-order accuracy, provided that the general numerical flux is evaluated with the left and right states that are computed exactly for a quadratic function, which can be achieved effectively by the U-MUSCL scheme with κ = 1/2. Numerical results are presented to verify third-order accuracy with the HLLC and LDFSS flux functions on irregular tetrahedral grids.

</details>


### [58] [Geometric Generalization of Neural Operators from Kernel Integral Perspective](https://arxiv.org/abs/2602.01498)
*Mingyu Han,Daniel Zhengyu Huang,Yuhan Wang,Yanshu Zhang,Jiayi Zhou*

Main category: math.NA

TL;DR: Neural operators for parametric PDEs struggle with variable geometries. This work proposes a kernel integral approach connecting operator learning to fast kernel summation methods, enabling robust geometric generalization.


<details>
  <summary>Details</summary>
Motivation: Neural operators need to handle variable and nonparametric geometries in practical applications like engineering design, where generalization to unseen geometries remains a major challenge.

Method: Adopts a kernel integral perspective inspired by boundary integral formulations, connecting operator learning to fast kernel summation methods. Proposes a multiscale neural operator inspired by Ewald summation for learning and efficiently evaluating unknown kernel integrals.

Result: Provides theoretical accuracy guarantees for the approximation. Numerical experiments demonstrate robust generalization across diverse geometries for several commonly used kernels and for a large-scale 3D fluid dynamics example.

Conclusion: The kernel integral perspective clarifies geometric generalization mechanisms and establishes a direct connection between operator learning and fast kernel summation, enabling effective handling of variable geometries in practical applications.

Abstract: Neural operators are neural network-based surrogate models for approximating solution operators of parametric partial differential equations, enabling efficient many-query computations in science and engineering. Many applications, including engineering design, involve variable and often nonparametric geometries, for which generalization to unseen geometries remains a central practical challenge. In this work, we adopt a kernel integral perspective motivated by classical boundary integral formulations and recast operator learning on variable geometries as the approximation of geometry-dependent kernel operators, potentially with singularities. This perspective clarifies a mechanism for geometric generalization and reveals a direct connection between operator learning and fast kernel summation methods. Leveraging this connection, we propose a multiscale neural operator inspired by Ewald summation for learning and efficiently evaluating unknown kernel integrals, and we provide theoretical accuracy guarantees for the resulting approximation. Numerical experiments demonstrate robust generalization across diverse geometries for several commonly used kernels and for a large-scale three-dimensional fluid dynamics example.

</details>


### [59] [Well-posedness and Numerical Analysis of Mixed Variational-hemivariational Inequalities](https://arxiv.org/abs/2602.01529)
*Weimin Han,Jianguo Huang,Yuan Yao*

Main category: math.NA

TL;DR: This paper studies well-posedness and numerical solutions for elliptic mixed variational-hemivariational inequalities, applying results to Stokes equations with frictional slip conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework for analyzing and solving various mixed variational equations, inequalities, and hemivariational inequalities that appear in the literature, particularly for problems involving frictional contact conditions in fluid mechanics.

Method: Uses projection iteration technique to study well-posedness of mixed variational-hemivariational inequalities and their numerical approximations, with error analysis. Applies results to Stokes equations with frictional slip conditions using stable finite element space pairs.

Result: Establishes well-posedness of the general inequality class, derives optimal order error estimates for numerical methods under solution regularity assumptions, and demonstrates theoretical convergence orders through numerical experiments.

Conclusion: The paper provides a comprehensive framework for analyzing and numerically solving mixed variational-hemivariational inequalities, with successful application to Stokes flow problems with both monotone and non-monotone frictional slip conditions.

Abstract: The paper is devoted to well-posedness analysis and the numerical solution of a family of general elliptic mixed variational-hemivariational inequalities. Various mixed variational equations, mixed variational inequalities and mixed hemivariational inequalities found in the literature are special cases of the mixed variational-hemivariational inequalities. Well-posedness of the mixed variational-hemivariational inequalities and their numerical approximations are studied via the projection iteration technique. Error analysis of the numerical methods is presented. The results are applied to the study of a variational-hemivariational inequality of the Stokes equations for incompressible fluid flows subject to slip conditions of frictional type, both monotone and non-monotone. Optimal order error estimates are derived for the use of some stable finite element space pairs under certain solution regularity assumptions. Numerical results are reported demonstrating the theoretical prediction of convergence orders.

</details>


### [60] [Equilibrated-flux residual certification for verified existence and outputs](https://arxiv.org/abs/2602.01636)
*Hiroki Ishizaka*

Main category: math.NA

TL;DR: A rigorous certification workflow for nonlinear elliptic PDEs that upgrades standard FEM computations to provide existence proofs and guaranteed error bounds for quantities of interest.


<details>
  <summary>Details</summary>
Motivation: Standard finite element computations provide approximate solutions but lack rigorous guarantees about existence of true solutions and error bounds. There's a need for post-processing methods that can certify both existence and provide guaranteed enclosures for quantities of interest while maintaining computational efficiency.

Method: Uses a Newton-Kantorovich argument with three certified ingredients: 1) guaranteed dual-norm residual bounds via equilibrated-flux reconstruction exploiting nonconforming/mixed formulation relations, 2) computable lower bounds for linearised operator stability via coercivity bounds, and 3) Lipschitz bounds for derivatives. Verification radius determined by bracketing-bisection search. Adjoint-based correction tightens enclosures for quantities of interest.

Result: Numerical experiments on semilinear diffusion-reaction models show that certificates are informative and the adjoint enhancement substantially reduces enclosure widths while maintaining full mathematical rigor.

Conclusion: The proposed workflow successfully provides rigorous existence proofs and guaranteed error bounds for nonlinear elliptic PDEs, with adjoint-based corrections significantly improving enclosure tightness without sacrificing mathematical rigor.

Abstract: We present a post-processing certification workflow for nonlinear elliptic boundary value problems that upgrades a standard finite element computation to a rigorous existence and output certificate. For a given approximate discrete state, we verify existence and local uniqueness of a weak solution in a computable neighbourhood via a Newton--Kantorovich argument based on three certified ingredients: a guaranteed dual-norm residual bound, a computable lower bound for the stability constant of the linearised operator, and a Lipschitz bound for the derivative on the verification ball. The residual bound is obtained by an equilibrated-flux reconstruction exploiting an explicit relation between nonconforming and mixed formulations, yielding $H(\mathrm{div})$-conforming fluxes without local mixed solves. The stability ingredient follows from a computable coercivity lower bound for the linearisation. An admissible verification radius is selected by a simple bracketing--bisection search, justified for an affine Lipschitz model. Once the verification ball is certified, we derive guaranteed enclosures for quantities of interest using computable variation bounds; an adjoint-based correction, in the spirit of goal-oriented error estimation, tightens these intervals while retaining full rigour. Numerical experiments for semilinear diffusion--reaction models show that the certificates are informative and that the adjoint enhancement substantially reduces enclosure widths.

</details>


### [61] [Numerical methods for diffusion coefficient recovery](https://arxiv.org/abs/2602.01656)
*Sahat Pandapotan Nainggolan,Julius Fergy Tiongson Rabago,Hirofumi Notsu*

Main category: math.NA

TL;DR: Modified CCBM method for stable reconstruction of diffusion coefficients from boundary data using gradient-weighted regularization and Sobolev-gradient descent.


<details>
  <summary>Details</summary>
Motivation: To address the inverse problem of reconstructing spatially varying diffusion coefficients from boundary Cauchy data in stationary elliptic equations, seeking more stable and robust reconstruction methods compared to classical boundary-based formulations.

Method: Introduces gradient-weighted modification of coupled complex-boundary method (CCBM) with H¹-type regularization term, formulates as regularized optimization problem, uses Sobolev-gradient descent scheme for numerical reconstruction, and includes projection-based extension for piecewise-constant coefficients.

Result: Modified CCBM yields more stable reconstructions, reduces high-frequency artifacts with appropriate H¹-weights, demonstrates favorable stability/robustness across various noise levels and coefficient structures, and enables stable recovery of piecewise-constant coefficients when subdomains share boundary portions.

Conclusion: The proposed CCBM-based Tikhonov regularization with pick-a-point strategy provides stable and reliable reconstruction of diffusion parameters, with performance dependent on problem specifics and regularization parameters.

Abstract: We revisit the inverse problem of reconstructing a spatially varying diffusion coefficient in stationary elliptic equations from boundary Cauchy data. From a theoretical perspective, we introduce a gradient-weighted modification of the coupled complex-boundary method (CCBM) incorporating an \(H^1\)-type term, and formulate the reconstruction as a regularized optimization problem over bounded admissible coefficients. We establish continuity and differentiability of the forward map, Lipschitz continuity of the modified cost functional, existence of minimizers, stability with respect to noisy data, and convergence under vanishing noise. From a numerical perspective, reconstructions are computed using a Sobolev-gradient descent scheme and evaluated through extensive numerical experiments across a range of noise levels, boundary inputs, and coefficient structures. In the reported tests, for sufficiently large but not excessive $H^1$-weights, the modified CCBM is observed to yield more stable reconstructions and to reduce certain high-frequency artifacts. Across the numerical scenarios considered in this study, the method often demonstrates favorable stability and robustness properties relative to several classical boundary-based formulations, although performance remains problem- and parameter-dependent. A projection-based extension further supports stable recovery of piecewise-constant diffusion coefficients in multi-subregion test cases. Our results indicate that, as long as all subdomains share a portion of the boundary, the proposed CCBM-based Tikhonov regularization approach with a pick-a-point strategy enables stable and reliable reconstruction of diffusion parameters.

</details>


### [62] [Curvature Preserving Fractal Interpolation Functions: A Hybrid Geometric Approach](https://arxiv.org/abs/2602.01707)
*K R Tyada*

Main category: math.NA

TL;DR: A curvature-preserving fractal interpolation function (FIF) is introduced that maintains geometric fidelity by matching the curvature of classical cubic splines through a penalty-optimized iterated function system.


<details>
  <summary>Details</summary>
Motivation: Traditional fractal interpolation functions often neglect geometric fidelity like curvature, which is important for accurate shape representation in modeling applications.

Method: Develop a curvature-aware iterated function system (IFS) with parameters optimized via penalty-based approach to minimize deviation from classical cubic spline curvature, with theoretical conditions for interpolation and curvature approximation.

Result: The proposed FIF achieves both data interpolation and shape fidelity, preserving curvature more accurately than standard splines across multiple examples.

Conclusion: The curvature-preserving FIF approach has potential applications in geometric modeling, computer graphics, and scientific data interpolation where both interpolation accuracy and geometric fidelity are important.

Abstract: Fractal interpolation functions (FIFs) generated using iterated function systems (IFS) provide a powerful framework for modeling self-similar and irregular data, yet traditional constructions often neglect geometric fidelity such as curvature. In this paper, we introduce a curvature-preserving variant of FIFs built upon a classical cubic spline interpolant. We define a curvature-aware iterated function system (IFS) with parameters optimized via a penalty-based approach to minimize deviation from the curvature of the classical spline. Theoretical conditions for interpolation and curvature approximation are derived. We compare the curvature of the proposed FIF with that of the classical cubic spline and discrete data curvature across multiple examples. Our method achieves both data interpolation and shape fidelity, preserving curvature more accurately than standard splines. The approach has potential applications in geometric modeling, computer graphics, and scientific data interpolation.

</details>


### [63] [Scalable Pseudospectral Analysis via Low-Rank Approximations of Dynamical Systems](https://arxiv.org/abs/2602.01721)
*Vladimir R. Kostic,Dragana Lj. Cvetkovic,Ljiljana Cvetkovic*

Main category: math.NA

TL;DR: A low-rank framework for efficient pseudospectral analysis that reduces cubic scaling to rank-proportional cost, enabling analysis of high-dimensional systems in machine learning and data-driven dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional pseudospectral analysis scales cubically with dimension, making it prohibitive for large-scale systems. Existing sparse methods don't work for machine learning/data-driven systems where operators are dense but low-rank.

Method: Developed a low-rank framework with exact characterization of pseudospectrum for low-rank matrices, reducing resolvent norm evaluation to eigenvalue problems proportional to rank. Derived rigorous inclusion sets via truncated/randomized low-rank approximations with perturbation bounds.

Result: Achieved orders-of-magnitude speedups while preserving accuracy. Enables efficient computation of stability quantities (distance to instability, Kreiss constants) scaling with effective rank rather than ambient dimension.

Conclusion: The framework makes pseudospectral analysis tractable for previously intractable high-dimensional problems in computational PDEs, control theory, and data-driven dynamics, especially for dense low-rank operators common in machine learning.

Abstract: Pseudospectral analysis is fundamental for quantifying the sensitivity and transient behavior of nonnormal matrices, yet its computational cost scales cubically with dimension, rendering it prohibitive for large-scale systems. While existing research on scalable pseudospectral computation has focused on exploiting sparsity structures, common in discretizations of differential operators, these approaches are ill-suited for machine learning and data-driven dynamical systems, where operators are typically dense but approximately low-rank. In this paper, we develop a comprehensive low-rank framework that dramatically reduces this computational burden. Our core theoretical contribution is an exact characterization of the pseudospectrum of arbitrary low-rank matrices, reducing the evaluation of resolvent norms to eigenvalue problems of dimension proportional to the rank. Building on this foundation, we derive rigorous inclusion sets for the pseudospectra of general matrices via truncated and randomized low-rank approximations, with explicit perturbation bounds. These results enable efficient estimators for key stability quantities, including distance to instability and Kreiss constants, at a cost that scales with the effective rank rather than the ambient dimension. We further demonstrate how our framework naturally extends to data-driven settings, providing pseudospectral analysis of transfer operators learned from nonlinear and stochastic dynamical systems. Numerical experiments confirm orders-of-magnitude speedups while preserving accuracy, opening pseudospectral analysis to previously intractable high-dimensional problems in computational PDEs, control theory, and data-driven dynamics.

</details>


### [64] [PINN-Based Kolmogorov-Arnold Networks with RAR-D Adaptive Sampling for Solving Elliptic Interface Problems](https://arxiv.org/abs/2602.01876)
*Zijuan Xin,Chenyao Wang,Feng Shi,Yizhong Sun*

Main category: math.NA

TL;DR: Proposes KAN-based PINNs with dual KAN structure and adaptive sampling for elliptic interface problems, achieving better accuracy with smaller networks and faster convergence than standard PINNs.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs using MLPs require large networks and extensive training for complex interface problems. KANs offer more flexibility with activation functions and can represent functions with fewer parameters.

Method: Introduces a novel PINN architecture based on Kolmogorov-Arnold Networks (KANs) with dual KAN structure that couples two KANs across subdomains and explicitly enforces interface conditions. Integrates RAR-D adaptive sampling strategy for dynamic training point refinement.

Result: Numerical experiments on elliptic interface problems show more uniform error distributions across the computational domain. The KAN-based PINNs achieve superior accuracy with significantly smaller network sizes and faster convergence compared to standard PINNs.

Conclusion: The proposed KAN-based PINN framework provides an efficient alternative to traditional MLP-based PINNs for solving interface problems, offering better performance with reduced computational resources.

Abstract: Physics-Informed Neural Networks (PINNs) have become a popular and powerful framework for solving partial differential equations (PDEs), leveraging neural networks to approximate solutions while embedding PDE constraints, boundary conditions, and interface jump conditions directly into the loss function. However, most existing PINN approaches are based on multilayer perceptrons (MLPs), which may require large network sizes and extensive training to achieve high accuracy, especially for complex interface problems. In this work, we propose a novel PINN architecture based on Kolmogorov-Arnold Networks (KANs), which offer greater flexibility in choosing activation functions and can represent functions with fewer parameters. Specifically, we introduce a dual KANs structure that couples two KANs across subdomains and explicitly enforces interface conditions. To further boost training efficiency and convergence, we integrate the RAR-D adaptive sampling strategy to dynamically refine training points. Numerical experiments on the elliptic interface problems yield more uniform error distributions across the computational domain, which demonstrates that our PINN-based KANs achieve superior accuracy with significantly smaller network sizes and faster convergence compared to standard PINNs.

</details>


### [65] [Convergence of high-index saddle dynamics for degenerate saddle points on critical manifolds](https://arxiv.org/abs/2602.01883)
*Tao Luo,Jianyuan Yin,Lei Zhang,Shixue Zhang*

Main category: math.NA

TL;DR: Rigorous analysis of high-index saddle dynamics (HiSD) for degenerate saddle points using Morse-Bott functions, proving convergence and explaining gradient alignment behavior.


<details>
  <summary>Details</summary>
Motivation: HiSD has shown empirical success for degenerate saddle points (where Hessian has zero eigenvalues), but lacks mathematical and numerical analysis for these cases. The paper aims to provide rigorous theoretical foundations for HiSD in degenerate settings.

Method: Utilizes Morse-Bott functions to analyze HiSD for computing degenerate saddle points on critical manifolds. Analyzes both continuous HiSD dynamics and discrete HiSD algorithm, including momentum-accelerated variants.

Result: Proves local convergence of continuous HiSD and establishes linear convergence rate for discrete HiSD. Provides theoretical explanation for gradient alignment tendency (gradient asymptotically aligns with specific Hessian eigenvector). Explains flexibility in selecting index for HiSD in degenerate cases.

Conclusion: The analysis provides rigorous mathematical foundation for HiSD in degenerate settings, validated through numerical experiments on neural-network loss landscapes. Momentum-accelerated variants achieve rapid convergence to degenerate saddle points.

Abstract: The high-index saddle dynamics (HiSD) method provides a powerful framework for finding saddle points and constructing solution landscapes. While originally derived for nondegenerate critical points, HiSD has demonstrated empirical success in degenerate cases, where the Hessian matrix exhibits zero eigenvalues. However, the mathematical and numerical analysis of HiSD for degenerate saddle points remains unexplored. In this paper, utilizing Morse-Bott functions, we present a rigorous analysis of HiSD for computing degenerate saddle points on a critical manifold. We prove the local convergence of the continuous HiSD and establish the linear convergence rate of the discrete HiSD algorithm. Furthermore, we provide a theoretical explanation for the gradient alignment tendency, revealing that the gradient direction asymptotically aligns with a specific Hessian eigenvector. Our analysis also elucidates the flexibility in selecting the index for HiSD in the context of degenerate saddle points. We validate our analytical results through numerical experiments on neural-network loss landscapes and demonstrate that momentum-accelerated variants of HiSD achieve rapid convergence to degenerate saddle points.

</details>


### [66] [A monolithic localized high-order ALE finite element method for multi-scale fluid-structure interaction problems](https://arxiv.org/abs/2602.02003)
*Lingyue Shen,Qi Xin,Yan Chen,Jiarui Han,Yumiao Zhang,Jinchao Xu,Shihua Gong*

Main category: math.NA

TL;DR: MLH-ALE is a monolithic high-order arbitrary Lagrangian-Eulerian finite element method for multi-scale fluid-structure interaction, featuring localized updating for computational efficiency and achieving optimal convergence.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of multi-scale fluid-structure interaction problems where there's significant scale disparity between the fluid domain and moving structures, requiring efficient computational methods that can handle complex geometries while maintaining accuracy.

Method: The method combines: 1) Monolithic localized high-order arbitrary Lagrangian-Eulerian (ALE) finite elements using isoparametric P₂ elements for geometric fidelity, 2) Implicit-explicit partitioned Runge-Kutta (IMEX-PRK) scheme for temporal discretization, and 3) A localized updating strategy to focus computational resolution on moving structures to address scale disparity.

Result: Numerical benchmarks confirm optimal high-order convergence of the ALE scheme. Simulations of particle focusing in spiral microchannels show reliable numerical results that agree well with experimental observations, demonstrating feasibility for complex multi-scale applications.

Conclusion: The MLH-ALE framework provides an effective approach for multi-scale FSI problems, successfully handling scale disparity through localized updating while maintaining high-order accuracy and geometric fidelity, making it suitable for complex applications like microfluidic particle focusing.

Abstract: This paper presents MLH-ALE, a monolithic localized high-order arbitrary Lagrangian-Eulerian finite element method for multi-scale fluid-structure interaction (FSI). The framework employs isoparametric $\mathcal{P}_2$ elements for geometric fidelity and an implicit-explicit partitioned Runge-Kutta (IMEX-PRK) scheme for temporal discretization. To address scale disparity, a localized updating strategy is integrated to focus computational resolution on the moving structure. Numerical benchmarks confirm the optimal high-order convergence of the underlying ALE scheme. Furthermore, simulations of particle focusing in spiral microchannels demonstrate that the MLH-ALE approach provides reliable numerical results in good agreement with experimental observations, confirming its feasibility for complex multi-scale applications.

</details>


### [67] [Monotonicity-based regularization of inverse medium scattering for shape reconstruction](https://arxiv.org/abs/2602.02052)
*Roland Griesmaier,Bastian Harrach,Jianli Xiang*

Main category: math.NA

TL;DR: Monotonicity-based regularization scheme for inverse scattering: combines monotonicity shape reconstruction with one-step linearization to recover scatterer support from far field data, with stability and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inverse scattering problem of recovering the support of an inhomogeneous scattering obstacle from far field observations. This is a challenging problem in wave-based imaging where traditional methods may lack stability or require extensive computational resources.

Method: Proposes a qualitative monotonicity-based regularization scheme that combines monotonicity-based shape reconstruction with one-step linearization. The approach uses far field observations from all possible incident and observation directions, and the linearization step stabilizes the monotonicity approach for noisy data.

Result: Theoretical results show that the scheme recovers the correct shape for noise-free data and converges to the exact solution as noise level tends to zero. Numerical examples demonstrate the effectiveness of the proposed method.

Conclusion: The monotonicity-based regularization scheme provides a stable and convergent approach for shape reconstruction in inverse scattering problems, combining the advantages of monotonicity methods with stabilization through linearization for practical applications with noisy data.

Abstract: We consider the scattering of time-harmonic plane waves by a compactly supported inhomogeneous scattering obstacle governed by the Helmholtz equation. Given far field observations of the scattered fields corresponding to plane wave incident fields for all possible incident and observation directions we study the inverse problem to recover the support of the scatterer. We propose a qualitative monotonicity-based regularization scheme which combines monotonicity-based shape reconstruction with one-step linearization to reconstruct a discrete approximation of the shape of the scatterer from noisy far field data. The purpose of the one-step linearization is to stabilize the monotonicity approach to shape reconstruction. We show that the monotonicity-based regularization scheme recovers the correct shape of the scatterer for noise-free data. Furthermore, we establish that the solution of the monotonicity-based regularization converges to the exact solution as the noise level tends to zero. We present numerical examples to illustrate our theoretical findings.

</details>


### [68] [Approximation of Functions: Optimal Sampling and Complexity](https://arxiv.org/abs/2602.02066)
*David Krieg,Mario Ullrich*

Main category: math.NA

TL;DR: Survey paper on function approximation/recovery from finite evaluations, covering information-theoretic limits, algorithms, sampling strategies, and connections to information-based complexity.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the fundamental problem of approximating functions from limited data, highlighting recent insights and connections between optimal recovery, machine learning, and numerical analysis.

Method: Survey methodology examining different aspects: information-theoretic limits due to finite data, algorithms and sampling strategies, and broader sampling approaches including nonlinear, adaptive, and random measurements.

Result: Synthesizes recent fundamental insights about information-theoretic limits of function recovery, presents optimal sampling strategies that approach these limits, and establishes relationships between different measurement settings.

Conclusion: The paper serves as both an introductory guide and contemporary summary of function approximation from finite evaluations, connecting optimal recovery, machine learning, and numerical analysis through the lens of information-based complexity.

Abstract: We consider approximation or recovery of functions based on a finite number of function evaluations. This is a well-studied problem in optimal recovery, machine learning, and numerical analysis in general, but many fundamental insights were obtained only recently. We discuss different aspects of the information-theoretic limit that appears because of the limited amount of data available, as well as algorithms and sampling strategies that come as close to it as possible.
  We also discuss (optimal) sampling in a broader sense, allowing other types of measurements that may be nonlinear, adaptive and random, and present several relations between the different settings in the spirit of information-based complexity. We hope that this article provides both, a basic introduction to the subject and a contemporary summary of the current state of research.

</details>


### [69] [On the Numerical Treatment of an Abstract Nonlinear System of Coupled Hyperbolic Equations Associated with the Timoshenko Model](https://arxiv.org/abs/2602.02068)
*Jemal Rogava,Zurab Vashakidze*

Main category: math.NA

TL;DR: Develops a symmetric three-layer semi-discrete time-stepping scheme for nonlinear Timoshenko beam systems with second-order accuracy, combining temporal discretization with Legendre-Galerkin spectral spatial approximation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical scheme for solving the Cauchy problem of abstract nonlinear coupled hyperbolic equations associated with Timoshenko beam models, enabling parallel computation and accurate approximation.

Method: Proposes a symmetric three-layer semi-discrete time-stepping scheme with nonlinear term evaluated at temporal midpoint, reducing nonlinear problem to linear at each step. Uses Legendre-Galerkin spectral method for spatial discretization, creating sparse linear systems that can be efficiently decoupled.

Result: Proves convergence and establishes second-order accuracy with respect to time-step size on local temporal intervals. Extends results to spatially one-dimensional nonlinear dynamic Timoshenko beam systems and validates through numerical experiments on benchmark problems.

Conclusion: The proposed scheme successfully provides an efficient, parallelizable numerical method for nonlinear Timoshenko beam systems with proven convergence and second-order accuracy, validated by theoretical analysis and numerical experiments.

Abstract: The present work addresses the Cauchy problem for an abstract nonlinear system of coupled hyperbolic equations associated with the Timoshenko model in a real Hilbert space. Our purpose is to develop and delve into a temporal discretization scheme for approximating a solution to this problem. To this end, we propose a symmetric three-layer semi-discrete time-stepping scheme in which the nonlinear term is evaluated at the temporal midpoint. As a result, at each time step, this approach reduces the original nonlinear problem to a linear one and enables parallel computation of its solution. Convergence is proved, and second-order accuracy with respect to the time-step size is established on a local temporal interval. The proposed scheme is then applied to a spatially one-dimensional nonlinear dynamic Timoshenko beam system, and the results obtained for the abstract nonlinear system are extended to this setting. A Legendre-Galerkin spectral approximation is employed for the spatial discretization. By taking differences of Legendre polynomials within the Galerkin framework, the resulting linear system is sparse and can be efficiently decoupled. The convergence of the method is also investigated. Finally, several numerical experiments on carefully chosen benchmark problems are conducted to validate the proposed approach and to confirm the theoretical findings.

</details>


### [70] [Convex limiting for finite elements and its relationship to residual distribution](https://arxiv.org/abs/2602.02095)
*Dmitri Kuzmin*

Main category: math.NA

TL;DR: Review of element-based algebraic stabilization methods for continuous finite element discretizations of nonlinear hyperbolic problems, focusing on convex limiting techniques that constrain antidiffusive element contributions.


<details>
  <summary>Details</summary>
Motivation: To address stabilization challenges in continuous finite element discretizations of nonlinear hyperbolic problems, particularly for multidimensional applications where traditional flux-based limiting may not be optimal.

Method: Two convex limiting approaches: 1) Localized flux-corrected transport (FCT) algorithm with low-order predictor followed by antidiffusive correction, and 2) Monolithic convex limiting (MCL) procedure at spatial semi-discretization level. Both constrain antidiffusive element contributions rather than fluxes.

Result: The resulting schemes can be interpreted as residual distribution methods. Both approaches enforce generalized discrete maximum principles by imposing inequality constraints on scalar functions of intermediate states to stay in convex invariant sets.

Conclusion: Element-based algebraic stabilization with convex limiting provides effective methods for continuous finite element discretizations of nonlinear hyperbolic problems, offering two viable approaches (FCT and MCL) that ensure solution quality through discrete maximum principles.

Abstract: We review some recent advances in the field of element-based algebraic stabilization for continuous finite element discretizations of nonlinear hyperbolic problems. The main focus is on multidimensional convex limiting techniques designed to constrain antidiffusive element contributions rather than fluxes. We show that the resulting schemes can be interpreted as residual distribution methods. Two kinds of convex limiting can be used to enforce the validity of generalized discrete maximum principles in this context. The first approach has the structure of a localized flux-corrected transport (FCT) algorithm, in which the computation of a low-order predictor is followed by an antidiffusive correction stage. The second option is the use of a monolithic convex limiting (MCL) procedure at the level of spatial semi-discretization. In both cases, inequality constraints are imposed on scalar functions of intermediate states that are required to stay in convex invariant sets.

</details>


### [71] [Convergence of a least-squares splitting method for the Monge-Ampère equation](https://arxiv.org/abs/2602.02118)
*Anna Peruso,Massimo Sorella*

Main category: math.NA

TL;DR: The paper provides the first rigorous convergence proof for a nonlinear least-squares splitting method for the Monge-Ampère equation, showing linear convergence in H² for initial data near solutions.


<details>
  <summary>Details</summary>
Motivation: The splitting method performs well computationally for the Monge-Ampère equation, but a rigorous convergence theory has been lacking despite its practical success.

Method: Reformulates the iteration as an alternating-projection scheme on Sobolev spaces H^m. Shows the Gâteaux differentials at solutions are linear projections onto tangent spaces, proves these tangent spaces are transverse, and uses Hilbert-space alternating projection theory.

Result: Proves linear convergence in H² of the splitting method on the 2D torus for initial data sufficiently close to a solution u ∈ H⁴. This is the first rigorous convergence result for this method in the periodic setting.

Conclusion: The paper provides a functional-analytic explanation for the method's numerical robustness and establishes the first rigorous convergence theory for this splitting approach to the Monge-Ampère equation.

Abstract: We study the theoretical convergence of the nonlinear least-squares splitting method for the Monge-Ampère equation in which each iteration decouples the pointwise nonlinearity from the differential operator and consists of a local nonlinear update followed by the solution of two sequential Poisson-type elliptic problems. While the method performs well in computations, a rigorous convergence theory has remained unavailable. We observe that the iteration admits a reformulation as an alternating-projection scheme on Sobolev spaces $H^m$, $m\ge 0$. At a solution, the Gâteaux differentials of the projection maps are the linear projections onto the corresponding tangent spaces. We prove that these tangent spaces are transverse, and hence the linearization of the alternating-projection map is a contraction by classical Hilbert-space theory for alternating projections. Building on this geometric characterization, we prove linear convergence in $H^2$ of the splitting method on the two-dimensional torus $\mathbb{T}^2$ for initial data sufficiently close to a solution $u\in H^4$. To the best of our knowledge, this yields the first rigorous convergence result for this splitting method in the periodic setting and provides a functional-analytic explanation for its observed numerical robustness.

</details>


### [72] [Asymmetric Lévy walks driven by convex combination of fractional material derivatives](https://arxiv.org/abs/2602.02169)
*Łukasz Płociniczak,Marek A. Teuerle,Hubert Woszczek*

Main category: math.NA

TL;DR: The paper develops a probability-conserving finite-volume scheme for fractional PDEs describing Lévy walks, ensuring non-negativity and mass conservation while proving existence, stability, and convergence.


<details>
  <summary>Details</summary>
Motivation: To study anomalous transport governed by fractional dynamics in Lévy walks while preserving probabilistic properties (non-negativity and unit mass) in both theoretical analysis and numerical computations.

Method: Uses Fourier-Laplace transforms to prove existence of mild solutions, identifies conditions for probability density preservation, and constructs a finite-volume discretization that is probability conservative by construction with discrete stability and convergence analysis.

Result: Proved existence of mild solutions, identified necessary/sufficient conditions for probability density preservation, developed probability-conserving finite-volume scheme with proven stability and convergence, validated through extensive numerical experiments showing mass conservation, non-negativity, and agreement with analytic solutions.

Conclusion: The combined theoretical and numerical framework provides a reliable tool for studying anomalous transport governed by fractional dynamics, with rigorous preservation of probabilistic properties essential for modeling Lévy walk processes.

Abstract: We analyze a class of linear partial differential equations that arise as deterministic descriptions of the scaling limits of Lévy walks, in which transport is driven by a convex combination of fractional material derivatives and a source term. Using techniques of Fourier-Laplace transforms, we first prove the existence of mild solutions for continuous initial data. Using a recently obtained pointwise representation of the fractional material derivative, we then identify a necessary and sufficient condition on the source term that guaranties the solution to remain a probability density for all times (non-negativity and unit mass). Motivated by the need to preserve these probabilistic properties in computations, we construct a finite-volume discretization that is probability conservative by construction. We establish discrete stability and a convergence result for the continuous weak solution as space and time steps tend to zero. Extensive numerical experiments validate the scheme: total mass is conserved, non-negativity is maintained, and the computed solutions reproduce the known analytic representations of the probability density functions associated with the Lévy walk process. The combined theoretical and numerical framework provides a reliable tool for studying anomalous transport governed by fractional dynamics.

</details>


### [73] [A new Energy Equation Derivation for the Shallow Water Linearized Moment Equations](https://arxiv.org/abs/2602.02247)
*Julian Koellermeier*

Main category: math.NA

TL;DR: The paper presents a systematic derivation of the energy equation for Shallow Water Linearized Moment Equations (SWLME), which extend standard Shallow Water Equations to account for vertical velocity variations.


<details>
  <summary>Details</summary>
Motivation: Standard Shallow Water Equations assume depth-averaged vertical velocity profiles, which limits their accuracy for flows with vertical variations. SWME/SWLME address this limitation by allowing polynomial velocity profiles, but a systematic energy equation derivation was needed for better understanding and numerical implementation.

Method: The authors perform a new systematic derivation of the energy equation for SWLME based on the standard SWE energy equation derivation approach. The derivation includes the skew-symmetric formulation of the model and follows higher order depth integration principles used in SWME development.

Result: A new systematic derivation of the energy equation for Shallow Water Linearized Moment Equations is presented, providing a foundation for better understanding of the model's energy properties and enabling more robust numerical solutions.

Conclusion: The systematic derivation of the SWLME energy equation is beneficial for extending the approach to other SWME variants and improving their numerical solution methods, advancing the modeling capabilities for free-surface flows with vertical velocity variations.

Abstract: Shallow Water Moment Equations (SWME) are extensions to the well-known Shallow Water Equations (SWE) for the efficient modeling and numerical simulation of free-surface flows. While the SWE typically assume a depth-averaged vertical velocity profile, the SWME allow for vertical variations of the velocity profile. The SWME therefore assume a polynomial profile and then derive additional evolution equations for the polynomial coefficients via higher order depth integration. In this work, we perform a new systematic derivation of the energy equation for a specific variant of the SWME, called the Shallow Water Linearized Moment Equations (SWLME). The derivation is based on the standard SWE energy equation derivation and includes the skew-symmetric formulation of the model. The new systematic derivation is beneficial for the extension to other SWME variants and their numerical solution.

</details>


### [74] [A formula for Hermite multivariate interpolation problem and partial fraction decomposition](https://arxiv.org/abs/2602.02353)
*Hakop Hakopian*

Main category: math.NA

TL;DR: New formula for Hermite multivariate interpolation based on Chung-Yao interpolation, enabling direct partial fraction decomposition of rational functions.


<details>
  <summary>Details</summary>
Motivation: To develop an explicit solution for partial fraction decomposition of rational functions using multivariate interpolation techniques.

Method: Derives new Hermite multivariate interpolation formula based on Chung-Yao interpolation, then applies it to solve partial fraction decomposition problems directly.

Result: Provides explicit formula for Hermite multivariate interpolation and demonstrates its application to obtain direct partial fraction decomposition solutions.

Conclusion: The new interpolation formula offers a straightforward approach to partial fraction decomposition, connecting interpolation theory with rational function analysis.

Abstract: We present a new formula for the Hermite multivariate interpolation corresponding to the Chung-Yao interpolation. With the help of the respective univariate interpolation formula we give a direct and explicit solution to the problem of partial fraction decomposition of rational functions.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [75] [The metaplectic semigroup and its applications to time-frequency analysis and evolution operators](https://arxiv.org/abs/2601.22252)
*Gianluca Giacchi,Luigi Rodino,Davide Tramontana*

Main category: math.AP

TL;DR: Extends metaplectic theory to positive complex symplectic matrices using operator-theoretic approach, investigates semigroup structure, and applies to time-frequency analysis and parabolic equations.


<details>
  <summary>Details</summary>
Motivation: Existing literature focuses on propagators of quadratic evolution equations via Mehler formulas, but lacks systematic operator-theoretic analysis of the metaplectic semigroup for positive complex symplectic matrices beyond unitary settings.

Method: Operator-theoretic and symplectic approach adapting techniques from standard metaplectic group to broader framework, investigating generators, polar decomposition, and intertwining relations with complex conjugation and Wigner distribution.

Result: Provides deeper insight into metaplectic semigroup structure, characterizes time-frequency representations with prescribed properties, studies boundedness of propagators on modulation spaces, obtains operator norm estimates, and analyzes Wigner singularity propagation.

Conclusion: Systematic operator-theoretic framework extends metaplectic theory beyond differential problems, enabling new applications in time-frequency analysis and parabolic equations with complex quadratic Hamiltonians.

Abstract: We develop a systematic analysis of the metaplectic semigroup $\mathrm{Mp}_+(d,\mathbb{C})$ associated with positive complex symplectic matrices, a notion introduced almost simultaneously and independently by Hörmander, Brunet, Kramer, and Howe, thereby extending the classical metaplectic theory beyond the unitary setting.
  While the existing literature has largely focused on propagators of quadratic evolution equations, for which results are typically obtained via Mehler formulas, our approach is operator-theoretic and symplectic in spirit and adapts techniques from the standard metaplectic group $\mathrm{Mp}(d,\mathbb{R})$ to a substantially broader framework that is not driven by differential problems or particular propagators.
  This point of view provides deeper insight into the structure of the metaplectic semigroup, and allows us to investigate its generators, polar decomposition, and intertwining relations with complex conjugation and with the Wigner distribution. We then exploit these structural results to characterize, from a metaplectic perspective, classes of time-frequency representations satisfying prescribed structural properties. Finally, we discuss further implications for parabolic equations with complex quadratic Hamiltonians, we study the boundedness of their propagators on modulation spaces, we obtain estimates in time of their operator norms. Finally, we apply our theory to the study of propagation of Wigner singularities.

</details>


### [76] [A Generalized Analytical Heat Transfer Model for Enhanced Geothermal Systems: Capturing Fracture Interactions and Correcting Classical Optimistic Predictions](https://arxiv.org/abs/2601.22316)
*Nelson Barros-Galvis or Christine Ehlig-Economides or Cristi Darley Guevara*

Main category: math.AP

TL;DR: A new analytical heat transfer model for enhanced geothermal systems that accounts for fracture interactions, corrects optimistic bias in classical models, and is computationally efficient enough for spreadsheet implementation.


<details>
  <summary>Details</summary>
Motivation: Classical analytical models like Gringarten et al. 1975 overestimate thermal performance due to simplified assumptions, leading to unrealistic engineering decisions in geothermal design and feasibility studies.

Method: Developed a generalized analytical model based on Green's functions that explicitly captures thermal interactions between fractures while preserving analytical tractability. The solution avoids Laplace space transformations or numerical inversion algorithms.

Result: The model shows close agreement with numerical simulations (CMG STARS and Volsung software) in temperature evolution, including fracture interaction effects. It corrects optimistic bias of classical approaches and provides more reliable predictions of production temperature and energy recovery.

Conclusion: The proposed model bridges the gap between legacy analytical models and numerical/commercial tools, with direct implications for geothermal feasibility studies, well design, and power forecasting, while retaining analytical simplicity and practical applicability.

Abstract: Numerical analytical heat transfer models play a critical role in geothermal design and feasibility studies. Classical solutions, such as those proposed by Gringarten et al. 1975, rely on simplified assumptions and systematically overestimate thermal performance, which can lead to unrealistic engineering decisions.
  This study presents a generalized analytical model for enhanced geothermal systems that explicitly captures thermal interactions between fractures while preserving analytical tractability.
  The formulation is based on Greenś functions and reproduces realistic thermal behavior under conditions representative of fractured geothermal reservoirs. The resulting solution is computationally efficient and sufficiently simple to be implemented directly in standard spreadsheets, without requiring Laplace space transformations or numerical inversion algorithms.
  The model is validated against numerical simulations performed using CMG STARS and Volsung software, showing close agreement in temperature evolution, including the effects of interacting fractures. Compared with classical analytical approaches, the proposed model corrects optimistic bias and provides more reliable predictions of production temperature and energy recovery.
  These results have direct implications for geothermal feasibility studies, well design, and power forecasting, effectively bridging the gap between legacy analytical models and numerical or commercial engineering tools. Building on the analytical framework originally introduced by Gringarten et al. 1975, the proposed formulation generalizes classical heat transfer solutions to account for fracture interaction while retaining analytical simplicity and practical applicability.

</details>


### [77] [Local existence and nonexistence of solutions to the Hardy parabolic equation with general nonlinearity](https://arxiv.org/abs/2601.22520)
*Yo Tsusaka*

Main category: math.AP

TL;DR: Local existence and nonexistence results for Hardy parabolic equation with general nonlinearity, establishing optimal integrability conditions for initial data.


<details>
  <summary>Details</summary>
Motivation: To determine precise conditions on initial data that guarantee existence or nonexistence of local-in-time nonnegative solutions for Hardy parabolic equations with general nonlinearities.

Method: Supersolution method for proving existence results, establishing optimal integrability conditions on initial functions.

Result: Obtained optimal integrability conditions for initial data that determine when local-in-time nonnegative solutions exist or fail to exist.

Conclusion: The paper establishes sharp conditions for local existence of solutions to Hardy parabolic equations, with the supersolution method providing existence proofs under optimal integrability assumptions.

Abstract: In this paper, we consider the Cauchy problem for the Hardy parabolic equation with general nonlinearity and establish the local existence and nonexistence results. Our results provide the optimal integrability conditions on initial function for the existence of a local-in-time nonnegative solution. The proof of the existence result is based on the supersolution method.

</details>


### [78] [Transmission and Reflection coefficients for Schrödinger Operators with Truncated Periodic Potentials that support defect states](https://arxiv.org/abs/2601.22544)
*Joseph C. Stellman,Jeremy L. Marzuola*

Main category: math.AP

TL;DR: Study of scattering through truncated periodic potentials with perturbations supporting localized gap eigenstates, proving existence of zero reflection states near bound states and analyzing transmission/reflection behavior.


<details>
  <summary>Details</summary>
Motivation: To understand how scattering waves behave through truncated periodic potentials with perturbations that support localized gap eigenstates, particularly focusing on transmission resonances and their relationship to bound states.

Method: Mathematical analysis proving existence of zero reflection states (transmission resonances) in complex neighborhoods around positive bound states of model operators, comparing to previously found scattering resonances, and analyzing transmission/reflection coefficients near bound states.

Result: Proves existence of distinct zero reflection states near assumed bound states, establishes relationship between transmission resonances and scattering resonances, and provides analysis of transmission/reflection coefficient behavior near bound states.

Conclusion: The study provides theoretical understanding of scattering through perturbed truncated periodic potentials, demonstrating how transmission resonances emerge near bound states and offering analytical framework for studying transmission/reflection properties in such systems, with applications to both crystalline and harmonic oscillator cases.

Abstract: We consider scattering waves through truncated periodic potentials with perturbations that support localized gap eigenstates. In a small complex neighborhood around an assumed positive bound state of the model operator, we prove the existence of a distinct zero reflection state, or transmission resonance. We compare its location to a previously found scattering resonance and use the properties of solutions near these interesting points to analyze the behavior of transmission and reflection coefficients of scattering solutions near the assumed bound state. By example, we also discuss the truncated simple harmonic oscillator and compare the analysis to the crystalline case.

</details>


### [79] [Spectral properties and bound states of the Dirac equation on periodic quantum graphs](https://arxiv.org/abs/2601.22603)
*Zhipeng Yang,Ling Zhu*

Main category: math.AP

TL;DR: Variational approach for nonlinear Dirac equations on periodic quantum graphs yields existence and multiplicity of bound states.


<details>
  <summary>Details</summary>
Motivation: Study nonlinear Dirac equations on periodic quantum graphs to understand existence and multiplicity of bound states, which are important for quantum mechanics and condensed matter physics applications.

Method: Introduce Dirac operator on periodic graph with periodic potential, describe spectral decomposition, work in natural energy space, use variational approach with linking geometry and Cerami-type compactness modulo translations.

Result: Prove existence of at least one bound state for asymptotically linear or superquadratic nonlinearities; when nonlinearity is even, prove existence of infinitely many geometrically distinct bound states.

Conclusion: Variational methods successfully establish existence and multiplicity results for nonlinear Dirac equations on periodic quantum graphs, with translation-invariance playing crucial role in compactness.

Abstract: We investigate nonlinear Dirac equations on a periodic quantum graph $G$ and develop a variational approach to the existence and multiplicity of bound states. After introducing the Dirac operator on $G$ with a $\mathbb Z^{d}$-periodic potential, we describe its spectral decomposition and work in the natural energy space. Under asymptotically linear or superquadratic assumptions on the nonlinearity, we establish the required linking geometry and a Cerami-type compactness property modulo $\mathbb Z^{d}$-translations. As a consequence, we prove the existence of at least one bound state and, when the nonlinearity is even, infinitely many geometrically distinct bound states.

</details>


### [80] [Weighted estimates for Hodge-Maxwell systems](https://arxiv.org/abs/2601.22604)
*Rohit Mahato,Swarnendu Sil*

Main category: math.AP

TL;DR: The paper establishes boundary regularity estimates in weighted L^p spaces for weak solutions to Hodge systems with various boundary conditions, leading to solvability of Hodge-Maxwell systems and Hodge decomposition theorems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop regularity theory for Hodge systems in weighted L^p spaces with Muckenhoupt weights, which is important for understanding partial differential equations in geometric analysis and mathematical physics, particularly for Hodge-Maxwell systems that arise in electromagnetism and related fields.

Method: The authors avoid potential theory and representation formulas, instead using decay estimates in the spirit of the 'Campanato method' to establish weighted L^p estimates. They work with Muckenhoupt weights A_p and consider two types of boundary conditions: either ν∧ω and ν∧d*(Bω) or ν⌟Bω and ν⌟Adω prescribed on the boundary.

Result: The main results are up-to-the-boundary regularity estimates in weighted L^p spaces for weak solutions to Hodge systems. As consequences, the authors prove solvability of Hodge-Maxwell systems and derive Hodge decomposition theorems in weighted Lebesgue spaces.

Conclusion: The paper successfully establishes a comprehensive regularity theory for Hodge systems in weighted function spaces using Campanato-type methods, providing new tools for analyzing Hodge-Maxwell systems and extending Hodge decomposition to weighted settings.

Abstract: We establish up to the boundary regularity estimates in weighted $L^{p}$ spaces with Muckenhoupt weights $A_{p}$ for weak solutions to the Hodge systems
  \begin{align*}
  d^{\ast}\left(Adω\right) + B^{\intercal}dd^{\ast}\left(Bω\right) = λBω+ f \quad \text{ in } Ω
  \end{align*}
  with either $ν\wedge ω$ and $ν\wedge d^{\ast}\left(Bω\right)$ or $ν\lrcorner Bω$ and $ν\lrcorner Adω$ prescribed on $\partialΩ.$ As a consequence, we prove the solvability of Hodge-Maxwell systems and derive Hodge decomposition theorems in weighted Lebesgue spaces. Our proof avoids potential theory, does not rely on representation formulas and instead uses decay estimates in the spirit of `Campanato method' to establish weighted $L^{p}$ estimates.

</details>


### [81] [Simplicity of eigenvalues for elliptic problems with mixed Steklov-Robin boundary condition](https://arxiv.org/abs/2601.22829)
*Marco Ghimenti,Anna Maria Micheletti,Angela Pistoia*

Main category: math.AP

TL;DR: The paper proves that for generic domains, all eigenvalues of elliptic problems with mixed Steklov-Robin boundary conditions are simple.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral properties of elliptic problems with mixed Steklov-Robin boundary conditions and establish generic simplicity of eigenvalues.

Method: Domain perturbation techniques and analysis of operator transversality.

Result: For a generic domain, all eigenvalues of the studied elliptic problems are simple.

Conclusion: The spectral simplicity property holds generically for elliptic problems with mixed Steklov-Robin boundary conditions.

Abstract: This paper investigates the spectral properties of two classes of elliptic problems characterized by mixed Steklov-Robin boundary conditions. Our main objective is to prove that, for a generic domain, all the eigenvalues are simple. This result is established by employing domain perturbation techniques and analyzing the transversality of the associated operators.

</details>


### [82] [Unconditional well-posedness of the master equation for monotone mean field games of controls](https://arxiv.org/abs/2601.22845)
*Joe Jackson,Alpár R. Mészáros*

Main category: math.AP

TL;DR: First unconditional well-posedness for master equations in mean field games of controls, covering displacement/Lasry-Lions monotone data and small time horizons, without requiring a priori regularity on fixed-point mappings.


<details>
  <summary>Details</summary>
Motivation: Previous results on master equations for mean field games of controls required additional structural assumptions on fixed-point mappings arising from control interactions. The authors aim to establish unconditional well-posedness where all assumptions are imposed only at the Lagrangian and terminal cost level.

Method: Bottom-up approach using N-player Nash systems: 1) Show solutions of N-player systems are compact via uniform-in-N decay estimates for derivatives of value functions, 2) Prove subsequential limit points must be solutions to the master equation, 3) Build classical solution without relying on generalized method of characteristics.

Result: First unconditional well-posedness result for master equations in mean field games of controls, covering displacement monotone or Lasry-Lions monotone data, and small time horizons. Results allow for non-degenerate idiosyncratic Brownian noise and common noise with constant intensity.

Conclusion: The paper establishes a fundamental well-posedness result for master equations in mean field games of controls using a novel bottom-up approach, eliminating the need for additional structural assumptions on fixed-point mappings and providing a more natural framework for analyzing these games.

Abstract: We establish the first unconditional well-posedness result for the master equation associated with a general class of mean field games of controls. Our analysis covers games with displacement monotone or Lasry--Lions monotone data, as well as those with a small time horizon. By unconditional, we mean that all assumptions are imposed solely at the level of the Lagrangian and the terminal cost. In particular, we do not require any a priori regularity or structural assumptions on the additional fixed-point mappings arising from the control interactions; instead we show that these fixed-point mappings are well-behaved as a consequence of the regularity and the monotonicity of the data. Our approach is bottom-up in nature, unlike most previous results which rely on a generalized method of characteristics. In particular, we build a classical solution of the master equation by showing that the solutions of the corresponding $N$-player Nash systems are compact, in an appropriate sense, and that their subsequential limit points must be solutions to the master equation. Compactness is obtained via uniform-in-$N$ decay estimates for derivatives of the $N$-player value functions. The underlying games are driven by non-degenerate idiosyncratic Brownian noise, and our results allow for the presence of common noise with constant intensity.

</details>


### [83] [Existence of a solution of the TV Wasserstein gradient flow](https://arxiv.org/abs/2601.22847)
*Kexin Lin,Filippo Santambrogio*

Main category: math.AP

TL;DR: Existence of TV Wasserstein gradient flow solutions on flat torus for initial densities bounded away from zero, with preservation of bounds and BV norm decay rates.


<details>
  <summary>Details</summary>
Motivation: Generalize previous work by Carlier and Poon that only fully proved results in 1D and didn't handle non-BV initial densities. Extend existence theory for TV Wasserstein gradient flows to arbitrary dimensions with minimal regularity assumptions.

Method: Use approximated TV-JKO scheme that artificially imposes a lower bound on density, enabling construction of continuous-in-time solutions regular enough to propagate initial bounds and analyze BV norm decay.

Result: Prove existence of solutions for initial densities bounded from below and above by positive constants on flat torus in any dimension. Solutions preserve density bounds and show BV norm decay: t^{-1/3} as t→0 for non-BV initial data, t^{-1} as t→∞.

Conclusion: Successfully extend TV Wasserstein gradient flow theory to arbitrary dimensions with weak regularity assumptions, improving upon previous 1D-only results and handling non-BV initial data cases.

Abstract: On the flat torus in any dimension we prove existence of a solution to the TV Wasserstein gradient flow equation, only assuming that the initial density $ρ_0$ is bounded from below and above by strictly positive constants. This solution preserves upper and lower bounds of the densities, and shows a certain decay of the BV norm (of the order of $t^{-1/3}$ for $t\to 0$ -- if $ρ_0\notin BV$, otherwise the BV norm is of course bounded -- and of the order of $t^{-1}$ as $t\to\infty$). This generalizes a previous result by Carlier and Poon, who only gave a full proof in one dimension of space and did not consider the case $ρ_0\notin BV$.
  The main tool consists in considering an approximated TV-JKO scheme which artificially imposes a lower bound on the density and allows to find a continuous-in-time solution regular enough to prove that the lower bounds of the initial datum propagates in time, and study on this approximated equation the decay of the BV norm.

</details>


### [84] [Global Well-posedness of Strong Solutions to the Cauchy Problem of 2D Nonhomogeneous Navier-Stokes Equations with Density-Dependent Viscosity and Vacuum](https://arxiv.org/abs/2601.22877)
*Bing Yuan,Rong Zhang,Peng Zhou*

Main category: math.AP

TL;DR: Global existence of strong solutions for 2D nonhomogeneous Navier-Stokes equations with density-dependent viscosity, allowing arbitrarily large initial data and vacuum regions, plus large-time asymptotic behavior analysis.


<details>
  <summary>Details</summary>
Motivation: To study the Cauchy problem for modified 2D nonhomogeneous incompressible Navier-Stokes equations with density-dependent viscosity, addressing challenges with large initial data and vacuum regions where density vanishes.

Method: Fully utilizes the structure of the system to obtain key estimates of $\|\nabla ρ\|_{L_t^\infty L_x^q},q>2$ without smallness assumptions on initial data, enabling global existence proofs.

Result: Establishes global existence of strong solutions for both vacuum and nonvacuum far-field density cases, allows arbitrarily large initial data and vanishing initial density, and proves large-time asymptotic behavior of velocity and pressure gradients.

Conclusion: The paper successfully overcomes traditional smallness assumptions, providing global existence results for challenging scenarios in 2D nonhomogeneous Navier-Stokes equations with density-dependent viscosity, including vacuum cases and large initial data.

Abstract: This paper is concerned with the Cauchy problem for the modified two-dimensional (2D) nonhomogeneous incompressible Navier-Stokes equations with density-dependent viscosity. By fully using the structure of the system, we can obtain the key estimates of $\|\nabla ρ\|_{L_t^\infty L_x^q},q>2$ without any smallness asuumption on the initial data, and thus establish the global existence of the strong solutions with the far-field density being either vacuum or nonvacuum. Notably, the initial data can be arbitrarily large and the initial density is allowed to vanish. Furthermore, the large-time asymptotic behavior of the gradients of the velocity and the pressure is also established.

</details>


### [85] [Local Well-posedness and Blow-up for the Restricted Fourth-Order Prandtl Equation](https://arxiv.org/abs/2601.22940)
*Ik Hyun Choi*

Main category: math.AP

TL;DR: Local well-posedness and finite-time blow-up proved for a restricted fourth-order Prandtl equation on half-line with clamped boundary conditions.


<details>
  <summary>Details</summary>
Motivation: The equation arises from a two-dimensional fourth-order Prandtl system via ansatz reduction, featuring nonlinearity with nonlocal integral term. Understanding well-posedness and blow-up behavior is important for analyzing this higher-order boundary layer model.

Method: Use Duhamel fixed-point argument requiring uniform L¹ bounds for half-line biharmonic heat kernel. Establish uniform L¹ estimates for kernel and derivatives, show semigroup preserves spatial regularity under compatibility conditions using alternative representation via integration by parts.

Result: Proved local existence and uniqueness for restricted model, and constructed solutions that exhibit finite-time blow-up.

Conclusion: Successfully analyzed the restricted fourth-order Prandtl equation, establishing both local well-posedness and finite-time blow-up phenomena through careful kernel estimates and fixed-point methods.

Abstract: We prove local well-posedness and finite-time blow-up for a restricted fourth-order Prandtl equation posed on the half-line with clamped boundary conditions. The equation arises from a two-dimensional fourth-order Prandtl system via an ansatz reduction, and its nonlinearity involves a nonlocal integral term. To close a Duhamel fixed-point argument, we need uniform $L^1$ bounds for the associated half-line biharmonic heat kernel. We establish uniform $L^1$ estimates for the kernel and its derivatives, and we show that the semigroup preserves spatial regularity under appropriate compatibility conditions, using an alternative representation derived by integration by parts. These kernel estimates yield local existence and uniqueness for the restricted model and allow us to construct solutions that blow-up in finite time.

</details>


### [86] [Instability of two-dimensional Taylor-Green Vortices](https://arxiv.org/abs/2601.23040)
*Gonzalo Cao-Labora,Maria Colombo,Michele Dolce,Paolo Ventura*

Main category: math.AP

TL;DR: The paper develops a general criterion for characterizing unstable eigenvalues of linear Hamiltonian operators as zeros of a holomorphic function, and applies it to prove spectral instability of the Taylor-Green vortex in 2D ideal fluids.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical framework for analyzing spectral instability of steady states in Hamiltonian systems, particularly for fluid dynamics problems like the Taylor-Green vortex in ideal fluids.

Method: Develops a general criterion that characterizes unstable eigenvalues as zeros of a holomorphic function (determinant of finite-dimensional matrix). Applies this to Taylor-Green vortex by analyzing different invariant subspaces (odd/even perturbations) and combines analytical methods with rigorous computer-assisted arguments.

Result: Proves linear stability of odd perturbations (no unstable spectrum on real axis), detects real instabilities in rescaled vortices, and fully characterizes unstable spectrum in even function subspace using computer-assisted root finding.

Conclusion: The developed criterion successfully characterizes spectral instability of the Taylor-Green vortex, demonstrating both analytical stability results and computer-assisted detection of instabilities in different perturbation subspaces.

Abstract: For a wide class of linear Hamiltonian operators we develop a general criterion that characterizes the unstable eigenvalues as the zeros of a holomorphic function given by the determinant of a finite-dimensional matrix. We apply the latter result to prove the spectral instability of the Taylor-Green vortex in two-dimensional ideal fluids. The linearized Euler operator at this steady state possesses different invariant subspaces, within which we apply our criterion to rule out or detect instabilities. We show linear stability of odd perturbations, for which the unstable spectrum can appear only on the real axis. We exclude this possibility by applying our stability criterion. Real instabilities, instead, exist and can be detected with the same criterion if we consider suitable rescalings of the Taylor-Green vortex. In the subspace of functions even in both variables, the problem is reduced to finding a single complex root of our stability function. We successfully locate this value by combining our general criterion with a rigorous computer-assisted argument. As a consequence, we fully characterize the unstable spectrum of the Taylor-Green vortex.

</details>


### [87] [Existence of Traveling Waves in Infinite Range FPUT Lattices](https://arxiv.org/abs/2601.23091)
*Michael Herrmann,Karsten Matthies,Jan-Patrick Meyer*

Main category: math.AP

TL;DR: Existence of solitary waves in a lattice with long-range repulsive interactions, proven via constrained optimization yielding unimodal solutions.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of solitary wave solutions in systems with long-range repulsive interactions, which is mathematically challenging due to the non-local nature of the forces.

Method: Variational existence proof using constrained optimization techniques to find solitary wave solutions as critical points of an energy functional subject to constraints.

Result: Proved existence of a one-parameter family of unimodal solitary wave solutions and characterized asymptotic behavior of large, fast, high-energy waves.

Conclusion: Solitary waves can exist in lattices with purely repulsive long-range interactions, contrary to intuition, with specific mathematical properties and asymptotic behaviors.

Abstract: We prove the existence of solitary waves in a lattice where all particles interact with each other by pair-wise repulsive forces that decay with distance. The variational existence proof is based on constrained optimization and provides a one-parameter family of unimodal solutions. We also describe the asymptotic behavior of large, fast, high-energy waves.

</details>


### [88] [Nonlinear Schrödinger Equation with magnetic potential on metric graphs](https://arxiv.org/abs/2601.23115)
*Riccardo Adami,Nicolò Cangiotti,Ivan Gallo,David Spitzkopf*

Main category: math.AP

TL;DR: The paper proves that magnetic Schrödinger equations on metric graphs can be reduced to non-magnetic problems with effective repulsive potentials determined by Aharonov-Bohm flux, enabling extension of existence criteria and revealing mass-dependent phase transitions.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence of ground states for nonlinear magnetic Schrödinger equations on noncompact metric graphs, which are important in quantum mechanics and condensed matter physics for modeling systems with nontrivial topology and magnetic fields.

Method: Proves variational equivalence between magnetic Hamiltonian and non-magnetic operator with additional repulsive potentials supported on graph cycles, where the effective potential is strictly determined by Aharonov-Bohm flux through topological loops.

Result: Extends classical existence criteria to magnetic setting, characterizes ground state structure on tadpole graph revealing mass-dependent phase transition, shows ground states exist for small repulsion in intermediate mass regime, and demonstrates strong flux prevents ground state formation.

Conclusion: The reduction technique enables systematic study of magnetic ground states on metric graphs, revealing how Aharonov-Bohm flux creates effective repulsive potentials that control existence and properties of ground states, with applications to specific graph topologies.

Abstract: In this manuscript, we shall investigate the Nonlinear Magnetic Schrödinger Equation on noncompact metric graphs, focusing on the existence of ground states. We prove that the magnetic Hamiltonian is variationally equivalent to a non-magnetic operator with additional repulsive potentials supported on the graph's cycles. This effective potential is strictly determined by the Aharonov-Bohm flux through the topological loops. Leveraging this reduction, we extend classical existence criteria to the magnetic setting. As a key application, we characterize the ground state structure on the tadpole graph, revealing a mass-dependent phase transition. The ground states exist for sufficiently small repulsion in an intermediate regime of masses while sufficiently strong flux prevents the formation of ground states.

</details>


### [89] [Hyperbolic partial differential equations with complex characteristics on Fourier Lebesgue spaces](https://arxiv.org/abs/2601.23138)
*Duván Cardona,William Obeng-Denteh,Frederick Opoku*

Main category: math.AP

TL;DR: Establishes well-posedness for hyperbolic PDEs with complex characteristics on Fourier Lebesgue spaces using Fourier integral operators with complex-valued phase functions.


<details>
  <summary>Details</summary>
Motivation: To develop well-posedness theory for hyperbolic partial differential equations with complex characteristics in Fourier Lebesgue spaces, which requires extending harmonic analysis techniques to handle complex-valued phase functions.

Method: Uses harmonic analysis approach to study boundedness properties of Fourier integral operators with complex-valued phase functions on Fourier Lebesgue spaces, Besov spaces, and Triebel-Lizorkin spaces. These operators serve as propagators for the PDEs.

Result: Proves new boundedness results for Fourier integral operators under the spatial smooth factorization condition for their canonical relations, establishing well-posedness for the considered hyperbolic PDEs.

Conclusion: The harmonic analysis approach successfully establishes well-posedness for hyperbolic PDEs with complex characteristics in Fourier Lebesgue spaces through boundedness properties of Fourier integral operators with complex phase functions.

Abstract: The aim of this paper is to establish well-posedness properties for hyperbolic PDEs on Fourier Lebesgue spaces. We consider hyperbolic operators with complex characteristics. Since our approach comes from harmonic analysis, we establish boundedness properties of Fourier integral operators with complex-valued phase functions on Fourier Lebesgue spaces, Besov spaces and Triebel-Lizorkin spaces. Indeed, these classes of operators serve as propagators of the considered PDE problems. In terms of the boundedness properties, we prove new results in the case where the canonical relation of the operator is assumed to satisfy the {\it spatial smooth factorization condition}

</details>


### [90] [Non-uniformly elliptic variational problems on BV](https://arxiv.org/abs/2601.23195)
*Lisa Beck,Franz Gmeineder,Mathias Schäffner*

Main category: math.AP

TL;DR: The paper establishes W¹,¹-regularity and higher gradient integrability for relaxed minimizers of convex integral functionals on BV spaces, extending results to functionals with linear growth from below but not necessarily from above.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend regularity theory beyond classical examples like minimal surface integrands to more general convex integral functionals that only require linear growth from below, which typically exhibit non-uniformly degenerate elliptic behavior.

Method: The authors study relaxed minimizers of convex integral functionals on BV (bounded variation) spaces, focusing on functionals with linear growth from below but not necessarily from above, addressing the non-uniformly degenerate elliptic behavior that arises.

Result: The main results establish W¹,¹-regularity and higher gradient integrability for these relaxed minimizers, extending available bounds from the superlinear growth case in a sharp way.

Conclusion: The paper successfully extends regularity theory to a broader class of convex integral functionals with linear growth conditions, providing sharp bounds that generalize previous results from the superlinear growth case to include functionals with non-uniformly degenerate elliptic behavior.

Abstract: We establish $\mathrm{W}^{1,1}$-regularity and higher gradient integrability for relaxed minimizers of convex integral functionals on $\mathrm{BV}$. Unlike classical examples such as the minimal surface integrand, we only require linear growth from below but not necessarily from above. This typically comes with a non-uniformly degenerate elliptic behaviour, for which our results extend the presently available bounds from the superlinear growth case in a sharp way.

</details>


### [91] [The metaplectic semigroup and its applications to time-frequency analysis and evolution operators](https://arxiv.org/abs/2601.22252)
*Gianluca Giacchi,Luigi Rodino,Davide Tramontana*

Main category: math.AP

TL;DR: Extends metaplectic theory to positive complex symplectic matrices using operator-theoretic approach, enabling analysis of generators, polar decomposition, and applications to time-frequency representations and parabolic equations.


<details>
  <summary>Details</summary>
Motivation: Existing literature focuses on propagators of quadratic evolution equations via Mehler formulas, but lacks a systematic operator-theoretic approach to the metaplectic semigroup beyond unitary settings.

Method: Operator-theoretic and symplectic approach adapting techniques from standard metaplectic group to broader framework, analyzing generators, polar decomposition, and intertwining relations with complex conjugation and Wigner distribution.

Result: Characterizes classes of time-frequency representations from metaplectic perspective, studies boundedness of propagators on modulation spaces, obtains operator norm estimates, and applies to propagation of Wigner singularities.

Conclusion: Provides deeper structural insight into metaplectic semigroup and enables applications to time-frequency analysis and parabolic equations with complex quadratic Hamiltonians.

Abstract: We develop a systematic analysis of the metaplectic semigroup $\mathrm{Mp}_+(d,\mathbb{C})$ associated with positive complex symplectic matrices, a notion introduced almost simultaneously and independently by Hörmander, Brunet, Kramer, and Howe, thereby extending the classical metaplectic theory beyond the unitary setting.
  While the existing literature has largely focused on propagators of quadratic evolution equations, for which results are typically obtained via Mehler formulas, our approach is operator-theoretic and symplectic in spirit and adapts techniques from the standard metaplectic group $\mathrm{Mp}(d,\mathbb{R})$ to a substantially broader framework that is not driven by differential problems or particular propagators.
  This point of view provides deeper insight into the structure of the metaplectic semigroup, and allows us to investigate its generators, polar decomposition, and intertwining relations with complex conjugation and with the Wigner distribution. We then exploit these structural results to characterize, from a metaplectic perspective, classes of time-frequency representations satisfying prescribed structural properties. Finally, we discuss further implications for parabolic equations with complex quadratic Hamiltonians, we study the boundedness of their propagators on modulation spaces, we obtain estimates in time of their operator norms. Finally, we apply our theory to the study of propagation of Wigner singularities.

</details>


### [92] [A Generalized Analytical Heat Transfer Model for Enhanced Geothermal Systems: Capturing Fracture Interactions and Correcting Classical Optimistic Predictions](https://arxiv.org/abs/2601.22316)
*Nelson Barros-Galvis or Christine Ehlig-Economides or Cristi Darley Guevara*

Main category: math.AP

TL;DR: A new analytical heat transfer model for enhanced geothermal systems that captures fracture interactions, corrects optimistic bias in classical models, and is computationally efficient enough for spreadsheet implementation.


<details>
  <summary>Details</summary>
Motivation: Classical geothermal heat transfer models (like Gringarten et al. 1975) rely on simplified assumptions and systematically overestimate thermal performance, leading to unrealistic engineering decisions. There's a need for models that capture realistic thermal interactions between fractures while maintaining analytical tractability.

Method: Developed a generalized analytical model based on Green's functions that explicitly captures thermal interactions between fractures. The solution preserves analytical tractability, doesn't require Laplace space transformations or numerical inversion algorithms, and is simple enough for spreadsheet implementation.

Result: The model shows close agreement with numerical simulations (CMG STARS and Volsung software) in temperature evolution, including fracture interaction effects. It corrects the optimistic bias of classical approaches and provides more reliable predictions of production temperature and energy recovery.

Conclusion: The proposed model bridges the gap between legacy analytical models and numerical/commercial tools, with direct implications for geothermal feasibility studies, well design, and power forecasting. It generalizes classical solutions while retaining analytical simplicity and practical applicability.

Abstract: Numerical analytical heat transfer models play a critical role in geothermal design and feasibility studies. Classical solutions, such as those proposed by Gringarten et al. 1975, rely on simplified assumptions and systematically overestimate thermal performance, which can lead to unrealistic engineering decisions.
  This study presents a generalized analytical model for enhanced geothermal systems that explicitly captures thermal interactions between fractures while preserving analytical tractability.
  The formulation is based on Greenś functions and reproduces realistic thermal behavior under conditions representative of fractured geothermal reservoirs. The resulting solution is computationally efficient and sufficiently simple to be implemented directly in standard spreadsheets, without requiring Laplace space transformations or numerical inversion algorithms.
  The model is validated against numerical simulations performed using CMG STARS and Volsung software, showing close agreement in temperature evolution, including the effects of interacting fractures. Compared with classical analytical approaches, the proposed model corrects optimistic bias and provides more reliable predictions of production temperature and energy recovery.
  These results have direct implications for geothermal feasibility studies, well design, and power forecasting, effectively bridging the gap between legacy analytical models and numerical or commercial engineering tools. Building on the analytical framework originally introduced by Gringarten et al. 1975, the proposed formulation generalizes classical heat transfer solutions to account for fracture interaction while retaining analytical simplicity and practical applicability.

</details>


### [93] [Local existence and nonexistence of solutions to the Hardy parabolic equation with general nonlinearity](https://arxiv.org/abs/2601.22520)
*Yo Tsusaka*

Main category: math.AP

TL;DR: Establishes optimal integrability conditions for local existence/nonexistence of nonnegative solutions to Hardy parabolic equation with general nonlinearity.


<details>
  <summary>Details</summary>
Motivation: To determine precise conditions under which the Cauchy problem for Hardy parabolic equations with general nonlinearity admits local-in-time nonnegative solutions, addressing the critical integrability requirements for initial data.

Method: Uses supersolution method to prove existence results, establishing optimal integrability conditions on initial functions for local solution existence.

Result: Obtains optimal integrability conditions on initial function for local existence of nonnegative solutions, with both existence and nonexistence results that define the boundary between solvable and unsolvable cases.

Conclusion: The paper successfully characterizes the exact integrability conditions needed for local solvability of Hardy parabolic equations with general nonlinearity, providing a complete picture of when solutions exist and when they don't.

Abstract: In this paper, we consider the Cauchy problem for the Hardy parabolic equation with general nonlinearity and establish the local existence and nonexistence results. Our results provide the optimal integrability conditions on initial function for the existence of a local-in-time nonnegative solution. The proof of the existence result is based on the supersolution method.

</details>


### [94] [Transmission and Reflection coefficients for Schrödinger Operators with Truncated Periodic Potentials that support defect states](https://arxiv.org/abs/2601.22544)
*Joseph C. Stellman,Jeremy L. Marzuola*

Main category: math.AP

TL;DR: Analysis of scattering waves through truncated periodic potentials with perturbations supporting localized gap eigenstates, proving existence of zero reflection states near bound states and comparing to scattering resonances.


<details>
  <summary>Details</summary>
Motivation: To understand how perturbations in truncated periodic potentials affect wave scattering, particularly the existence and properties of transmission resonances near bound states in the gap.

Method: Mathematical analysis in complex neighborhoods around positive bound states, proving existence of zero reflection states, comparing to scattering resonances, and analyzing transmission/reflection coefficients near bound states.

Result: Existence of distinct zero reflection states (transmission resonances) near assumed bound states, with analysis of their location relative to scattering resonances and behavior of transmission/reflection coefficients.

Conclusion: Truncated periodic potentials with perturbations supporting localized gap eigenstates exhibit interesting transmission resonances near bound states, with distinct properties from scattering resonances, demonstrated through mathematical analysis and examples including the truncated harmonic oscillator.

Abstract: We consider scattering waves through truncated periodic potentials with perturbations that support localized gap eigenstates. In a small complex neighborhood around an assumed positive bound state of the model operator, we prove the existence of a distinct zero reflection state, or transmission resonance. We compare its location to a previously found scattering resonance and use the properties of solutions near these interesting points to analyze the behavior of transmission and reflection coefficients of scattering solutions near the assumed bound state. By example, we also discuss the truncated simple harmonic oscillator and compare the analysis to the crystalline case.

</details>


### [95] [Spectral properties and bound states of the Dirac equation on periodic quantum graphs](https://arxiv.org/abs/2601.22603)
*Zhipeng Yang,Ling Zhu*

Main category: math.AP

TL;DR: Nonlinear Dirac equations on periodic quantum graphs: existence and multiplicity of bound states via variational methods.


<details>
  <summary>Details</summary>
Motivation: To study nonlinear Dirac equations on periodic quantum graphs and develop a variational framework for proving existence and multiplicity of bound states (solutions).

Method: Introduce Dirac operator on periodic graph with Z^d-periodic potential, describe spectral decomposition, work in natural energy space. Use variational approach with linking geometry and Cerami-type compactness modulo translations.

Result: Prove existence of at least one bound state under asymptotically linear or superquadratic nonlinearity. When nonlinearity is even, prove existence of infinitely many geometrically distinct bound states.

Conclusion: Developed variational framework for nonlinear Dirac equations on periodic quantum graphs, establishing existence and multiplicity results for bound states using linking geometry and compactness arguments.

Abstract: We investigate nonlinear Dirac equations on a periodic quantum graph $G$ and develop a variational approach to the existence and multiplicity of bound states. After introducing the Dirac operator on $G$ with a $\mathbb Z^{d}$-periodic potential, we describe its spectral decomposition and work in the natural energy space. Under asymptotically linear or superquadratic assumptions on the nonlinearity, we establish the required linking geometry and a Cerami-type compactness property modulo $\mathbb Z^{d}$-translations. As a consequence, we prove the existence of at least one bound state and, when the nonlinearity is even, infinitely many geometrically distinct bound states.

</details>


### [96] [Weighted estimates for Hodge-Maxwell systems](https://arxiv.org/abs/2601.22604)
*Rohit Mahato,Swarnendu Sil*

Main category: math.AP

TL;DR: The paper establishes boundary regularity estimates in weighted L^p spaces with Muckenhoupt weights for Hodge systems, leading to solvability of Hodge-Maxwell systems and Hodge decomposition theorems in weighted Lebesgue spaces.


<details>
  <summary>Details</summary>
Motivation: To develop regularity theory for Hodge systems with boundary conditions in weighted function spaces, which is important for applications in partial differential equations and geometric analysis where weighted spaces naturally arise.

Method: Uses decay estimates in the spirit of the 'Campanato method' to establish weighted L^p estimates, avoiding potential theory and representation formulas. The approach works with Muckenhoupt weights A_p.

Result: Proves up-to-the-boundary regularity estimates in weighted L^p spaces for weak solutions to Hodge systems with prescribed boundary conditions. Establishes solvability of Hodge-Maxwell systems and derives Hodge decomposition theorems in weighted Lebesgue spaces.

Conclusion: The paper provides a new approach to regularity theory for Hodge systems in weighted spaces, offering tools for analyzing PDEs and geometric problems in weighted function spaces without relying on traditional potential theory methods.

Abstract: We establish up to the boundary regularity estimates in weighted $L^{p}$ spaces with Muckenhoupt weights $A_{p}$ for weak solutions to the Hodge systems
  \begin{align*}
  d^{\ast}\left(Adω\right) + B^{\intercal}dd^{\ast}\left(Bω\right) = λBω+ f \quad \text{ in } Ω
  \end{align*}
  with either $ν\wedge ω$ and $ν\wedge d^{\ast}\left(Bω\right)$ or $ν\lrcorner Bω$ and $ν\lrcorner Adω$ prescribed on $\partialΩ.$ As a consequence, we prove the solvability of Hodge-Maxwell systems and derive Hodge decomposition theorems in weighted Lebesgue spaces. Our proof avoids potential theory, does not rely on representation formulas and instead uses decay estimates in the spirit of `Campanato method' to establish weighted $L^{p}$ estimates.

</details>


### [97] [Simplicity of eigenvalues for elliptic problems with mixed Steklov-Robin boundary condition](https://arxiv.org/abs/2601.22829)
*Marco Ghimenti,Anna Maria Micheletti,Angela Pistoia*

Main category: math.AP

TL;DR: The paper proves that for generic domains, all eigenvalues of elliptic problems with mixed Steklov-Robin boundary conditions are simple.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral properties of elliptic problems with mixed Steklov-Robin boundary conditions and establish generic simplicity of eigenvalues.

Method: Domain perturbation techniques and analysis of transversality of associated operators.

Result: For generic domains, all eigenvalues of the elliptic problems with mixed Steklov-Robin boundary conditions are simple.

Conclusion: The spectral simplicity holds generically for these classes of elliptic boundary value problems, establishing a fundamental property of their eigenvalue structure.

Abstract: This paper investigates the spectral properties of two classes of elliptic problems characterized by mixed Steklov-Robin boundary conditions. Our main objective is to prove that, for a generic domain, all the eigenvalues are simple. This result is established by employing domain perturbation techniques and analyzing the transversality of the associated operators.

</details>


### [98] [Unconditional well-posedness of the master equation for monotone mean field games of controls](https://arxiv.org/abs/2601.22845)
*Joe Jackson,Alpár R. Mészáros*

Main category: math.AP

TL;DR: First unconditional well-posedness for master equations in mean field games of controls, covering displacement/Lasry-Lions monotone data or small time horizons, without requiring a priori regularity on fixed-point mappings.


<details>
  <summary>Details</summary>
Motivation: Previous results for master equations in mean field games of controls required additional structural assumptions on fixed-point mappings arising from control interactions. The authors aim to establish unconditional well-posedness where all assumptions are imposed only at the Lagrangian and terminal cost level.

Method: Bottom-up approach using N-player Nash systems: 1) Show solutions of N-player systems are compact via uniform-in-N decay estimates for derivatives of value functions, 2) Prove subsequential limit points must be solutions to the master equation, 3) Build classical solution through this limiting process.

Result: Established first unconditional well-posedness for master equations in mean field games of controls, covering displacement monotone or Lasry-Lions monotone data, as well as small time horizons. Results allow for common noise with constant intensity.

Conclusion: The bottom-up approach via N-player approximations provides a robust framework for establishing well-posedness of master equations without requiring a priori regularity on fixed-point mappings, demonstrating that such regularity emerges naturally from the data's monotonicity and regularity.

Abstract: We establish the first unconditional well-posedness result for the master equation associated with a general class of mean field games of controls. Our analysis covers games with displacement monotone or Lasry--Lions monotone data, as well as those with a small time horizon. By unconditional, we mean that all assumptions are imposed solely at the level of the Lagrangian and the terminal cost. In particular, we do not require any a priori regularity or structural assumptions on the additional fixed-point mappings arising from the control interactions; instead we show that these fixed-point mappings are well-behaved as a consequence of the regularity and the monotonicity of the data. Our approach is bottom-up in nature, unlike most previous results which rely on a generalized method of characteristics. In particular, we build a classical solution of the master equation by showing that the solutions of the corresponding $N$-player Nash systems are compact, in an appropriate sense, and that their subsequential limit points must be solutions to the master equation. Compactness is obtained via uniform-in-$N$ decay estimates for derivatives of the $N$-player value functions. The underlying games are driven by non-degenerate idiosyncratic Brownian noise, and our results allow for the presence of common noise with constant intensity.

</details>


### [99] [Existence of a solution of the TV Wasserstein gradient flow](https://arxiv.org/abs/2601.22847)
*Kexin Lin,Filippo Santambrogio*

Main category: math.AP

TL;DR: Existence of TV-Wasserstein gradient flow solutions on flat tori for initial densities bounded away from zero, with preservation of bounds and BV norm decay rates.


<details>
  <summary>Details</summary>
Motivation: Generalize previous work by Carlier and Poon that only fully proved results in 1D and didn't handle non-BV initial densities. Extend existence theory for TV-Wasserstein gradient flows to higher dimensions with minimal regularity assumptions.

Method: Use approximated TV-JKO scheme that artificially imposes a lower density bound, enabling construction of continuous-in-time solutions. This scheme allows propagation of initial density bounds and analysis of BV norm decay.

Result: Prove existence of solutions on flat tori in any dimension for initial densities bounded away from zero. Solutions preserve upper/lower bounds and show BV norm decay: t^{-1/3} as t→0 (if ρ₀∉BV) and t^{-1} as t→∞.

Conclusion: Successfully extend TV-Wasserstein gradient flow theory to higher dimensions with weak regularity assumptions, improving upon previous 1D results and handling non-BV initial densities through an approximated JKO scheme.

Abstract: On the flat torus in any dimension we prove existence of a solution to the TV Wasserstein gradient flow equation, only assuming that the initial density $ρ_0$ is bounded from below and above by strictly positive constants. This solution preserves upper and lower bounds of the densities, and shows a certain decay of the BV norm (of the order of $t^{-1/3}$ for $t\to 0$ -- if $ρ_0\notin BV$, otherwise the BV norm is of course bounded -- and of the order of $t^{-1}$ as $t\to\infty$). This generalizes a previous result by Carlier and Poon, who only gave a full proof in one dimension of space and did not consider the case $ρ_0\notin BV$.
  The main tool consists in considering an approximated TV-JKO scheme which artificially imposes a lower bound on the density and allows to find a continuous-in-time solution regular enough to prove that the lower bounds of the initial datum propagates in time, and study on this approximated equation the decay of the BV norm.

</details>


### [100] [Global Well-posedness of Strong Solutions to the Cauchy Problem of 2D Nonhomogeneous Navier-Stokes Equations with Density-Dependent Viscosity and Vacuum](https://arxiv.org/abs/2601.22877)
*Bing Yuan,Rong Zhang,Peng Zhou*

Main category: math.AP

TL;DR: Global existence of strong solutions for 2D nonhomogeneous Navier-Stokes equations with density-dependent viscosity, allowing arbitrarily large initial data and vacuum, plus large-time asymptotic behavior analysis.


<details>
  <summary>Details</summary>
Motivation: To establish global existence of strong solutions for modified 2D nonhomogeneous incompressible Navier-Stokes equations with density-dependent viscosity, overcoming challenges of large initial data and vacuum conditions.

Method: Leverage the structure of the system to obtain key estimates of ∇ρ in mixed Lebesgue spaces (∥∇ρ∥_{L_t^∞L_x^q}, q>2) without smallness assumptions on initial data.

Result: Prove global existence of strong solutions for both vacuum and nonvacuum far-field density cases, with arbitrarily large initial data and vanishing initial density allowed.

Conclusion: Successfully establish global well-posedness for the system and additionally prove large-time asymptotic behavior of velocity and pressure gradients.

Abstract: This paper is concerned with the Cauchy problem for the modified two-dimensional (2D) nonhomogeneous incompressible Navier-Stokes equations with density-dependent viscosity. By fully using the structure of the system, we can obtain the key estimates of $\|\nabla ρ\|_{L_t^\infty L_x^q},q>2$ without any smallness asuumption on the initial data, and thus establish the global existence of the strong solutions with the far-field density being either vacuum or nonvacuum. Notably, the initial data can be arbitrarily large and the initial density is allowed to vanish. Furthermore, the large-time asymptotic behavior of the gradients of the velocity and the pressure is also established.

</details>


### [101] [Local Well-posedness and Blow-up for the Restricted Fourth-Order Prandtl Equation](https://arxiv.org/abs/2601.22940)
*Ik Hyun Choi*

Main category: math.AP

TL;DR: Local well-posedness and finite-time blow-up proven for a restricted fourth-order Prandtl equation on half-line with clamped boundary conditions.


<details>
  <summary>Details</summary>
Motivation: The equation arises from a two-dimensional fourth-order Prandtl system via ansatz reduction, featuring nonlocal integral nonlinearity. Understanding well-posedness and blow-up behavior is crucial for analyzing this higher-order boundary layer model.

Method: Duhamel fixed-point argument requiring uniform L¹ bounds for half-line biharmonic heat kernel. Established kernel estimates and derivatives, showed semigroup preserves spatial regularity using integration by parts representation.

Result: Proved local existence and uniqueness for restricted model, and constructed solutions that exhibit finite-time blow-up.

Conclusion: The paper successfully establishes both local well-posedness and finite-time blow-up behavior for the restricted fourth-order Prandtl equation, providing complete analysis of solution behavior for this higher-order boundary layer model.

Abstract: We prove local well-posedness and finite-time blow-up for a restricted fourth-order Prandtl equation posed on the half-line with clamped boundary conditions. The equation arises from a two-dimensional fourth-order Prandtl system via an ansatz reduction, and its nonlinearity involves a nonlocal integral term. To close a Duhamel fixed-point argument, we need uniform $L^1$ bounds for the associated half-line biharmonic heat kernel. We establish uniform $L^1$ estimates for the kernel and its derivatives, and we show that the semigroup preserves spatial regularity under appropriate compatibility conditions, using an alternative representation derived by integration by parts. These kernel estimates yield local existence and uniqueness for the restricted model and allow us to construct solutions that blow-up in finite time.

</details>


### [102] [Instability of two-dimensional Taylor-Green Vortices](https://arxiv.org/abs/2601.23040)
*Gonzalo Cao-Labora,Maria Colombo,Michele Dolce,Paolo Ventura*

Main category: math.AP

TL;DR: The paper develops a general criterion for characterizing unstable eigenvalues of linear Hamiltonian operators and applies it to prove spectral instability of the Taylor-Green vortex in 2D ideal fluids.


<details>
  <summary>Details</summary>
Motivation: To analyze the spectral stability of the Taylor-Green vortex, a classical steady state in 2D ideal fluid dynamics, by developing a mathematical framework for characterizing unstable eigenvalues in linear Hamiltonian systems.

Method: Develops a general criterion that characterizes unstable eigenvalues as zeros of a holomorphic function (determinant of finite-dimensional matrix). Applies this to Taylor-Green vortex by analyzing different invariant subspaces: odd perturbations, even functions, and rescaled vortices. Combines analytical criterion with rigorous computer-assisted arguments.

Result: Proves linear stability of odd perturbations (unstable spectrum only possible on real axis, excluded by criterion). Shows real instabilities exist for rescaled vortices. For even functions, reduces problem to finding single complex root, successfully located via computer-assisted argument. Fully characterizes unstable spectrum of Taylor-Green vortex.

Conclusion: The developed criterion successfully characterizes spectral instability of the Taylor-Green vortex, demonstrating both stability in certain subspaces (odd perturbations) and instability in others (even functions, rescaled vortices), providing complete spectral analysis of this classical fluid dynamics problem.

Abstract: For a wide class of linear Hamiltonian operators we develop a general criterion that characterizes the unstable eigenvalues as the zeros of a holomorphic function given by the determinant of a finite-dimensional matrix. We apply the latter result to prove the spectral instability of the Taylor-Green vortex in two-dimensional ideal fluids. The linearized Euler operator at this steady state possesses different invariant subspaces, within which we apply our criterion to rule out or detect instabilities. We show linear stability of odd perturbations, for which the unstable spectrum can appear only on the real axis. We exclude this possibility by applying our stability criterion. Real instabilities, instead, exist and can be detected with the same criterion if we consider suitable rescalings of the Taylor-Green vortex. In the subspace of functions even in both variables, the problem is reduced to finding a single complex root of our stability function. We successfully locate this value by combining our general criterion with a rigorous computer-assisted argument. As a consequence, we fully characterize the unstable spectrum of the Taylor-Green vortex.

</details>


### [103] [Existence of Traveling Waves in Infinite Range FPUT Lattices](https://arxiv.org/abs/2601.23091)
*Michael Herrmann,Karsten Matthies,Jan-Patrick Meyer*

Main category: math.AP

TL;DR: Existence of solitary waves in lattices with long-range repulsive interactions proven via variational methods, yielding one-parameter family of unimodal solutions with asymptotic analysis of large, fast, high-energy waves.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of solitary wave solutions in lattice systems where particles interact through long-range repulsive forces, which is a fundamental question in nonlinear lattice dynamics and wave propagation.

Method: Variational existence proof using constrained optimization techniques to find solitary wave solutions as critical points of an energy functional under appropriate constraints.

Result: Proved existence of solitary waves, obtained a one-parameter family of unimodal solutions, and characterized the asymptotic behavior of large, fast, high-energy waves.

Conclusion: Solitary waves exist in lattices with long-range repulsive interactions, forming a continuum family with predictable asymptotic properties for extreme parameter regimes.

Abstract: We prove the existence of solitary waves in a lattice where all particles interact with each other by pair-wise repulsive forces that decay with distance. The variational existence proof is based on constrained optimization and provides a one-parameter family of unimodal solutions. We also describe the asymptotic behavior of large, fast, high-energy waves.

</details>


### [104] [Nonlinear Schrödinger Equation with magnetic potential on metric graphs](https://arxiv.org/abs/2601.23115)
*Riccardo Adami,Nicolò Cangiotti,Ivan Gallo,David Spitzkopf*

Main category: math.AP

TL;DR: The paper proves that magnetic Schrödinger equations on graphs can be reduced to non-magnetic problems with effective repulsive potentials, allowing extension of existence criteria to magnetic settings, with applications showing mass-dependent phase transitions on tadpole graphs.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence of ground states for Nonlinear Magnetic Schrödinger Equations on noncompact metric graphs, particularly understanding how magnetic fields affect ground state formation in these geometric settings.

Method: Proves variational equivalence between magnetic Hamiltonian and non-magnetic operator with additional repulsive potentials determined by Aharonov-Bohm flux through topological loops, then extends classical existence criteria to magnetic setting.

Result: Shows ground states exist for sufficiently small repulsion in intermediate mass regimes, while sufficiently strong flux prevents ground state formation. Characterizes ground state structure on tadpole graph revealing mass-dependent phase transition.

Conclusion: Magnetic effects on graphs can be effectively modeled as repulsive potentials, enabling application of classical existence theory to magnetic problems, with important implications for ground state formation in quantum graphs under magnetic fields.

Abstract: In this manuscript, we shall investigate the Nonlinear Magnetic Schrödinger Equation on noncompact metric graphs, focusing on the existence of ground states. We prove that the magnetic Hamiltonian is variationally equivalent to a non-magnetic operator with additional repulsive potentials supported on the graph's cycles. This effective potential is strictly determined by the Aharonov-Bohm flux through the topological loops. Leveraging this reduction, we extend classical existence criteria to the magnetic setting. As a key application, we characterize the ground state structure on the tadpole graph, revealing a mass-dependent phase transition. The ground states exist for sufficiently small repulsion in an intermediate regime of masses while sufficiently strong flux prevents the formation of ground states.

</details>


### [105] [Hyperbolic partial differential equations with complex characteristics on Fourier Lebesgue spaces](https://arxiv.org/abs/2601.23138)
*Duván Cardona,William Obeng-Denteh,Frederick Opoku*

Main category: math.AP

TL;DR: Establishes well-posedness for hyperbolic PDEs with complex characteristics on Fourier Lebesgue spaces using harmonic analysis approach and Fourier integral operators.


<details>
  <summary>Details</summary>
Motivation: To develop well-posedness theory for hyperbolic partial differential equations with complex characteristics on Fourier Lebesgue spaces, extending analysis beyond classical Sobolev spaces.

Method: Uses harmonic analysis approach to study boundedness properties of Fourier integral operators with complex-valued phase functions on Fourier Lebesgue spaces, Besov spaces, and Triebel-Lizorkin spaces. These operators serve as propagators for the PDE problems.

Result: Proves new boundedness results for Fourier integral operators when the canonical relation satisfies the spatial smooth factorization condition, establishing well-posedness properties for the hyperbolic PDEs.

Conclusion: The harmonic analysis approach successfully establishes well-posedness for hyperbolic PDEs with complex characteristics on Fourier Lebesgue spaces through boundedness properties of Fourier integral operators under spatial smooth factorization conditions.

Abstract: The aim of this paper is to establish well-posedness properties for hyperbolic PDEs on Fourier Lebesgue spaces. We consider hyperbolic operators with complex characteristics. Since our approach comes from harmonic analysis, we establish boundedness properties of Fourier integral operators with complex-valued phase functions on Fourier Lebesgue spaces, Besov spaces and Triebel-Lizorkin spaces. Indeed, these classes of operators serve as propagators of the considered PDE problems. In terms of the boundedness properties, we prove new results in the case where the canonical relation of the operator is assumed to satisfy the {\it spatial smooth factorization condition}

</details>


### [106] [Non-uniformly elliptic variational problems on BV](https://arxiv.org/abs/2601.23195)
*Lisa Beck,Franz Gmeineder,Mathias Schäffner*

Main category: math.AP

TL;DR: The paper establishes W¹,¹-regularity and higher gradient integrability for relaxed minimizers of convex integral functionals on BV spaces, extending results to functionals with linear growth from below but not necessarily from above.


<details>
  <summary>Details</summary>
Motivation: To extend regularity results for convex integral functionals beyond the classical uniformly elliptic case, particularly for functionals with linear growth from below but potentially non-uniformly degenerate elliptic behavior.

Method: Analyzes relaxed minimizers of convex integral functionals on BV (bounded variation) spaces, focusing on functionals with linear growth from below but not necessarily from above, dealing with non-uniformly degenerate elliptic behavior.

Result: Establishes W¹,¹-regularity and higher gradient integrability for such minimizers, extending available bounds from the superlinear growth case in a sharp way.

Conclusion: The results provide regularity theory for a broader class of convex integral functionals that only require linear growth from below, going beyond classical uniformly elliptic examples like minimal surface integrands.

Abstract: We establish $\mathrm{W}^{1,1}$-regularity and higher gradient integrability for relaxed minimizers of convex integral functionals on $\mathrm{BV}$. Unlike classical examples such as the minimal surface integrand, we only require linear growth from below but not necessarily from above. This typically comes with a non-uniformly degenerate elliptic behaviour, for which our results extend the presently available bounds from the superlinear growth case in a sharp way.

</details>


### [107] [Three self-similar solutions of Yang-Mills equations in high odd dimensions](https://arxiv.org/abs/2602.00345)
*Piotr Bizoń,Irfan Glogić,Arthur Wasserman*

Main category: math.AP

TL;DR: For odd dimensions d≥11, there exist exactly N smooth self-similar solutions to spherically symmetric Yang-Mills equations with gauge group SO(d), where N equals the number of zeros of an explicit polynomial P_m(z) in (0,1).


<details>
  <summary>Details</summary>
Motivation: To establish existence and uniqueness of self-similar solutions to Yang-Mills equations in higher-dimensional Minkowski spacetime, specifically for spherically symmetric configurations with SO(d) gauge group.

Method: Analysis of spherically symmetric Yang-Mills equations with gauge group SO(d) in d+1 dimensional Minkowski spacetime. The study focuses on self-similar solutions and relates their count to zeros of an explicit polynomial P_m(z) of degree m=(d-5)/2 in the interval (0,1).

Result: For odd d≥11, there exist exactly N smooth self-similar solutions (modulo reflection symmetry), where N equals the number of zeros of P_m(z) in (0,1). Computations show N=3 for all integer m from 3 to 15, suggesting this pattern holds for all odd d≥11, though this remains unproven.

Conclusion: The paper establishes a precise count of self-similar solutions to Yang-Mills equations in higher dimensions, with computational evidence suggesting exactly three solutions for all odd dimensions d≥11, though a general proof remains an open problem.

Abstract: We consider spherically symmetric Yang-Mills equations with gauge group $SO(d)$ in $d+1$ dimensional Minkowski spacetime. For any given odd $d\geq 11$, we establish existence and uniqueness (modulo reflection symmetry) of exactly $N$ smooth self-similar solutions, where $N$ is the number of zeros of an explicit polynomial $P_m(z)$ of degree $m=(d-5)/2$ in the interval $0<z<1$. The number $N$ can be determined algorithmically by an explicit computation. We find that $N=3$ for all integer $m$ from $3$ to $15$, the upper bound being merely limited by the extent of our computations. A proof that $N=3$ for all odd $d\ge 11$ remains an open problem.

</details>


### [108] [Homogenization of an optimal control problem for nonlocal semilinear elasticity with soft inclusions](https://arxiv.org/abs/2602.00373)
*Amartya Chakrabortty,Abu Sufian*

Main category: math.AP

TL;DR: Asymptotic analysis of optimal control for high-contrast elastic media with soft periodic inclusions, studying convergence as periodicity and contrast parameters approach zero with specific scaling.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of optimal control problems in composite materials with periodic microstructures and high contrast between phases, particularly when both the period size and material contrast become small.

Method: Uses homogenization theory to derive limit state system, formulates limit optimal control problem, and employs Γ-convergence to prove convergence of microscopic optimal controls to macroscopic ones.

Result: Derives homogenized state system for the scaling regime, formulates limit optimal control problem, and proves that microscopic optimal controls converge to optimal controls for the limit problem.

Conclusion: The asymptotic analysis provides rigorous justification for using homogenized models in optimal control of high-contrast elastic media with periodic microstructure, establishing convergence properties under specific scaling conditions.

Abstract: This paper investigates the asymptotic analysis of an optimal control problem (OCP) posed on a high-contrast elastic medium with soft periodic inclusions, governed by a semilinear elasticity system with a nonlocal term. The domain consists of a connected matrix phase and a soft inclusion phase. The model depends on two independent small parameters: the periodicity $\varepsilon>0$ and the contrast $δ>0$, and the distributed control acts only in the inclusion region. We consider an $L^2$-tracking cost on the displacement and analyze the limit as $(\varepsilon,δ)\to(0,0)$ in the regime
  \[
  \lim_{(\varepsilon,δ)\to(0,0)}\fracδ{\varepsilon}=κ\in(0,+\infty].
  \]
  First, we derive the homogenized (limit) state system associated with this scaling. We then formulate the limit OCP and prove that the limit of the microscopic optimal controls is an optimal control for the limit problem, using a $Γ$-convergence approach.

</details>


### [109] [Time Asymptotics and Scaling Limits for a Nonlocal Fokker-Planck Equation with Heavy-Tailed Kernel](https://arxiv.org/abs/2602.00375)
*Niccolò Tassi*

Main category: math.AP

TL;DR: Nonlocal Fokker-Planck equations with heavy-tailed kernels achieve exponential convergence to equilibrium with rate independent of scaling parameter ε and fractional index s, enabling uniform-in-time convergence to limiting equations.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of solutions to nonlocal Fokker-Planck equations with nonsingular, heavy-tailed convolution kernels, and to establish convergence properties as parameters approach their limits (ε→0 and s→1).

Method: Employ a suitable version of the generalized central limit theorem for heavy-tailed distributions and use Harris's theorem to analyze convergence properties.

Result: Prove exponential convergence to equilibrium with a rate that is independent of both scaling parameter ε and fractional index s, enabling uniform-in-time convergence for both ε→0 and s→1.

Conclusion: The analysis establishes robust convergence properties for nonlocal Fokker-Planck equations, recovering limiting equations through uniform-in-time convergence as parameters approach their boundary values.

Abstract: We investigate the asymptotic behaviour of solutions of a class of nonlocal Fokker--Planck equations defined by nonsingular, heavy-tailed convolution kernels and characterised by a scaling parameter $\e\in(0,1]$ and a fractional index $s\in(1/2,1)$. By employing a suitable version of the generalised central limit for heavy-tailed distributions and the use of Harris's theorem, we prove exponential convergence to the equilibrium with a rate that is independent of both $\e$ and $s$. This allows us to show uniform--in--time convergence for both $\e\to 0$ and $s\to1$ recovering the limiting equations.

</details>


### [110] [Global regularity of the multi-dimensional compressible Navier-Stokes-Korteweg system](https://arxiv.org/abs/2602.00455)
*Xiangdi Huang,Weili Meng,Xueyao Zhang*

Main category: math.AP

TL;DR: Global existence of strong solutions for compressible Navier-Stokes-Korteweg system with arbitrarily large initial data on periodic torus.


<details>
  <summary>Details</summary>
Motivation: The compressible Navier-Stokes-Korteweg system describes fluids with capillarity effects, but establishing global existence of strong solutions for large initial data has been a challenging problem.

Method: Established a novel critical control relation between effective velocity and density lower bound: upper bound of effective velocity is controlled by square root of logarithm of reciprocal of density's lower bound.

Result: Proved global existence of strong solutions for arbitrarily large initial data in both 2D and 3D on periodic torus.

Conclusion: The critical control relation between effective velocity and density lower bound is key to overcoming the challenge of large initial data and establishing global existence of strong solutions.

Abstract: In this paper, we investigate the 2D and 3D compressible Navier-Stokes-Korteweg system derived by Dunn and Serrin [Arch. Ration. Mech. Anal. 88(2):95-133, 1985], which is widely used to describe compressible fluids with capillarity effects. Specifically, we prove that strong solutions exist globally for arbitrarily large initial data on the periodic torus. One of the key ingredients in the proof lies in establishing a novel critical control relation between the effective velocity and the lower bound of the density, i.e., the upper bound of the effective velocity is controlled by the square root of the logarithm of the reciprocal of the density's lower bound, which plays a crucial role in deriving the lower bound of the density.

</details>


### [111] [New characterizations of BLO spaces by heat semigroups and applications](https://arxiv.org/abs/2602.00479)
*Shaohong Liang,Dongyong Yang,Chao Zhang*

Main category: math.AP

TL;DR: The paper provides two new characterizations of Bounded Lower Oscillation (BLO) space using Gaussian heat semigroup, applies them to prove regularity of heat equation solutions with BLO boundary values, and reproves BMO-BLO boundedness of Littlewood-Paley g-function via semigroup method.


<details>
  <summary>Details</summary>
Motivation: To develop new characterizations of BLO space using Gaussian heat semigroup tools, which can then be applied to analyze regularity properties of heat equations with BLO boundary conditions and establish boundedness results for Littlewood-Paley operators.

Method: Uses Gaussian heat semigroup approach to characterize BLO space, applies these characterizations to study heat equation solutions with BLO boundary values, and employs semigroup method to reprove boundedness of Littlewood-Paley g-function between BMO and BLO spaces.

Result: Two new characterizations of BLO space via Gaussian heat semigroup, proof of regularity properties for heat equation solutions with BLO boundary conditions, and alternative proof of BMO-BLO boundedness for Littlewood-Paley g-function using semigroup techniques.

Conclusion: The Gaussian heat semigroup provides effective tools for characterizing BLO spaces and analyzing related PDE problems, offering new insights into regularity properties and operator boundedness in function space theory.

Abstract: In this paper, we give two new characterizations of the bounded lower oscillation(BLO) space by using the Gaussian heat semigroup. By the new characterizations, we prove the regularity property of the solutions to the heat equation with BLO boundary value. Also, we reprove the BMO-BLO boundedness of the Littlewood-Paley $g$-function by using the semigroup method.

</details>


### [112] [Gaffney's Inequality and the Closed Range Property of the de Rham Complex in Unbounded Domains](https://arxiv.org/abs/2602.00581)
*Dirk Pauly,Marcus Waurick*

Main category: math.AP

TL;DR: The paper establishes closed range results for differential operators in the de Rham complex based on domain boundedness directions, with applications to Maxwell equations stability.


<details>
  <summary>Details</summary>
Motivation: To extend classical Poincaré estimates and characterize closed range properties of differential operators (like rot) in terms of domain geometry, particularly directions of boundedness, for applications in Maxwell equations stability analysis.

Method: Uses Gaffney's inequality extended to unbounded domains, stability of closed range results under bi-Lipschitz transformations, and characterizations based on domain boundedness directions. Provides accessible proofs and includes mixed boundary conditions analysis.

Result: Establishes that rot-operator has closed range if and only if domain is bounded in two directions. Characterizes closed range for all de Rham complex operators via domain boundedness. Proves existence of spectral gap for Maxwell operator enabling exponential stability with damping.

Conclusion: Closed range properties of differential operators are fundamentally tied to domain geometry (directions of boundedness). These results enable rigorous stability analysis for Maxwell equations and provide tools for PDE analysis on unbounded domains.

Abstract: The classical Poincaré estimate establishes closedness of the range of the gradient in unweighted $L^2(Ω)$-spaces as long as $Ω\subseteq\mathbb{R}^3$ is contained in a slab, that is, $Ω$ is bounded in one direction. Here, as a main observation, we provide closed range results for the $\operatorname{rot}$-operator, if (and only if) $Ω$ is bounded in two directions. Along the way, we characterise closed range results for all the differential operators of the primal and dual de Rham complex in terms of directions of boundedness of the underlying domain.
  As a main application, one obtains the existence of a spectral gap near the $0$ of the Maxwell operator allowing for exponential stability results for solutions of Maxwell's equations with sufficient damping in the conductivity.
  Our results are based on the validity of Gaffney's (in)equality and the transition of the same to unbounded (simple) domains as well as on the stability of closed range results under bi-Lipschitz regular transformations. The latter technique is well-known and detailed in the appendix; for the results concerning Gaffney's estimate, we shall provide accessible, simple proofs using mere standard results.
  Moreover, we shall present non-trivial examples and a closed range result for $\operatorname{rot}$ with mixed boundary conditions on a set bounded in one direction only.

</details>


### [113] [Liouville Type Theorem for the Fractional MHD and Hall-MHD equations in $\mathbb{R}^{3}](https://arxiv.org/abs/2602.00584)
*Weihua Wang,Zhenyuan Liu*

Main category: math.AP

TL;DR: The paper studies Liouville-type problems for stationary fractional MHD and Hall-MHD equations, with Navier-Stokes results as a byproduct.


<details>
  <summary>Details</summary>
Motivation: To investigate Liouville-type theorems for stationary fractional magnetohydrodynamics equations, addressing the challenges posed by non-local operators in these fluid dynamics models.

Method: Uses Caffarelli-Silvestre extension to handle the non-local operator (-Δ)^s, combined with Yuan and Xiao's method from their 2020 paper.

Result: Presents results on Liouville-type problems for stationary fractional MHD and Hall-MHD equations, with additional results for Navier-Stokes equations as a byproduct.

Conclusion: The combination of Caffarelli-Silvestre extension with Yuan and Xiao's method successfully addresses the challenges of non-local operators in establishing Liouville-type theorems for fractional MHD systems.

Abstract: In this paper, we are mainly concerned with the Liouville type problem for the stationary fractional magnetohydrodynamics(MHD) and stationary fractional Hall-MHD equations. In addition, we present the results of the Navier-Stokes equation as a byproduct. The key point is to use the Caffarelli-Sivestre extension to overcome the difficulty caused by the non-local operator $(-\triangle)^{s}$ and combined with Yuan and Xiao's method (J. Math. Anal. Appl. 491 (2020) 124343).

</details>


### [114] [Decay of solutions of nonlinear Dirac equations: the 2D case](https://arxiv.org/abs/2602.00599)
*Sebastian Herr,Christopher Maulén,Claudio Muñoz*

Main category: math.AP

TL;DR: The paper studies long-time behavior of small solutions to 2D Dirac-type equations, proving decay results for various nonlinearity powers and ruling out localized structures like breathers.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of small solutions to 2D Dirac-type equations, which are important for physical models like the Dirac equation with honeycomb potentials studied by Fefferman and Weinstein.

Method: Introduces new virial identities with specific algebraic structure applied directly to the Dirac model, without using the nonlinear Klein-Gordon equation. Analyzes radial solutions with vorticity S ≠ -1,0.

Result: For radial solutions: with p≥5 (massless) and p≥7 (massive), small globally bounded solutions decay to zero locally in L²_loc. With weighted H¹ boundedness, decay extends to p≥3 (massless) and p≥5 (massive). Rules out existence of small localized structures like breathers or solitary waves.

Conclusion: The paper establishes decay properties for small solutions of 2D Dirac-type equations and shows no small localized structures exist in these regimes, with applications to physical models including honeycomb potential systems.

Abstract: We study the long-time behavior of small solutions for a broad class of 2D Dirac-type equations with suitable nonlinearities. First, we prove that for nonlinearities with power $p\geq 5$ (massless case) and $p\geq7$ (massive case), any small globally bounded radial solution with vorticity $S\ne -1,0$ decays to zero locally in $L^2_{loc}$, as time tends to infinity. For solutions uniformly bounded in time in a weighted $H^1$ space, this decay result extends to lower powers $p\geq 3$ (massless) and $p\geq5$ (massive). Our main results apply to several physical models of current interest, such as the 2D Dirac equation with a honeycomb potential described by Fefferman and Weinstein. Finally, we rule out the existence of small, localized structures such as standing breathers or solitary waves in the 2D regimes considered. To prove these results, we introduce new virial identities with a particular algebra that are applied directly to the Dirac model, and without resorting to the nonlinear Klein-Gordon equation.

</details>


### [115] [On the blow-up of the vectorial Bernoulli free boundary problem](https://arxiv.org/abs/2602.00741)
*Giovanni Siclari,Bozhidar Velichkov*

Main category: math.AP

TL;DR: Completes classification of blow-up limits for vectorial Bernoulli free boundary problem minimizers and studies asymptotic behavior as measure constraint approaches domain size.


<details>
  <summary>Details</summary>
Motivation: To fully characterize the blow-up limits of minimizers in vectorial Bernoulli free boundary problems, which is essential for understanding singular behavior and solution structure near free boundaries.

Method: Studies the vectorial Bernoulli free boundary problem in bounded domains with measure constraints, analyzes asymptotic behavior as constraint approaches domain volume, and uses linear boundary data to characterize singular homogeneous global solutions.

Result: Completes the classification of blow-up limits for minimizers, providing complete characterization of singular homogeneous global solutions in vectorial Bernoulli problems.

Conclusion: The paper achieves complete classification of blow-up limits for vectorial Bernoulli free boundary problems, establishing fundamental understanding of singular solution behavior through asymptotic analysis with measure constraints.

Abstract: In this paper, we complete the classification of the blow-up limits of minimizers of the vectorial Bernoulli free boundary problem. Furthermore, we study the vectorial Bernoulli free boundary problem in a bounded box $D$, with a constraint $m$ on the measure of the positivity set, and the asymptotic of minimizers as the measure constraint $m$ tends to $|D|$. Such a study with a linear datum on the fixed boundary is the main ingredient for the characterization of the singular homogeneous global solutions of the vectorial problem and, thus, for the classification of the blow-up limits.

</details>


### [116] [Renormalization of contact vector fields with horizontal Sobolev regularity in Heisenberg groups](https://arxiv.org/abs/2602.00804)
*Luigi Ambrosio,Gianluca Somma,Simone Verzellesi,Davide Vittone*

Main category: math.AP

TL;DR: First proof of well-posedness for transport/continuity equations in Heisenberg groups for contact vector fields, extending classical Euclidean theory to sub-Riemannian geometry.


<details>
  <summary>Details</summary>
Motivation: Extend classical Euclidean transport equation theory to genuine sub-Riemannian settings, specifically Heisenberg groups, where existing Euclidean results don't apply due to different geometric structure.

Method: Adapt the mollification strategy from classical Euclidean theory [18] to the geometry of Heisenberg groups, handling the unique sub-Riemannian structure and commutator relations.

Result: Obtained well-posedness of transport and continuity equations in Heisenberg groups for a class of contact vector fields under natural regularity assumptions not covered by Euclidean theory.

Conclusion: First successful extension of transport equation theory to genuine sub-Riemannian setting, with comparison showing why results differ from Euclidean BV case and alternative interpolation approaches.

Abstract: In this paper we obtain the well-posedness of the transport and continuity equations in the Heisenberg groups $\mathbb{H}^n$ for a class of contact vector fields $\mathbf b$, under natural assumptions on the regularity of $\mathbf b$ not covered by the, now classical, Euclidean theory [18]. It is the first example of well-posedness in a genuine sub-Riemannian setting, that we obtain adapting to the $\mathbb{H}^n$ geometry the mollification strategy of [18]. In the final part of the paper we illustrate why our result is not covered by the Euclidean $BV$ case solved by the first author in [1], and we compare it with the strategy of [7], based on the representation of the commutator by interpolation à la Bakry-Émery and an integral representation of the symmetrized derivative of $\mathbf b$.

</details>


### [117] [Traveling waves near shear flows for the inhomogeneous Euler equations with non-constant density](https://arxiv.org/abs/2602.00824)
*Qi Zhao,Weiren Zhao*

Main category: math.AP

TL;DR: The paper studies traveling waves near monotonic shear flows with non-constant density for 2D inhomogeneous Euler equations in a finite channel, showing inviscid damping failure at certain regularities while proving nonexistence in higher regularity spaces.


<details>
  <summary>Details</summary>
Motivation: To understand the existence and nonexistence of traveling wave solutions near monotonic shear flows with non-constant background density for the 2D inhomogeneous Euler equations, particularly investigating when inviscid damping fails and establishing regularity thresholds for solution existence.

Method: For any small τ>0, first construct nontrivial traveling waves with velocity in H^{5/2-τ} and density in H^{3/2-τ} to demonstrate inviscid damping failure. Second, when the distorted Rayleigh operator has no eigenvalues, prove that such traveling wave solutions cannot exist in higher regularity spaces (H^{5/2+τ} for velocity and H^{3/2+τ} for density).

Result: 1) Constructed nontrivial traveling waves at H^{5/2-τ} velocity and H^{3/2-τ} density regularity, showing inviscid damping fails at these regularities. 2) Proved that when the distorted Rayleigh operator has no eigenvalues, such traveling wave solutions cannot exist in higher regularity spaces (H^{5/2+τ} for velocity and H^{3/2+τ} for density).

Conclusion: There exists a sharp regularity threshold for traveling wave solutions near monotonic shear flows with non-constant density: solutions exist at H^{5/2-τ} velocity/H^{3/2-τ} density regularity but cannot exist at H^{5/2+τ} velocity/H^{3/2+τ} density regularity when the distorted Rayleigh operator has no eigenvalues, establishing precise conditions for inviscid damping failure.

Abstract: We investigate the existence and nonexistence of traveling wave solutions near monotonic shear flows with non-constant background density for the two-dimensional inhomogeneous Euler equations in a finite channel. For any small $τ>0$, first, we construct nontrivial traveling waves with velocity and density in $H^{5/2-τ}$ and $H^{3/2-τ}$, respectively, showing that inviscid damping fails at these regularities. Second, when the distorted Rayleigh operator has no eigenvalues, we prove that such traveling wave solutions cannot exist in higher regularity spaces ($H^{5/2+τ}$ for velocity and $H^{3/2+τ}$ for density).

</details>


### [118] [Geometric Integration by Parts and Sobolev Spaces on Vector Bundles: A Unified Global Approach](https://arxiv.org/abs/2602.01016)
*Velázquez-Mendoza Carlos Daniel,Sandoval-Romero María de los Ángeles*

Main category: math.AP

TL;DR: Develops a unified framework for Sobolev spaces on vector bundles over Riemannian manifolds with rigorous geometric integration by parts formula, establishing sharp norm estimates and streamlined proofs for key theorems.


<details>
  <summary>Details</summary>
Motivation: To provide a modern, accessible foundation for Sobolev spaces on bundles that prioritizes intrinsic global arguments over ad hoc coordinate patching, addressing the lack of explicit, direct proofs in the literature.

Method: Develops a rigorous higher-order geometric integration by parts formula characterizing the formal adjoint of covariant derivative; establishes sharp local-to-global norm equivalence estimates; provides streamlined, self-contained proofs using intrinsic global arguments.

Result: Establishes integration by parts formula for arbitrary manifolds (no completeness/compactness assumptions); proves Meyers-Serrin theorem on general manifolds; proves Sobolev embedding and Rellich-Kondrashov theorems for compact case.

Conclusion: Provides a unified, modern framework for Sobolev spaces on vector bundles that is geometrically intuitive and accessible, bridging the gap between sophisticated machinery and underlying geometry in global analysis.

Abstract: This article develops a unified framework for the theory of Sobolev spaces on vector bundles over Riemannian manifolds. The analytical core of our approach is a rigorous higher-order geometric integration by parts formula, which characterizes the formal adjoint of the covariant derivative. This identity is established for arbitrary manifolds, requiring no assumptions on completeness or compactness. While these results are fundamental to global analysis, explicit and direct proofs are often elusive in the literature or rely on overly sophisticated machinery that overshadows the underlying geometry. To bridge this gap, we establish sharp local-to-global norm equivalence estimates and provide streamlined, self-contained proofs for the Meyers-Serrin theorem on general manifolds, as well as the Sobolev embedding and Rellich-Kondrashov theorems for the compact case. By prioritizing intrinsic global arguments over ad hoc coordinate patching, this work provides a modern and accessible foundation for the study of Sobolev spaces on bundles.

</details>


### [119] [Vortex Stretching in the Navier-Stokes Equations and Information Dissipation in Diffusion Models: A Reformulation from a Partial Differential Equation Viewpoint](https://arxiv.org/abs/2602.01071)
*Tsuyoshi Yoneda*

Main category: math.AP

TL;DR: New inverse-time formulation for vortex stretching in Navier-Stokes using score-based diffusion framework, enabling backward-time particle trajectory reconstruction via learned neural network score function.


<details>
  <summary>Details</summary>
Motivation: To develop a method for reconstructing backward-time particle trajectories in turbulent flows by addressing the ill-posed nature of time reversal in Navier-Stokes equations, particularly for vortex stretching phenomena.

Method: Inverse-time formulation inspired by score-based diffusion models, absorbing backward Laplacian into drift term via score function. Uses discrete Lagrangian flow of axisymmetric vortex-stretching field, learns score function with neural network to construct backward-time particle trajectories.

Result: Numerical results show rapid loss of initial position information in compressive direction, but relatively good preservation in stretching direction, demonstrating the method's capability to track backward-time evolution.

Conclusion: The score-based inverse-time formulation provides a viable approach for reconstructing backward-time particle trajectories in vortex stretching flows, with differential information preservation depending on flow directionality.

Abstract: We present a new inverse-time formulation of vortex stretching in the Navier-Stokes equations, based on a PDE framework inspired by score-based diffusion models. By absorbing the ill-posed backward Laplacian arising from time reversal into a drift term expressed through a score function, the inverse-time dynamics are formulated in a Lagrangian manner. Using a discrete Lagrangian flow of an axisymmetric vortex-stretching field, the score function is learned with a neural network and employed to construct backward-time particle trajectories. Numerical results demonstrate that information about initial positions is rapidly lost in the compressive direction, whereas it is relatively well preserved in the stretching direction.

</details>


### [120] [Fully discrete follow-the-leader approximation of one-dimensional scalar conservation laws with vacuum](https://arxiv.org/abs/2602.01145)
*M. Di Francesco,S. Fagioli,V. Iorio,M. D. Rosini*

Main category: math.AP

TL;DR: A fully discrete particle method for 1D scalar conservation laws that converges to entropy weak solutions


<details>
  <summary>Details</summary>
Motivation: To develop a particle-based numerical method for scalar conservation laws that maintains accuracy and convergence properties while being compatible with vacuum regions

Method: Constructs a vacuum-compatible family of time-discrete particle equations under monotonicity assumptions on macroscopic velocity, with piecewise-constant density reconstruction from particle positions

Result: The piecewise-constant density reconstruction from the particle scheme converges to the unique entropy weak solution of the macroscopic scalar conservation law

Conclusion: The proposed fully discrete particle approximation provides a valid numerical method for scalar conservation laws that handles vacuum regions and converges to the correct entropy solution

Abstract: We present a fully discrete particle approximation for one-dimensional scalar conservation laws. Under suitable monotonicity assumptions on the macroscopic velocity, we construct a vacuum-compatible family of time-discrete particle equations and show that an appropriate piecewise-constant density reconstruction from the particle setting converges to the unique entropy weak solution of the macroscopic scalar conservation law.

</details>


### [121] [Long-time asymptotics of (1,3)-sign solitary waves for the damped nonlinear Klein-Gordon equation](https://arxiv.org/abs/2602.01205)
*Kenjiro Ishizuka*

Main category: math.AP

TL;DR: The paper proves that for a damped nonlinear Klein-Gordon equation, solutions asymptotic to four solitons (three with same sign, one opposite) evolve into an equilateral triangle configuration centered around the opposite-signed soliton.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time dynamics and spatial organization of multi-soliton solutions in damped nonlinear Klein-Gordon equations, specifically how solitons with different signs arrange themselves geometrically.

Method: Mathematical analysis of the damped nonlinear Klein-Gordon equation with damping parameter α>0, in dimensions 2≤d≤5, for energy sub-critical exponents p>2. The proof likely involves stability analysis, modulation theory, and geometric arguments about soliton interactions.

Result: Proves that solutions approaching a superposition of four solitons (three with same sign, one with opposite sign) evolve such that the three like-signed solitons form an equilateral triangle configuration centered at the oppositely signed soliton.

Conclusion: The damped nonlinear Klein-Gordon equation exhibits geometric organization of multi-soliton solutions, with like-signed solitons arranging themselves in symmetric patterns around opposite-signed solitons, specifically forming equilateral triangles in the case of three versus one.

Abstract: We consider the damped nonlinear Klein-Gordon equation: \begin{align*} \partial_{t}^2u-Δu+2α\partial_{t}u+u-|u|^{p-1}u=0, \ & (t,x) \in \mathbb{R} \times \mathbb{R}^d, \end{align*} where $α>0$, $2\leq d\leq 5$ and energy sub-critical exponents $p>2$. In this paper, we prove that any solution which is asymptotic to a superposition of four solitons with exactly one soliton of opposite sign evolves so that the three like-signed solitons spread out in an equilateral-triangle configuration centered at the oppositely signed soliton.

</details>


### [122] [On the nodal set conjecture for the $p$-Laplacian in circularly symmetric domains](https://arxiv.org/abs/2602.01210)
*Vladimir Bobkov*

Main category: math.AP

TL;DR: Extends Pütter's 1990 result on second eigenfunctions of Dirichlet Laplacian to p-Laplacian in higher dimensions for circularly symmetric domains.


<details>
  <summary>Details</summary>
Motivation: Generalize Pütter's classical result about nodal lines intersecting boundaries for second eigenfunctions from the standard Laplacian to the p-Laplacian, and extend from planar domains to arbitrary higher dimensions.

Method: Adopts the method of moving polarization to analyze nodal sets of second eigenfunctions of Dirichlet p-Laplacian on circularly symmetric domains.

Result: Establishes that the nodal set of second eigenfunctions of Dirichlet p-Laplacian intersects the boundary for circularly symmetric domains in arbitrary higher dimensions.

Conclusion: Successfully extends Pütter's classical planar result to the nonlinear p-Laplacian case in higher dimensions, showing boundary intersection of nodal sets under circular symmetry.

Abstract: In 1990, Pütter shown that the nodal line of any second eigenfunction of the Dirichlet Laplacian on a planar bounded simply connected domain $Ω$ intersects the boundary $\partialΩ$ provided $Ω$ has the circular symmetry. By adopting the method of moving polarization, we establish similar information on the nodal set of second eigenfunctions of the Dirichlet $p$-Laplacian on circularly symmetric domains in arbitrary higher dimension.

</details>


### [123] [Regularity to Thin Obstacle Problem in Orlicz spaces](https://arxiv.org/abs/2602.01255)
*Junior da Silva Bessa,Paulo Henryque da Costa Silva,Alan Pio Sousa*

Main category: math.AP

TL;DR: Proves Lipschitz continuity and Hölder gradient continuity for minimizers of thin obstacle problems in Orlicz spaces using De Giorgi techniques, with nodal set characterization.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties for minimizers of energy functionals in thin obstacle problems within the more general framework of Orlicz spaces, extending classical results.

Method: Uses techniques from De Giorgi's classical regularity theory to analyze minimizers of the energy functional associated with thin obstacle problems in Orlicz spaces.

Result: Proves Lipschitz continuity of minimizers and Hölder continuity of their gradients. Also provides characterization of the structure of nodal sets as a byproduct.

Conclusion: Successfully establishes regularity results for thin obstacle problems in Orlicz spaces, demonstrating that De Giorgi's techniques remain effective in this more general setting.

Abstract: In this work, we establish regularity results for minimizers of the energy functional associated with the thin obstacle problem in Orlicz spaces. More precisely, we prove the Lipschitz continuity and the Hölder continuity of the gradient of minimizers. The analysis is based on techniques from De Giorgi's classical regularity theory. As a byproduct of our results, we also provide a characterization of the structure of the nodal sets of the minimizers.

</details>


### [124] [Ground states for the NLS equation with combined nonlinearity on periodic metric graphs](https://arxiv.org/abs/2602.01336)
*Nicola Soave,Lorenzo Villata*

Main category: math.AP

TL;DR: Study of ground states with prescribed mass for NLS with combined nonlinearities on 1- and 2-periodic metric graphs, extending previous work on homogeneous NLS on periodic graphs and combined nonlinearities on finite graphs.


<details>
  <summary>Details</summary>
Motivation: To understand how the interplay between different nonlinearities affects ground state existence on periodic metric graphs, extending previous studies on homogeneous NLS on periodic graphs and NLS with combined nonlinearities on finite graphs.

Method: Analysis of Non-Linear Schrödinger energy with combined nonlinearities on 1- and 2-periodic metric graphs, building on previous work on homogeneous NLS and using techniques from metric graph theory.

Result: The interplay between different nonlinearities creates new phenomena compared to homogeneous settings, with dimensional crossover occurring for 2-periodic graphs. Extends existing results for homogeneous NLS on square and honeycomb grids to general 2-periodic graphs, and improves previous results for inhomogeneous NLS on noncompact finite graphs.

Conclusion: Periodic metric graphs with combined nonlinearities exhibit novel phenomena due to the interaction between different nonlinearities, with dimensional crossover being a key feature in 2-periodic cases, advancing understanding beyond homogeneous and finite graph settings.

Abstract: We investigate the existence of ground states with prescribed mass for the Non-Linear Schrödinger energy with combined nonlinearities on $1$ and $2$-periodic metric graphs. This is the natural prosecution of previous studies concerning on the one hand the homogeneous NLS equation on periodic graphs, and on the other hand the NLS equation with combined nonlinearity on noncompact metric graphs with finitely many vertexes and edges. As in the latter case, it turns out that the interplay between different nonlinearities creates new phenomena with respect to the homogenous setting, but, due to the periodicity, in a quite different way; in particular, for $2$-periodic graphs, the so called dimensional crossover occurs.
  As a by-product, we extend existing results for the homogeneous NLS on the square and honeycomb grids to general $2$-periodic graphs. Furthermore, we also improve previous results obtained for the inhomogeneous NLS on noncompact graphs with finitely many vertexes and edges.

</details>


### [125] [Semigroup Solutions for A Multilayered Filtration System](https://arxiv.org/abs/2602.01403)
*George Avalos,Galen Richard,Justin T. Webster*

Main category: math.AP

TL;DR: Analysis of strong solutions for a multilayered filtration system coupling Stokes flow, Biot poroelasticity, and poroplate interface dynamics using semigroup theory.


<details>
  <summary>Details</summary>
Motivation: To mathematically analyze the complex interaction between viscous incompressible flow and bulk poroelasticity through a poroelastic interface in multilayered filtration systems, extending previous weak solution theory to strong solutions.

Method: Uses Hilbert space framework and Lumer-Phillips theorem to establish existence of strong solutions. Employs nonstandard mixed variational formulation for resolvent analysis, characterizes infinitesimal generator, and generates C₀-semigroup on finite-energy space. Uses perturbation theory to handle elastic nonlinearities.

Result: Successfully establishes generation of C₀-semigroup for the linear Cauchy problem, enabling treatment of elastic nonlinearities through perturbation theory. Provides explicit characterization of infinitesimal generator and strong solution framework.

Conclusion: The semigroup approach enables analysis of structural nonlinearities in multilayer filtration systems, opens possibilities for future stability and regularity analyses, and allows comparison between different filtration configurations while suggesting poroplate dynamics may have regularizing and stabilizing effects.

Abstract: We investigate solutions to a coupled system of partial differential equations that describe a multilayered filtration system. Namely, we study the interaction of a viscous incompressible flow with bulk poroelasticity, via a poroelastic interface. The configuration consists of two 3D toroidal subdomains connected via a plate interface, which permits elastic deformation and perfusive fluid dynamics. The governing dynamics comprise Stokes equations in the bulk fluid region, Biot's equations in the bulk poroelastic region, and the recent poroplate of Mikelić at the interface. Coupling occurs on the top and lower surfaces of the plate, and involves conservation of mass, stress balance, and a certain slip condition for the fluid free-flow.
  We seek strong (and mild) solutions in the Hilbert space framework via the Lumer-Phillips theorem. The resolvent analysis employs a nonstandard mixed variational formulation which captures the complex, multi-physics coupling at the interface. We explicitly characterize the infinitesimal generator associated to the linear Cauchy problem and establish the generation of a $C_0$-semigroup on a suitably chosen finite-energy space. With the semigroup in hand, we may treat elastic nonlinearities for plate displacements through perturbation theory. These result parallel those for Biot-Stokes filtration systems, and complement the recently established weak solution theory for multilayer filtrations. The agency of the semigroup straightforwardly admits structural (plate) nonlinearity into the dynamics. Future stability and regularity analyses for multilayer filtrations are also made possible by these results, as well as a comparison of spectral and regularity properties between filtration configurations, and the elucidation of the mitigating poroplate dynamics as possibly regularizing and stabilizing.

</details>


### [126] [Existence of pure capillary solitary waves in constant vorticity flows](https://arxiv.org/abs/2602.01431)
*Ting-Yang Hsiao,Zhengjun Liang,Giang To,Ye Zhang*

Main category: math.AP

TL;DR: Existence of pure capillary solitary waves in 2D finite-depth Euler equations with constant vorticity, contrasting with irrotational nonexistence results.


<details>
  <summary>Details</summary>
Motivation: Previous work (Ifrim--Pineau--Tataru--Taylor) showed nonexistence of solitary waves in irrotational pure-capillary regime. This paper investigates whether constant vorticity enables solitary wave formation in this regime.

Method: Uses spatial-dynamics Hamiltonian formulation, nonlinear change of variables to flatten free surface and put symplectic form into Darboux coordinates. Performs center-manifold reduction near distinguished parameter curve, cubic normal-form expansion, and long-wave scaling to derive KdV-type profile equation.

Result: Proves existence of pure capillary solitary waves for 2D finite-depth Euler equations with nonzero constant vorticity, establishing constant vorticity as enabling mechanism for solitary waves in pure-capillary regime.

Conclusion: Constant vorticity serves as a mechanism that enables solitary wave formation in pure-capillary regime, where such waves were previously shown not to exist in irrotational case.

Abstract: We prove the existence of pure capillary solitary waves for the 2D finite-depth Euler equations with nonzero constant vorticity. In the irrotational case, nonexistence of solitary waves was established by Ifrim--Pineau--Tataru--Taylor, so our theorem isolates constant vorticity as a mechanism that enables solitary waves in the pure-capillary regime. The proof uses a spatial-dynamics Hamiltonian formulation of the travelling-wave equations and a nonlinear change of variables that flattens the free surface while putting the symplectic form into Darboux coordinates. Near a distinguished curve in the vorticity--capillarity parameter space, the linearization has a two-dimensional center subspace; a parameter-dependent center-manifold reduction yields a canonical planar Hamiltonian system. A cubic normal-form expansion and long-wave scaling produce a KdV-type profile equation with a reversible homoclinic orbit, which persists under the full dynamics and generates the solitary-wave solutions.

</details>


### [127] [Instability of solutions in a degenerate reaction diffusion equation](https://arxiv.org/abs/2602.01487)
*R. Marangell,J. J. Wylie,B. H. Bradshaw-Hajek*

Main category: math.AP

TL;DR: Analysis of spectral stability for traveling and stationary front/pulse solutions in degenerate reaction-diffusion systems, with both analytical and numerical approaches.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of traveling and stationary wave solutions (fronts and pulses) in degenerate reaction-diffusion systems, which is important for predicting pattern formation and wave propagation behavior.

Method: Characterized essential spectrum of linearized operator, identified stability conditions, used analytical methods (explicit Evans functions) for special cases, and employed numerical Riccati-Evans function approach for regimes where analytical methods are unavailable.

Result: Found that stable traveling fronts can occur, while traveling pulses are typically unstable. Obtained complete spectral descriptions for certain stationary waves and analytical results for special cases.

Conclusion: Degenerate reaction-diffusion systems can support stable traveling fronts but generally produce unstable traveling pulses, with the spectral analysis providing comprehensive understanding of wave solution stability.

Abstract: We study the spectral stability of travelling and stationary front and pulse solutions in a class of degenerate reaction-diffusion systems. We characterise the essential spectrum of the linearised operator in full generality and identify conditions under which it lies entirely in the left-half plane. For a number of special cases we obtain analytical results, including explicit Evans functions, and complete spectral descriptions for certain stationary waves. In regimes where analytical methods are not available, we compute the point spectrum numerically using a Riccati-Evans function approach. Our results show that stable travelling fronts can occur, while travelling pulses are typically unstable.

</details>


### [128] [Unveiling Traffic Wave of Linear Adaptive Cruise Control: A Second-order Macroscopic Traffic Flow Model](https://arxiv.org/abs/2602.01506)
*Zihao Li,Quyuan Lin,Fan Pu,Soyoung Ahn,Yunlong Zhang,Jiwan Jiang,Yang Zhou*

Main category: math.AP

TL;DR: This paper develops a second-order macroscopic traffic flow model that directly incorporates Adaptive Cruise Control (ACC) feedback laws, enabling analysis of traffic wave propagation under ACC systems.


<details>
  <summary>Details</summary>
Motivation: Existing traffic analyses focus on steady-state metrics and neglect ACC control parameters that shape traffic dynamics. As ACC systems become widespread, understanding how traffic waves evolve under ACC control is crucial for improving traffic efficiency and safety.

Method: The authors embed ACC control laws directly into the momentum equation while preserving mass conservation, creating a second-order macroscopic model governed by a second-order PDE equivalent to linear ACC feedback. They analyze wave propagation characteristics, linear degeneracy, admissible discontinuities, and their connection to ACC string stability.

Result: The model is strictly hyperbolic, preserving anisotropy and physical consistency. Traffic wave evolution depends on both initial state and ACC control parameters. Numerical experiments show the second-order model yields significantly lower vehicle-pair speed deviations along wave paths compared to first-order models under the same disturbances.

Conclusion: A second-order treatment is necessary for accurate analysis of traffic dynamics under ACC systems, and the proposed framework provides a physically consistent approach to understanding how ACC parameters influence traffic wave propagation.

Abstract: Traffic waves, the spatiotemporal propagation of congestion, are a key feature of traffic flow. As Adaptive Cruise Control (ACC) systems gain widespread adoption and show promise for improving both efficiency and safety, understanding how these waves evolve under ACC becomes increasingly important. Yet most existing analyses rely on steady-state metrics (e.g., equilibrium spacing) and neglect the ACC control-law parameters, such as feedback gains, that fundamentally shape higher-order traffic dynamics. To overcome this limitation, we embed the ACC control law directly into the momentum equation while retaining mass conservation law. The result is a higher-order macroscopic model whose dynamics are governed by a second-order partial differential equation equivalent to the linear ACC feedback law. Analyzing the flux Jacobian confirms that the system is strictly hyperbolic, thereby preserving anisotropy and ensuring physical consistency. The derivation also shows that traffic wave evolution depends on both the initial state and the ACC control parameters. We analyze wave-propagation characteristics, linear degeneracy, admissible discontinuities, and their connection to ACC string stability, with the corresponding derivations. Numerical experiments confirm that the second-order model yields markedly lower vehicle-pair speed deviations along wave paths than a first-order model subject to the same non-steady disturbances, underscoring both the necessity of a second-order treatment and the soundness of the proposed framework.

</details>


### [129] [Vacuum initial data with minimal decay and borderline decay](https://arxiv.org/abs/2602.01557)
*Dawei Shen,Jingbo Wan*

Main category: math.AP

TL;DR: The paper extends the conical solution-operator method to construct vacuum asymptotically flat initial data at minimal and borderline decay thresholds for Minkowski spacetime stability.


<details>
  <summary>Details</summary>
Motivation: To apply the conical solution-operator method (originally developed by Mao-Tao) to construct vacuum initial data that matches the decay thresholds established in previous global and exterior stability results for Minkowski spacetime.

Method: Extends the conical solution-operator method from Mao-Tao's work to handle minimal and borderline decay thresholds for asymptotically flat vacuum initial data construction.

Result: Successfully constructs vacuum asymptotically flat initial data at the minimal and borderline decay thresholds corresponding to the global and exterior stability results for Minkowski spacetime.

Conclusion: The conical solution-operator method provides a simple construction approach for vacuum initial data that aligns with the decay thresholds established in previous Minkowski stability theorems, demonstrating the method's applicability to these critical cases.

Abstract: In this note, we show that the conical solution-operator method of Mao-Tao in [Localized initial data for Einstein equations] applies to a simple construction of vacuum asymptotically flat initial data at minimal and borderline decay thresholds, corresponding to the global and exterior stability of Minkowski spacetime proved by the first named author in [Global stability of Minkowski spacetime with minimal decay] and [Exterior stability of Minkowski spacetime with borderline decay].

</details>


### [130] [Regularization for Multi-Phase 2D Euler Equations via Competing Transport Markers](https://arxiv.org/abs/2602.01569)
*Trinh T. Nguyen*

Main category: math.AP

TL;DR: A novel regularization framework for 2D incompressible Euler equations that preserves transport structure of multi-phase vorticity fields using passively advected scalar markers with selection-based regularization.


<details>
  <summary>Details</summary>
Motivation: To develop a regularization method for multi-phase vortex patch solutions that exactly preserves the transport structure without introducing spatial diffusion or mollification, addressing the challenge of maintaining sharp interfaces in Euler equations.

Method: Reformulate multi-phase vortex patch data using finite family of passively advected scalar marker functions with smooth pointwise selection rule. Regularization originates solely from marker selection mechanism as sharpness parameter β→∞.

Result: Prove uniform convergence of transported marker functions on finite time intervals. Under geometric nondegeneracy condition, establish Hausdorff convergence of evolving interfaces and exponential-in-β pointwise convergence of regularized vorticity away from tie sets to sharp multi-phase vortex patch solution.

Conclusion: The framework provides diffusion-free regularization preserving transport structure exactly. Loss of pointwise convergence coincides precisely with onset of geometric degeneracy in Euler interface dynamics, establishing connection between convergence failure and interface degeneracy.

Abstract: We introduce a novel regularization framework for the two-dimensional incompressible Euler equation that exactly preserves the transport structure of multi-phase vorticity fields. The key step is a reformulation of multi-phase vortex patch data in terms of a finite family of passively advected scalar marker functions: at each point, the local vorticity is determined by a smooth, pointwise selection rule arising from competition among these markers. The scheme introduces no spatial diffusion or mollification; all regularization originates solely from the marker selection mechanism. As the sharpness parameter $β\to\infty$, we prove uniform convergence of the transported marker functions on finite time intervals. Moreover, under a geometric nondegeneracy condition on the underlying Euler interface network, we establish Hausdorff convergence of the evolving interfacial structures and exponential-in-$β$ pointwise convergence of the regularized vorticity away from the tie sets to the corresponding sharp multi-phase vortex patch solution. Finally, we show that the loss of pointwise convergence coincides precisely with the onset of geometric degeneracy in the Euler interface dynamics.

</details>


### [131] [Gradient Existence and Energy Finiteness of Local Minimizers in the Wasserstein $L^\infty$ Topology for Binary-Star Systems](https://arxiv.org/abs/2602.01678)
*Hangsheng Chen*

Main category: math.AP

TL;DR: This paper refines McCann's results on binary-star systems using Euler-Poisson equations, focusing on properties of local energy minimizers under Wasserstein L∞ topology.


<details>
  <summary>Details</summary>
Motivation: To extend McCann's work on binary-star systems by considering more general equations of state and to deeply investigate properties of local energy minimizers in the Wasserstein L∞ topology, addressing gaps in understanding gradient existence, L∞ function neighborhoods, and energy finiteness.

Method: Uses variational methods to obtain solutions as local energy minimizers under Wasserstein L∞ topology. Examines three key aspects: (1) proving gradient existence to connect Euler-Lagrange and Euler-Poisson equations, (2) analyzing existence of L∞ functions in neighborhoods, and (3) studying energy finiteness of minimizers in different topologies.

Result: The paper demonstrates that local energy minimizers exist in Wasserstein L∞ topology, shows gradient existence connecting variational and PDE formulations, establishes L∞ function neighborhoods, and reveals that finite-energy local minimizers exist in Wasserstein topology while they don't exist in topological vector space topologies.

Conclusion: The Wasserstein L∞ topology provides a suitable framework for studying binary-star systems via variational methods, enabling rigorous connections between Euler-Lagrange and Euler-Poisson formulations and supporting finite-energy local minimizers that don't exist in other topologies.

Abstract: In this paper, we refine and complement McCann's results on binary-star systems \cite{McC06}, a compressible fluid model governed by the Euler-Poisson equations. We consider a general form of the equation of state that includes polytropic gaseous stars indexed by $γ$ as a special case. Beyond revisiting McCann's framework and conclusions -- where solutions to the Euler-Poisson equations are obtained as local energy minimizers via variational methods under the topology induced by the Wasserstein $L^\infty$ distance -- we focus on a detailed exploration of the properties of local energy minimizers in this topology, addressing three key aspects: (1) the feasibility of transitioning from the Euler-Lagrange equation to the Euler-Poisson equation by demonstrating gradient existence; (2) the existence of $L^\infty$ functions within neighborhoods in this topology; and (3) the finiteness of the energy of local minimizers in this topology, contrasted with the non-existence of finite-energy local minimizers and the existence of infinite-energy weak local minimizers in the topology inherited from topological vector spaces.

</details>


### [132] [Blow-up suppression for the nematic liquid crystal flow via Couette flow on $\mathbb{R}^2$](https://arxiv.org/abs/2602.01823)
*Yubo Chen,Wendong Wang,Juncheng Wei,Guoxu Yang*

Main category: math.AP

TL;DR: Large-scale Couette flow can suppress blow-up in 2D harmonic heat/liquid crystal flows even when initial energy exceeds 8π threshold.


<details>
  <summary>Details</summary>
Motivation: Previous work showed small-scale velocity fields can create singular solutions. This paper investigates whether large-scale velocities might stabilize the system and prevent blow-up concentration.

Method: Analyze harmonic heat flow/liquid crystal flow with Couette flow. Use weak assumption on anisotropic norm of initial data. Construct examples with initial energy > 8π that satisfy assumptions.

Result: Blow-up can be suppressed by sufficiently large-amplitude Couette flow. Constructed examples with initial energy exceeding 8π that meet the assumptions.

Conclusion: Large-scale Couette flow has stabilizing effect that can prevent blow-up in 2D harmonic heat/liquid crystal flows, even for initial energies above the critical 8π threshold.

Abstract: As is well known, for the harmonic heat flow or liquid crystal flow in two-dimension, the solution may blow up when the initial energy is greater than $8π$. Motivated by Lai--Lin--Wang--Wei--Zhou (CPAM, 2022), where singular solutions were constructed in the presence of small-scale velocity fields, it is natural to ask whether large-scale velocities may play a stabilizing role, preventing the concentration of blow-up. Here we show that the blow-up phenomenon can be suppressed by a Couette flow whose amplitude is large enough under a weak assumption on the anisotropic norm of the initial data. In particular, we construct examples with initial energy exceeding $8π$ that satisfy our assumptions.

</details>


### [133] [Constant potentials do not minimize the fundamental gap on convex domains in negatively curved Hadamard manifolds](https://arxiv.org/abs/2602.01866)
*Frieder Jäckel*

Main category: math.AP

TL;DR: The paper disproves the second part of the fundamental gap conjecture in negatively curved Hadamard manifolds by constructing convex domains with potentials that yield smaller fundamental gaps than the Laplacian alone.


<details>
  <summary>Details</summary>
Motivation: The fundamental gap conjecture concerns the spectral gap between the first two eigenvalues of Schrödinger operators. While the first part (lower bound) was proven, the second part (upper bound) remained open, particularly in negatively curved manifolds beyond symmetric cases like hyperbolic space.

Method: For any negatively curved Hadamard manifold X and diameter D, the authors construct a convex domain Ω ⊆ X with diameter D and a convex potential V on Ω. They analyze the fundamental gap of -Δ+V and show it can be strictly smaller than that of -Δ alone, requiring PDE analysis due to lack of symmetry (unlike the ODE case in hyperbolic space).

Result: The second part of the fundamental gap conjecture is false in every negatively curved manifold. The constructed examples demonstrate that convex potentials can reduce the fundamental gap below the Laplacian-only case.

Conclusion: The fundamental gap conjecture's second part does not hold in negatively curved Hadamard manifolds, resolving a long-standing question and highlighting the complexity introduced by curvature and lack of symmetry in spectral analysis.

Abstract: We show that for every negatively curved Hadamard manifold $X$ and every $D > 0$ there exists a convex domain $Ω\subseteq X$ with diameter $D$ and a convex potential $V$ on $Ω$ such that the fundamental gap of the operator $-Δ+V$ is strictly smaller than the fundamental gap of $-Δ$. This shows that the second part of the fundamental gap conjecture is wrong in every negatively curved manifold. This is significantly harder than in the previously known case of hyperbolic space because, due to the lack of symmetry, one has to study a true PDE, and not just an ODE.

</details>


### [134] [On the relaxation towards mechanical equilibrium for two-pressure compressible flows](https://arxiv.org/abs/2602.01890)
*Cosmin Burtea,Timothée Crin-Barat,Pierre Gonin--Joubert*

Main category: math.AP

TL;DR: Symmetrization of Baer-Nunziato model enables zero compaction viscosity limit to Kapila model and reveals pressure-induced stabilization for global existence near constant states.


<details>
  <summary>Details</summary>
Motivation: To develop a symmetrized version of the Baer-Nunziato model for compressible fluid mixtures that enables rigorous analysis of the zero compaction viscosity limit and reveals stabilization mechanisms for global existence.

Method: Introduces a symmetrization technique for the one-velocity two-pressures Baer-Nunziato type model for mixtures of barotropic compressible fluids.

Result: The symmetrization justifies the zero compaction viscosity limit to recover solutions of the Kapila model, and reveals a pressure-induced stabilization mechanism that enables global-in-time existence for initial data close to constant states.

Conclusion: The symmetrization approach provides both mathematical justification for the Kapila model limit and reveals fundamental stabilization properties that ensure global existence near equilibrium states.

Abstract: We introduce a symmetrization of a one-velocity two-pressures Baer-Nunziato type model for mixtures of barotropic compressible fluids. It allows us to justify the zero compaction viscosity limit and to recover a solution of the so-called Kapila model. On the other hand, the symmetrization highlights a pressure-induced stabilization mechanism which allows us to recover a global-in-time existence result for initial data close to constant states.

</details>


### [135] [Local bounds for nonlinear higher-order vector fields for the p-Laplace equation](https://arxiv.org/abs/2602.01926)
*Felice Iandoli,Giuseppe Spadaro,Domenico Vuono*

Main category: math.AP

TL;DR: The paper proves higher regularity for weak solutions of the p-Laplace equation when p is close to 2, showing that certain nonlinear quantities involving the gradient and Hessian belong to local Sobolev spaces with uniform bounds.


<details>
  <summary>Details</summary>
Motivation: The p-Laplace equation is a fundamental nonlinear elliptic PDE that appears in various applications. Understanding the regularity of its solutions, especially near critical points where the gradient vanishes, is challenging but important for analysis and applications.

Method: The authors study weak solutions of the p-Laplace equation with p sufficiently close to 2. They assume the source term f satisfies appropriate Sobolev and Hölder regularity conditions, then use analytical techniques to prove regularity results for nonlinear quantities involving the gradient and Hessian.

Result: For m ≥ 3, they prove that |∇u|^{m-2}∇u ∈ W^{m-1,q}_{loc}(Ω) and |∇u|^{m-2}D²u ∈ W^{m-2,q}_{loc}(Ω) for any q ≥ 2. They also obtain uniform L^∞ bounds for weighted (m-1)-th derivatives of these quantities, providing quantitative control even near critical points of ∇u.

Conclusion: The paper establishes improved regularity results for p-Laplace equations when p is near 2, overcoming difficulties near critical points through careful analysis of weighted derivatives and obtaining uniform bounds that remain valid even where the gradient vanishes.

Abstract: We study higher regularity for weak solutions of the $p$-Laplace equation $-Δ_p u = f$ in a domain $Ω\subset \mathbb{R}^n$ for $p$ sufficiently close to 2. For $m \ge 3$, assuming that $f$ satisfies suitable Sobolev and Hölder regularity conditions, we prove that the nonlinear quantity $|\nabla u|^{m-2}\nabla u$ belongs to $W^{m-1,q}_{loc}(Ω)$, and that $|\nabla u|^{m-2} D^2u$ belongs to $W^{m-2,q}_{loc}(Ω)$, for any $q\ge 2$. Furthermore, we obtain uniform $L^\infty$ bounds for the weighted $(m-1)$-th derivatives of $|\nabla u|^{m-2}\nabla u$ and $|\nabla u|^{m-2} D^2u$, providing quantitative control even near critical points of $\nabla u$.

</details>


### [136] [Low Mach number limit and optimal time decay rates of the compressible Navier-Stokes-transport system in critical Besov spaces](https://arxiv.org/abs/2602.02019)
*Fucai Li,Jinkai Ni,Yuzhu Wang*

Main category: math.AP

TL;DR: Global well-posedness and low Mach number limit for compressible Navier-Stokes-Transport system in Besov spaces, with temperature transport instead of dissipation.


<details>
  <summary>Details</summary>
Motivation: Study the Navier-Stokes-Transport system where temperature satisfies transport equation rather than dissipative parabolic equation, leading to fundamentally different behavior from Navier-Stokes-Fourier system.

Method: Use Besov spaces framework, establish global well-posedness of strong solutions in critical Besov spaces, introduce Mach number parameter, develop refined energy analysis to handle lack of dissipation on density and temperature.

Result: Prove global well-posedness in critical Besov spaces, establish low Mach number limit to incompressible inhomogeneous Navier-Stokes system (even for ill-prepared data), obtain optimal time decay rates in dimensions d≥3, show density remains uniformly bounded unlike NSF system.

Conclusion: The NST system exhibits fundamentally different asymptotic behavior from NSF system due to transport-type temperature equation, with density remaining bounded rather than decaying, and the low Mach number limit holds globally even for ill-prepared initial data.

Abstract: In this paper, we investigate the Navier-Stokes-Transport (NST) system in the framework of Besov spaces. This system contains of a compressible Navier-Stokes system for the density and momentum of a fluid, and a transport equation for the potential temperature of the fluid. In stark contrast to the well-known Navier-Stokes-Fourier (NSF) system where the temperature satisfies a parabolic type equation providing dissipative effect for the temperature and the density, the temperature in our NST system enjoys a transport equation which precludes a dissipative mechanism for the density, leading to significant different effects to the whole system. We first establish the global well-posedness of strong solutions to the compressible NST system in critical Besov spaces over $\mathbb{R}^d$ with $d \geq 2$. Furthermore, by introducing the Mach number $\varepsilon > 0$, we rigorously prove the low Mach number limit as $\varepsilon \to 0$, showing that the solutions converge to that of the incompressible inhomogeneous Navier-Stokes system. This singular limit holds globally in time, even for {\it ill-prepared} initial data. To address the challenge posed by the lack of dissipation on the density and temperature, we develop a refined energy analysis and establish optimal time decay rates for strong solutions in $\mathbb{R}^d$ with $d \geq 3$. Notably, the density remains uniformly bounded in time, displaying asymptotic behavior fundamentally distinct from that in the NSF system, where the density possesses a dissipative structure via the momentum and temperature equations and exhibits temporal decay.

</details>


### [137] [Ground state solutions of mixed local-nonlolcal equations with Hartree type nonlinearities](https://arxiv.org/abs/2602.02168)
*Gurdev Chand Anthal,Prashanta Garain,Nidhi Nidhi*

Main category: math.AP

TL;DR: The paper proves existence of ground state solutions for mixed local-nonlocal equations with Hartree-type nonlinearities, establishing regularity, Pohožaev identity, and symmetry properties.


<details>
  <summary>Details</summary>
Motivation: To study a class of mixed local-nonlocal equations combining classical and fractional Laplacians with Hartree-type nonlinearities, which arise in various physical contexts and present mathematical challenges due to the combination of different types of operators and nonlocal nonlinearities.

Method: The authors use variational methods to prove existence of ground state solutions. They establish regularity properties for weak solutions, derive a Pohožaev-type identity for general weak solutions, and employ polarization methods to obtain symmetry properties of ground state solutions.

Result: The main result is the existence of ground state solutions for the mixed local-nonlocal equation with Hartree-type nonlinearities. Additionally, the paper establishes regularity properties, a Pohožaev-type identity, and symmetry properties of these ground state solutions.

Conclusion: The paper successfully proves the existence of ground state solutions for mixed local-nonlocal equations with Hartree-type nonlinearities, providing important mathematical tools including regularity analysis, Pohožaev identity, and symmetry results that contribute to the understanding of such equations.

Abstract: We study a class of mixed local-nonlocal equations with Hartree-type nonlinearities of the form \begin{equation}\label{meqnab} -Δu + (-Δ)^s u + u = (I_α* F(u))\,F'(u) \quad \text{in } \mathbb{R}^N, \end{equation} where $N \geq 3$, $s \in (0,1)$, and $F \in C^1(\mathbb{R},\mathbb{R})$ satisfies Berestycki-Lions type assumptions. The equation combines the classical Laplacian with the fractional Laplacian, while the Hartree-type nonlinearity is given by a nonlocal convolution term involving the Riesz potential $I_α$, with $α\in (0,N)$. We prove the existence of ground state solutions. To this end, we establish regularity properties and derive a Pohožaev-type identity for general weak solutions. Moreover, we obtain symmetry properties of ground state solutions via polarization methods.

</details>


### [138] [A note on harmonic polynomials on Heisenberg and Carnot groups](https://arxiv.org/abs/2602.02200)
*Francesco Paolo Maiale*

Main category: math.AP

TL;DR: The paper establishes spherical harmonic decompositions on Heisenberg and Carnot groups, analogous to classical Euclidean spherical harmonics, showing orthogonal decompositions of L² spaces on spheres defined by homogeneous norms.


<details>
  <summary>Details</summary>
Motivation: To extend classical spherical harmonic theory from Euclidean spaces to non-commutative settings like Heisenberg groups and general Carnot groups, establishing analogous orthogonal decompositions for harmonic functions in these geometric contexts.

Method: Uses homogeneous norms (Korányi-Folland norm for Heisenberg groups, canonical homogeneous norms for Carnot groups) to define spheres, studies homogeneous Δ_H-harmonic polynomials, proves orthogonal decompositions via direct sum representations and extends results using fundamental solutions of sub-Laplacians.

Result: Proves L²(S_ρ,σ) decomposes as orthogonal direct sum of finite-dimensional spaces H_m(S_ρ) of spherical harmonics of degree m on Heisenberg groups; establishes unique decomposition of homogeneous polynomials; extends results to general Carnot groups with dense span in L²(S_N,σ_N).

Conclusion: The paper successfully generalizes classical spherical harmonic theory to Heisenberg and Carnot groups, establishing analogous orthogonal decompositions and density results that parallel Euclidean harmonic analysis in these non-commutative geometric settings.

Abstract: In this paper, we consider homogeneous $Δ_H$-harmonic polynomials on the first Heisenberg group $\mathbb H$ and their traces on the unit sphere $S_ρ$ associated with the Korányi--Folland homogeneous norm $ρ$. We prove that $L^2(S_ρ,σ)$ decomposes as the orthogonal Hilbert direct sum of finite-dimensional spaces $H_m(S_ρ)$ of spherical harmonics of degree $m$, in direct analogy with the classical Euclidean spherical harmonic decomposition. We also show that, for the polynomial gauge $η_+^2(z,t)=|z|^2+4t$, every homogeneous polynomial on $\mathbb H$ admits a unique decomposition $$ P_m(\mathbb H) = H_m(\mathbb H)\oplus η_+^2 P_{m-2}(\mathbb H). $$ Finally, we extend the spherical $L^2$-decomposition to general Carnot groups $G$ equipped with a canonical homogeneous norm $N$ associated with a fundamental solution of a fixed sub-Laplacian $Δ_G$. The traces on $S_N$ of homogeneous $Δ_G$-harmonic polynomials of degree $m$ form pairwise orthogonal eigenspaces of the spherical operator on $S_N$, and their span is dense in $L^2(S_N,σ_N)$.

</details>


### [139] [Statistical solutions to the Euler system of gas dynamics](https://arxiv.org/abs/2602.02205)
*Eduard Feireisl*

Main category: math.AP

TL;DR: A framework for constructing statistical solutions to the Euler system using dissipative measure-valued solutions, Bregman divergence minimization, and Markov semigroups.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic approach for constructing statistical solutions to the complete Euler system for compressible perfect fluids, which is challenging due to the complexity of the equations and the need for statistical descriptions in fluid dynamics.

Method: Three main components: 1) Using dissipative measure-valued solutions to the Euler system, 2) A single-step selection procedure based on minimizing Bregman divergence to maximal entropy equilibrium, 3) Constructing a Markov semigroup via push-forward measures.

Result: Proposes a comprehensive platform/framework for building statistical solutions, though specific numerical or analytical results aren't detailed in the abstract.

Conclusion: The paper presents a novel mathematical framework that combines measure-valued solutions, entropy principles, and Markov processes to address the statistical description of compressible fluid flows governed by the Euler equations.

Abstract: We consider the (complete) Euler system describing the motion of a compressible perfect fluid. We propose a platform suitable for constructing the statistical solutions. The main ingredients of our approach include:
  1. The concept of dissipative (measure{valued) solution to the Euler system. 2. A single step selection procedure based on minimizing the Bregman divergence of a
  given solution to the maximal entropy equilibrium. 3. A construction of a Markov semigroup via push forward measures.

</details>


### [140] [On a system of equations arising in meteorology: Well-posedness and data assimilation](https://arxiv.org/abs/2602.02328)
*Eduard Feireisl,Piotr Gwiazda,Agnieszka Świerczewska-Gwiazda*

Main category: math.AP

TL;DR: The paper develops a continuous data assimilation framework for a simplified model derived from 3D compressible Navier-Stokes-Fourier with rotation and temperature gradient, then extends results to the full compressible setting.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous analytical study of data assimilation for complex weather prediction models by working with a simplified limit system that preserves essential physics while being mathematically tractable.

Method: Uses a singular limit of 3D compressible Navier-Stokes-Fourier system with rotation driven by temperature gradient to obtain a reduced 2.5D structure, then applies nudging scheme for data assimilation with relative entropy arguments to extend to full model.

Result: Established well-posedness of global-in-time solutions, compact trajectory attractor, stability and convergence results for nudging scheme on limit system, and extension to full 3D compressible setting via relative entropy arguments.

Conclusion: The approach successfully bridges simplified and complete physical models for data assimilation, providing rigorous mathematical foundation for incorporating observational data into complex weather prediction systems while maintaining physical relevance.

Abstract: Data assimilation plays a crucial role in modern weather prediction, providing a systematic way to incorporate observational data into complex dynamical models. The paper addresses continuous data assimilation for a model arising as a singular limit of the three-dimensional compressible Navier-Stokes-Fourier system with rotation driven by temperature gradient. The limit system preserves the essential physical mechanisms of the original model, while exhibiting a reduced, effectively two-and-a-half-dimensional structure. This simplified framework allows for a rigorous analytical study of the data assimilation process while maintaining a direct physical connection to the full compressible model. We establish well posedness of global-in-time solutions and a compact trajectory attractor, followed by the stability and convergence results for the nudging scheme applied to the limiting system. Finally, we demonstrate how these results can be combined with a relative entropy argument to extend the assimilation framework to the full three-dimensional compressible setting, thereby establishing a rigorous connection between the reduced and physically complete models.

</details>


### [141] [A Priori Estimates for Maximally Subelliptic Quadratic Forms](https://arxiv.org/abs/2602.02441)
*Brian Street*

Main category: math.AP

TL;DR: Proves a priori subelliptic estimates for heat operators near non-characteristic boundaries in maximally subelliptic quadratic forms.


<details>
  <summary>Details</summary>
Motivation: To establish subelliptic estimates for heat operators in boundary value problems, continuing a series on maximally subelliptic boundary value problems.

Method: Analytic proof of a priori subelliptic estimates near non-characteristic boundary points for heat operators associated with maximally subelliptic quadratic forms.

Result: Successfully proves subelliptic estimates for heat operators in this class of boundary value problems.

Conclusion: This work advances the understanding of maximally subelliptic boundary value problems by establishing key estimates for heat operators, contributing to the broader theory.

Abstract: We prove a priori subelliptic estimates, near a non-characteristic boundary point, for the heat operators associated to a wide class of maximally subelliptic quadratic forms. This is the third paper in a series devoted to studying general maximally subelliptic boundary value problems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [142] [An Oscillation-Free Real Fluid Quasi-Conservative Finite Volume Method for Transcritical and Phase-Change Flows](https://arxiv.org/abs/2602.00658)
*Haotong Bai,Wenjia Xie,Yixin Yang,Ping Yi,Mingbo Sun*

Main category: physics.comp-ph

TL;DR: A quasi-conservative finite volume method for real fluids that eliminates spurious pressure oscillations in transcritical and phase-change flows with shock waves by linearizing EoS and evolving frozen parameters.


<details>
  <summary>Details</summary>
Motivation: To address numerical issues in simulating real fluids with shock waves in transcritical and phase-change flows, particularly eliminating spurious pressure oscillations that occur in fully conservative schemes.

Method: Extends the classic five-equation quasi-conservative model to real fluids with arbitrary EoS. Locally linearizes EoS at each grid point/time step, constructs and evolves frozen Grüneisen coefficient Γ and linearization remainder E₀ via advection equations, then reconstructs oscillation-free pressure field and performs thermodynamic re-projection.

Result: Theoretical analysis shows energy conservation error is high-order in smooth regions and determined by entropy increase rate in discontinuities. Numerical tests demonstrate robust simulation of transcritical flows, phase transitions, and shock-interface interactions with minor energy conservation errors.

Conclusion: The RFQC method is both accurate and robust for capturing shock waves and phase transitions in real fluid simulations, effectively eliminating spurious pressure oscillations while maintaining acceptable conservation properties.

Abstract: A new Real Fluid Quasi-Conservative (RFQC) finite volume method is developed to address the numerical simulation of real fluids involving shock waves in transcritical and phase-change flows. To eliminate the spurious pressure oscillations inherent in fully conservative schemes, we extend the classic five-equation quasi-conservative model, originally designed for two-phase flows, to real fluids governed by arbitrary equations of state (EoS). The RFQC method locally linearizes the real fluid EoS at each grid point and time step, constructing and evolving the frozen Grüneisen coefficient $Γ$ and the linearization remainder $E_0$ via two advection equations. At the end of each time step, the evolved $Γ$ and $E_0$ are utilized to reconstruct the oscillation-free pressure field, followed by a thermodynamic re-projection applied to the conserved variables. Theoretical analysis demonstrates that, in smooth regions, the energy conservation error of the RFQC method is a high-order term relative to the spatial reconstruction truncation error. In discontinuous regions, this error is determined by the entropy increase rate, thereby maintaining consistency with the inherent truncation error of shock-capturing methods. A series of numerical tests verifies that the method can robustly simulate complex flow processes with only minor energy conservation errors, including transcritical flows, phase transitions, and shock-interface interactions. The RFQC method is proven to be both accurate and robust in capturing shock waves and phase transitions.

</details>


### [143] [Semi-implicit Lax-Wendroff kinetic scheme for electron-phonon coupling](https://arxiv.org/abs/2602.01220)
*Jiaming Li,Hong Liang,Meng Lian,Chuang Zhang,Jiangrong Xu*

Main category: physics.comp-ph

TL;DR: A semi-implicit Lax-Wendroff scheme for electron-phonon coupling in metals using two-temperature kinetic equations, with time steps not limited by relaxation time or mean free path constraints.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for modeling electron-phonon coupling processes in metals that can handle regimes from ballistic to diffusive transport without being constrained by traditional time step limitations.

Method: Semi-implicit Lax-Wendroff scheme based on two-temperature kinetic equations. Integrates evolution information into numerical modeling, uses finite difference method for reconstructing interfacial distribution functions, and couples particle migration, scattering, and electron-phonon coupling within single time steps.

Result: Numerical tests show the method efficiently captures electron-phonon coupling and heat conduction processes across ballistic to diffusive regimes, with time steps not limited by relaxation time or mean free path constraints.

Conclusion: The method provides a new computational tool for describing electron-phonon coupling and thermal management in microelectronic devices, offering efficient simulation across different transport regimes.

Abstract: A semi-implicit Lax-Wendroff scheme is developed for electron-phonon coupling process in metals based on the two-temperature kinetic equations. The core of this method is to integrate the evolution information of physical equations into the numerical modeling process, which leads to that the time step or cell size is not limited by the relaxation time and mean free path. Specifically, the finite difference method is used to solve the kinetic model again when reconstructing the interfacial distribution function, through which the particle migration, scattering and electron-phonon coupling processes are coupled together within a single time step. Numerical tests demonstrate that this method could efficiently capture electron-phonon coupling or heat conduction processes from the ballistic to diffusive regimes. It provides a new tool for describing electron-phonon coupling or thermal management in microelectronic devices.

</details>


### [144] [A differentiation formula, with application to the two-dimensional Schrödinger equation](https://arxiv.org/abs/1410.5402)
*Alexander Pikovski*

Main category: physics.comp-ph

TL;DR: A method using generalized divided differences for derivative discretization formulas, applied to 2D Schrödinger eigenvalue problems where it outperforms standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard methods for solving the eigenvalue problem of the two-dimensional Schrödinger equation converge very slowly, requiring a more efficient discretization approach for derivatives.

Method: Generalization of divided differences through modification of the dependent variable to obtain discretization formulas for derivatives.

Result: The proposed approach gives accurate results for the 2D Schrödinger eigenvalue problem, outperforming standard methods that converge slowly.

Conclusion: The generalized divided differences method provides an effective discretization technique for derivative approximations, particularly valuable for challenging problems like the 2D Schrödinger equation where conventional methods perform poorly.

Abstract: A method for obtaining discretization formulas for the derivatives of a function is presented, which relies on a generalization of divided differences. These modified divided differences essentially correspond to a change of the dependent variable. This method is applied to the numerical solution of the eigenvalue problem for the two-dimensional Schrödinger equation, where standard methods converge very slowly while the approach proposed here gives accurate results.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [145] [Batch Bayesian optimization of attosecond betatron pulses from laser wakefield acceleration](https://arxiv.org/abs/2601.22794)
*Dominika Maslarova,Albert Hansson,Mufei Luo,Vojtěch Horný,Julien Ferri,Istvan Pusztai,Tünde Fülöp*

Main category: physics.plasm-ph

TL;DR: Using Bayesian optimization to enhance attosecond betatron radiation from laser wakefield acceleration by optimizing plasma density spikes, achieving >10x power improvement.


<details>
  <summary>Details</summary>
Motivation: Laser wakefield acceleration produces femtosecond-scale broadband X-ray betatron radiation suitable for advanced imaging. Recent laser technology advances enable attosecond regimes where increased pulse energy would benefit practical applications.

Method: Numerical simulations combined with batch Bayesian optimization to efficiently explore multi-parameter space and identify optimal plasma density spike configurations.

Result: Identified regime where plasma density spike triggers high-charge electron beam generation, resulting in >10x improvement in on-axis time-averaged power within central time containing half radiated energy compared to reference case without density spike.

Conclusion: Bayesian optimization effectively enhances attosecond betatron radiation sources, demonstrating significant power improvements through optimized plasma density spike configurations.

Abstract: Laser wakefield acceleration can generate a femtosecond-scale broadband X-ray betatron radiation pulse from electrons accelerated by an intense laser pulse in a plasma. The micrometer-scale of the source makes wakefield betatron radiation well-suited for advanced imaging techniques, including diffraction and phase-contrast imaging. Recent progress in laser technology can expand these capabilities into the attosecond regime, where the practical applications would significantly benefit from the increased energy contained within the pulse. Here we use numerical simulations combined with batch Bayesian optimization to enhance the radiation produced by an attosecond betatron source. The method enables an efficient exploration of a multi-parameter space and identifies a regime in which a plasma density spike triggers the generation of a high-charge electron beam. This results in an improvement of more than one order of magnitude in the on-axis time-averaged power within the central time containing half of the radiated energy, compared to the reference case without the density spike.

</details>


### [146] [A predictive formula for the H-mode electron separatrix density: Bridging regression and physics-based models across C-Mod, AUG and JET tokamaks](https://arxiv.org/abs/2601.23140)
*D. Silvagni,O. Grover,A. Stagni,J. W. Hughes,M. A. Miller,B. Lomanowski,L. Balbinot,G. Ciraolo,W. Dekeyser,M. Dunne,L. Frassinetti,C. Giroud,T. Happel,I. Jepu,A. Kallenbach,A. Kirjasuo,A. Kuang,T. Luda,D. Moulton,O. Pan,C. Perez von Thun,T. Puetterich,G. Rubino,S. A. Silburn,H. J. Sun,D. Umezaki,H. Zohm,the ASDEX Upgrade team,JET contributors,the EUROfusion tokamak exploitation team*

Main category: physics.plasm-ph

TL;DR: Researchers developed a predictive formula for separatrix electron density in tokamaks by combining regression analysis of multi-machine data with theoretical two-point model, achieving accurate predictions across current and future devices.


<details>
  <summary>Details</summary>
Motivation: The separatrix electron density is crucial for balancing energy confinement, detachment achievement, and ELM suppression in tokamaks, affecting core-edge integration. Understanding what determines this key parameter is essential for predicting performance in current and future fusion devices.

Method: Created a database of H-mode separatrix density measurements from Alcator C-Mod, ASDEX Upgrade, and JET using consistent analysis methods. Derived regression scaling from engineering parameters, compared with two-point model predictions, and combined both approaches into a predictive formula.

Result: Found remarkable agreement between regression scaling and two-point model predictions. The combined formula estimates separatrix density within factor of 1.5 across three machines and provides projections for next-step devices (ITER, SPARC, DTT, JT-60SA, COMPASS-U) consistent with SOLPS simulations.

Conclusion: The study successfully developed a predictive formula for separatrix electron density that bridges empirical data and theoretical modeling, providing reliable projections for future fusion devices and advancing understanding of core-edge integration in tokamaks.

Abstract: The electron density at the separatrix ($n_{e,\mathrm{sep}}$) plays a central role in balancing energy confinement, detachment achievement, and ELM suppression in tokamaks, thereby influencing core-edge integration. To study what determines this key parameter, a database of H-mode separatrix density measurements from Alcator C-Mod, ASDEX Upgrade, and JET tokamaks has been assembled using a consistent analysis method across all devices. This dataset is used to derive a regression scaling expression based solely on engineering parameters, and the results are compared to predictions from the two-point model. The agreement found is remarkable: both the regression and model provide similar parameter dependencies and tokamak-specific multiplicative constants. Building on this agreement, a fully predictive formula that combines the regression dependencies and the two-point model multiplicative constant is proposed. This formula is able to estimate $n_{e,\mathrm{sep}}$ across the three machines within a factor of 1.5, and provides projections to next-step devices (ITER, SPARC, DTT, JT-60SA and COMPASS-U) that are in agreement with available SOLPS simulations.

</details>


### [147] [Time-Resolved Interferometric Measurements of Plasma Density Evolution in Laser-Driven Capacitor-Coil Targets](https://arxiv.org/abs/2601.23271)
*Yang Zhang,Ryo Omura,Rinya Akematsu,King Fai Farley Law,Brandon K. Russell,Geoffrey Pomraning,Kian Orr,Kai Kimura,Muhammad Fauzan Syahbana,Yuga Karaki,Hiroki Matsubara,Ryuya Yamada,Jinyuan Dun,Ryunosuke Takizawa,Yasunobu Arikawa,Tatiana Pikuz,Yuji Fukuda,Lan Gao,Hantao Ji,Shinsuke Fujioka*

Main category: physics.plasm-ph

TL;DR: Researchers used interferometry to measure plasma density evolution in laser-driven capacitor-coil targets, revealing two distinct plasma sources that load the coil region.


<details>
  <summary>Details</summary>
Motivation: While laser-driven capacitor-coil targets are widely used for generating strong magnetic fields in magnetized high-energy-density plasma experiments, there's limited direct, time-resolved measurements of the plasma density surrounding the coil, which can influence physical processes and interact with secondary targets or external plasmas.

Method: The researchers conducted interferometric measurements of plasma density evolution in laser-driven capacitor-coil targets irradiated by the University of Osaka LFEX laser, producing two-dimensional electron density maps.

Result: The measurements revealed two distinct plasma sources loading the coil region: plasma generated in the coil itself and plasma produced by laser ablation of the target plates.

Conclusion: These results provide quantitative information on plasma loading and evolution in capacitor-coil targets, which is directly relevant to the design and modeling of magnetized high-energy-density plasma experiments.

Abstract: Laser-driven capacitor-coil targets provide a compact platform for generating strong magnetic fields and are widely used in magnetized high-energy-density plasma experiments. In addition to magnetic-field generation, these targets also produce plasma in the coil region, which can influence the subject physical processes, interact with secondary targets or external plasmas in their applications. However, direct, time-resolved measurements of the plasma density surrounding the coil remain limited. Here, we report interferometric measurements of the plasma density evolution in laser-driven capacitor-coil targets irradiated by the University of Osaka LFEX laser. Two-dimensional electron density maps reveal two distinct plasma sources loading the coil region: plasma generated in the coil itself and plasma produced by laser ablation of the target plates. These results provide quantitative information on plasma loading and evolution in capacitor-coil targets and are directly relevant to the design and modeling of magnetized high-energy-density plasma experiments.

</details>


### [148] [Batch Bayesian optimization of attosecond betatron pulses from laser wakefield acceleration](https://arxiv.org/abs/2601.22794)
*Dominika Maslarova,Albert Hansson,Mufei Luo,Vojtěch Horný,Julien Ferri,Istvan Pusztai,Tünde Fülöp*

Main category: physics.plasm-ph

TL;DR: Using Bayesian optimization to enhance attosecond betatron radiation from laser wakefield acceleration by optimizing plasma density profiles, achieving >10x power improvement.


<details>
  <summary>Details</summary>
Motivation: Laser wakefield acceleration produces femtosecond-scale broadband X-ray betatron radiation suitable for advanced imaging. Recent laser technology advances enable attosecond regimes where increased pulse energy would benefit practical applications.

Method: Numerical simulations combined with batch Bayesian optimization to efficiently explore multi-parameter space and identify optimal plasma density profiles. Specifically uses a plasma density spike to trigger high-charge electron beam generation.

Result: Identified a regime where plasma density spike triggers high-charge electron beam generation, resulting in more than one order of magnitude improvement in on-axis time-averaged power within the central time containing half of the radiated energy compared to reference case without density spike.

Conclusion: Bayesian optimization combined with numerical simulations effectively enhances attosecond betatron radiation sources, enabling significant power improvements through optimized plasma density profiles for advanced imaging applications.

Abstract: Laser wakefield acceleration can generate a femtosecond-scale broadband X-ray betatron radiation pulse from electrons accelerated by an intense laser pulse in a plasma. The micrometer-scale of the source makes wakefield betatron radiation well-suited for advanced imaging techniques, including diffraction and phase-contrast imaging. Recent progress in laser technology can expand these capabilities into the attosecond regime, where the practical applications would significantly benefit from the increased energy contained within the pulse. Here we use numerical simulations combined with batch Bayesian optimization to enhance the radiation produced by an attosecond betatron source. The method enables an efficient exploration of a multi-parameter space and identifies a regime in which a plasma density spike triggers the generation of a high-charge electron beam. This results in an improvement of more than one order of magnitude in the on-axis time-averaged power within the central time containing half of the radiated energy, compared to the reference case without the density spike.

</details>


### [149] [A predictive formula for the H-mode electron separatrix density: Bridging regression and physics-based models across C-Mod, AUG and JET tokamaks](https://arxiv.org/abs/2601.23140)
*D. Silvagni,O. Grover,A. Stagni,J. W. Hughes,M. A. Miller,B. Lomanowski,L. Balbinot,G. Ciraolo,W. Dekeyser,M. Dunne,L. Frassinetti,C. Giroud,T. Happel,I. Jepu,A. Kallenbach,A. Kirjasuo,A. Kuang,T. Luda,D. Moulton,O. Pan,C. Perez von Thun,T. Puetterich,G. Rubino,S. A. Silburn,H. J. Sun,D. Umezaki,H. Zohm,the ASDEX Upgrade team,JET contributors,the EUROfusion tokamak exploitation team*

Main category: physics.plasm-ph

TL;DR: Researchers developed a predictive formula for separatrix electron density in tokamaks by combining regression analysis from multi-machine data with the two-point model, achieving accurate predictions across current and future devices.


<details>
  <summary>Details</summary>
Motivation: The separatrix electron density is crucial for balancing energy confinement, detachment achievement, and ELM suppression in tokamaks, affecting core-edge integration. Understanding what determines this key parameter is essential for optimizing tokamak performance and predicting behavior in next-step devices.

Method: Assembled a database of H-mode separatrix density measurements from Alcator C-Mod, ASDEX Upgrade, and JET using consistent analysis methods. Derived regression scaling based on engineering parameters and compared results to predictions from the two-point model.

Result: Found remarkable agreement between regression and two-point model predictions, with similar parameter dependencies and tokamak-specific multiplicative constants. Developed a predictive formula combining regression dependencies with two-point model constants that estimates separatrix density within factor of 1.5 across three machines.

Conclusion: The proposed formula successfully predicts separatrix density across current tokamaks and provides projections for next-step devices (ITER, SPARC, DTT, JT-60SA, COMPASS-U) that align with available SOLPS simulations, offering a valuable tool for tokamak design and operation.

Abstract: The electron density at the separatrix ($n_{e,\mathrm{sep}}$) plays a central role in balancing energy confinement, detachment achievement, and ELM suppression in tokamaks, thereby influencing core-edge integration. To study what determines this key parameter, a database of H-mode separatrix density measurements from Alcator C-Mod, ASDEX Upgrade, and JET tokamaks has been assembled using a consistent analysis method across all devices. This dataset is used to derive a regression scaling expression based solely on engineering parameters, and the results are compared to predictions from the two-point model. The agreement found is remarkable: both the regression and model provide similar parameter dependencies and tokamak-specific multiplicative constants. Building on this agreement, a fully predictive formula that combines the regression dependencies and the two-point model multiplicative constant is proposed. This formula is able to estimate $n_{e,\mathrm{sep}}$ across the three machines within a factor of 1.5, and provides projections to next-step devices (ITER, SPARC, DTT, JT-60SA and COMPASS-U) that are in agreement with available SOLPS simulations.

</details>


### [150] [Time-Resolved Interferometric Measurements of Plasma Density Evolution in Laser-Driven Capacitor-Coil Targets](https://arxiv.org/abs/2601.23271)
*Yang Zhang,Ryo Omura,Rinya Akematsu,King Fai Farley Law,Brandon K. Russell,Geoffrey Pomraning,Kian Orr,Kai Kimura,Muhammad Fauzan Syahbana,Yuga Karaki,Hiroki Matsubara,Ryuya Yamada,Jinyuan Dun,Ryunosuke Takizawa,Yasunobu Arikawa,Tatiana Pikuz,Yuji Fukuda,Lan Gao,Hantao Ji,Shinsuke Fujioka*

Main category: physics.plasm-ph

TL;DR: Time-resolved interferometric measurements reveal two distinct plasma sources in laser-driven capacitor-coil targets, providing quantitative data on plasma loading for magnetized HEDP experiments.


<details>
  <summary>Details</summary>
Motivation: While laser-driven capacitor-coil targets are widely used for magnetic field generation in magnetized high-energy-density plasma experiments, there's limited direct, time-resolved measurement of plasma density in the coil region, which can significantly influence physical processes and interactions with secondary targets.

Method: Used interferometric measurements of plasma density evolution in laser-driven capacitor-coil targets irradiated by the University of Osaka LFEX laser, producing two-dimensional electron density maps.

Result: Revealed two distinct plasma sources loading the coil region: plasma generated in the coil itself and plasma produced by laser ablation of the target plates, providing quantitative information on plasma loading and evolution.

Conclusion: These results provide crucial quantitative data on plasma loading in capacitor-coil targets, directly relevant to the design and modeling of magnetized high-energy-density plasma experiments.

Abstract: Laser-driven capacitor-coil targets provide a compact platform for generating strong magnetic fields and are widely used in magnetized high-energy-density plasma experiments. In addition to magnetic-field generation, these targets also produce plasma in the coil region, which can influence the subject physical processes, interact with secondary targets or external plasmas in their applications. However, direct, time-resolved measurements of the plasma density surrounding the coil remain limited. Here, we report interferometric measurements of the plasma density evolution in laser-driven capacitor-coil targets irradiated by the University of Osaka LFEX laser. Two-dimensional electron density maps reveal two distinct plasma sources loading the coil region: plasma generated in the coil itself and plasma produced by laser ablation of the target plates. These results provide quantitative information on plasma loading and evolution in capacitor-coil targets and are directly relevant to the design and modeling of magnetized high-energy-density plasma experiments.

</details>


### [151] [Self ordering to imposed ordering of dust -- a continuous spatial phase transition experiment in MDPX](https://arxiv.org/abs/2602.00312)
*Siddharth Bachoti,Saikat Chakraborty Thakur,Rahul Banka,Cameron Royer,Edward Thomas*

Main category: physics.plasm-ph

TL;DR: Dust particles transition from hexagonal Coulomb crystal to 4-fold symmetry aligned with mesh geometry under strong magnetic field, revealing "imposed ordering" phenomenon.


<details>
  <summary>Details</summary>
Motivation: To investigate the transition phenomenon where dust particles align with conducting mesh geometry under strong magnetic fields, moving from natural hexagonal ordering to imposed 4-fold symmetry.

Method: Conducted transition experiments in Magnetized Dusty Plasma eXperiment (MDPX), starting with classical 2D Coulomb crystal in unmagnetized plasma, then applying magnetic field perpendicular to mesh to observe dust transition to mesh-aligned flow.

Result: Dust transitions from 6-fold hexagonal symmetry to 4-fold symmetry matching mesh geometry, with identified critical magnetic field value for onset of imposed ordering; elongated electric potential structures from mesh drive dust motion.

Conclusion: Beyond certain magnetization, mesh potential structures impose 4-fold ordering on dust particles, demonstrating transition from self-ordering to imposed ordering with quantifiable critical magnetic field threshold.

Abstract: Previous experiments conducted in the Magnetized Dusty Plasma eXperiment (MDPX) revealed an intriguing phenomenon first referred to as imposed ordering. This occurs when micron-sized dust particles become aligned with the geometry of a conducting mesh placed above the dust (at a distance much larger than the plasma Debye length or the ion-neutral or electron-neutral mean free paths) in the presence of a strong magnetic field perpendicular to the mesh. In this work, results of a transition experiment are presented wherein starting from a classical two-dimensional Coulomb crystal with hexagonal symmetry in an unmagnetized plasma $(B = 0\,T)$, dust transitions to a state in which it flows along the geometry of a conducting mesh placed above it, mapping out the 4-fold symmetry of the boundary condition. It is hypothesized that beyond a certain magnetization, elongated electric potential structures emanating from the mesh drive the dust motion to reflect the mesh morphology, transitioning from a 6-fold self ordering to 4-fold imposed ordering. The various dust phases are quantified and a critical value of magnetic field is identified in the transition experiment indicating the onset of imposed ordering.

</details>


### [152] [Drift-kinetic PIC model for simulations of longitudinal plasma confinement in mirror traps](https://arxiv.org/abs/2602.00552)
*V. V. Glinskiy,I. V. Timofeev,V. V. Prikhodko*

Main category: physics.plasm-ph

TL;DR: The paper presents a 1D2V electrostatic PIC model for simulating classical longitudinal plasma transport in open traps, featuring energy/charge conservation, Coulomb collisions, and correct modeling of near-wall electric potential jumps without requiring strict quasi-neutrality.


<details>
  <summary>Details</summary>
Motivation: To develop a more accurate simulation model for classical longitudinal plasma transport in axially symmetric open traps that can correctly handle boundary conditions, Coulomb collisions, and self-consistent electron kinetics, particularly for near-wall phenomena like Debye sheaths and ambipolar potential profiles.

Method: A 1D2V (one-dimensional in space, two-dimensional in velocity) electrostatic Particle-In-Cell (PIC) model with drift-kinetic description of all particle types. The model generalizes semi-implicit PIC method with exact conservation of energy and charge to collisional plasma, adapts it to boundary conditions on perfectly conducting walls with floating potential, and implements Coulomb collisions.

Result: The model successfully reproduces: 1) temperature relaxation in two-component plasma matching analytical theory, 2) correct ambipolar electric potential profiles up to walls, 3) Debye sheath theory and Bohm criterion, and 4) shows 15% differences in electron temperature, potential, and density compared to hybrid code MIDAS when self-consistently considering electron kinetics in expanders.

Conclusion: The developed 1D2V electrostatic PIC model effectively simulates classical longitudinal plasma transport in open traps, correctly handling boundary conditions, collisions, and near-wall phenomena while maintaining computational efficiency through large grid steps. Self-consistent electron kinetics significantly impacts plasma parameters compared to hybrid approaches.

Abstract: The paper presents a 1D2V electrostatic PIC model with a drift-kinetic description of all particle types aiming at simulating classical longitudinal plasma transport in axially symmetric open traps. The model generalizes the semi-implicit particle-in-cell method with exact conservation of energy and charge to the case of collisional plasma and adapts it to boundary conditions on perfectly conducting walls with a floating potential. Implementation of Coulomb collisions is tested on the problem of temperature relaxation in a two-component plasma and demonstrates good agreement with the analytical theory. Since quasi-neutrality of plasma is not strictly determined, the model is able to correctly reproduce the ambipolar electric potential profile up to the walls. At the same time, the main advantage of implicit PIC simulations - the ability to use large grid steps, many times larger than the Debye radius - does not prevent the correct modeling of the near-wall electric potential jump. The model satisfactorily reproduces the known results of the Debye sheath theory and the Bohm criterion. A comparison of stationary plasma profiles formed in a mirror trap in the presence of a constant particle source with the results of simulations using the hybrid code MIDAS showed that self-consistent consideration of electron kinetics in expanders leads to noticeable (at the level of 15 %) differences in the electron temperature, potential, and density of the confined plasma.

</details>


### [153] [von Neumann entropy of phase space structures in gyrokinetic plasma turbulence](https://arxiv.org/abs/2602.00600)
*Go Yatomi,Motoki Nakata*

Main category: physics.plasm-ph

TL;DR: SVD-based von Neumann entropy quantifies velocity-space complexity in gyrokinetic turbulence, showing wavenumber-dependent phase mixing with FLR effects at k⊥ρti∼1 and parallel Landau damping at higher k.


<details>
  <summary>Details</summary>
Motivation: Need a compact, basis-independent measure to quantify kinetic complexity and map wavenumber dependence of phase-mixing processes in gyrokinetic turbulence.

Method: Combine SVD with information-theoretic von Neumann entropy applied to nonlinear flux-tube simulations of ion distribution function; analyze velocity-space structure via Hermite/Laguerre decompositions.

Result: vNE increases across k⊥ρti∼1 due to FLR phase mixing; systematic vNE increase correlates with Hermite spectrum broadening, indicating enhanced parallel phase mixing (Landau resonance) as primary mechanism.

Conclusion: SVD-based vNE provides basis-independent measure of kinetic complexity and enables global mapping of wavenumber-dependent phase-mixing processes in gyrokinetic turbulence.

Abstract: We introduce a data-driven diagnostic that combines the singular value decomposition (SVD) with an information-theoretic entropy to quantify the phase-space complexity of perturbed distribution functions in gyrokinetic turbulence. Applying this framework to nonlinear flux-tube simulations that solve the time evolution of the ion distribution function represented by Fourier modes with the wavenumber for real space, we define the von Neumann entropy (vNE) to analyze the velocity-space structure. A global survey in the wavenumber space reveals a wavenumber-dependent variation of the vNE in velocity-space structure: the vNE remains low at low wavenumber but increases across $k_\perpρ_{t\mathrm{i}}\sim 1$. Hermite/Laguerre decompositions revealed that the finite Larmor radius (FLR) phase mixing in the perpendicular (magnetic-moment) direction is active. Simultaneously, the systematic increase in vNE for $k_\perpρ_{t\mathrm{i}}$ correlates with the broadening of the Hermite spectrum, suggesting enhanced parallel phase mixing (Landau resonance) as the primary mechanism for the observed wave number dependence. These results demonstrate that the SVD-based vNE provides a compact measure of kinetic complexity without assuming a predefined basis and enables a global mapping of its wavenumber dependence of phase-mixing processes in gyrokinetic turbulence.

</details>


### [154] [Linear Magnetohydrodynamic Waves in a Magneto-Lattice: A Unified Theoretical Framework and Numerical Validation](https://arxiv.org/abs/2602.00649)
*Shiyu Sun,Peifeng Fan,Yulei Wang,Qiang Chen,Xingkai Li,Weihua Wang*

Main category: physics.plasm-ph

TL;DR: Study of MHD wave propagation in periodic magnetic fields (magneto-lattices) showing bandgaps, cutoff phenomena, and Alfvén wave splitting.


<details>
  <summary>Details</summary>
Motivation: To understand how periodic magnetic field structures affect MHD wave propagation and enable manipulation of waves in structured plasmas.

Method: Used plane wave expansion (PWE) method to establish two types of central equations in terms of (ρ,B,v) and perturbation displacement ξ, validated with numerical examples.

Result: Identified intrinsic frequency bandgaps and cutoff phenomena; bandgap width increases with magnetic modulation ratio Bm; periodicity causes Alfvén waves to split into multiple branches.

Conclusion: Periodic magnetic fields enable manipulation of MHD waves through bandgap engineering and mode splitting, offering new insights for structured plasma applications.

Abstract: We present a systematic theoretical and numerical investigation of the propagation properties of linear magnetohydrodynamic (MHD) waves in a spatially periodic magnetic field, referred to as a magneto-lattice. Two types of central equations, expressed in terms of $\left(ρ,\boldsymbol{B},\boldsymbol{v}\right)$ (where $ρ$ is perturbed mass density, $\boldsymbol{B}$ is perturbed magnetic field, and $\boldsymbol{v}$ is perturbed velocity) and the perturbation displacement $\boldsymbolξ$, are established using the plane wave expansion (PWE) method. The validity of both equations is demonstrated through two numerical examples. This framework enables the identification of intrinsic frequency bandgaps and cutoff phenomena within the system. Our numerical results show that the bandgap width increases with the magnetic modulation ratio $B_{m}$, leading to the suppression of specific MHD wave modes. Furthermore, the periodicity of the magnetic field induces the splitting of Alfvén waves into multiple branches\textemdash a phenomenon absent in uniform plasmas. These findings provide new insights for manipulating MHD waves in a crystalline lattice framework of structured plasmas.

</details>


### [155] [Irreversibility and Entropy Exclusion in Collisionless Plasmas](https://arxiv.org/abs/2602.01314)
*Alexander G. Tevzadze*

Main category: physics.plasm-ph

TL;DR: Non-conservative moment closures in collisionless plasmas generate residual entropy from irreversible information loss, creating statistical realizability constraints that exclude certain macroscopic states regardless of dynamical stability.


<details>
  <summary>Details</summary>
Motivation: To understand entropy production in reduced descriptions of collisionless plasmas and establish statistical realizability as an organizing principle for macroscopic plasma states.

Method: Introduce a closure-dependent entropy hierarchy, analyze non-conservative vs invariant moment closures, evaluate entropy production in second-order moment closures for anisotropic plasmas, and identify contributions from transport and magnetic field inhomogeneity.

Result: Non-conservative closures generate residual entropy from irreversible information loss, creating monotonic entropy growth that imposes statistical realizability constraints excluding certain phase space regions. For anisotropic plasmas, the resulting entropy exclusion boundary aligns with observed space plasma anisotropy distributions.

Conclusion: Statistical realizability emerges as a fundamental organizing principle for reduced collisionless plasma descriptions, providing constraints on macroscopic states independent of dynamical stability considerations.

Abstract: We examine entropy production in reduced descriptions of collisionless plasmas. Introducing a closure dependent entropy hierarchy, we show that non-conservative moment closures generate a residual entropy associated with irreversible information loss, whereas invariant closures remain reversible. The monotonic growth of this residual entropy imposes a statistical realizability constraint on macroscopic plasma states, excluding regions of phase space independent of dynamical stability. For anisotropic plasmas, we evaluate entropy production within a second-order moment closure, identifying contributions from transport and magnetic field inhomogeneity. The resulting entropy exclusion boundary is broadly consistent with observed anisotropy distributions in space plasmas. Statistical realizability thus emerges as an organizing principle for reduced collisionless plasma descriptions.

</details>


### [156] [A statistical theory of electronic degrees of freedom in wave packet molecular dynamics](https://arxiv.org/abs/2602.02088)
*Daniel Plummer,Pontus Svensson,Wiktor Jasniak,Patrick Hollebon,Sam M. Vinko,Gianluca Gregori*

Main category: physics.plasm-ph

TL;DR: Statistical distributions derived for wave packet widths in molecular dynamics models, validated against warm dense matter data to guide parameter constraints.


<details>
  <summary>Details</summary>
Motivation: To provide theoretical understanding and practical guidance for constraining the confining potential parameter in wave packet molecular dynamics models, which is currently an empirical parameter.

Method: Developed a theoretical framework for statistical distributions of Gaussian wavepacket widths in both isotropic and anisotropic formulations, then validated against molecular dynamics data under warm dense matter conditions.

Result: The derived distribution functions show good agreement with molecular dynamics data, providing practical guidance for constraining the confining potential parameter.

Conclusion: The statistical distributions for wave packet degrees of freedom successfully guide parameter constraints in molecular dynamics models and influence effective Coulomb interactions in warm dense matter systems.

Abstract: We derive statistical distributions for the degrees of freedom in wave packet molecular dynamics models. Specifically, a theory is developed for the width distributions of Gaussian wavepackets in both isotropic and anisotropic formulations. The resulting distribution functions show good agreement with molecular dynamics data under warm dense matter conditions, providing practical guidance for constraining the confining potential, an empirical parameter in the model. We also discuss how these distributions influence the resulting effective Coulomb interactions.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [157] [Mixing times for the stochastic $p$-Laplace equation](https://arxiv.org/abs/2602.00853)
*Gerardo Barrera,Jonas M. Tölle*

Main category: math.PR

TL;DR: This paper provides a comprehensive overview and quantitative analysis of long-time behavior for stochastic p-Laplace equations with additive Wiener noise, including explicit mixing time estimates and asymptotic results.


<details>
  <summary>Details</summary>
Motivation: To systematically review and synthesize existing quantitative results on the long-time behavior of stochastic p-Laplace equations, which are important nonlinear stochastic partial differential equations with applications in various fields.

Method: The authors conduct a comprehensive literature review, summarize existing results in tabular form, and derive explicit quantitative upper and lower bounds for ε-mixing times of stochastic p-Laplace equations for p>1.

Result: The paper provides explicit quantitative estimates for mixing times and summarizes mixing time asymptotics in a table, offering a clear overview of the current state of knowledge in this area.

Conclusion: This work consolidates existing quantitative results on stochastic p-Laplace equations, providing valuable reference material and explicit mixing time estimates that advance understanding of their long-time behavior.

Abstract: We give an overview on existing quantitative results on long-time behavior of the stochastic $p$-Laplace equation with additive Wiener noise, $p>1$. We summarize the existing results in a table. We give explicit quantitative upper and lower estimates for the $\varepsilon$-mixing times of the stochastic $p$-Laplace equations for $p>1$. We summarize the mixing time asymptotics in a table.

</details>


### [158] [Rigorous derivation of the mean-field limit for the signal-dependent Keller-Segel system](https://arxiv.org/abs/2602.01138)
*Jinhuan Wang,Keyu Li,Hui Huang*

Main category: math.PR

TL;DR: Derivation of 2D Keller-Segel system from stochastic particle model with improved algebraic convergence rate.


<details>
  <summary>Details</summary>
Motivation: To rigorously connect microscopic stochastic particle models with macroscopic Keller-Segel type equations, improving upon existing logarithmic convergence rates to achieve algebraic convergence.

Method: 1) Use stopping times to prove convergence of interacting particle system to mean-field limit equations in probability under algebraic scaling. 2) Apply relative-entropy method to obtain strong L¹ propagation of chaos and establish algebraic convergence rate.

Result: Successfully derived 2D Keller-Segel system with signal-dependent sensitivity from stochastic particle model. Achieved algebraic convergence rate, improving upon previous logarithmic scaling results.

Conclusion: The paper establishes a rigorous connection between microscopic stochastic particle systems and macroscopic Keller-Segel equations with improved convergence rates, providing stronger mathematical foundation for these biological models.

Abstract: We rigorously derive a two-dimensional Keller-Segel type system with signal-dependent sensitivity from a stochastic interacting particle model. By employing suitably defined stopping times, we prove that the convergence of the interacting particle system towards the corresponding mean-field limit equations in probability under an algebraic scaling regime which improves upon existing results with logarithmic scaling. Building on this, we apply the relative-entropy method to obtain strong $L^1$ propagation of chaos, and establish an algebraic convergence rate.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [159] [Maximal regularity for evolution equations with critical singular perturbations](https://arxiv.org/abs/2602.00895)
*Esmée Theewis,Mark Veraar*

Main category: math.FA

TL;DR: The paper studies perturbations of operators with maximal L^p-regularity by time-dependent unbounded operators satisfying critical L^q-integrability, establishing two main results: endpoint maximal regularity and weighted theory for mixed-scale perturbations.


<details>
  <summary>Details</summary>
Motivation: To extend maximal L^p-regularity theory to perturbations by time-dependent unbounded operators with critical integrability conditions, motivated by applications in stochastic PDEs and large deviations theory.

Method: Analyzes perturbations of operators A with maximal L^p-regularity by time-dependent operators B that are unbounded and satisfy critical L^q-integrability in time. Uses functional analytic techniques to handle endpoint cases and develops weighted maximal regularity theory for mixed-scale perturbations.

Result: Two main results: (1) Proves maximal L^p-regularity for the critical endpoint case, generalizing Prüss and Schnaubelt (2001); (2) Develops a weighted maximal regularity theory for mixed-scale perturbations relevant to linearized skeleton equations in large deviations theory for stochastic PDEs.

Conclusion: The paper successfully extends maximal regularity theory to handle critical perturbations by time-dependent unbounded operators, providing tools applicable to stochastic PDEs and large deviations theory, particularly for linearized skeleton equations.

Abstract: Assuming $A$ has maximal $L^p$-regularity, this paper investigates perturbations of $A$ by time-dependent operators $B$ that are unbounded and satisfy a critical $L^q$-integrability condition in time. We establish two main results. The first proves maximal $L^p$-regularity for the critical endpoint case, generalizing previous work by Prüss and Schnaubelt (2001). The second develops a weighted maximal regularity theory for mixed-scale perturbations, motivated by the linearized skeleton equations appearing in large deviations theory for stochastic PDEs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [160] [From Block Diagrams to Bloch Spheres: Graphical Quantum Circuit Simulation in LabVIEW](https://arxiv.org/abs/2602.00643)
*Murtaza Vefadar*

Main category: quant-ph

TL;DR: QuVI is a visual quantum circuit toolkit built in LabVIEW that uses dataflow programming to make quantum computing more accessible to engineers and students by providing intuitive graphical representation of quantum circuits.


<details>
  <summary>Details</summary>
Motivation: There's a need for more accessible quantum simulation tools that bridge the gap between abstract linear algebra and practical implementation. Text-based frameworks like Qiskit and Cirq have a steep learning curve for engineers accustomed to graphical system design.

Method: Developed QuVI (Quantum Virtual Instrument) as an open-source quantum circuit toolkit natively within NI LabVIEW environment. Leverages LabVIEW's "dataflow" paradigm where wires represent data and nodes represent operations, creating a visual analog to standard quantum circuit notation. Enables seamless integration of classical control structures like loops and conditionals.

Result: Demonstrated capabilities through construction and visualization of fundamental quantum algorithms, verifying results against theoretical predictions. Successfully translates "Block Diagrams" directly into quantum state evolutions ("Bloch Spheres").

Conclusion: QuVI offers educators and researchers a powerful platform for prototyping quantum logic within the graphical engineering workspace, making quantum computing more accessible to those familiar with visual programming environments.

Abstract: As quantum computing transitions from theoretical physics to engineering applications, there is a growing need for accessible simulation tools that bridge the gap between abstract linear algebra and practical implementation. While text-based frameworks (like Qiskit or Cirq) are standard, they often present a steep learning curve for students and engineers accustomed to graphical system design. This paper introduces QuVI (Quantum Virtual Instrument), an open-source quantum circuit toolkit developed natively within the NI LabVIEW environment. Moving beyond initial proof-of-concept models, QuVI establishes a robust framework that leverages LabVIEW's "dataflow" paradigm-where wires represent data and nodes represent operations-to provide an intuitive, visual analog to standard quantum circuit notation while enabling the seamless integration of classical control structures like loops and conditionals. The toolkit's capabilities are demonstrated through the construction and visualization of fundamental quantum algorithms, verifying results against theoretical predictions. By translating "Block Diagrams" directly into quantum state evolutions ("Bloch Spheres"), QuVI offers educators and researchers a powerful platform for prototyping quantum logic without leaving the graphical engineering workspace.

</details>


### [161] [Methods for non-variational heuristic quantum optimisation](https://arxiv.org/abs/2602.01353)
*Stuart Ferguson,Petros Wallden*

Main category: quant-ph

TL;DR: Quantum-enhanced optimization algorithms (QeSA and QePT) using MCMC techniques show superior scaling over classical methods for hard optimization problems, offering noise resilience and parallel execution capabilities for near-term quantum devices.


<details>
  <summary>Details</summary>
Motivation: Current quantum optimization research is dominated by variational approaches, leaving alternative methods underexplored. There's a need for noise-resilient quantum algorithms that can leverage near-term quantum devices for solving large-scale optimization problems.

Method: Developed hybrid quantum-classical optimization heuristics based on Markov Chain Monte Carlo (MCMC) techniques, specifically Quantum-enhanced Simulated Annealing (QeSA) and Quantum-enhanced Parallel Tempering (QePT), which forgo the variational framework.

Result: Validated the algorithms on hard Sherrington-Kirkpatrick instances and demonstrated superior scaling over classical benchmarks. The algorithms show inherent noise robustness and support parallel execution across quantum and classical resources.

Conclusion: These MCMC-based quantum optimization algorithms offer a scalable and competitive route for solving large-scale optimization problems with near-term quantum devices, providing an alternative to variational approaches.

Abstract: Optimisation plays a central role in a wide range of scientific and industrial applications, and quantum computing has been widely proposed as a means to achieve computational advantages in this domain. To date, research into the design of noise-resilient quantum algorithms has been dominated by variational approaches, while alternatives remain relatively unexplored. In this work, we introduce a novel class of quantum optimisation heuristics that forgo this variational framework in favour of a hybrid quantum-classical approach built upon Markov Chain Monte Carlo (MCMC) techniques. We introduce Quantum-enhanced Simulated Annealing (QeSA) and Quantum-enhanced Parallel Tempering (QePT), before validating these heuristics on hard Sherrington-Kirkpatrick instances and demonstrate their superior scaling over classical benchmarks. These algorithms are expected to exhibit inherent robustness to noise and support parallel execution across both quantum and classical resources with only classical communication required. As such, they offer a scalable and potentially competitive route toward solving large-scale optimisation problems with near-term quantum devices.

</details>


### [162] [Sampling two-dimensional isometric tensor network states](https://arxiv.org/abs/2602.02245)
*Alec Dektor,Eugene Dumitrescu,Chao Yang*

Main category: quant-ph

TL;DR: Novel sampling algorithms for 2D isometric tensor network states that extend 1D tensor network sampling methods


<details>
  <summary>Details</summary>
Motivation: Sampling quantum probability distributions is crucial for quantum advantage experiments and quantum Monte Carlo algorithms, but existing methods are well-established only for 1D tensor networks

Method: Two algorithms for 2D isometric tensor network states: 1) independent sampling for single configurations with probabilities, and 2) greedy search for K high-probability configurations

Result: Numerical results demonstrate effectiveness across quantum states with varying entanglement and system sizes

Conclusion: The paper introduces practical sampling algorithms for 2D tensor networks, extending capabilities beyond 1D systems for quantum computational tasks

Abstract: Sampling a quantum systems underlying probability distributions is an important computational task, e.g., for quantum advantage experiments and quantum Monte Carlo algorithms. Tensor networks are an invaluable tool for efficiently representing states of large quantum systems with limited entanglement. Algorithms for sampling one-dimensional (1D) tensor networks are well-established and utilized in several 1D tensor network methods. In this paper we introduce two novel sampling algorithms for two-dimensional (2D) isometric tensor network states (isoTNS) that can be viewed as extensions of algorithms for 1D tensor networks. The first algorithm we propose performs independent sampling and yields a single configuration together with its associated probability. The second algorithm employs a greedy search strategy to identify K high-probability configurations and their corresponding probabilities. Numerical results demonstrate the effectiveness of these algorithms across quantum states with varying entanglement and system size.

</details>


### [163] [Analysis of Hessian Scaling for Local and Global Costs in Variational Quantum Algorithm](https://arxiv.org/abs/2602.00783)
*Yihan Huang,Yangshuai Wang*

Main category: quant-ph

TL;DR: Hessian entry resolvability in VQAs has two scaling regimes: exponential shot requirements for global objectives vs polynomial for local objectives with bounded-depth circuits.


<details>
  <summary>Details</summary>
Motivation: To understand when curvature-based optimization is feasible for Variational Quantum Algorithms, since vanishing gradients (barren plateaus) don't necessarily mean the Hessian is statistically resolvable against shot noise.

Method: Leverage exact second-order parameter-shift rules to derive structural representation of Hessian entries as covariance quadratic forms of shifted cost evaluations. Analyze variance scaling for different objective types and circuit depths.

Result: Global objectives require exponentially many shots (O(e^{αn})) to resolve Hessian entries, while termwise local objectives in bounded-depth circuits require only polynomial shots (O(poly(n))), controlled by backward lightcone growth.

Conclusion: Provides rigorous criterion for computational tractability of second-order methods: local objectives with bounded-depth circuits maintain statistically accessible curvature information, while global objectives face exponential sampling costs.

Abstract: Barren plateaus are typically characterized by vanishing gradients, yet the feasibility of curvature-based optimization fundamentally relies on the statistical resolvability of the Hessian matrix. In this work, we quantify the entrywise resolvability of the Hessian for Variational Quantum Algorithms at random initialization. By leveraging exact second-order parameter-shift rules, we derive a structural representation that reduces the variance of Hessian entries to a finite covariance quadratic form of shifted cost evaluations. This framework reveals two distinct scaling regimes that govern the sample complexity required to resolve Hessian entries against shot noise. For global objectives, we prove that Hessian variances are exponentially suppressed, implying that the number of measurement shots must scale as $O(e^{αn})$ with the number of qubits $n$ to maintain a constant signal-to-noise ratio. In contrast, for termwise local objectives in bounded-depth circuits, the variance decay is polynomial and explicitly controlled by the backward lightcone growth on the interaction graph, ensuring that curvature information remains statistically accessible with $O(\mathrm{poly}(n))$ shots. Extensive numerical experiments across varying system sizes and circuit depths demonstrate these theoretical bounds and the associated sampling costs. Our results provide a rigorous criterion for the computational tractability of second-order methods at initialization.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [164] [Numerical Simulations for Time-Fractional Black-Scholes Equations](https://arxiv.org/abs/2602.00201)
*Neetu Garg,A. S. V. Ravi Kanth*

Main category: q-fin.CP

TL;DR: Efficient numerical algorithm for time-fractional Black-Scholes model using Crank-Nicolson time discretization and exponential B-spline space approximation, proven unconditionally stable with superior performance.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical solution for the time-fractional Black-Scholes model governing European options, addressing the computational challenges of fractional derivatives in financial modeling.

Method: Combines Crank-Nicolson approach for time variable discretization with exponential B-spline approximation for space variable. The method is proven to be unconditionally stable.

Result: Numerical examples confirm theoretical analysis. Comparisons with other methods demonstrate the supremacy of the proposed approach in terms of accuracy and efficiency.

Conclusion: The developed algorithm provides an effective and stable numerical solution for the time-fractional Black-Scholes model, offering superior performance for European option pricing with fractional derivatives.

Abstract: This paper implements an efficient numerical algorithm for the time-fractional Black-Scholes model governing European options. The proposed method comprises the Crank-Nicolson approach to discretize the time variable and exponential B-spline approximation for the space variable. The implemented method is unconditionally stable. We present few numerical examples to confirm the theory. Numerical simulations with comparisons exhibit the supremacy of the proposed approach.

</details>


<div id='cond-mat.quant-gas'></div>

# cond-mat.quant-gas [[Back]](#toc)

### [165] [Phase Dynamics of Self-Accelerating Bose-Einstein Condensates](https://arxiv.org/abs/2602.01406)
*Maximilian L. D. D. Pellner,Georgi Gary Rozenman*

Main category: cond-mat.quant-gas

TL;DR: Researchers develop methods to extract cubic phase dynamics from Airy-shaped Bose-Einstein condensates using interference fringes, enabling measurement of weak nonlinear interactions.


<details>
  <summary>Details</summary>
Motivation: To develop practical methods for probing weak mean-field nonlinearities in self-accelerating matter waves by accessing and quantifying the cubic Kennard phase, which is challenging to measure directly.

Method: Reconstruct relative phase of simulated Airy-shaped BECs from interference fringes using two phase-extraction methods (heterodyne-based and density-based) with windowed polynomial fits and systematic uncertainty estimates. Compare Airy-Gaussian vs Airy-Airy collision geometries.

Result: Airy-Gaussian geometry shows substantially improved robustness to fit-window selection compared to Airy-Airy collisions. Extracted cubic coefficient responds linearly to effective 1D interaction strength in weakly interacting regime.

Conclusion: The approach successfully turns cubic phase dynamics into a practical probe for measuring weak mean-field nonlinearities in self-accelerating Bose-Einstein condensates, with potential applications in microgravity experiments.

Abstract: Self-accelerating Airy matter waves offer a clean setting to access the cubic Kennard phase. Here we reconstruct the relative phase of simulated Airy-shaped Bose-Einstein condensates in free space, a regime approached in microgravity, from interference fringes. The cubic phase dynamics are quantified via windowed polynomial fits with systematics-aware uncertainty estimates that account for window-induced correlations. We compare two experimentally feasible phase-extraction methods - heterodyne-based and density-based - and show that an Airy-Gaussian geometry yields substantially improved robustness to fit-window selection relative to an Airy-Airy collision. In the weakly interacting regime, the extracted cubic coefficient responds linearly to the effective one-dimensional interaction strength. Our approach turns cubic phase dynamics into a practical probe of weak mean-field nonlinearities in self-accelerating condensates.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [166] [Synthesis of Monolayer Ice on a Hydrophobic Metal Surface](https://arxiv.org/abs/2601.22460)
*Qiaoxiao Zhao,Meiling Xu,Dong Li,Zhicheng Gao,Yudian Zhou,Wenbo Liu,Jingyan Chen,Peng Cheng,Sheng Meng,Kehui Wu,Yanchao Wang,Lan Chen,Baojie Feng*

Main category: cond-mat.mtrl-sci

TL;DR: Researchers synthesized a monolayer ice phase on hydrophobic Au(111) using low-energy-electron-assisted growth, challenging previous assumptions about water assembly on hydrophobic metals.


<details>
  <summary>Details</summary>
Motivation: Understanding water-metal interactions is crucial for catalysis, electrochemistry, and atmospheric science. While monolayer ice phases are known on hydrophilic surfaces, their formation on hydrophobic metals was considered thermodynamically unfavorable, with water typically forming amorphous films, 3D crystallites, or bilayer ice instead.

Method: Used low-energy-electron-assisted growth method on hydrophobic Au(111) surface. Combined experimental techniques including low-energy electron diffraction (LEED), angle-resolved photoemission spectroscopy (ARPES), and X-ray photoelectron spectroscopy (XPS) with first-principles calculations.

Result: Successfully synthesized a monolayer ice phase on hydrophobic Au(111). Experimental characterizations and calculations proved that the monolayer ice consists of intact water molecules, demonstrating that ordered 2D ice can be stabilized on inert substrates.

Conclusion: The low-energy-electron-assisted growth method provides a generalizable strategy for stabilizing ordered two-dimensional ice on hydrophobic substrates. This work offers new insights into water-low-energy electron interactions at hydrophobic interfaces and challenges previous assumptions about water assembly on hydrophobic metals.

Abstract: Understanding water-metal interactions is central to disciplines spanning catalysis, electrochemistry, and atmospheric science. Monolayer ice phases are well established on hydrophilic surfaces, where strong water-substrate interactions stabilize ordered hydrogen-bond networks. In contrast, their formation on hydrophobic metals has been deemed ther-modynamically unfavourable, with water typically assembling into amorphous films, three-dimensional crystallites, or interlocked bilayer ice. Here, we demonstrate the synthesis of a monolayer ice phase on the hydrophobic Au(111) surface using a low-energy-electron-assisted growth method. Combined experimental characterizations including low-energy electron diffraction, angle-resolved photoemission spectroscopy, and X-ray photoelectron spectroscopy, complemented by first-principles calculations, prove that the monolayer ice phase composes of intact water molecules. This approach provides a generalizable strategy for stabilizing ordered two-dimensional ice on inert substrates and offers new insight into the interplay between water and low-energy electrons at hydrophobic interfaces.

</details>


### [167] [Unlocking the Power of Orbital-Free Density Functional Theory to Explore the Electronic Structure Under Extreme Conditions](https://arxiv.org/abs/2601.23002)
*Cheng Ma,Qiang Xu,Zhenhao Zhang,Ke Wang,Ying Sun,Wenhui Mi,Zhandos A. Moldabekov,Tobias Dornheim,Jan Vorberger,Sebastian Schwalbe,Xuecheng Shao*

Main category: cond-mat.mtrl-sci

TL;DR: A new orbital-free DFT framework achieves KSDFT accuracy with 100x speedup for extreme condition simulations, validated against quantum Monte Carlo and experimental data.


<details>
  <summary>Details</summary>
Motivation: X-ray diagnostics enable probing extreme conditions (stellar interiors, fusion experiments), but KSDFT is too computationally expensive for routine use, while OFDFT lacks accuracy needed for electronic structure description.

Method: Developed a non-empirical Kohn-Sham-assisted orbital-free density functional framework that combines efficiency of OFDFT with accuracy of KSDFT for extreme conditions calculations.

Result: Achieves KSDFT-level accuracy for electron densities, structure factors, and equations of state with speedups up to several hundred times compared to KSDFT, validated against quantum Monte Carlo data and Rayleigh weight measurements.

Conclusion: The framework enables efficient high-accuracy simulations at extreme conditions, and demonstrates that quantum non-locality remains essential even at temperatures around 100 eV for dense hydrogen electronic structure.

Abstract: Recent advances in X-ray free-electron laser diagnostics have enabled direct probing of the electronic structure under extreme pressures and temperatures, such as those encountered in stellar interiors and inertial confinement fusion experiments, challenging theoretical models for interpreting experimental data. Kohn-Sham density functional theory (KSDFT) has been successfully applied to analyze experimental X-ray scattering measurements, but its high computational cost renders routine application impractical. Orbital-free DFT (OFDFT) is a substantially more efficient alternative, with computational cost scaling linearly with system size and a weak temperature dependence, yet it often lacks the accuracy required for electronic structure description. Overcoming this limitation, we present a non-empirical Kohn-Sham-assisted orbital-free density functional framework for calculations at extreme conditions, which enables efficient OFDFT simulations with KSDFT-level accuracy for electron densities, electron-ion structure factors, and equations of state across a broad range of conditions. Benchmark comparisons with quantum Monte Carlo data for dense hydrogen and validation against Rayleigh weight measurements of hot dense beryllium demonstrate the reliability of the framework and speedups of up to several hundred times compared with KSDFT. We further show that even at temperatures on the order of 100 eV, quantum non-locality remains essential for correctly describing the electronic structure of dense hydrogen.

</details>


### [168] [Synthesis of Monolayer Ice on a Hydrophobic Metal Surface](https://arxiv.org/abs/2601.22460)
*Qiaoxiao Zhao,Meiling Xu,Dong Li,Zhicheng Gao,Yudian Zhou,Wenbo Liu,Jingyan Chen,Peng Cheng,Sheng Meng,Kehui Wu,Yanchao Wang,Lan Chen,Baojie Feng*

Main category: cond-mat.mtrl-sci

TL;DR: Researchers synthesized monolayer ice on hydrophobic Au(111) using low-energy-electron-assisted growth, challenging previous assumptions that such ordered structures couldn't form on hydrophobic metals.


<details>
  <summary>Details</summary>
Motivation: Understanding water-metal interactions is crucial for catalysis, electrochemistry, and atmospheric science. While monolayer ice phases are known on hydrophilic surfaces, their formation on hydrophobic metals was considered thermodynamically unfavorable, with water typically forming amorphous films, 3D crystallites, or bilayer ice instead.

Method: Used low-energy-electron-assisted growth method on Au(111) surface. Combined experimental techniques: low-energy electron diffraction (LEED), angle-resolved photoemission spectroscopy (ARPES), and X-ray photoelectron spectroscopy (XPS), complemented by first-principles calculations.

Result: Successfully synthesized a monolayer ice phase on hydrophobic Au(111). Experimental characterizations and calculations proved the monolayer ice consists of intact water molecules, forming an ordered two-dimensional structure.

Conclusion: The low-energy-electron-assisted growth provides a generalizable strategy for stabilizing ordered 2D ice on inert substrates and offers new insights into water-low-energy electron interactions at hydrophobic interfaces.

Abstract: Understanding water-metal interactions is central to disciplines spanning catalysis, electrochemistry, and atmospheric science. Monolayer ice phases are well established on hydrophilic surfaces, where strong water-substrate interactions stabilize ordered hydrogen-bond networks. In contrast, their formation on hydrophobic metals has been deemed ther-modynamically unfavourable, with water typically assembling into amorphous films, three-dimensional crystallites, or interlocked bilayer ice. Here, we demonstrate the synthesis of a monolayer ice phase on the hydrophobic Au(111) surface using a low-energy-electron-assisted growth method. Combined experimental characterizations including low-energy electron diffraction, angle-resolved photoemission spectroscopy, and X-ray photoelectron spectroscopy, complemented by first-principles calculations, prove that the monolayer ice phase composes of intact water molecules. This approach provides a generalizable strategy for stabilizing ordered two-dimensional ice on inert substrates and offers new insight into the interplay between water and low-energy electrons at hydrophobic interfaces.

</details>


### [169] [Unlocking the Power of Orbital-Free Density Functional Theory to Explore the Electronic Structure Under Extreme Conditions](https://arxiv.org/abs/2601.23002)
*Cheng Ma,Qiang Xu,Zhenhao Zhang,Ke Wang,Ying Sun,Wenhui Mi,Zhandos A. Moldabekov,Tobias Dornheim,Jan Vorberger,Sebastian Schwalbe,Xuecheng Shao*

Main category: cond-mat.mtrl-sci

TL;DR: A new orbital-free DFT framework achieves KSDFT-level accuracy with 100x speedup for extreme conditions like stellar interiors and fusion experiments.


<details>
  <summary>Details</summary>
Motivation: X-ray diagnostics can now probe extreme conditions in stellar interiors and fusion experiments, but KSDFT is too computationally expensive for routine use, while OFDFT lacks accuracy needed for electronic structure description.

Method: Developed a non-empirical Kohn-Sham-assisted orbital-free density functional framework that combines efficiency of OFDFT with accuracy of KSDFT for extreme conditions calculations.

Result: Achieved KSDFT-level accuracy for electron densities, electron-ion structure factors, and equations of state with speedups up to several hundred times compared to KSDFT. Validated against quantum Monte Carlo data for dense hydrogen and Rayleigh weight measurements for hot dense beryllium.

Conclusion: The framework enables efficient high-accuracy simulations for extreme conditions, and shows that quantum non-locality remains essential even at temperatures around 100 eV for correctly describing dense hydrogen electronic structure.

Abstract: Recent advances in X-ray free-electron laser diagnostics have enabled direct probing of the electronic structure under extreme pressures and temperatures, such as those encountered in stellar interiors and inertial confinement fusion experiments, challenging theoretical models for interpreting experimental data. Kohn-Sham density functional theory (KSDFT) has been successfully applied to analyze experimental X-ray scattering measurements, but its high computational cost renders routine application impractical. Orbital-free DFT (OFDFT) is a substantially more efficient alternative, with computational cost scaling linearly with system size and a weak temperature dependence, yet it often lacks the accuracy required for electronic structure description. Overcoming this limitation, we present a non-empirical Kohn-Sham-assisted orbital-free density functional framework for calculations at extreme conditions, which enables efficient OFDFT simulations with KSDFT-level accuracy for electron densities, electron-ion structure factors, and equations of state across a broad range of conditions. Benchmark comparisons with quantum Monte Carlo data for dense hydrogen and validation against Rayleigh weight measurements of hot dense beryllium demonstrate the reliability of the framework and speedups of up to several hundred times compared with KSDFT. We further show that even at temperatures on the order of 100 eV, quantum non-locality remains essential for correctly describing the electronic structure of dense hydrogen.

</details>


### [170] [FluxNet: Learning Capacity-Constrained Local Transport Operators for Conservative and Bounded PDE Surrogates](https://arxiv.org/abs/2602.01941)
*Zishuo Lan,Junjie Li,Lei Wang,Jincheng Wang*

Main category: cond-mat.mtrl-sci

TL;DR: FluxNet: A framework for learning conservative transport operators on grids that guarantees discrete conservation and state bounds by construction, using local transport operators instead of direct next-state prediction.


<details>
  <summary>Details</summary>
Motivation: Autoregressive learning of time-stepping operators for PDE simulation often violates conservation laws and state bounds (like nonnegative mass or bounded concentrations), leading to unstable long-horizon rollouts. Direct next-state regression struggles to enforce these coupled constraints.

Method: Inspired by lattice Boltzmann methods, the framework learns local transport operators that update cells through neighborhood exchanges, guaranteeing discrete conservation by construction. For bounded quantities, transport is parameterized within capacity-constrained feasible sets, enforcing bounds structurally rather than through post-hoc clipping.

Result: FluxNet shows improved rollout stability and physical consistency on 1D convection-diffusion, 2D shallow water equations, 1D traffic flow, and 2D spinodal decomposition. It enables large time-steps with long-range transport for phase-field spinodal decomposition while preserving microstructure evolution.

Conclusion: The framework provides a principled approach to learning conservative transport operators that structurally enforce conservation laws and state bounds, leading to more stable and physically consistent simulations across various PDE applications.

Abstract: Autoregressive learning of time-stepping operators offers an effective approach to data-driven PDE simulation on grids. For conservation laws, however, long-horizon rollouts are often destabilized when learned updates violate global conservation and, in many applications, additional state bounds such as nonnegative mass and densities or concentrations constrained to [0,1]. Enforcing these coupled constraints via direct next-state regression remains difficult. We introduce a framework for learning conservative transport operators on regular grids, inspired by lattice Boltzmann-style discrete-velocity transport representations. Instead of predicting the next state, the model outputs local transport operators that update cells through neighborhood exchanges, guaranteeing discrete conservation by construction. For bounded quantities, we parameterize transport within a capacity-constrained feasible set, enforcing bounds structurally rather than by post-hoc clipping. We validate FluxNet on 1D convection-diffusion, 2D shallow water equations, 1D traffic flow, and 2D spinodal decomposition. Experiments on shallow-water equations and traffic flow show improved rollout stability and physical consistency over strong baselines. On phase-field spinodal decomposition, the method enables large time-steps with long-range transport, accelerating simulation while preserving microstructure evolution in both pointwise and statistical measures.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [171] [Denoising deterministic networks using iterative Fourier transforms](https://arxiv.org/abs/2602.00790)
*H. Robert Frost*

Main category: eess.SP

TL;DR: IterativeFT method uses iterative Fourier transforms to denoise and recover network structure from noisy adjacency matrices with missing edges.


<details>
  <summary>Details</summary>
Motivation: Network data often contains both noise (spurious edges) and missing edges, making it challenging to identify true deterministic network structure. Existing denoising methods may not effectively handle both problems simultaneously.

Method: Iterative execution of forward/inverse 2D Fourier transforms on adjacency matrix with sparsification operations in both real and frequency domains. Convergence achieved when real domain sparsity pattern stabilizes.

Result: IterativeFT outperforms comparison methods (real/frequency domain thresholding, reduced rank reconstruction, locally adaptive sparsification) on lattice and Kautz networks, with competitive performance on tree and bipartite networks. Effectively filters noisy edges and recovers missing true edges.

Conclusion: IterativeFT is an effective Fourier-based approach for network denoising and structure recovery that handles both edge pruning and Gaussian noise, particularly suitable for deterministic network models.

Abstract: We detail a novel Fourier-based approach (IterativeFT) for identifying deterministic network structure in the presence of both edge pruning and Gaussian noise. This technique involves the iterative execution of forward and inverse 2D discrete Fourier transforms on a target network adjacency matrix. The denoising ability of the method is achieved via the application of a sparsification operation to both the real and frequency domain representations of the adjacency matrix with algorithm convergence achieved when the real domain sparsity pattern stabilizes. To demonstrate the effectiveness of the approach, we apply it to noisy versions of several deterministic models including Kautz, lattice, tree and bipartite networks. For contrast, we also evaluate preferential attachment networks to illustrate the behavior on stochastic graphs. We compare the performance of IterativeFT against simple real domain and frequency domain thresholding, reduced rank reconstruction and locally adaptive network sparsification. Relative to the comparison network denoising approaches, the proposed IterativeFT method provides the best overall performance for lattice and Kuatz networks with competitive performance on tree and bipartite networks. Importantly, the InterativeFT technique is effective at both filtering noisy edges and recovering true edges that are missing from the observed network.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [172] [Predicting the hydrogen bond strength from water reorientation dynamics at short timescales](https://arxiv.org/abs/2602.00311)
*Frederik Zysk,Ana Vila Verde,Naveen K. Kaliannan,Kristof Karhan,Thomas D. Kühne*

Main category: physics.chem-ph

TL;DR: The study connects hydrogen bond strength and asymmetry at water/air interface to experimental observables (SFG spectra, reorientation dynamics) using path-integral MD and energy decomposition analysis.


<details>
  <summary>Details</summary>
Motivation: To establish quantitative relationships between hydrogen bond properties (strength, asymmetry, delocalization) at water/air interfaces and experimentally measurable observables like SFG spectra and reorientation dynamics, enabling prediction of H-bond strength from experimental data.

Method: Combined path-integral molecular dynamics simulations with electronic structure-based energy decomposition analysis (EDA) using absolutely localized molecular orbitals, validated against sum-frequency generation (SFG) spectra for distinct interfacial layers.

Result: Found red-shift in SFG spectra from interface to bulk, strongly bonded water peak at 3250 cm⁻¹ near bulk, slower reorientation dynamics from interface to bulk, strong decline in total delocalization energy and H-bond strength from bulk to interface, increased asymmetry of strongest interactions, and correlation between strongest H-bond donor/acceptor strength and librational motions.

Conclusion: Established quantitative relationship between H-bond strength and short-time reorientation dynamics at water/air interface, providing framework to predict H-bond strength from experimental observables that could extend to other hydrophobic systems.

Abstract: Path-integral molecular dynamics simulations and electronic structure-based energy decomposition analysis (EDA) are employed to connect hydrogen bond (H-bond) strength, its asymmetry, and the total delocalization energy at the water/air interface to experimentally measurable observables, such as the reorientation dynamics and the sum-frequency generation (SFG) spectrum. Using SFG spectra for distinct layers at the water/air interface, we validate the accuracy of our simulations and report a red-shift from the interface to bulk and a strongly bonded water peak at around 3250 cm$^{-1}$ in the layer closest to bulk. The reorientation dynamics of water molecules slow down from the interface to bulk, which correlates with the SFG results. From our EDA based on absolutely localized molecular orbitals, we observe a strong decline in total delocalization energy from bulk to the interface, as well as a decline in the strength of the strongest donor and acceptor interactions. The asymmetry between the two strongest interactions similarly rises towards the interface, while the importance of interactions from the outer solvation shells is greatly diminished and is lower than previously reported. Finally, we find that the strength of the strongest H-bond donor/acceptor is best correlated with the local minimum of the autocorrelation function resembling the L2 band librational motions. Following that, we propose a simple yet quantitative relationship between H-bond strength and the short-time reorientation dynamics at the water/air interface that could potentially be extended to predict H-bond strength in other hydrophobic systems from experimentally obtainable observables.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [173] [aurel: A Python package for automatic relativistic calculations](https://arxiv.org/abs/2602.00155)
*Robyn L. Munoz,Christian T. Byrnes,Will J. Roper*

Main category: astro-ph.IM

TL;DR: aurel is an open-source Python package for automated relativistic calculations with symbolic and numerical capabilities, supporting both analytical expressions and NR simulation data.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient, flexible tool for handling the highly nonlinear nature of general relativity calculations, addressing the increasing use of Numerical Relativity simulations and supporting the popularization of the field.

Method: Uses an efficient caching and dependency-tracking system, extends SymPy for symbolic tensorial calculations, and implements finite-difference methods for numerical computations directly from spacetime and matter data arrays.

Result: Developed a comprehensive package that computes curvature, matter kinematics, and other tensorial quantities from both analytical expressions and NR simulation data, with helper functions for standard NR code data formats.

Conclusion: aurel provides a timely post-processing tool for Numerical Relativity that combines symbolic and numerical capabilities with user-friendly dependency management, supporting the growing field of relativistic simulations.

Abstract: \texttt{aurel} is an open-source Python package designed to \emph{au}tomatically calculate \emph{rel}ativistic quantities. It uses an efficient, flexible and user-friendly caching and dependency-tracking system, ideal for managing the highly nonlinear nature of general relativity. The package supports both symbolic and numerical calculations. The symbolic part extends \texttt{SymPy} with additional tensorial calculations. The numerical part computes a wide range of tensorial quantities, such as curvature, matter kinematics and much more, directly from any spacetime and matter data arrays using finite-difference methods. Inputs can be either generated from analytical expressions or imported from Numerical Relativity (NR) simulations, with helper functions provided to read in data from standard NR codes. Given the increasing use of NR, \texttt{aurel} offers a timely post-processing tool to support the popularisation of this field.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [174] [Adaptive Benign Overfitting (ABO): Overparameterized RLS for Online Learning in Non-stationary Time-series](https://arxiv.org/abs/2601.22200)
*Luis Ontaneda Mijares,Nick Firoozye*

Main category: q-fin.ST

TL;DR: QR-based exponentially weighted RLS algorithm enables stable online learning with benign overfitting in overparameterized models, achieving 20-40% speed improvements while maintaining accuracy comparable to kernel methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of conventional learning theory being challenged by overparameterized models exhibiting improved generalization beyond interpolation (benign overfitting), and to enable online adaptation under non-stationary conditions while preventing numerical divergence.

Method: Introduces Adaptive Benign Overfitting (ABO) extending recursive least-squares (RLS) with QR-based exponentially weighted RLS (QR-EWRLS) algorithm. Combines random Fourier feature mappings with forgetting-factor regularization, uses orthogonal-triangular updates for numerical stability, and prevents covariance-form RLS divergence while adapting to evolving data distributions.

Result: Experiments show bounded residuals and stable condition numbers while reproducing double-descent behavior. Applications to foreign exchange and electricity demand forecasting achieve accuracy comparable to baseline kernel methods with 20-40% speed improvements. The approach maintains adaptability under non-stationary conditions.

Conclusion: Provides a unified framework linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning system that combines numerical stability with the generalization benefits of overparameterized models.

Abstract: Overparameterized models have recently challenged conventional learning theory by exhibiting improved generalization beyond the interpolation limit, a phenomenon known as benign overfitting. This work introduces Adaptive Benign Overfitting (ABO), extending the recursive least-squares (RLS) framework to this regime through a numerically stable formulation based on orthogonal-triangular updates. A QR-based exponentially weighted RLS (QR-EWRLS) algorithm is introduced, combining random Fourier feature mappings with forgetting-factor regularization to enable online adaptation under non-stationary conditions. The orthogonal decomposition prevents the numerical divergence associated with covariance-form RLS while retaining adaptability to evolving data distributions. Experiments on nonlinear synthetic time series confirm that the proposed approach maintains bounded residuals and stable condition numbers while reproducing the double-descent behavior characteristic of overparameterized models. Applications to forecasting foreign exchange and electricity demand show that ABO is highly accurate (comparable to baseline kernel methods) while achieving speed improvements of between 20 and 40 percent. The results provide a unified view linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning framework.

</details>


### [175] [Adaptive Benign Overfitting (ABO): Overparameterized RLS for Online Learning in Non-stationary Time-series](https://arxiv.org/abs/2601.22200)
*Luis Ontaneda Mijares,Nick Firoozye*

Main category: q-fin.ST

TL;DR: QR-based exponentially weighted RLS algorithm enables stable online learning with benign overfitting in non-stationary environments, achieving 20-40% speed improvements while maintaining accuracy comparable to kernel methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of conventional learning theory being challenged by overparameterized models exhibiting improved generalization (benign overfitting), and to extend this capability to online adaptation under non-stationary conditions while maintaining numerical stability.

Method: Introduces Adaptive Benign Overfitting (ABO) through a numerically stable QR-based exponentially weighted RLS (QR-EWRLS) algorithm that combines random Fourier feature mappings with forgetting-factor regularization, using orthogonal-triangular updates to prevent numerical divergence.

Result: The approach maintains bounded residuals and stable condition numbers while reproducing double-descent behavior; achieves 20-40% speed improvements over baseline kernel methods with comparable accuracy in foreign exchange and electricity demand forecasting.

Conclusion: Provides a unified framework linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning system that enables efficient adaptation to evolving data distributions.

Abstract: Overparameterized models have recently challenged conventional learning theory by exhibiting improved generalization beyond the interpolation limit, a phenomenon known as benign overfitting. This work introduces Adaptive Benign Overfitting (ABO), extending the recursive least-squares (RLS) framework to this regime through a numerically stable formulation based on orthogonal-triangular updates. A QR-based exponentially weighted RLS (QR-EWRLS) algorithm is introduced, combining random Fourier feature mappings with forgetting-factor regularization to enable online adaptation under non-stationary conditions. The orthogonal decomposition prevents the numerical divergence associated with covariance-form RLS while retaining adaptability to evolving data distributions. Experiments on nonlinear synthetic time series confirm that the proposed approach maintains bounded residuals and stable condition numbers while reproducing the double-descent behavior characteristic of overparameterized models. Applications to forecasting foreign exchange and electricity demand show that ABO is highly accurate (comparable to baseline kernel methods) while achieving speed improvements of between 20 and 40 percent. The results provide a unified view linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning framework.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [176] [Leveraging Interactions for Efficient Swarm-Based Brownian Computing](https://arxiv.org/abs/2601.22874)
*Alessandro Pignedoli,Atreya Majumdar,Karin Everschor-Sitte*

Main category: cond-mat.stat-mech

TL;DR: Brownian quasiparticles with short-range attractive interactions form energy-efficient swarms that outperform non-interacting searchers in finding global optima in temperature-defined optimization landscapes.


<details>
  <summary>Details</summary>
Motivation: To develop energy-efficient unconventional computing platforms inspired by swarm intelligence, leveraging emergent cooperative behavior in physical systems for optimization tasks.

Method: Use thermally driven Brownian quasiparticles with short-range attractive interactions to form swarms. Define optimization tasks using spatially varying temperature landscapes. Coarse-grain dynamics onto sensor lattice to emulate experimental particle-tracking measurements.

Result: Interacting swarms reliably identify global optima and significantly outperform non-interacting searchers within specific regimes of interaction strength and swarm size. Swarms adapt robustly to time-evolving landscapes.

Conclusion: Interacting Brownian quasiparticles provide a physical platform for scalable, energy-efficient unconventional computing through emergent cooperative optimization behavior.

Abstract: Drawing inspiration from swarm intelligence, we show that short-range attractive interactions between thermally driven Brownian quasiparticles enable energy-efficient optimization. As quasiparticles can be generated directly within a material, the swarm size can be adjusted with minimal energy overhead. Using an optimization task defined by a spatially varying temperature landscape, we quantitatively show that interacting swarms reliably identify global optima and significantly outperform non-interacting searchers within a well-defined regime of interaction strength and swarm size. This improvement arises from emergent cooperative behavior, where local interactions guide the swarm toward high-quality solutions without central coordination. To link our physical model to experimental realizations, we coarse-grain the quasiparticle dynamics onto a sensor lattice and generate trajectories emulating particle-tracking measurements. We further show that the interacting swarm adapts robustly to landscapes that evolve over time. These findings establish interacting Brownian quasiparticles as a physical platform for scalable and energy-efficient unconventional computing.

</details>


### [177] [Leveraging Interactions for Efficient Swarm-Based Brownian Computing](https://arxiv.org/abs/2601.22874)
*Alessandro Pignedoli,Atreya Majumdar,Karin Everschor-Sitte*

Main category: cond-mat.stat-mech

TL;DR: Brownian quasiparticles with short-range attractive interactions enable energy-efficient optimization by forming cooperative swarms that outperform non-interacting searchers in finding global optima in temperature landscapes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop energy-efficient unconventional computing platforms inspired by swarm intelligence, leveraging the natural dynamics of Brownian quasiparticles within materials to perform optimization tasks with minimal energy overhead.

Method: The researchers use thermally driven Brownian quasiparticles with short-range attractive interactions, studying their behavior in spatially varying temperature landscapes. They coarse-grain the dynamics onto a sensor lattice to emulate experimental particle-tracking measurements and test adaptation to time-evolving landscapes.

Result: Interacting swarms reliably identify global optima and significantly outperform non-interacting searchers within specific regimes of interaction strength and swarm size. The emergent cooperative behavior guides swarms toward high-quality solutions without central coordination, and the system adapts robustly to evolving landscapes.

Conclusion: Interacting Brownian quasiparticles represent a scalable, energy-efficient physical platform for unconventional computing, demonstrating that swarm intelligence principles can be implemented in material systems for optimization tasks.

Abstract: Drawing inspiration from swarm intelligence, we show that short-range attractive interactions between thermally driven Brownian quasiparticles enable energy-efficient optimization. As quasiparticles can be generated directly within a material, the swarm size can be adjusted with minimal energy overhead. Using an optimization task defined by a spatially varying temperature landscape, we quantitatively show that interacting swarms reliably identify global optima and significantly outperform non-interacting searchers within a well-defined regime of interaction strength and swarm size. This improvement arises from emergent cooperative behavior, where local interactions guide the swarm toward high-quality solutions without central coordination. To link our physical model to experimental realizations, we coarse-grain the quasiparticle dynamics onto a sensor lattice and generate trajectories emulating particle-tracking measurements. We further show that the interacting swarm adapts robustly to landscapes that evolve over time. These findings establish interacting Brownian quasiparticles as a physical platform for scalable and energy-efficient unconventional computing.

</details>


### [178] [Internal Trajectories and Observation Effects in Langevin Splitting Schemes](https://arxiv.org/abs/2602.01923)
*Bettina G. Keller*

Main category: cond-mat.stat-mech

TL;DR: Analysis of Langevin splitting schemes focusing on internal trajectories and observation points, revealing subtle biases at high friction/time steps.


<details>
  <summary>Details</summary>
Motivation: To understand Langevin integrators from the perspective of their internal trajectories and observation points, complementing existing generator-based analyses, and to clarify when accuracy differences between splitting schemes matter in practice.

Method: Examines Langevin splitting schemes by exploiting merging, splitting, and cyclic permutation of elementary update operators to group formally distinct schemes according to identical or closely related trajectories. Quantifies accuracy differences arising from momentum updates and observation points.

Result: Modern Langevin integrators are remarkably stable under standard simulation conditions, but subtle but systematic biases emerge at large friction coefficients and time steps. Accuracy differences are quantified for configurational sampling, free-energy estimates, and transition rates.

Conclusion: The results clarify when accuracy differences between splitting schemes matter in practice and provide an intuitive framework for understanding observation effects in Langevin dynamics simulations.

Abstract: Langevin integrators based on operator splitting are widely used in molecular dynamics. This work examines Langevin splitting schemes from the perspective of their internal trajectories and observation points, complementing existing generator-based analyses. By exploiting merging, splitting, and cyclic permutation of elementary update operators, formally distinct schemes can be grouped according to identical or closely related trajectories. Accuracy differences arising from momentum updates and observation points are quantified for configurational sampling, free-energy estimates, and transition rates. While modern Langevin integrators are remarkably stable under standard simulation conditions, subtle but systematic biases emerge at large friction coefficients and time steps. These results clarify when accuracy differences between splitting schemes matter in practice and provide an intuitive framework for understanding observation effects.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [179] [Parametric vector flows for registration fields in bounded domains with applications to nonlinear interpolation of shock-dominated flows](https://arxiv.org/abs/2601.22712)
*Jon Labatut,Jean-Baptiste Chapelier,Angelo Iollo,Tommaso Taddei*

Main category: physics.flu-dyn

TL;DR: A registration method for parametric model order reduction that aligns coherent structures (shocks, shear layers) across parameters using diffeomorphisms, combined with nonlinear interpolation for accurate shock-dominated fluid dynamics.


<details>
  <summary>Details</summary>
Motivation: To improve parametric model order reduction for fluid dynamics problems with moving coherent structures like shocks, which are difficult to approximate with linear subspaces due to their parameter-dependent positions.

Method: Develops an expectation-maximization procedure to find diffeomorphisms (vector flows of velocity fields) that align point clouds extracted from solution snapshots, combined with nonlinear interpolation for shock-dominated fields.

Result: Demonstrates effectiveness on 2D inviscid transonic flow past a NACA airfoil and 3D viscous transonic flow past an ONERA M6 wing, showing accurate interpolation of shock-dominated fluid dynamic fields.

Conclusion: The registration method combined with nonlinear interpolation enables accurate parametric MOR for fluid dynamics with moving shocks and coherent structures, overcoming limitations of linear subspace approximations.

Abstract: We present a registration procedure for parametric model order reduction (MOR) in two- and three-dimensional bounded domains. In the MOR framework, registration methods exploit solution snapshots to identify a parametric coordinate transformation that improves the approximation of the solution set through linear subspaces. For each training parameter, optimization-based (or variational) registration methods minimize a target function that measures the alignment of the coherent structures of interest (e.g., shocks, shear layers, cracks) for different parameter values, over a family of bijections of the computational domain $Ω$. We consider diffeomorphisms $Φ$ that are vector flows of given velocity fields $v$ with vanishing normal component on $\partial Ω$; we rely on a sensor to extract appropriate point clouds from the solution snapshots and we develop an expectation-maximization procedure to simultaneously solve the point cloud matching problem and to determine the velocity $v$ (and thus the bijection $Φ$); finally, we combine our registration method with the nonlinear interpolation technique of [Iollo, Taddei, J. Comput. Phys., 2022] to perform accurate interpolations of fluid dynamic fields in the presence of shocks. Numerical results for a two-dimensional inviscid transonic flow past a NACA airfoil and a three-dimensional viscous transonic flow past an ONERA M6 wing illustrate the many elements of the methodology and demonstrate the effectiveness of nonlinear interpolation for shock-dominated fields.

</details>


### [180] [Parametric vector flows for registration fields in bounded domains with applications to nonlinear interpolation of shock-dominated flows](https://arxiv.org/abs/2601.22712)
*Jon Labatut,Jean-Baptiste Chapelier,Angelo Iollo,Tommaso Taddei*

Main category: physics.flu-dyn

TL;DR: Registration method for parametric model order reduction using diffeomorphisms to align coherent structures (shocks, shear layers) across parameters, combined with nonlinear interpolation for accurate shock-dominated fluid dynamics.


<details>
  <summary>Details</summary>
Motivation: Parametric MOR struggles with coherent structures like shocks that move with parameters. Registration methods can improve approximation by aligning these structures before linear subspace approximation.

Method: 1) Use diffeomorphisms as vector flows of velocity fields with vanishing normal boundary conditions; 2) Extract point clouds from solution snapshots using sensors; 3) Develop expectation-maximization procedure to solve point cloud matching and determine velocity field; 4) Combine with nonlinear interpolation technique for shock-dominated fields.

Result: Method demonstrated on 2D inviscid transonic flow past NACA airfoil and 3D viscous transonic flow past ONERA M6 wing. Shows effectiveness of nonlinear interpolation for shock-dominated fluid dynamic fields.

Conclusion: Registration procedure combined with nonlinear interpolation enables accurate parametric MOR for fluid dynamics with moving shocks and coherent structures, improving approximation quality for shock-dominated fields.

Abstract: We present a registration procedure for parametric model order reduction (MOR) in two- and three-dimensional bounded domains. In the MOR framework, registration methods exploit solution snapshots to identify a parametric coordinate transformation that improves the approximation of the solution set through linear subspaces. For each training parameter, optimization-based (or variational) registration methods minimize a target function that measures the alignment of the coherent structures of interest (e.g., shocks, shear layers, cracks) for different parameter values, over a family of bijections of the computational domain $Ω$. We consider diffeomorphisms $Φ$ that are vector flows of given velocity fields $v$ with vanishing normal component on $\partial Ω$; we rely on a sensor to extract appropriate point clouds from the solution snapshots and we develop an expectation-maximization procedure to simultaneously solve the point cloud matching problem and to determine the velocity $v$ (and thus the bijection $Φ$); finally, we combine our registration method with the nonlinear interpolation technique of [Iollo, Taddei, J. Comput. Phys., 2022] to perform accurate interpolations of fluid dynamic fields in the presence of shocks. Numerical results for a two-dimensional inviscid transonic flow past a NACA airfoil and a three-dimensional viscous transonic flow past an ONERA M6 wing illustrate the many elements of the methodology and demonstrate the effectiveness of nonlinear interpolation for shock-dominated fields.

</details>


### [181] [Parametrization of subgrid scales in long-term simulations of the shallow-water equations using machine learning and convex limiting](https://arxiv.org/abs/2602.00378)
*Md Amran Hossan Mojamder,Zhihang Xu,Min Wang,Ilya Timofeyev*

Main category: physics.flu-dyn

TL;DR: A neural network-based method for parametrizing sub-grid processes in Shallow Water equations that learns local sub-grid fluxes using coarse variables and spatial averages, improving energy balance in turbulent simulations.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable parametrization method for sub-grid processes in Shallow Water equations that overcomes limitations of globally coupled parametrizations and works well even in untrained dynamical regimes.

Method: Define coarse variables and local spatial averages, then use a feed-forward neural network to learn sub-grid fluxes. The method produces a local parametrization using a four-point computational stencil, which can be combined with flux limiting techniques.

Result: The method improves energy balance in long-term turbulent simulations, accurately reproduces individual solutions, reduces oscillations near shocks when combined with flux limiting, and provides reliable parametrizations even in dynamical regimes not included in training data.

Conclusion: The neural network-based local parametrization approach offers significant advantages over globally coupled methods, providing robust and accurate sub-grid modeling for Shallow Water equations with good generalization to untrained regimes.

Abstract: We present a method for parametrizing sub-grid processes in the Shallow Water equations. We define coarse variables and local spatial averages and use a feed-forward neural network to learn sub-grid fluxes. Our method results in a local parametrization that uses a four-point computational stencil, which has several advantages over globally coupled parametrizations. We demonstrate numerically that our method improves energy balance in long-term turbulent simulations and also accurately reproduces individual solutions. The neural network parametrization can be easily combined with flux limiting to reduce oscillations near shocks. More importantly, our method provides reliable parametrizations, even in dynamical regimes that are not included in the training data.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [182] [On two-dimensional Dirac operators with critical delta-shell interactions](https://arxiv.org/abs/2601.23053)
*William Borrelli,Pietro Carimati,Davide Fermi*

Main category: math.SP

TL;DR: 2D Dirac operators with singular interactions on lines/circles show critical points in essential spectrum - infinite multiplicity eigenvalue for lines vs. accumulation point of eigenvalues for circles.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties of Dirac operators with singular interactions in different geometries, particularly the nature of critical points in the essential spectrum that appear within the mass gap.

Method: Study two-dimensional Dirac operators with electrostatic and Lorentz-scalar singular interactions supported on straight lines and circles, analyzing spectral properties for critical interaction strengths.

Result: For straight line: critical point is eigenvalue of infinite multiplicity with detailed eigenfunction analysis. For circle: critical point is not an eigenvalue but accumulation point of double sequence of simple eigenvalues.

Conclusion: Different geometries yield fundamentally different spectral behaviors for critical singular interactions, suggesting conjectures about general smooth curves.

Abstract: We study two-dimensional Dirac operators with singular interactions of electrostatic and Lorentzscalar type, supported either on a straight line or a circle. For certain critical values of the interaction strengths, the essential spectrum of such operators comprises an isolated point lying within the mass gap. We clarify the nature of this point in both geometries. For the straight line model, this point is known to be an eigenvalue of infinite multiplicity, and we provide a detailed analysis of the corresponding eigenfunctions. By contrast, in the case of a circle, we show that the said point is not itself an eigenvalue, but rather an accumulation point of a double sequence of simple eigenvalues. In view of the high degree of symmetry of the configurations under analysis, this behavior is unexpected and our findings lead us to formulate some conjectures concerning critical singular interactions supported on generic smooth curves.

</details>


### [183] [On two-dimensional Dirac operators with critical delta-shell interactions](https://arxiv.org/abs/2601.23053)
*William Borrelli,Pietro Carimati,Davide Fermi*

Main category: math.SP

TL;DR: 2D Dirac operators with singular interactions on lines/circles show critical cases where essential spectrum has isolated point in mass gap - behaves differently in line vs circle geometries.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties of 2D Dirac operators with singular electrostatic and Lorentz-scalar interactions, particularly when critical interaction strengths create isolated points in the essential spectrum within the mass gap.

Method: Study Dirac operators with singular interactions supported on straight lines and circles, analyze spectral properties for critical interaction strengths, examine eigenfunctions for line case and eigenvalue accumulation for circle case.

Result: For straight line: isolated point is eigenvalue of infinite multiplicity with detailed eigenfunction analysis. For circle: isolated point is not eigenvalue but accumulation point of double sequence of simple eigenvalues - unexpected due to symmetry.

Conclusion: Geometry dramatically affects spectral properties of critical singular interactions; line vs circle show fundamentally different behavior despite high symmetry, leading to conjectures about generic smooth curves.

Abstract: We study two-dimensional Dirac operators with singular interactions of electrostatic and Lorentzscalar type, supported either on a straight line or a circle. For certain critical values of the interaction strengths, the essential spectrum of such operators comprises an isolated point lying within the mass gap. We clarify the nature of this point in both geometries. For the straight line model, this point is known to be an eigenvalue of infinite multiplicity, and we provide a detailed analysis of the corresponding eigenfunctions. By contrast, in the case of a circle, we show that the said point is not itself an eigenvalue, but rather an accumulation point of a double sequence of simple eigenvalues. In view of the high degree of symmetry of the configurations under analysis, this behavior is unexpected and our findings lead us to formulate some conjectures concerning critical singular interactions supported on generic smooth curves.

</details>


### [184] [On the discrete spectrum of non-selfadjoint operators with applications to Schrödinger operators with complex potentials](https://arxiv.org/abs/2602.02359)
*Sabine Bögli,Sukrid Petpradittha*

Main category: math.SP

TL;DR: Upper bound on discrete eigenvalues for perturbed non-negative selfadjoint operators using Birman-Schwinger operator partial trace, with applications to Schrödinger operators and generalizations of Cwikel-Lieb-Rozenblum inequality.


<details>
  <summary>Details</summary>
Motivation: To establish eigenvalue bounds for perturbed non-negative selfadjoint operators, particularly in half-planes away from the positive real axis, which extends classical results to more general settings.

Method: Use relatively form-compact perturbations and derive bounds via partial trace of the real part of the Birman-Schwinger operator (or appropriate rotations). Apply to Schrödinger operators with complex potentials.

Result: Obtain upper bound on number of discrete eigenvalues in specified half-planes. Generalize Cwikel-Lieb-Rozenblum inequality to complex potentials and derive new Lieb-Thirring type inequalities.

Conclusion: The approach provides effective eigenvalue bounds for perturbed operators, extending classical inequalities to complex potential settings with applications in spectral theory of Schrödinger operators.

Abstract: For relatively form-compact perturbations of non-negative selfadjoint operators, we obtain an upper bound on the number of discrete eigenvalues in half-planes separated from the positive real axis. The bound is given in terms of a partial trace of the real part of the Birman--Schwinger operator, or an appropriate rotation thereof. As an application to Schrödinger operators, we generalise the Cwikel--Lieb--Rozenblum inequality to complex potentials and derive new Lieb--Thirring type inequalities.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [185] [Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations](https://arxiv.org/abs/2602.00474)
*Yang Xu,Vaneet Aggarwal*

Main category: stat.ML

TL;DR: The paper addresses the ill-posedness of Poisson equations in average-reward RL beyond ergodic Markov chains, developing a method to handle reducible/periodic chains through quotient space analysis and gauge-fixed stochastic approximation.


<details>
  <summary>Details</summary>
Motivation: Poisson equations are fundamental to average-reward reinforcement learning but become ill-posed for non-ergodic Markov chains (reducible or periodic), where solutions are non-unique and standard fixed-point methods oscillate. This limits RL applications beyond ergodic settings.

Method: 1) Analyze Markov chains to capture non-decaying modes via real peripheral invariant subspace K(P). 2) Show induced operator on quotient space R^n/K(P) is strictly contractive, enabling unique quotient solutions. 3) Develop pipeline: learn chain structure, estimate anchor-based gauge map, run projected stochastic approximation to estimate gauge-fixed representative with peripheral residual.

Result: Prove O~(T^{-1/2}) convergence up to projection estimation error, enabling stable Poisson equation learning for multichain and periodic regimes. This extends performance evaluation capabilities for average-reward RL beyond ergodicity.

Conclusion: The framework successfully addresses ill-posedness in Poisson equations for non-ergodic Markov chains through quotient space theory and gauge fixing, providing stable learning with theoretical guarantees for broader RL applications including multichain and periodic systems.

Abstract: Poisson equations underpin average-reward reinforcement learning, but beyond ergodicity they can be ill-posed, meaning that solutions are non-unique and standard fixed point iterations can oscillate on reducible or periodic chains. We study finite-state Markov chains with $n$ states and transition matrix $P$. We show that all non-decaying modes are captured by a real peripheral invariant subspace $\mathcal{K}(P)$, and that the induced operator on the quotient space $\mathbb{R}^n/\mathcal{K}(P)$ is strictly contractive, yielding a unique quotient solution. Building on this viewpoint, we develop an end-to-end pipeline that learns the chain structure, estimates an anchor based gauge map, and runs projected stochastic approximation to estimate a gauge-fixed representative together with an associated peripheral residual. We prove $\widetilde{O}(T^{-1/2})$ convergence up to projection estimation error, enabling stable Poisson equation learning for multichain and periodic regimes with applications to performance evaluation of average-reward reinforcement learning beyond ergodicity.

</details>


### [186] [Training-free score-based diffusion for parameter-dependent stochastic dynamical systems](https://arxiv.org/abs/2602.02113)
*Minglei Yang,Sicheng He*

Main category: stat.ML

TL;DR: Training-free conditional diffusion model for parameter-dependent SDEs using kernel-weighted Monte Carlo estimator to approximate conditional score functions, enabling interpolation across state space and continuous parameter domains without retraining.


<details>
  <summary>Details</summary>
Motivation: Simulating parameter-dependent SDEs is computationally expensive as separate high-fidelity simulations are needed for each parameter value. Existing machine learning methods either require expensive neural network training for score function estimation or cannot handle continuous parameter dependence.

Method: Proposes a training-free conditional diffusion model framework with a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values. This enables interpolation across both state space and continuous parameter domains.

Result: The generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. Demonstrated via three numerical examples of increasing complexity with accurate approximation of conditional distributions.

Conclusion: The framework provides an efficient, training-free approach for learning stochastic flow maps of parameter-dependent SDEs, enabling interpolation across continuous parameter domains and accelerating computational studies without the need for expensive neural network training or separate simulations for each parameter value.

Abstract: Simulating parameter-dependent stochastic differential equations (SDEs) presents significant computational challenges, as separate high-fidelity simulations are typically required for each parameter value of interest. Despite the success of machine learning methods in learning SDE dynamics, existing approaches either require expensive neural network training for score function estimation or lack the ability to handle continuous parameter dependence. We present a training-free conditional diffusion model framework for learning stochastic flow maps of parameter-dependent SDEs, where both drift and diffusion coefficients depend on physical parameters. The key technical innovation is a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values, enabling interpolation across both state space and the continuous parameter domain. Once trained, the resulting generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. The performance of the proposed approach is demonstrated via three numerical examples of increasing complexity, showing accurate approximation of conditional distributions across varying parameter values.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [187] [Dynamical stability of various convex graphical translators](https://arxiv.org/abs/2601.22368)
*Junyoung Park*

Main category: math.DG

TL;DR: Existence of longtime MCF solutions from continuous graphs over slabs, plus dynamical stability for various graphical translators.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous existence results for mean curvature flow starting from continuous initial data and to understand the stability properties of important special solutions (translators) in the flow.

Method: Two-part approach: 1) Prove existence of longtime solutions to mean curvature flow starting from graphs of continuous functions over slabs. 2) Establish dynamical stability results for graphical translators using analytical techniques.

Result: Successfully proved existence of longtime solutions from continuous initial data and established dynamical stability for grim reaper, 2D graphical translators, and asymptotically cylindrical translators.

Conclusion: The paper provides important existence and stability results for mean curvature flow, extending the theory to continuous initial data and establishing robustness properties for key translator solutions.

Abstract: In the first part of the paper, we prove the existence of longtime solution to mean curvature flow starting from a graph of a continuous function defined over a slab. Then, we establish dynamical stability results for various types of graphical translators to mean curvature flow, namely the grim reaper, two dimensional graphical translators, and asymptotically cylindrical translators.

</details>


### [188] [Sharp thresholds for Escobar and Gagliardo-Nirenberg functionals: the Escobar-Willmore mass, geometric selection, and compactness trichotomy](https://arxiv.org/abs/2601.22665)
*Mayukh Mukherjee,Utsab Sarkar*

Main category: math.DG

TL;DR: A unified framework for sharp threshold phenomena in boundary-critical variational problems on Riemannian manifolds, covering Escobar quotient and Gagliardo-Nirenberg inequalities, with geometric selection governed by mean curvature and Willmore-type anisotropy.


<details>
  <summary>Details</summary>
Motivation: To develop a quantitative framework for understanding sharp threshold phenomena in boundary-critical variational problems on compact Riemannian manifolds, particularly addressing the dichotomy between attainment (compactness) and bubbling (non-compactness) in critical Sobolev-type inequalities.

Method: Uses transfer-stability-reduction approach to obtain attainment-versus-bubbling alternatives, H¹-compactness, and finite-dimensional reductions. Geometric selection is governed by mean curvature H_g and a Willmore-type anisotropy from |ĪĪ|². Introduces renormalized boundary mass 𝔖_g involving Ricci curvature, scalar curvature, and traceless second fundamental form.

Result: Identifies threshold dichotomy: if first nonvanishing coefficient among {ρ_n^conf H_g, 𝔖_g, Θ_g} is negative somewhere, then C^*_Esc(M,g) < S_* and sequences are precompact. At threshold, blow-up concentrates where H_g is critical. In multi-bubble regime, dynamics governed by 𝒲_k produce k-bubble critical points. Resolves question about sharp constant divergence with small Dirichlet windows.

Conclusion: Provides a comprehensive framework for sharp threshold phenomena in boundary-critical problems, establishing precise geometric criteria for compactness versus bubbling, with applications to entropy inequalities, curvature-driven NLS ground states, and Euler characteristic recovery from Gagliardo-Nirenberg measurements.

Abstract: We develop a unified quantitative framework for sharp threshold phenomena in boundary-critical variational problems on compact Riemannian manifolds, covering the Escobar quotient and Gagliardo-Nirenberg inequalities. Via transfer-stability-reduction, we obtain attainment-versus-bubbling alternatives, $H^1$-compactness, and finite-dimensional reductions. Geometric selection is governed by mean curvature $H_g$ and a Willmore-type anisotropy from $|\mathring{\mathrm{II}}|^2$.
  At hemisphere threshold $S_\ast=C^*_{\mathrm{Esc}}(\mathbb S^n_+)$ for $n\ge5$ on $H_g\equiv0$, we identify a renormalized boundary mass $\mathfrak R_g=κ_1(n)\,\mathrm{Ric}_g(ν,ν)+κ_2(n)\,\mathrm{Scal}_{g|\partial M}+κ_3(n)\,|\mathring{\mathrm{II}}|^2$, $κ_3(n)<0$, yielding one-bubble expansions and energy-only estimators. Threshold dichotomy: if the first nonvanishing coefficient among $\{ρ_n^{\mathrm{conf}}H_g,\mathfrak R_g,Θ_g\}$ is negative somewhere, then $C^*_{\mathrm{Esc}}(M,g)<S_\ast$ and sequences are precompact. At threshold, blow-up concentrates where $H_g$ is critical; on $H_g\equiv0$, stationarity forces $\mathfrak R_g(p)=\nabla_\partial\mathfrak R_g(p)=0$. If $H_g$ is Morse and $\mathfrak R_g>0$ at all critical points, no bubbling occurs. In multi-bubble regime ($n\ge5$), dynamics governed by $\mathcal W_k=\sum_{i=1}^k\mathfrak R_g(x_i)$ produce $k$-bubble critical points at levels $k^{1/(n-1)}S_\ast$. In the degenerate case we obtain conformal hemispherical rigidity.
  The GN track yields analogous dichotomies and resolves a question of Christianson et al.: the sharp constant with small Dirichlet windows diverges at optimal capacitary rate, relating threshold to spectral/isoperimetric invariants. Applications include entropy inequalities for fast diffusion, curvature-driven NLS ground states, and (in $n=2$) Euler characteristic recovery from GN measurements.

</details>


### [189] [Prescribed $T$-curvature flow on the four-dimensional unit ball](https://arxiv.org/abs/2601.22934)
*Pak Tung Ho,Cheikh Birahim Ndiaye,Liming Sun,Heming Wang*

Main category: math.DG

TL;DR: Study of prescribed T-curvature problem on 4D unit ball using T-curvature flow, establishing existence results via Morse theory and proving exponential convergence of the flow.


<details>
  <summary>Details</summary>
Motivation: To solve the prescribed T-curvature problem on the 4-dimensional unit ball, which involves finding metrics with specific curvature properties, using a flow approach that provides both existence results and convergence properties.

Method: Combines Ache-Chang's inequality with Malchiodi-Struwe's Morse-theoretic approach to establish existence results under strong Morse-type inequalities at infinity. Uses T-curvature flow analysis starting from Q-flat minimal metrics conformal to Euclidean metric.

Result: Established existence results for prescribed T-curvature problem under strong Morse-type inequalities at infinity. Proved exponential convergence of T-curvature flow on 4D ball from Q-flat minimal metrics to extremal metrics of Ache-Chang's inequality.

Conclusion: The T-curvature flow approach successfully solves prescribed curvature problems on 4D balls, with both existence theorems and convergence results to known extremal metrics, connecting geometric analysis with Morse theory.

Abstract: In this paper, we study the prescribed $T$-curvature problem on the unit ball $\mathbb{B}^4$ of $\mathbb{R} ^4$ via the $T$-curvature flow approach. By combining Ache-Chang's inequality with the Morse-theoretic approach of Malchiodi-Struwe, we establish existence results under strong Morse-type inequalities at infinity. As a byproduct of our argument, we also prove the exponential convergence of the $T$-curvature flow on $\mathbb{B}^4$, starting from a $Q$-flat and minimal metric conformal to the standard Euclidean metric, to an extremal metric of Ache-Chang's inequality whose explicit expression was derived by Ndiaye-Sun.

</details>


### [190] [Dynamical stability of various convex graphical translators](https://arxiv.org/abs/2601.22368)
*Junyoung Park*

Main category: math.DG

TL;DR: Existence of longtime MCF solutions from continuous graphs over slabs, plus dynamical stability analysis for various graphical translators.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for mean curvature flow starting from general initial conditions (continuous graphs) and to understand the stability properties of important special solutions (translators) that move by translation under MCF.

Method: Two-part approach: 1) Prove existence of longtime solutions for MCF starting from continuous graphs over slabs using analytical techniques; 2) Establish dynamical stability results for graphical translators (grim reaper, 2D translators, asymptotically cylindrical translators) through stability analysis methods.

Result: Successfully proved existence of longtime MCF solutions from continuous initial graphs. Demonstrated dynamical stability for various types of graphical translators, providing mathematical justification for their persistence under perturbations.

Conclusion: The paper establishes important existence results for MCF with general initial data and provides stability analysis for key translator solutions, contributing to the understanding of long-time behavior in mean curvature flow.

Abstract: In the first part of the paper, we prove the existence of longtime solution to mean curvature flow starting from a graph of a continuous function defined over a slab. Then, we establish dynamical stability results for various types of graphical translators to mean curvature flow, namely the grim reaper, two dimensional graphical translators, and asymptotically cylindrical translators.

</details>


### [191] [Sharp thresholds for Escobar and Gagliardo-Nirenberg functionals: the Escobar-Willmore mass, geometric selection, and compactness trichotomy](https://arxiv.org/abs/2601.22665)
*Mayukh Mukherjee,Utsab Sarkar*

Main category: math.DG

TL;DR: Sharp threshold phenomena in boundary-critical variational problems on Riemannian manifolds, covering Escobar quotient and Gagliardo-Nirenberg inequalities, with geometric selection governed by mean curvature and Willmore-type anisotropy.


<details>
  <summary>Details</summary>
Motivation: To develop a unified quantitative framework for understanding sharp threshold phenomena in boundary-critical variational problems on compact Riemannian manifolds, particularly focusing on when solutions exist versus when bubbling occurs, and how geometric quantities like mean curvature influence these thresholds.

Method: Uses transfer-stability-reduction approach to obtain attainment-versus-bubbling alternatives, H¹-compactness, and finite-dimensional reductions. Geometric selection is governed by mean curvature H_g and a Willmore-type anisotropy from |II̊|². Introduces renormalized boundary mass 𝔖_g involving Ricci curvature, scalar curvature, and traceless second fundamental form.

Result: Identifies threshold dichotomy: if first nonvanishing coefficient among {ρ_n^conf H_g, 𝔖_g, Θ_g} is negative somewhere, then C^*_Esc(M,g) < S_* and sequences are precompact. At threshold, blow-up concentrates where H_g is critical. In multi-bubble regime (n≥5), dynamics governed by 𝒲_k produce k-bubble critical points. For GN inequalities, resolves question about sharp constant divergence with small Dirichlet windows.

Conclusion: Develops comprehensive framework for sharp threshold phenomena in boundary-critical problems, revealing how geometric quantities control existence versus bubbling behavior, with applications to entropy inequalities, curvature-driven NLS ground states, and Euler characteristic recovery from Gagliardo-Nirenberg measurements.

Abstract: We develop a unified quantitative framework for sharp threshold phenomena in boundary-critical variational problems on compact Riemannian manifolds, covering the Escobar quotient and Gagliardo-Nirenberg inequalities. Via transfer-stability-reduction, we obtain attainment-versus-bubbling alternatives, $H^1$-compactness, and finite-dimensional reductions. Geometric selection is governed by mean curvature $H_g$ and a Willmore-type anisotropy from $|\mathring{\mathrm{II}}|^2$.
  At hemisphere threshold $S_\ast=C^*_{\mathrm{Esc}}(\mathbb S^n_+)$ for $n\ge5$ on $H_g\equiv0$, we identify a renormalized boundary mass $\mathfrak R_g=κ_1(n)\,\mathrm{Ric}_g(ν,ν)+κ_2(n)\,\mathrm{Scal}_{g|\partial M}+κ_3(n)\,|\mathring{\mathrm{II}}|^2$, $κ_3(n)<0$, yielding one-bubble expansions and energy-only estimators. Threshold dichotomy: if the first nonvanishing coefficient among $\{ρ_n^{\mathrm{conf}}H_g,\mathfrak R_g,Θ_g\}$ is negative somewhere, then $C^*_{\mathrm{Esc}}(M,g)<S_\ast$ and sequences are precompact. At threshold, blow-up concentrates where $H_g$ is critical; on $H_g\equiv0$, stationarity forces $\mathfrak R_g(p)=\nabla_\partial\mathfrak R_g(p)=0$. If $H_g$ is Morse and $\mathfrak R_g>0$ at all critical points, no bubbling occurs. In multi-bubble regime ($n\ge5$), dynamics governed by $\mathcal W_k=\sum_{i=1}^k\mathfrak R_g(x_i)$ produce $k$-bubble critical points at levels $k^{1/(n-1)}S_\ast$. In the degenerate case we obtain conformal hemispherical rigidity.
  The GN track yields analogous dichotomies and resolves a question of Christianson et al.: the sharp constant with small Dirichlet windows diverges at optimal capacitary rate, relating threshold to spectral/isoperimetric invariants. Applications include entropy inequalities for fast diffusion, curvature-driven NLS ground states, and (in $n=2$) Euler characteristic recovery from GN measurements.

</details>


### [192] [Prescribed $T$-curvature flow on the four-dimensional unit ball](https://arxiv.org/abs/2601.22934)
*Pak Tung Ho,Cheikh Birahim Ndiaye,Liming Sun,Heming Wang*

Main category: math.DG

TL;DR: Existence of prescribed T-curvature metrics on the 4D unit ball via T-curvature flow, using Ache-Chang inequality and Morse theory, with exponential convergence to extremal metrics.


<details>
  <summary>Details</summary>
Motivation: To solve the prescribed T-curvature problem on the 4-dimensional unit ball, which involves finding metrics with specific curvature properties, using flow methods rather than variational approaches.

Method: Combines Ache-Chang's inequality with Malchiodi-Struwe's Morse-theoretic approach to establish existence under strong Morse-type inequalities at infinity. Uses T-curvature flow starting from Q-flat minimal metrics conformal to Euclidean metric.

Result: Proves existence results for prescribed T-curvature metrics under certain conditions. As a byproduct, establishes exponential convergence of T-curvature flow to extremal metrics of Ache-Chang's inequality (explicitly derived by Ndiaye-Sun).

Conclusion: The T-curvature flow approach successfully solves prescribed curvature problems on the 4D ball, with exponential convergence to known extremal metrics, providing both existence results and flow convergence properties.

Abstract: In this paper, we study the prescribed $T$-curvature problem on the unit ball $\mathbb{B}^4$ of $\mathbb{R} ^4$ via the $T$-curvature flow approach. By combining Ache-Chang's inequality with the Morse-theoretic approach of Malchiodi-Struwe, we establish existence results under strong Morse-type inequalities at infinity. As a byproduct of our argument, we also prove the exponential convergence of the $T$-curvature flow on $\mathbb{B}^4$, starting from a $Q$-flat and minimal metric conformal to the standard Euclidean metric, to an extremal metric of Ache-Chang's inequality whose explicit expression was derived by Ndiaye-Sun.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [193] [High-Efficiency Hexagonal Nanowire MAPbI3 Perovskite Solar Cell with Broadband Light Trapping](https://arxiv.org/abs/2601.23191)
*Kawshik Nath,Bibekananda Nath,Ahmed Zubair*

Main category: physics.optics

TL;DR: Hexagonal nanowire perovskite solar cell achieves 24.2% efficiency through enhanced light absorption and polarization insensitivity.


<details>
  <summary>Details</summary>
Motivation: Perovskite solar cells show great promise for next-gen photovoltaics due to excellent light absorption and low-cost manufacturing, but further improvements in light management and charge collection are needed for higher performance.

Method: Designed hexagonal nanowire (HNW) perovskite solar cell structure with CH3NH3PbI3 material, optimized geometric parameters (diameter, period, fill ratio), and embedded SiO2 dielectric spheres in ITO layer to enhance photon confinement. Used FDTD method for optical analysis and solved coupled drift-diffusion and Poisson equations for electrical performance.

Result: Achieved polarization-independent broadband absorption, high optical short-circuit current density of 29.53 mA/cm², and power conversion efficiency of 24.2% through optimized light-matter interaction and carrier transport.

Conclusion: The HNW perovskite solar cell structure demonstrates strong potential for high-performance photovoltaic systems by effectively combining optical confinement with efficient carrier transport, making it suitable for scalable thin-film solar technologies.

Abstract: Perovskite solar cells (PSCs) have emerged as strong contenders for the next generation of photovoltaic (PV) technologies due to their exceptional light absorption properties, tunability, and affordability in manufacturing. Here, we presented an ingenious hexagonal nanowire (HNW)-based PSC that achieves broadband absorption, minimizes reflectance, and offers robust polarization insensitivity by improving light-matter interaction and increasing charge-collection efficiency. The rotational symmetry of the HNW configuration yielded polarization-independent absorbance under both TE and TM illumination across the visible and near-infrared spectra. The optimization of the geometrical parameters of CH3NH3PbI3-based HNW structure, including diameter, period, and fill ratio, offered a wide rangeof variations that influenced both optical properties and device performance. To further intensify photon confinement, a dielectric SiO2 sphere is partially embedded in the ITO layer, improving long-wavelength absorbance and increasing electron-hole pair generation near the active region. We analyzed the finite-difference time-domain (FDTD) method to examine the optical properties of our proposed structure. This study demonstrates that our proposed structure has achieved a higher generation rate, enhanced absorbance, and a higher optical short-circuit current density (Jsc) of 29.53 mA/cm2. Electrical performance is assessed by solving the coupled drift-diffusion and Poisson equations for the dynamics of carrier transport. The optimized HNW structure achieved a notable power conversion efficiency of 24.2%, highlighting a strong connection between optical confinement and effective carrier transport. These attributes render the proposed HNW PSC a viable option for high-performance PV systems and scalable thin-film solar technologies.

</details>


### [194] [High-Efficiency Hexagonal Nanowire MAPbI3 Perovskite Solar Cell with Broadband Light Trapping](https://arxiv.org/abs/2601.23191)
*Kawshik Nath,Bibekananda Nath,Ahmed Zubair*

Main category: physics.optics

TL;DR: Hexagonal nanowire perovskite solar cells achieve 24.2% efficiency through enhanced light absorption and charge collection.


<details>
  <summary>Details</summary>
Motivation: Perovskite solar cells show promise for next-gen photovoltaics but need improved light absorption and charge collection efficiency to reach higher performance levels.

Method: Designed hexagonal nanowire (HNW) perovskite structure with CH3NH3PbI3, optimized geometrical parameters (diameter, period, fill ratio), and embedded SiO2 spheres in ITO layer for photon confinement. Used FDTD method for optical analysis and solved drift-diffusion/Poisson equations for electrical performance.

Result: Achieved polarization-insensitive broadband absorption, optical short-circuit current density of 29.53 mA/cm², and power conversion efficiency of 24.2% through optimized structure.

Conclusion: The HNW perovskite solar cell demonstrates strong optical-electrical coupling, making it a viable option for high-performance, scalable thin-film photovoltaic systems.

Abstract: Perovskite solar cells (PSCs) have emerged as strong contenders for the next generation of photovoltaic (PV) technologies due to their exceptional light absorption properties, tunability, and affordability in manufacturing. Here, we presented an ingenious hexagonal nanowire (HNW)-based PSC that achieves broadband absorption, minimizes reflectance, and offers robust polarization insensitivity by improving light-matter interaction and increasing charge-collection efficiency. The rotational symmetry of the HNW configuration yielded polarization-independent absorbance under both TE and TM illumination across the visible and near-infrared spectra. The optimization of the geometrical parameters of CH3NH3PbI3-based HNW structure, including diameter, period, and fill ratio, offered a wide rangeof variations that influenced both optical properties and device performance. To further intensify photon confinement, a dielectric SiO2 sphere is partially embedded in the ITO layer, improving long-wavelength absorbance and increasing electron-hole pair generation near the active region. We analyzed the finite-difference time-domain (FDTD) method to examine the optical properties of our proposed structure. This study demonstrates that our proposed structure has achieved a higher generation rate, enhanced absorbance, and a higher optical short-circuit current density (Jsc) of 29.53 mA/cm2. Electrical performance is assessed by solving the coupled drift-diffusion and Poisson equations for the dynamics of carrier transport. The optimized HNW structure achieved a notable power conversion efficiency of 24.2%, highlighting a strong connection between optical confinement and effective carrier transport. These attributes render the proposed HNW PSC a viable option for high-performance PV systems and scalable thin-film solar technologies.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [195] [Sculpting of Martian brain terrain reveals the drying of ancient Mars](https://arxiv.org/abs/2601.22606)
*Shenyi Zhang,Lei Zhang,Yutian Ke,Jinhai Zhang*

Main category: physics.geo-ph

TL;DR: Martian brain terrain formation involves two-stage process: initial freeze-thaw patterning requiring liquid water, followed by sublimation sculpting in dry conditions, providing evidence for Mars' climate transition from wet to hyper-arid.


<details>
  <summary>Details</summary>
Motivation: Martian brain terrain (MBT) resembles Earth's self-organized patterned ground, suggesting shared formation mechanisms. However, lack of quantitative descriptions and physical modeling limits understanding of thermal/aqueous conditions during MBT formation.

Method: Established specialized quantitative system to extract MBT morphological features in northern Arabia Terra, then employed numerical model to investigate formation mechanisms through simulation.

Result: Simulations accurately replicate MBT morphology with <10% deviation in key metrics, but show self-organized transport alone produces <0.5m relief, insufficient for MBT's 3.29±0.65m average relief. Sublimation sculpting explains discrepancy, indicating ~3m subsurface ice loss over ~3 million years.

Conclusion: MBT formation is multi-stage: initial freeze-thaw patterning (requiring liquid water) followed by sublimation sculpting (dry environment). This provides physical evidence for Mars' climate transition from wetter period to colder hyper-arid state.

Abstract: The Martian brain terrain (MBT), characterized by its unique brain-like morphology, is a potential geological archive for finding hints of paleoclimatic conditions during its formation period. The morphological similarity of MBT to self-organized patterned ground on Earth suggests a shared formation mechanism. However, the lack of quantitative descriptions and robust physical modeling of self-organized stone transport jointly limits the study of the thermal and aqueous conditions governing MBT's formation. Here we established a specialized quantitative system for extracting the morphological features of MBT, taking a typical region located in the northern Arabia Terra as an example, and then employed a numerical model to investigate its formation mechanisms. Our simulation results accurately replicate the observed morphology of MBT, matching its key geometric metrics with deviations $<10\%$. Crucially, however, we find that the self-organized transport can solely produce relief $<0.5$ m, insufficient to explain the formation of MBT with average relief of $3.29 \pm 0.65$ m. We attribute this discrepancy to sculpting driven by late-stage sublimation, constraining cumulative subsurface ice loss in this region to $\sim 3$ meters over the past $\sim 3$ Ma. These findings demonstrate that MBT's formation is a multi-stage process: initial patterning driven by freeze-thaw cycles (implying liquid water) followed by vertical sculpting via sublimation (requiring a dry environment). This evolution provides physical evidence for the transition of the ancient Martian climate from a wetter period to a colder hyper-arid state.

</details>


### [196] [Sculpting of Martian brain terrain reveals the drying of ancient Mars](https://arxiv.org/abs/2601.22606)
*Shenyi Zhang,Lei Zhang,Yutian Ke,Jinhai Zhang*

Main category: physics.geo-ph

TL;DR: Martian brain terrain formation involves two stages: initial freeze-thaw patterning requiring liquid water, followed by sublimation sculpting in dry conditions, revealing Mars' climate transition from wet to hyper-arid.


<details>
  <summary>Details</summary>
Motivation: Brain terrain (MBT) morphology resembles Earth's patterned ground, suggesting shared self-organization mechanisms, but lack of quantitative analysis and physical modeling limits understanding of Martian paleoclimate conditions during MBT formation.

Method: Developed quantitative system to extract MBT morphological features from Arabia Terra region, then used numerical modeling to simulate formation mechanisms and compare with observed geometry.

Result: Simulations matched observed MBT morphology with <10% deviation in key metrics, but self-organization alone produces <0.5m relief vs. observed 3.29±0.65m. Discrepancy attributed to ~3m ice sublimation over ~3 million years.

Conclusion: MBT formation is two-stage: initial freeze-thaw cycles (requiring liquid water) create patterning, followed by sublimation sculpting (dry environment). This provides physical evidence for Mars' climate transition from wet to hyper-arid state.

Abstract: The Martian brain terrain (MBT), characterized by its unique brain-like morphology, is a potential geological archive for finding hints of paleoclimatic conditions during its formation period. The morphological similarity of MBT to self-organized patterned ground on Earth suggests a shared formation mechanism. However, the lack of quantitative descriptions and robust physical modeling of self-organized stone transport jointly limits the study of the thermal and aqueous conditions governing MBT's formation. Here we established a specialized quantitative system for extracting the morphological features of MBT, taking a typical region located in the northern Arabia Terra as an example, and then employed a numerical model to investigate its formation mechanisms. Our simulation results accurately replicate the observed morphology of MBT, matching its key geometric metrics with deviations $<10\%$. Crucially, however, we find that the self-organized transport can solely produce relief $<0.5$ m, insufficient to explain the formation of MBT with average relief of $3.29 \pm 0.65$ m. We attribute this discrepancy to sculpting driven by late-stage sublimation, constraining cumulative subsurface ice loss in this region to $\sim 3$ meters over the past $\sim 3$ Ma. These findings demonstrate that MBT's formation is a multi-stage process: initial patterning driven by freeze-thaw cycles (implying liquid water) followed by vertical sculpting via sublimation (requiring a dry environment). This evolution provides physical evidence for the transition of the ancient Martian climate from a wetter period to a colder hyper-arid state.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [197] [Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS](https://arxiv.org/abs/2602.02234)
*Andong Hu,Luca Pennati,Stefano Markidis,Ivy Peng*

Main category: cs.DC

TL;DR: Integration of AI deep potentials into GROMACS MD code via DeePMD-kit, enabling high-performance inference of DPA2 and DPA3 models for protein simulations with DPA2 showing 3-4x higher throughput than DPA3 on modern GPUs.


<details>
  <summary>Details</summary>
Motivation: AI deep potentials offer near-quantum mechanical accuracy at much lower computational cost than traditional first-principles methods like DFT, but need integration into production-level molecular dynamics software like GROMACS for practical applications.

Method: Integrated DeePMD-kit's C++/CUDA backend with GROMACS Neural Network Potentials to enable inference across multiple DP model families. Evaluated DPA2 (attention-based) and DPA3 (GNN-based) architectures on four protein-in-water benchmarks using NVIDIA A100 and GH200 GPUs.

Result: DPA2 delivers 4.23x higher throughput than DPA3 on A100 GPUs and 3.18x higher on GH200 GPUs. Characterization study reveals kernel-launch overhead and domain-decomposed inference as main optimization priorities for production MD simulations.

Conclusion: Successful integration of AI deep potentials into GROMACS enables production-level MD simulations with ab initio quality. DPA2 outperforms DPA3 significantly in throughput, and optimization should focus on reducing kernel-launch overhead and improving domain-decomposed inference.

Abstract: State-of-the-art AI deep potentials provide ab initio-quality results, but at a fraction of the computational cost of first-principles quantum mechanical calculations, such as density functional theory. In this work, we bring AI deep potentials into GROMACS, a production-level Molecular Dynamics (MD) code, by integrating with DeePMD-kit that provides domain-specific deep learning (DL) models of interatomic potential energy and force fields. In particular, we enable AI deep potentials inference across multiple DP model families and DL backends by coupling GROMACS Neural Network Potentials with the C++/CUDA backend in DeePMD-kit. We evaluate two recent large-atom-model architectures, DPA2 that is based on the attention mechanism and DPA3 that is based on GNN, in GROMACS using four ab initio-quality protein-in-water benchmarks (1YRF, 1UBQ, 3LZM, 2PTC) on NVIDIA A100 and GH200 GPUs. Our results show that DPA2 delivers up to 4.23x and 3.18x higher throughput than DPA3 on A100 and GH200 GPUs, respectively. We also provide a characterization study to further contrast DPA2 and DPA3 in throughput, memory usage, and kernel-level execution on GPUs. Our findings identify kernel-launch overhead and domain-decomposed inference as the main optimization priorities for AI deep potentials in production MD simulations.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [198] [Flow Generation via Catastrophic Loss of Equilibrium in Weakly-Rotating Self-Gravitating Fluids: A Minimal Idealized Model](https://arxiv.org/abs/2602.00786)
*L. Gudushauri,N. L. Shatashvili,G. Shekiladze,S. M. Mahajan*

Main category: astro-ph.HE

TL;DR: The paper explores catastrophic energy transformations in weakly rotating self-gravitating fluids, showing how gravitational energy converts to kinetic energy in flows, with applications to astrophysical systems.


<details>
  <summary>Details</summary>
Motivation: To understand catastrophic energy transformations in weakly rotating self-gravitating fluids/gases near massive compact objects, and how these processes can explain flow generation in various high-energy astrophysical systems.

Method: Parallel application of methods developed for investigating catastrophic relaxation in stellar plasmas to weakly rotating self-gravitating fluids, analyzing equilibrium loss and transformation to less complex states.

Result: The study shows that catastrophic transformations in weakly rotating self-gravitating fluids convert significant gravitational energy into kinetic energy in flows, explaining flow generation mechanisms in astrophysical systems.

Conclusion: The revealed energy transformation processes advance understanding of various astrophysical flows, including galactic structures, accretion discs, and rotating star dynamics, highlighting the importance of catastrophic energy conversions in astrophysics.

Abstract: This paper explores the catastrophic energy transformations, in particular the ones leading to the generation of a flow in a weakly rotating self-gravitating fluid/gas found, for instance, in the vicinity of a massive compact object. Because of the similarity in the governing equations, the system dynamics is worked out exactly in parallel to the methods developed for investigating catastrophic relaxation in stellar plasmas [1-3]. In the latter a more ``complex" equilibrium state, on slow changes in the environment, can lose its equilibrium (catastrophe), and transform to a less complex state with a very different energy mix from the original. It is shown that a similar transformation in the weakly rotating self-gravitating fluid/gas will convert much of its gravitation energy into kinetic energy in the flow. Since flows are a perennial ingredient of high-energy astrophysical systems, the energy transformation processes revealed in present study, can advance our understanding of a variety of them. Some particularly relevant examples are: macro-scale flows / structures in galaxies, accretion discs, and the dynamics and stability of a rotating star / its atmosphere.

</details>


### [199] [Radiation-Driven Origin of Super-Equipartition Magnetic Fields in Accretion Discs and Outflows](https://arxiv.org/abs/2602.01097)
*Mukesh Kumar Vyas,Asaf Pe'er*

Main category: astro-ph.HE

TL;DR: Radiation fields in black hole accretion discs can generate strong magnetic fields (~10⁸ G) that reach equipartition and magnetize outflows, providing an internal origin for large-scale magnetic structures without needing external flux.


<details>
  <summary>Details</summary>
Motivation: Magnetic fields are crucial in black hole accretion physics, but their physical origin within accretion flows remains poorly understood. The paper aims to investigate whether radiation fields can serve as a primary generator for these magnetic fields.

Method: The authors self-consistently evolve magnetic fields using generalized MHD equations including advection, shear-driven induction, and Hall effects. They study anisotropic radiation fields in black hole accretion discs with compact rotating inner coronas, where radiation acts as the field generator and azimuthal rotation provides amplification.

Result: Radiation-generated magnetic fields efficiently develop dominant toroidal components through Keplerian rotation, reaching strengths of ~10⁸ G near a 10 solar mass black hole. These fields achieve or exceed local equipartition estimates and develop within viscous timescales. When vertical outflows are included, amplified fields are advected into coronas, magnetizing disc-launched winds and jet precursors.

Conclusion: Radiation is not just a passive component but provides a robust, unavoidable trigger for generating dynamically significant magnetic fields. This offers a physically grounded explanation for large-scale magnetic field origins in accretion discs without requiring externally supplied flux, with implications for X-ray binaries, AGN, and GRBs.

Abstract: Magnetic fields play a central role in accretion physics around black holes, yet their physical origin within accretion flows remains an open problem. In this work, we investigate the generation and subsequent evolution of magnetic fields triggered by anisotropic radiation fields in black hole accretion discs with compact rotating inner corona. We self-consistently evolve the magnetic field using the generalized field evolution MHD equation, including advection, shear-driven induction, and Hall effects. The radiation field acts as a primary field generator, while azimuthal rotation in the magnetized plasma provides rapid amplification. We find that radiation-generated fields efficiently reach a dominant toroidal component by Keplerian rotation, leading to magnetic field strengths of order $\sim 10^{8}\,\mathrm{G}$ in the vicinity of a 10 solar mass black hole and accretion disc-corona emitting at luminosity equivalent to the Eddington unit. These magnetic fields are achieved within viscous timescales and reach or exceed local equipartition estimates based on gas pressure. When vertical outflows are included, the amplified magnetic fields are advected into the corona, magnetizing disc-launched winds and jet precursors with field strengths of similar order. Our results demonstrate that radiation is not merely a passive component of accretion flows, but provides a robust and unavoidable trigger for the generation of dynamically significant magnetic fields. Our results provide a physically grounded explanation for the origin of large-scale, structured magnetic fields in and around accretion discs. This mechanism offers a pathway for magnetizing accretion discs and their outflows without invoking externally supplied magnetic flux, with broad implications for X-ray binaries, active galactic nuclei and other transients such as gamma-ray bursts (GRBs).

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [200] [Understanding the sign problem from an exact Path Integral Monte Carlo model of interacting harmonic fermions](https://arxiv.org/abs/2601.22559)
*Siu A. Chin*

Main category: cond-mat.str-el

TL;DR: Operator contraction identity for harmonic oscillator path integrals can be applied to fermions, creating an exactly solvable model for studying the sign problem in Path Integral Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: To develop an exactly solvable model for studying the fermion sign problem in Path Integral Monte Carlo simulations, allowing analytical understanding of how interactions affect the sign problem.

Method: Apply operator contraction identity (originally for harmonic oscillator) to fermions in any dimension, use fourth-order and variable-bead algorithms for numerical computations, compare with neural network results.

Result: Repulsive interactions shift sign problem to larger imaginary time, attractive to smaller time, but don't worsen it compared to non-interacting case. For closed-shell fermion numbers, sign problem disappears at large imaginary time. Successfully computed ground state energies of quantum dots with up to 110 electrons.

Conclusion: The operator contraction approach provides an exactly solvable framework for understanding fermion sign problems, revealing that interactions don't fundamentally worsen the sign problem and that closed-shell systems can avoid it at large imaginary times.

Abstract: This work shows that the recently discovered operator contraction identity for solving the discreet Path Integral of the harmonic oscillator can be applied equally to fermions in any dimension. This then yields an exactly solvable model for studying the sign problem where the Path Integral Monte Carlo energy at any time step for any number of fermions is known analytically, or can be computed numerically. It is found that repulsive/attractive pairwise interaction shifts the sign problem to larger/smaller imaginary time, but does not make it more severe than the non-interacting case. More surprisingly, for closed-shell number of fermions, the sign problem goes away at large imaginary time. Fourth-order and newly found variable-bead algorithms are used to compute ground state energies of quantum dots with up to 110 electrons and compared to results obtained by modern neural networks.

</details>


### [201] [Understanding the sign problem from an exact Path Integral Monte Carlo model of interacting harmonic fermions](https://arxiv.org/abs/2601.22559)
*Siu A. Chin*

Main category: cond-mat.str-el

TL;DR: The paper shows that operator contraction identity for harmonic oscillator path integrals can be applied to fermions, creating an exactly solvable model for studying the sign problem in Path Integral Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: To develop an exactly solvable model for studying the fermion sign problem in Path Integral Monte Carlo, which is a major computational challenge in quantum many-body physics.

Method: Apply operator contraction identity from harmonic oscillator path integrals to fermions in any dimension, use fourth-order and variable-bead algorithms for numerical computations.

Result: Repulsive/attractive interactions shift sign problem to larger/smaller imaginary time but don't worsen it; for closed-shell fermion numbers, sign problem disappears at large imaginary time; computed ground state energies of quantum dots with up to 110 electrons.

Conclusion: The operator contraction approach provides an exactly solvable model for studying fermion sign problem, revealing interesting interaction effects and enabling comparison with modern neural network methods.

Abstract: This work shows that the recently discovered operator contraction identity for solving the discreet Path Integral of the harmonic oscillator can be applied equally to fermions in any dimension. This then yields an exactly solvable model for studying the sign problem where the Path Integral Monte Carlo energy at any time step for any number of fermions is known analytically, or can be computed numerically. It is found that repulsive/attractive pairwise interaction shifts the sign problem to larger/smaller imaginary time, but does not make it more severe than the non-interacting case. More surprisingly, for closed-shell number of fermions, the sign problem goes away at large imaginary time. Fourth-order and newly found variable-bead algorithms are used to compute ground state energies of quantum dots with up to 110 electrons and compared to results obtained by modern neural networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [202] [Exact closed-form Gaussian moments of residual layers](https://arxiv.org/abs/2601.22307)
*Simon Kuang,Xinfan Lin*

Main category: cs.LG

TL;DR: Exact moment matching for Gaussian propagation through deep neural networks with various activation functions, achieving orders-of-magnitude improvements in KL divergence over alternatives.


<details>
  <summary>Details</summary>
Motivation: To address the longstanding gap in propagating mean and covariance of multivariate Gaussian distributions through deep neural networks using layer-by-layer moment matching, particularly for various activation functions in both feedforward and residual architectures.

Method: Derived exact moment matching for probit, GeLU, ReLU (as limit of GeLU), Heaviside (as limit of probit), and sine activation functions; applied to both feedforward and generalized residual layers using layer-by-layer Gaussian propagation.

Result: Achieved orders-of-magnitude improvements (up to millionfold) in KL divergence error on random networks, competitive statistical calibration for epistemic uncertainty inference on real data, and hundredfold improvements over state-of-the-art deterministic inference in variational Bayes networks.

Conclusion: The method provides exact moment matching for important activation functions, significantly outperforming existing alternatives in accuracy for uncertainty propagation through deep neural networks, with applications to both random networks and real-world inference tasks.

Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.

</details>


### [203] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: MSN-PINN is a physics-informed neural network that learns power-law scaling exponents as trainable parameters, enabling recovery of singular solution structures with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard neural networks fail to explicitly capture power-law scaling behavior near singularities, interfaces, and critical points, leaving important physical exponents implicit rather than learnable.

Method: Introduces physics-informed Müntz-Szász Networks (MSN-PINN) with power-law basis functions where scaling exponents are trainable parameters. Uses constraint-aware training to encode physical requirements like boundary condition compatibility.

Result: Achieves single-exponent recovery with 1-5% error under noise and sparse sampling. Recovers corner singularity exponents for 2D Laplace equation with 0.009% error, matches classical Kondrat'ev (1967) results, and achieves 100% success rate on 40-configuration wedge benchmark with 0.022% mean error.

Conclusion: MSN-PINN combines neural network expressiveness with asymptotic analysis interpretability, producing physically meaningful parameters and improving accuracy by three orders of magnitude over naive training through constraint-aware methods.

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [204] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: DDIS: A decoupled diffusion framework for inverse PDE problems that separates coefficient prior learning from physics modeling, achieving superior data efficiency and accuracy with limited supervision.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion posterior samplers for inverse PDE problems require substantial paired supervision and implicitly represent physics through joint coefficient-solution modeling, which suffers from guidance attenuation when training data is scarce.

Method: Decoupled Diffusion Inverse Solver (DDIS) uses two components: 1) an unconditional diffusion model learns the coefficient prior, and 2) a neural operator explicitly models the forward PDE for guidance. This enables Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing issues in Diffusion Posterior Sampling (DPS).

Result: DDIS achieves state-of-the-art performance under sparse observation, improving l₂ error by 11% and spectral error by 54% on average. With only 1% training data, DDIS maintains accuracy with 40% advantage in l₂ error compared to joint models.

Conclusion: The decoupled design enables superior data efficiency and effective physics-informed learning while avoiding guidance attenuation failure, making DDIS particularly effective for inverse PDE problems with limited training data.

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


### [205] [Exact closed-form Gaussian moments of residual layers](https://arxiv.org/abs/2601.22307)
*Simon Kuang,Xinfan Lin*

Main category: cs.LG

TL;DR: Exact moment matching for Gaussian propagation through deep neural networks with various activation functions, showing orders-of-magnitude improvement over alternatives.


<details>
  <summary>Details</summary>
Motivation: To address the longstanding gap in propagating mean and covariance of multivariate Gaussian distributions through deep neural networks using exact moment matching, which has practical applications for uncertainty quantification and Bayesian inference.

Method: Layer-by-layer moment matching approach with exact derivations for probit, GeLU, ReLU (as limit of GeLU), Heaviside (as limit of probit), and sine activation functions for both feedforward and generalized residual layers.

Result: Orders-of-magnitude improvements (up to millionfold) in KL divergence error on random networks, competitive statistical calibration for epistemic uncertainty inference on real data, and hundredfold improvements over state-of-the-art deterministic inference methods on variational Bayes networks.

Conclusion: The proposed exact moment matching method provides significant improvements in uncertainty propagation through deep neural networks, closing a longstanding theoretical gap and offering practical benefits for Bayesian inference and uncertainty quantification.

Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.

</details>


### [206] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: MSN-PINN is a physics-informed neural network that learns power-law scaling exponents as trainable parameters, enabling simultaneous recovery of solutions and their scaling structure near singularities.


<details>
  <summary>Details</summary>
Motivation: Standard neural networks fail to explicitly capture power-law scaling behavior near singularities, interfaces, and critical points, leaving important physical exponents implicit rather than learnable.

Method: Introduces physics-informed Müntz-Szász Networks (MSN-PINN) with power-law basis functions where scaling exponents are trainable parameters. Uses constraint-aware training to encode physical requirements like boundary condition compatibility.

Result: Achieves single-exponent recovery with 1-5% error under noise/sparse sampling. Recovers corner singularity exponents for 2D Laplace equation with 0.009% error, matches Kondrat'ev (1967) classical result, and handles singular Poisson problems with 0.03-0.05% errors. On 40-configuration wedge benchmark: 100% success rate with 0.022% mean error.

Conclusion: MSN-PINN combines neural network expressiveness with asymptotic analysis interpretability, producing physically meaningful parameters. Proves identifiability with error scaling O(|μ-α|²), showing constraint-aware training improves accuracy by three orders of magnitude.

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [207] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: DDIS: A decoupled diffusion framework for inverse PDE problems that separates coefficient prior learning from physics modeling, achieving superior data efficiency and accuracy with limited supervision.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion posterior samplers for inverse PDE problems require substantial paired supervision and implicitly represent physics through joint coefficient-solution modeling, leading to poor performance when training data is scarce.

Method: Decoupled Diffusion Inverse Solver (DDIS) with two components: 1) unconditional diffusion model learns coefficient prior, 2) neural operator explicitly models forward PDE for guidance. Uses Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing.

Result: State-of-the-art performance under sparse observation: 11% improvement in l2 error, 54% improvement in spectral error on average. With only 1% training data, maintains 40% advantage in l2 error over joint models.

Conclusion: DDIS provides a data-efficient, physics-aware framework for inverse PDE problems that avoids guidance attenuation failure in joint models, enabling accurate solutions with limited supervision through explicit physics modeling.

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


### [208] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: MF-BPINN: A multi-fidelity Bayesian physics-informed neural network framework that combines low-fidelity simulations with sparse high-fidelity data using hierarchical neural architecture and adaptive residual learning with Bayesian uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: PINNs are powerful for solving PDEs but remain computationally prohibitive for high-fidelity parametric systems requiring multiple evaluations across varying parameters. There's a need to efficiently leverage abundant low-fidelity data alongside sparse high-fidelity measurements.

Method: MF-BPINN combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. It uses hierarchical neural architecture to learn nonlinear correlations across fidelity levels, adaptive residual network with learnable gating mechanisms to balance linear/nonlinear fidelity discrepancies, and Hamiltonian Monte Carlo for rigorous Bayesian inference.

Result: The paper presents a novel framework but doesn't include specific numerical results in the abstract. The method is described as computationally efficient for solving high-fidelity PDEs by leveraging multi-fidelity data.

Conclusion: MF-BPINN provides an efficient multi-fidelity framework that synergistically combines physics-informed learning with Bayesian uncertainty quantification, enabling computationally feasible solutions for high-fidelity parametric PDE systems through adaptive residual learning and hierarchical modeling.

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [209] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

TL;DR: Backpropagation emerges exactly as finite-time relaxation of a physical dynamical system, with exact recovery of standard backpropagation in 2L steps for L-layer networks.


<details>
  <summary>Details</summary>
Motivation: To establish backpropagation as a physical dynamical process rather than just symbolic computation, providing foundation for exact gradient computation in analog/neuromorphic hardware.

Method: Formulate feedforward inference as continuous-time process, apply Lagrangian theory of non-conservative systems, derive global energy functional on doubled state space encoding activations and sensitivities, analyze saddle-point dynamics.

Result: Prove unit-step Euler discretization recovers standard backpropagation exactly in 2L steps for L-layer network, with no approximations. Unlike prior methods, guarantees exact gradients in finite time.

Conclusion: Backpropagation is digitally optimized shadow of continuous physical relaxation, providing rigorous foundation for exact gradient computation in analog/neuromorphic substrates.

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [210] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: MFL-DA dynamics for regularized two-player games are locally exponentially stable near the unique Nash equilibrium, answering an open question about local convergence rates.


<details>
  <summary>Details</summary>
Motivation: The long-time behavior of mean-field Langevin descent-ascent (MFL-DA) dynamics for general nonconvex-nonconcave games was largely unknown, despite the existence of a unique mixed Nash equilibrium. Wang and Chizat (COLT 2024) posed an open question about whether this equilibrium is locally stable and what convergence rates could be established.

Method: The authors prove local exponential stability by establishing a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. This coercivity reveals a local displacement convex-concave structure that drives contraction in the dynamics.

Result: The equilibrium is locally exponentially stable: if initialization is sufficiently close in Wasserstein metric, the dynamics converges to equilibrium at an exponential rate. This settles the local stability and quantitative rate questions posed by Wang and Chizat.

Conclusion: The paper provides a partial resolution to the open question, establishing local exponential stability of the equilibrium for MFL-DA dynamics. However, global convergence remains an open challenge for future research.

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [211] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

TL;DR: RMFlow improves 1-NFE MeanFlow generation by adding a noise-injection refinement step, achieving near SOTA results with comparable computational cost.


<details>
  <summary>Details</summary>
Motivation: MeanFlow enables efficient 1-NFE image generation but often produces suboptimal results. The authors aim to improve 1-NFE MeanFlow generation quality while maintaining efficiency.

Method: RMFlow integrates coarse 1-NFE MeanFlow transport with tailored noise-injection refinement. It approximates average velocity of flow path using neural network trained with new loss function balancing Wasserstein distance minimization and likelihood maximization.

Result: RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, with computational cost comparable to baseline MeanFlows.

Conclusion: RMFlow successfully addresses MeanFlow's 1-NFE quality limitations through refinement integration, enabling efficient high-quality multimodal generation across diverse domains.

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [212] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

TL;DR: Proposes an efficient multiscale graph-based learning framework for proteins that uses hierarchical graphs (fine-grained subgraphs for secondary structures and coarse-grained graph connecting motifs) with two GNNs to capture both local interactions and higher-level structural relationships.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based methods for protein structure learning face challenges in learning multiscale representations and modeling long-range dependencies efficiently. There's a need for a more efficient framework that can capture both local and global structural information in proteins.

Method: 1) Constructs hierarchical graph representation with fine-grained subgraphs for secondary structure motifs (α-helices, β-strands, loops) and a coarse-grained graph connecting these motifs based on spatial arrangement and orientation. 2) Uses two GNNs: first operates within individual secondary motifs for local interactions, second models higher-level structural relationships across motifs. The framework allows flexible choice of GNN in each stage.

Result: Theoretically, the hierarchical framework preserves maximal expressiveness without loss of critical structural information. Empirically, integrating baseline GNNs into this multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

Conclusion: The proposed multiscale graph-based learning framework provides an efficient and effective approach for protein structure analysis by capturing both local interactions within secondary structure motifs and global relationships between motifs, offering improved accuracy and computational efficiency.

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [213] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: The paper introduces a new training objective for flow-based generative models that simultaneously matches both the flow and its divergence, improving accuracy without sacrificing efficiency.


<details>
  <summary>Details</summary>
Motivation: Conditional flow matching (CFM) is efficient but insufficient for ensuring accuracy in learning probability paths, leading to a need for improved training methods that better capture the underlying probability distributions.

Method: The authors develop a PDE characterization of the error between learned and exact probability paths, derive bounds on the total variation gap, and design a new objective function that matches both the flow and its divergence simultaneously.

Result: The new approach improves performance of flow-based generative models by a noticeable margin across several benchmark tasks including dynamical systems, DNA sequences, and videos, while maintaining generation efficiency.

Conclusion: Simultaneous matching of flow and divergence provides a theoretically grounded improvement over standard CFM, enhancing generative model performance without computational trade-offs.

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [214] [On the Convergence of Jacobian-Free Backpropagation for Optimal Control Problems with Implicit Hamiltonians](https://arxiv.org/abs/2602.00921)
*Eric Gelphman,Deepanshu Verma,Nicole Tianjiao Yang,Stanley Osher,Samy Wu Fung*

Main category: math.OC

TL;DR: JFB method converges to stationary points in stochastic minibatch setting for optimal control with implicit Hamiltonians, scaling to high-dimensional problems.


<details>
  <summary>Details</summary>
Motivation: Optimal feedback control with implicit Hamiltonians lacks closed-form optimal control laws, making learning-based value function methods challenging. Previous JFB approach only had sample-wise descent guarantees.

Method: Jacobian-Free Backpropagation (JFB) in stochastic minibatch setting for optimal control with implicit Hamiltonians.

Result: Established convergence guarantees for JFB to stationary points of expected optimal control objective. Demonstrated scalability on high-dimensional problems including multi-agent optimal consumption and swarm-based quadrotor/bicycle control.

Conclusion: JFB provides both theoretical justification and empirical evidence for high-dimensional optimal control with implicit Hamiltonians.

Abstract: Optimal feedback control with implicit Hamiltonians poses a fundamental challenge for learning-based value function methods due to the absence of closed-form optimal control laws. Recent work~\cite{gelphman2025end} introduced an implicit deep learning approach using Jacobian-Free Backpropagation (JFB) to address this setting, but only established sample-wise descent guarantees. In this paper, we establish convergence guarantees for JFB in the stochastic minibatch setting, showing that the resulting updates converge to stationary points of the expected optimal control objective. We further demonstrate scalability on substantially higher-dimensional problems, including multi-agent optimal consumption and swarm-based quadrotor and bicycle control. Together, our results provide both theoretical justification and empirical evidence for using JFB in high-dimensional optimal control with implicit Hamiltonians.

</details>


### [215] [Global stabilization and finite element analysis of the viscous Burgers' equation with memory subject to Neumann boundary feedback control](https://arxiv.org/abs/2602.01321)
*Shishu Pal Singh,Sudeep Kundu*

Main category: math.OC

TL;DR: Global stabilization of viscous Burgers' equation with memory term using Neumann boundary feedback control, with analysis of continuous and semi-discrete schemes, and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To develop stabilization methods for the viscous Burgers' equation with memory term using boundary feedback control, addressing both theoretical stabilization and practical numerical implementation with unknown diffusion coefficients.

Method: Construct feedback control inputs using control Lyapunov functional; establish existence/uniqueness via Faedo-Galerkin method; apply C⁰-conforming finite element method for spatial discretization; use Ritz-Volterra projection for error analysis.

Result: Achieved global stabilization in L², H¹, and H²-norms; established stabilization with unknown diffusion coefficient; obtained optimal error estimates for state variable in L∞, L², H¹-norms; derived error estimates for feedback control laws; validated with numerical simulations.

Conclusion: The proposed Neumann boundary feedback control effectively stabilizes the viscous Burgers' equation with memory term, with rigorous theoretical guarantees and practical numerical implementation validated through simulations.

Abstract: This paper presents a global stabilization result of the viscous Burgers' equation with the memory term by applying Neumann boundary feedback control laws. We construct suitable feedback control inputs using the control Lyapunov functional and establish stabilization in the \(L^{2}, H^{1},\) and \(H^{2}\)-norms. The existence and uniqueness of the solution are established through the Faedo-Galerkin method. Moreover, we show the global stabilization where the diffusion coefficient $ν$ is unknown. Then, we apply a \(C^{0}\)-conforming finite element method to the spatial variable while keeping the time variable continuous. Furthermore, we obtain global stabilization of the semi-discrete scheme and optimal error estimates for the state variable in the \(L^{\infty}\), \(L^{2}\), and \(H^{1}\)-norms, using the Ritz-Volterra projection. Additionally, error estimates for the feedback control laws are established. Lastly, we present some numerical simulations to demonstrate the theoretical findings.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [216] [Revisiting the energy-momentum squared gravity](https://arxiv.org/abs/2601.22333)
*Mihai Marciu*

Main category: gr-qc

TL;DR: Energy-momentum squared gravity theory revisited with second derivative corrections from thermodynamics, analyzed via linear stability theory to show cosmological compatibility with Universe expansion and matter-to-acceleration transitions.


<details>
  <summary>Details</summary>
Motivation: To revisit and extend energy-momentum squared gravity theory by incorporating second derivative terms of matter Lagrangian with respect to metric, motivated by thermodynamic considerations, and to analyze the physical implications of these corrections.

Method: Derived scalar tensor representation of energy-momentum squared gravity with new thermodynamic corrections, then analyzed physical implications using linear stability theory to examine cosmological system behavior.

Result: The cosmological system is compatible with Universe expansion for specific matter Lagrangians, explaining emergence of matter domination era and transition to late-time accelerated expansion approaching de-Sitter phenomenology.

Conclusion: The extended energy-momentum squared gravity theory with thermodynamic corrections provides a viable framework explaining cosmological evolution from matter domination to late-time acceleration, consistent with de-Sitter phenomenology.

Abstract: In this paper we have revisited the energy-momentum squared gravity theory, by taking into account the second derivative of the matter Lagrangian with respect to the metric, encapsulating relations originated from thermodynamical grounds. After obtaining the scalar tensor representation of the energy-momentum squared gravity with the new corrections, we have analyzed the physical implications by relying on the linear stability theory. The results show that the current cosmological system is compatible with the expansion of the Universe for some specific matter Lagrangians, explaining the emergence of matter domination era, approaching the late time accelerated expansion era close to the de-Sitter phenomenology.

</details>


### [217] [Revisiting the energy-momentum squared gravity](https://arxiv.org/abs/2601.22333)
*Mihai Marciu*

Main category: gr-qc

TL;DR: Energy-momentum squared gravity theory revisited with second derivative corrections from thermodynamics, showing cosmological compatibility with Universe expansion and explaining matter domination to late-time acceleration.


<details>
  <summary>Details</summary>
Motivation: To revisit energy-momentum squared gravity theory by incorporating second derivative terms of matter Lagrangian with respect to metric, motivated by thermodynamic considerations, and analyze physical implications for cosmological evolution.

Method: Derived scalar tensor representation of energy-momentum squared gravity with new thermodynamic corrections, then analyzed physical implications using linear stability theory to examine cosmological system behavior.

Result: The cosmological system is compatible with Universe expansion for specific matter Lagrangians, explaining emergence of matter domination era and approaching late-time accelerated expansion close to de-Sitter phenomenology.

Conclusion: The revised energy-momentum squared gravity with thermodynamic corrections successfully describes key cosmological epochs - matter domination and late-time acceleration - aligning with observed Universe expansion patterns.

Abstract: In this paper we have revisited the energy-momentum squared gravity theory, by taking into account the second derivative of the matter Lagrangian with respect to the metric, encapsulating relations originated from thermodynamical grounds. After obtaining the scalar tensor representation of the energy-momentum squared gravity with the new corrections, we have analyzed the physical implications by relying on the linear stability theory. The results show that the current cosmological system is compatible with the expansion of the Universe for some specific matter Lagrangians, explaining the emergence of matter domination era, approaching the late time accelerated expansion era close to the de-Sitter phenomenology.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [218] [What is a POLYNOMIAL-TIME Computable L2-Function?](https://arxiv.org/abs/2601.17078)
*Aras Bacho,Svetlana Selivanova,Martin Ziegler*

Main category: cs.CC

TL;DR: Two natural definitions of polynomial-time computability for L2 functions are shown to be incomparable unless FP₁ includes #P₁.


<details>
  <summary>Details</summary>
Motivation: To establish a clear computational complexity theory for L2 functions by examining different natural definitions of polynomial-time computability and understanding their relationships.

Method: The paper proposes two natural definitions for polynomial-time computability of L2 functions and then proves their incomparability through theoretical analysis, with the condition that FP₁ does not include #P₁.

Result: The two definitions of polynomial-time computability for L2 functions are shown to be incomparable, meaning neither definition implies the other, unless the complexity class FP₁ contains #P₁.

Conclusion: The paper establishes that there is no single natural notion of polynomial-time computability for L2 functions that subsumes all others, highlighting the complexity of defining efficient computation for this function class.

Abstract: We give two natural definitions of polynomial-time computability for L2 functions; and we show them incomparable (unless complexity class FP_1 includes #P_1).

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [219] [Simultaneous Estimation of Seabed and Its Roughness With Longitudinal Waves](https://arxiv.org/abs/2602.01099)
*Babak Maboudi Afkham,Ana Carpio*

Main category: stat.AP

TL;DR: Infinite-dimensional Bayesian framework for acoustic seabed tomography that simultaneously estimates seabed properties and roughness using wave scattering and statistical isotropy assumptions.


<details>
  <summary>Details</summary>
Motivation: Acoustic seabed tomography is ill-posed with multiple seabed configurations producing similar measurements. Need for robust methods to simultaneously estimate seabed properties and roughness while quantifying uncertainties.

Method: Infinite-dimensional Bayesian framework leveraging wave scattering, statistical isotropy of seabed, and fractional differentiability to identify seabed roughness. Includes robust numerical algorithm for estimation.

Result: Extensive numerical experiments validate the method's effectiveness. The approach successfully estimates seabed properties and roughness while quantifying uncertainties.

Conclusion: The proposed framework offers a promising avenue for large-scale seabed exploration by providing robust seabed estimation with uncertainty quantification.

Abstract: This paper introduces an infinite-dimensional Bayesian framework for acoustic seabed tomography, leveraging wave scattering to simultaneously estimate the seabed and its roughness. Tomography is considered an ill-posed problem where multiple seabed configurations can result in similar measurement patterns. We propose a novel approach focusing on the statistical isotropy of the seabed. Utilizing fractional differentiability to identify seabed roughness, the paper presents a robust numerical algorithm to estimate the seabed and quantify uncertainties. Extensive numerical experiments validate the effectiveness of this method, offering a promising avenue for large-scale seabed exploration.

</details>
