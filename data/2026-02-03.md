<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 19]
- [math.AP](#math.AP) [Total: 16]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.DG](#math.DG) [Total: 3]
- [cs.LG](#cs.LG) [Total: 3]
- [math.SP](#math.SP) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Large Language Models: A Mathematical Formulation](https://arxiv.org/abs/2601.22170)
*Ricardo Baptista,Andrew Stuart,Son Tran*

Main category: math.NA

TL;DR: A mathematical framework for understanding large language models (LLMs) that explains token encoding, next-token prediction architecture, learning processes, and deployment for various tasks using accessible mathematical concepts.


<details>
  <summary>Details</summary>
Motivation: To provide a clear mathematical foundation for understanding how LLMs work, despite their empirical success, there's a need for a structured framework that explains their underlying mechanisms using accessible mathematics.

Method: Develops a mathematical framework describing: 1) text encoding into token sequences, 2) next-token prediction model architecture, 3) learning from data processes, and 4) deployment for various tasks. Uses concepts from information theory, probability, and optimization.

Result: Establishes a platform for analyzing LLM accuracy, efficiency, and robustness, and suggests directions for developing modified and new methodologies. The framework makes complex LLM algorithms accessible through straightforward mathematical concepts.

Conclusion: The mathematical framework provides a foundation for understanding, analyzing, and improving LLMs, enabling systematic investigation of their properties and guiding future methodological developments in language modeling.

Abstract: Large language models (LLMs) process and predict sequences containing text to answer questions, and address tasks including document summarization, providing recommendations, writing software and solving quantitative problems. We provide a mathematical framework for LLMs by describing the encoding of text sequences into sequences of tokens, defining the architecture for next-token prediction models, explaining how these models are learned from data, and demonstrating how they are deployed to address a variety of tasks. The mathematical sophistication required to understand this material is not high, and relies on straightforward ideas from information theory, probability and optimization. Nonetheless, the combination of ideas resting on these different components from the mathematical sciences yields a complex algorithmic structure; and this algorithmic structure has demonstrated remarkable empirical successes. The mathematical framework established here provides a platform from which it is possible to formulate and address questions concerning the accuracy, efficiency and robustness of the algorithms that constitute LLMs. The framework also suggests directions for development of modified and new methodologies.

</details>


### [2] [On the $L^p$-Convergence and Denoising Performance of Durrmeyer-Type Max-Min Neural Network Operators](https://arxiv.org/abs/2601.22174)
*Berke Şahin,İsmail Aslan*

Main category: math.NA

TL;DR: Durrmeyer-type max-min neural network operators are introduced, proven to converge in L^p norm, with quantitative convergence rates and demonstrated superior smoothing and filtering performance compared to existing operators.


<details>
  <summary>Details</summary>
Motivation: To develop improved neural network operators that combine Durrmeyer-type generalizations with maximum-minimum operations for better approximation properties and signal processing capabilities.

Method: Generalize maximum-minimum neural network operators using Durrmeyer-type modifications, analyze sigmoidal functions and max-min operations, establish convergence properties in various norms, and derive quantitative convergence rates.

Result: Proved convergence in pointwise, supremum, and L^p norms for functions in L^p([a,b],[0,1]), derived quantitative convergence rates, and demonstrated superior smoothing and filtering performance compared to Kantorovich-type and standard max-min operators.

Conclusion: Durrmeyer-type max-min neural network operators provide effective approximation tools with proven convergence properties, smoother approximations, and superior filtering capabilities for both approximation theory and signal processing applications.

Abstract: In this paper, we investigate Durrmeyer-type generalizations of maximum-minimum neural network operators. The primary objective of this study is to establish the convergence of these operators in the $L^{p}$ norm for functions $f\in L^{p}([a,b],[0,1])$ with $1\leq p<\infty$. To this end, we analyze the properties of sigmoidal functions and maximum-minimum operations, subsequently establishing the convergence of the proposed operator in pointwise, supremum, and $L^{p}$ norms. Furthermore, we derive quantitative estimates for the rates of convergence. In the applications section, numerical and graphical examples demonstrate that the proposed Durrmeyer-type operators provide smoother approximations compared to Kantorovich-type and standard max-min operators. Finally, we highlight the superior filtering performance of these operators in signal analysis, validating their effectiveness in both approximation and data processing tasks.

</details>


### [3] [Convergence Analysis of the Discrete Constrained Saddle Dynamics and Their Momentum Variants](https://arxiv.org/abs/2601.22341)
*Qiang Du,Baoming Shi*

Main category: math.NA

TL;DR: Constrained saddle dynamics with momentum acceleration for finding saddle points on manifolds, with linear convergence rates and reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Need efficient methods to locate saddle points on manifolds for various applications, with challenges in convergence rates and computational cost.

Method: Discrete constrained saddle dynamics and momentum variants on manifolds, with theoretical analysis of convergence rates and eigenvector updates.

Result: Proved local linear convergence with rate depending on Hessian condition number, momentum accelerates convergence, single-step eigenvector update sufficient.

Conclusion: Momentum-based constrained saddle dynamics effectively accelerate convergence, especially for ill-conditioned problems, with reduced computational requirements.

Abstract: We study the discrete constrained saddle dynamics and their momentum variants for locating saddle points on manifolds. Under the assumption of exact unstable eigenvectors, we establish a local linear convergence of the discrete constrained saddle dynamics and show that the convergence rate depends on the condition number of the Riemannian Hessian. To mitigate this dependence, we introduce a momentum-based constrained saddle dynamics and prove local convergence of the continuous-time dynamics as well as the corresponding discrete scheme, which further demonstrates that momentum accelerates convergence, particularly in ill-conditioned settings. In addition, we show that a single-step eigenvector update is sufficient to guarantee local convergence; thus, the assumption of exact unstable eigenvectors is not necessary, which substantially reduces the computational cost. Finally, numerical experiments, including applications to the Thomson problem, the Rayleigh quotient on the Stiefel manifold, and the energy functional of Bose-Einstein condensates, are presented to complement the theoretical analysis.

</details>


### [4] [Low-Rank Approximation by Randomly Pivoted LU](https://arxiv.org/abs/2601.22344)
*Marc Aurèle Gilles,Heather Wilber*

Main category: math.NA

TL;DR: RPLU (Randomly Pivoted LU) provides efficient low-rank approximation with geometric convergence for matrices with rapidly decaying singular values, offering memory-efficient implementation and exploiting matrix structure.


<details>
  <summary>Details</summary>
Motivation: To develop a low-rank approximation algorithm that is both memory-efficient and can exploit matrix structure, addressing limitations of existing methods in constrained memory environments and for structured matrices.

Method: Randomly Pivoted LU (RPLU) - a Gaussian elimination variant where pivots are sampled proportional to squared entries of the Schur complement, enabling geometric convergence for matrices with rapidly decaying singular values.

Result: RPLU achieves geometric convergence in expectation, requires only O(k² + m + n) storage and O(k(m+n) + kM(A) + k³) operations for rank-k approximation, and outperforms existing methods for memory-constrained settings and structured matrices like Cauchy-like matrices.

Conclusion: RPLU is an effective low-rank approximation algorithm that combines memory efficiency with the ability to exploit matrix structure, making it suitable for applications in rational approximation and GPU-based linear system solving.

Abstract: The low-rank approximation properties of Randomly Pivoted LU (RPLU), a variant of Gaussian elimination where pivots are sampled proportional to the squared entries of the Schur complement, are analyzed. It is shown that the RPLU iterates converge geometrically in expectation for matrices with rapidly decaying singular values. RPLU outperforms existing low-rank approximation algorithms in two settings: first, when memory is limited, RPLU can be implemented with $\mathcal{O}(k^2 + m + n)$ storage and $\mathcal{O}( k(m + n)+ k\mathcal{M}(\mat{A}) + k^3)$ operations, where $\mathcal{M}(\mat{A})$ is the cost of a matvec with $\mat{A}\in\mathbb{C}^{n\times m}$ or its adjoint, for a rank-$k$ approximation. Second, when the matrix and its Schur complements share exploitable structure, such as for Cauchy-like matrices. The efficacy of RPLU is illustrated with several examples, including applications in rational approximation and solving large linear systems on GPUs.

</details>


### [5] [Forward-KL Convergence of Time-Inhomogeneous Langevin Diffusions](https://arxiv.org/abs/2601.22349)
*Andreas Habring,Martin Zach*

Main category: math.NA

TL;DR: Non-asymptotic analysis of time-dependent Langevin diffusions and their discretizations for annealing/tempering samplers, with convergence bounds in forward-KL divergence.


<details>
  <summary>Details</summary>
Motivation: Many practical samplers use time-dependent drifts (from annealing/tempering schedules) to improve exploration and stability, but lack unified non-asymptotic analysis of these Langevin diffusions and their discretizations.

Method: Provides unified non-asymptotic convergence analysis for continuous-time Langevin diffusions with time-dependent drifts and their Euler-Maruyama discretizations under abstract conditions on the drift, covering various annealing schemes.

Result: Derives convergence bounds in forward-Kullback-Leibler divergence for both continuous-time diffusion and discretized versions, applicable to practical annealing schemes like geometric tempering and annealed Langevin sampling.

Conclusion: The analysis provides theoretical foundation for time-dependent samplers, with numerical experiments comparing annealing schemes in low- and high-dimensional settings to validate the theory.

Abstract: Many practical samplers rely on time-dependent drifts -- often induced by annealing or tempering schedules -- to improve exploration and stability. This motivates a unified non-asymptotic analysis of the corresponding Langevin diffusions and their discretizations. We provide a convergence analysis that includes non-asymptotic bounds for the continuous-time diffusion and its Euler--Maruyama discretization in the forward-Kullback--Leibler divergence under a single set of abstract conditions on the time-dependent drift. The results apply to many practically-relevant annealing schemes, including geometric tempering and annealed Langevin sampling. In addition, we provide numerical experiments comparing the annealing schemes covered by our theory in low- and high-dimensional settings.

</details>


### [6] [Inverse acoustic scattering for random obstacles with multi-frequency data](https://arxiv.org/abs/2601.22560)
*Zhiqi Sun,Xiang Xu,Yiwen Lin*

Main category: math.NA

TL;DR: Two-stage inversion method for random obstacle scattering using Gaussian process modeling with KL expansion to recover both baseline shape and statistical characteristics from multi-frequency data.


<details>
  <summary>Details</summary>
Motivation: To develop an inverse scattering method for random obstacles where the scatterer boundary is modeled as a random process, requiring simultaneous recovery of both geometric shape and statistical properties of boundary fluctuations.

Method: Two-stage inversion: first stage reconstructs baseline shape using multi-frequency data; second stage estimates statistical characteristics (KL eigenvalues and covariance hyperparameters) of Gaussian process boundary fluctuations with modified covariance function.

Result: Theoretical justifications provided for model well-definedness, convergence of two-stage procedure, and uniqueness discussion. Numerical experiments show stable recovery of both geometric and statistical information for simple and complex obstacle shapes.

Conclusion: Proposed method successfully addresses inverse random obstacle scattering by combining Gaussian process modeling with two-stage inversion, enabling simultaneous recovery of deterministic baseline shape and statistical characteristics of random boundary fluctuations.

Abstract: We study an inverse random obstacle scattering problems in $\mathbb{R}^2$ where the scatterer is formulated by a Gaussian process defined on the angular parameter domain. Equipped with a modified covariance function which is mathematically well-defined and physically consistent, the Gaussian process admits a parameterization via Karhunen--Loève (KL) expansion. Based on observed multi-frequency data, we develop a two-stage inversion method: the first stage reconstructs the baseline shape of the random scatterer and the second stage estimates the statistical characteristics of the boundary fluctuations, including KL eigenvalues and covariance hyperparameters. We further provide theoretical justifications for the modeling and inversion pipeline, covering well-definedness of the Gaussian-process model, convergence for the two-stage procedure and a brief discussion on uniqueness. Numerical experiments demonstrate stable recovery of both geometric and statistical information for obstacles with simple and more complex shapes.

</details>


### [7] [An ultra-weak three-field finite element formulation for the biharmonic and extended Fisher--Kolmogorov equations](https://arxiv.org/abs/2601.22587)
*Rekha Khot,Bishnu P. Lamichhane,Ricardo Ruiz-Baier*

Main category: math.NA

TL;DR: Ultra-weak three-field formulation for biharmonic problem with solution, gradient, and Lagrange multiplier as unknowns; well-posedness established using saddle-point theory; conforming FEM with Raviart-Thomas discretization; extended to time-dependent semilinear extended Fisher-Kolmogorov equation.


<details>
  <summary>Details</summary>
Motivation: To develop a novel ultra-weak three-field formulation for the biharmonic problem that introduces the solution, its gradient, and an additional Lagrange multiplier as separate unknowns, allowing for more flexible discretization approaches.

Method: 1) Ultra-weak three-field formulation with solution, gradient, and Lagrange multiplier unknowns; 2) Well-posedness analysis using abstract saddle-point theory; 3) Conforming finite element scheme with Raviart-Thomas discretizations for auxiliary variables; 4) Discrete inf-sup condition for stability; 5) Extension to time-dependent semilinear extended Fisher-Kolmogorov equation.

Result: 1) Established well-posedness of continuous and discrete formulations; 2) Derived a priori error estimates; 3) Developed stable finite element scheme; 4) Successfully extended analysis to time-dependent semilinear case; 5) Demonstrated performance through numerical examples.

Conclusion: The proposed ultra-weak three-field formulation provides a robust framework for solving biharmonic problems with well-posed continuous and discrete formulations, stable finite element discretization, and extensibility to time-dependent semilinear equations, validated by numerical experiments.

Abstract: This paper discusses a so-called ultra-weak three-field formulation of the biharmonic problem where the solution, its gradient, and an additional Lagrange multiplier are the three unknowns. We establish the well-posedness of the problem using the abstract theory for saddle-point problems, and develop a conforming finite element scheme based on Raviart--Thomas discretisations of the two auxiliary variables. The well-posedness of the discrete formulation and the corresponding a priori error estimate are proved using a discrete inf-sup condition. We further extend the analysis to the time-dependent semilinear equation, namely extended Fisher--Kolmogorov equation. We present a few numerical examples to demonstrate the performance of our approach.

</details>


### [8] [An inertial minimal-deformation-rate framework for shape optimization](https://arxiv.org/abs/2601.22605)
*Falai Chen,Buyang Li,Jiajie Li,Rong Tang*

Main category: math.NA

TL;DR: A robust numerical framework combining inertial flow with mesh preservation for PDE-constrained shape optimization and Willmore-driven surface hole filling, achieving faster convergence and better mesh quality without remeshing.


<details>
  <summary>Details</summary>
Motivation: Address two key challenges in geometric evolution problems: (1) slow progress in flat energy landscapes leading to premature stagnation at suboptimal configurations, and (2) mesh deterioration during geometric evolution requiring frequent remeshing.

Method: Couples second-order inertial flow with minimal-deformation-rate (MDR) mesh motion strategy to accelerate convergence while preserving mesh quality. Incorporates surface-diffusion regularization within Barrett-Garcke-Nürberg (BGN) framework for robustness with non-smooth/non-convex initial geometries. Extends inertial MDR methodology to Willmore-type surface hole filling.

Result: Numerical experiments show markedly faster convergence to lower original objective values, with consistently superior mesh preservation throughout the evolution. Enables high-order smooth reconstructions even from incompatible initial data.

Conclusion: The proposed framework provides a robust solution for PDE-constrained shape optimization and surface hole filling problems, overcoming stagnation issues and mesh deterioration while eliminating the need for remeshing.

Abstract: We propose a robust numerical framework for PDE-constrained shape optimization and Willmore-driven surface hole filling. To address two central challenges -- slow progress in flat energy landscapes, which can trigger premature stagnation at suboptimal configurations, and mesh deterioration during geometric evolution -- we couple a second-order inertial flow with a minimal-deformation-rate (MDR) mesh motion strategy. This coupling accelerates convergence while preserving mesh quality and thus avoids remeshing. To further enhance robustness for non-smooth or non-convex initial geometries, we incorporate surface-diffusion regularization within the Barrett-Garcke-N"urnberg (BGN) framework. Moreover, we extend the inertial MDR methodology to Willmore-type surface hole filling, enabling high-order smooth reconstructions even from incompatible initial data. Numerical experiments demonstrate markedly faster convergence to lower original objective values, together with consistently superior mesh preservation throughout the evolution.

</details>


### [9] [A Mathematical Analysis of a Smooth-Convex-Concave Splitting Scheme for the Swift--Hohenberg Equation](https://arxiv.org/abs/2601.22687)
*Yuki Yonekura,Daiki Iwade,Shun Sato,Takayasu Matsuo*

Main category: math.NA

TL;DR: First linearly implicit finite difference scheme for 3D Swift-Hohenberg equation that preserves energy-dissipation law while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Existing structure-preserving schemes for the Swift-Hohenberg equation are fully implicit and computationally expensive, creating a need for more efficient methods that still preserve key physical properties.

Method: Introduces a design principle for dissipation-preserving finite difference schemes using discrete inequalities for the underlying energy, assuming Lipschitz continuous gradient and convexity/strong convexity of relevant terms. Results in a linearly implicit scheme for 3D Swift-Hohenberg equation.

Result: The proposed method preserves original energy-dissipation law, guarantees unique solvability, ensures boundedness of numerical solutions, and admits an a priori error estimate with sufficiently small time steps.

Conclusion: This is the first linearly implicit finite difference scheme for the Swift-Hohenberg equation that simultaneously achieves computational efficiency while preserving all key mathematical and physical properties.

Abstract: The Swift--Hohenberg equation is a widely studied fourth-order model, originally proposed to describe hydrodynamic fluctuations. It admits an energy-dissipation law and, under suitable assumptions, bounded solutions. Many structure-preserving numerical schemes have been proposed to retain such properties; however, existing approaches are often fully implicit and therefore computationally expensive. We introduce a simple design principle for constructing dissipation-preserving finite difference schemes and apply it to the Swift--Hohenberg equation in three spatial dimensions. Our analysis relies on discrete inequalities for the underlying energy, assuming a Lipschitz continuous gradient and either convexity or $μ$-strong convexity of the relevant terms. The resulting method is linearly implicit, yet it preserves the original energy-dissipation law, guarantees unique solvability, ensures boundedness of numerical solutions, and admits an a priori error estimate, provided that the time step is sufficiently small. To the best of our knowledge, this is the first linearly implicit finite difference scheme for the Swift--Hohenberg equation for which all of these properties are established.

</details>


### [10] [Numerical Differentiation of Functions of Two Variables Using Chebyshev Polynomials](https://arxiv.org/abs/2601.22762)
*Maksym Kyselov,Sergiy G. Solodky*

Main category: math.NA

TL;DR: A new truncation method using Chebyshev polynomial expansions and hyperbolic cross for numerical differentiation of bivariate functions from weighted Wiener classes.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for numerical differentiation of bivariate functions from weighted Wiener classes, addressing the challenge of reconstructing partial derivatives of arbitrary order with controlled error estimates.

Method: A new truncation method based on Chebyshev polynomial expansions and hyperbolic cross approach, exploiting Chebyshev polynomials' approximation properties and their connection to weighted spaces through the Chebyshev weight function.

Result: Derived a choice rule for truncation parameter as function of noise level, smoothness parameters, and differentiation order, enabling explicit error estimates in both weighted integral norms and uniform metric.

Conclusion: The proposed method provides a systematic approach for numerical differentiation of bivariate functions with rigorous error control in different metrics, leveraging Chebyshev polynomial properties and hyperbolic cross techniques.

Abstract: We investigate the problem of numerical differentiation of bivariate functions from weighted Wiener classes using Chebyshev polynomial expansions. We develop and analyze a new version of the truncation method based on Chebyshev polynomials and the idea of hyperbolic cross to reconstruct partial derivatives of arbitrary order. The method exploits the approximation properties of Chebyshev polynomials and their natural connection to weighted spaces through the Chebyshev weight function. We derive a choice rule for the truncation parameter as a function of the noise level, smoothness parameters of the function class, and the order of differentiation. This approach allows us to establish explicit error estimates in both weighted integral norms and uniform metric.

</details>


### [11] [Approximation of PDE solution manifolds: Sparse-grid interpolation and quadrature](https://arxiv.org/abs/2601.22825)
*Dinh Dũng,Van Kien Nguyen,Duong Thanh Pham,Christoph Schwab*

Main category: math.NA

TL;DR: The paper develops fully-discrete approximations and quadratures for infinite-variate functions in Bochner spaces using sparse-grid tensor-product polynomial interpolation based on Chebyshev points, with applications to parametric PDEs and holomorphic maps.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of approximating infinite-variate functions in high-dimensional parameter spaces, particularly for parametric PDEs and holomorphic maps, while avoiding the curse of dimensionality through efficient sparse-grid constructions.

Method: Uses sparse-grid tensor-product polynomial interpolation based on univariate Chebyshev points, with Jacobi generalized polynomial chaos expansions. Develops fully-discrete approximations via stable discretizations of Hilbert spaces and sparse-grid tensor-product projectors.

Result: Obtains convergence rates for approximations and quadratures free from the curse of dimensionality, verified for linear elliptic diffusion equations with affine-parametric coefficients and abstract holomorphic maps between Hilbert spaces.

Conclusion: The framework provides dimension-independent convergence rates for sparse-grid approximations and quadratures in infinite-variate settings, with applications to parametric PDEs and holomorphic maps, and lays groundwork for neural network extensions.

Abstract: We study fully-discrete approximations and quadratures of infinite-variate functions in abstract Bochner spaces associated with a Hilbert space $X$ and an infinite-tensor-product Jacobi measure. For target infinite-variate functions taking values in $X$ which admit absolutely convergent Jacobi generalized polynomial chaos expansions, with suitable weighted summability conditions for the coefficient sequences, we generalize and improve prior results on construction of sequences of finite sparse-grid tensor-product polynomial interpolation approximations and quadratures, based on the univariate Chebyshev points. For a generic stable discretization of $X$ in terms of a dense sequence $(V_m)_{m \in \mathbb{N}_0}$ of finite-dimensional subspaces, we obtain fully-discrete, linear approximations in terms of so-called sparse-grid tensor-product projectors, with convergence rates of approximations as well as of sparse-grid tensor-product quadratures of the target functions.
  We verify the abstract assumptions in two fundamental application settings: first, a linear elliptic diffusion equation with affine-parametric coefficients and second, abstract holomorphic maps between separable Hilbert spaces with affine-parametric input data encoding. For these settings, as in [37,20], cancellation of anti-symmetric terms in ultra-spherical Jacobi generalized polynomial chaos expansion coefficients implies crucially improved convergence rates of sparse-grid tensor-product quadrature with respect to the infinite-tensor-product Jacobi weight, free from the ``curse-of-dimension".
  Largely self-contained proofs of all results are developed. Approximation convergence rate results in the present setting which are based on construction of neural network surrogates, for unbounded parameter ranges with Gaussian measures, will be developed in extensions of the present work.

</details>


### [12] [On the convergence and efficiency of splitting schemes for the Cahn-Hilliard-Biot model](https://arxiv.org/abs/2601.22854)
*Cedric Riethmüller,Erlend Storvik*

Main category: math.NA

TL;DR: Novel solution strategy for Cahn-Hilliard-Biot model using semi-implicit time discretization and alternating minimization with proven convergence.


<details>
  <summary>Details</summary>
Motivation: The Cahn-Hilliard-Biot model presents significant challenges due to its coupled, nonlinear, and non-convex nature, requiring a consistent and efficient solution strategy for this three-way coupled system of solid phase separation, fluid dynamics, and elastic deformations in porous media.

Method: Introduces a semi-implicit time discretization that transforms the system into a convex minimization problem, then uses abstract convex theory to prove convergence of an alternating minimization method with flexible spatial discretization (requiring standard inverse inequalities).

Result: Numerical experiments demonstrate the promise of the proposed solution strategy in terms of both efficiency and robustness for solving the challenging Cahn-Hilliard-Biot system.

Conclusion: The paper presents a novel, theoretically sound solution strategy with proven convergence properties that effectively addresses the computational challenges of the coupled Cahn-Hilliard-Biot model.

Abstract: In this paper, we present a novel solution strategy for the Cahn-Hilliard-Biot model, a three-way coupled system that features the interplay of solid phase separation, fluid dynamics, and elastic deformations in porous media. It is a phase-field model that combines the Cahn-Hilliard regularized interface equation and Biot's equations of poroelasticity. Solving the system poses significant challenges due to its coupled, nonlinear, and non-convex nature. The main goal of this work is to provide a consistent and efficient solution strategy. With this in mind, we introduce a semi-implicit time discretization such that the resulting discrete system is equivalent to a convex minimization problem. Then, using abstract theory for convex problems, we prove the convergence of an alternating minimization method to the time-discrete system. The solution strategy is relatively flexible in terms of spatial discretization, although we require standard inverse inequalities for the guaranteed convergence of the alternating minimization method. Finally, we perform some numerical experiments that show the promise of the proposed solution strategy, both in terms of efficiency and robustness.

</details>


### [13] [Bayesian Interpolating Neural Network (B-INN): a scalable and reliable Bayesian model for large-scale physical systems](https://arxiv.org/abs/2601.22860)
*Chanwook Park,Brian Kim,Jiachen Guo,Wing Kam Liu*

Main category: math.NA

TL;DR: B-INN: A scalable Bayesian surrogate model combining interpolation theory with tensor decomposition for efficient uncertainty quantification in large-scale industrial simulations.


<details>
  <summary>Details</summary>
Motivation: Current neural networks and machine learning models for uncertainty quantification have limited scalability and poor reliability, making them impractical for industry-scale active learning where simulations take days/weeks and produce gigabytes of data.

Method: Combines high-order interpolation theory with tensor decomposition and alternating direction algorithm for effective dimensionality reduction while maintaining predictive accuracy. The model has linear complexity O(N) with respect to training samples.

Result: B-INNs are 20 to 10,000 times faster than Bayesian neural networks and Gaussian processes while providing robust uncertainty estimation. The function space is a subset of Gaussian processes.

Conclusion: B-INN provides a practical foundation for uncertainty-driven active learning in large-scale industrial simulations, offering computational efficiency and robust uncertainty calibration essential for industrial applications.

Abstract: Neural networks and machine learning models for uncertainty quantification suffer from limited scalability and poor reliability compared to their deterministic counterparts. In industry-scale active learning settings, where generating a single high-fidelity simulation may require days or weeks of computation and produce data volumes on the order of gigabytes, they quickly become impractical. This paper proposes a scalable and reliable Bayesian surrogate model, termed the Bayesian Interpolating Neural Network (B-INN). The B-INN combines high-order interpolation theory with tensor decomposition and alternating direction algorithm to enable effective dimensionality reduction without compromising predictive accuracy. We theoretically show that the function space of a B-INN is a subset of that of Gaussian processes, while its Bayesian inference exhibits linear complexity, $\mathcal{O}(N)$, with respect to the number of training samples. Numerical experiments demonstrate that B-INNs can be from 20 times to 10,000 times faster with a robust uncertainty estimation compared to Bayesian neural networks and Gaussian processes. These capabilities make B-INN a practical foundation for uncertainty-driven active learning in large-scale industrial simulations, where computational efficiency and robust uncertainty calibration are paramount.

</details>


### [14] [Randomized Methods for Kernelized DMD](https://arxiv.org/abs/2601.22867)
*Peter Oehme*

Main category: math.NA

TL;DR: The paper proposes using RPCholesky algorithm for kernelized DMD to accelerate processing of large-scale datasets through randomized low-rank approximations with better stability guarantees.


<details>
  <summary>Details</summary>
Motivation: Dynamic Mode Decomposition (DMD) needs acceleration for large-scale datasets, and existing randomized techniques for kernelized DMD lack stability guarantees and optimal tradeoffs between exploration and exploitation of data information.

Method: Apply RPCholesky algorithm to kernelized DMD (KDMD) using adaptive randomized sampling to approximate positive semidefinite kernel matrices, providing better stability than previous randomized methods.

Result: The proposed method demonstrates efficacy on established DMD benchmark problems with increasing dimensions, showing improved numerical stability and better tradeoff between exploration and exploitation.

Conclusion: RPCholesky algorithm combined with KDMD provides an effective randomized technique for accelerating DMD on large-scale datasets with enhanced stability guarantees compared to existing methods.

Abstract: Dynamic Mode Decomposition (DMD) is a data-driven method related to Koopman operator theory that extracts information about dominant dynamics from data snapshots. In this paper we examine techniques to accelerate the application of DMD to large-scale data sets with an eye on randomized techniques. Randomized techniques exploit low-rank matrix approximations at a much smaller computational cost, therefore permitting the use of increased data set sizes. In particular, we propose the application of the RPCholesky algorithm in the setting of kernelized DMD (KDMD). This algorithm relies on adaptive randomized sampling to approximate positive semidefinite kernel matrices and provides better stability guarantees than previously implemented randomized methods for KDMD. Differences between existing competitive randomized techniques and our proposed implementation are discussed with a focus on numerical stability and tradeoff between exploration and exploitation of information obtained from data. The efficacy of this new combination of algorithms is demonstrated on well-established benchmark problems from DMD literature increasing in problem dimension.

</details>


### [15] [FNWoS: Fractional Neural Walk-on-Spheres Methods for High-Dimensional PDEs Driven by $α$-stable Lévy Process on Irregular Domains](https://arxiv.org/abs/2601.22942)
*Ling Guo,Mingxin Qin,Changtao Sheng,Hao Wu,Fanhai Zeng*

Main category: math.NA

TL;DR: FNWoS is a derivative-free neural-enhanced Monte Carlo method for solving high-dimensional fractional Poisson equations on irregular domains, combining simplified walk-on-spheres with neural surrogates and buffered training.


<details>
  <summary>Details</summary>
Motivation: Solving high-dimensional fractional Poisson equations on irregular domains is computationally challenging due to the curse of dimensionality and complex boundary conditions. Traditional methods struggle with both high dimensions and irregular geometries.

Method: 1) Simplified fractional walk-on-spheres (FWoS) with constant weights to reduce per-trajectory cost; 2) Neural network surrogate integration (FNWoS) to amortize sampling; 3) Truncated path strategy for α near 2; 4) Buffered supervision (BFNWoS) with cached training pairs and progressive Monte Carlo target refinement.

Result: The method demonstrates accuracy, scalability, and computational efficiency in extensive numerical experiments, including tests on irregular domains and problems with dimensions up to 1000. It achieves more accurate evaluation with dramatically fewer trajectories than classical FWoS.

Conclusion: The proposed FNWoS and BFNWoS methods provide effective solutions for high-dimensional fractional Poisson equations on irregular domains, overcoming limitations of traditional Monte Carlo methods through neural network integration and buffered training strategies.

Abstract: In this paper, we develop a highly parallel and derivative-free fractional neural walk-on-spheres method (FNWoS) for solving high-dimensional fractional Poisson equations on irregular domains. We first propose a simplified fractional walk-on-spheres (FWoS) scheme that replaces the high-dimensional normalized weight integral with a constant weight and adopts a correspondingly simpler sampling density, substantially reducing per-trajectory cost. To mitigate the slow convergence of standard Monte Carlo sampling, FNWoS is then proposed via integrating this simplified FWoS estimator, derived from the Feynman-Kac representation, with a neural network surrogate. By amortizing sampling effort over the entire domain during training, FNWoS achieves more accurate evaluation at arbitrary query points with dramatically fewer trajectories than classical FWoS. To further enhance efficiency in regimes where the fractional order $α$ is close to 2 and trajectories become excessively long, we introduce a truncated path strategy with a prescribed maximum step count. Building on this, we propose a buffered supervision mechanism that caches training pairs and progressively refines their Monte Carlo targets during training, removing the need to precompute a highly accurate training set and yielding the buffered fractional neural walk-on-spheres method (BFNWoS). Extensive numerical experiments, including tests on irregular domains and problems with dimensions up to $1000$, demonstrate the accuracy, scalability, and computational efficiency of the proposed methods.

</details>


### [16] [Preconditioning and Numerical Stability in Neural Network Training for Parametric PDEs](https://arxiv.org/abs/2601.23185)
*Markus Bachmayr,Wolfgang Dahmen,Chenguang Duan,Mathias Oster*

Main category: math.NA

TL;DR: Preconditioning via well-conditioned frame representations improves neural network training for PDE approximations, with stable representations enabling low-precision computations without precision loss.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of neural network-based approximations for parameter-dependent PDE solutions by addressing numerical stability issues in preconditioning methods.

Method: Using well-conditioned frame representations of operators for preconditioning, and developing stable representations that enable computations with single- and half-precision floating point numbers.

Result: Significant improvement in training performance compared to standard methods, with stable representations allowing low-precision computations without loss of numerical precision.

Conclusion: Preconditioning via well-conditioned frame representations enhances neural network training for PDE approximations, and the proposed stable representations enable efficient low-precision computations while maintaining numerical stability.

Abstract: In the context of training neural network-based approximations of solutions of parameter-dependent PDEs, we investigate the effect of preconditioning via well-conditioned frame representations of operators and demonstrate a significant improvement on the performance of standard training methods. We also observe that standard representations of preconditioned matrices are insufficient for obtaining numerical stability and propose a generally applicable form of stable representations that enables computations with single- and half-precision floating point numbers without loss of precision.

</details>


### [17] [Applications of QR-based Vector-Valued Rational Approximation](https://arxiv.org/abs/2601.23237)
*Simon Dirckx*

Main category: math.NA

TL;DR: QR-AAA algorithm applied to multiple computational problems showing flexibility and effectiveness


<details>
  <summary>Details</summary>
Motivation: To demonstrate the practical utility and versatility of the QR-AAA algorithm across diverse computational applications

Method: QR-AAA algorithm - a greedy scheme for vector-valued rational approximation applied to various computational settings

Result: Successful applications in Stokes flow computation, multivariate rational approximation, function extension, novel quadrature methods, and near-field BEM approximation

Conclusion: QR-AAA algorithm proves to be flexible and practically effective across multiple computational domains

Abstract: Several applications of the QR-AAA algorithm, a greedy scheme for vector-valued rational approximation, are presented. The focus is on demonstrating the flexibility and practical effectiveness of QR-AAA in a variety of computational settings, including Stokes flow computation, multivariate rational approximation, function extension, the development of novel quadrature methods and near-field approximation in the boundary element method.

</details>


### [18] [A Primal-Dual Level Set Method for Computing Geodesic Distances](https://arxiv.org/abs/2601.23244)
*Hailiang Liu,Laura Zinnel*

Main category: math.NA

TL;DR: A primal-dual level set method for computing geodesic distances on surfaces using implicit surface representation and constraint minimization.


<details>
  <summary>Details</summary>
Motivation: Geodesic distance computation on surfaces is more complex than Euclidean distance due to surface geometry influence, with wide applications in various fields.

Method: Primal-dual level set method that implicitly represents surfaces as zero level sets, formulates constraint minimization problems, and uses regularization and acceleration techniques.

Result: The method is robust, efficient, easy to implement, with established convergence results for high-resolution PDE systems, and numerical evidence suggests convergence to geodesics in refinement limit.

Conclusion: The primal-dual level set approach provides an effective solution for geodesic distance computation on surfaces with theoretical convergence guarantees and practical implementation advantages.

Abstract: The numerical computation of shortest paths or geodesics on surfaces, along with the associated geodesic distance, has a wide range of applications. Compared to Euclidean distance computation, these tasks are more complex due to the influence of surface geometry on the behavior of shortest paths. This paper introduces a primal-dual level set method for computing geodesic distances. A key insight is that the underlying surface can be implicitly represented as a zero level set, allowing us to formulate a constraint minimization problem. We employ the primal-dual methodology, along with regularization and acceleration techniques, to develop our algorithm. This approach is robust, efficient, and easy to implement. We establish a convergence result for the high-resolution PDE system, and numerical evidence suggests that the method converges to a geodesic in the limit of refinement.

</details>


### [19] [Rank Reduction AutoEncoders for Mechanical Design: Advancing Novel and Efficient Data-Driven Topology Optimization](https://arxiv.org/abs/2601.23269)
*Ismael Ben-Yelun,Mohammed El Fallaki Idrissi,Jad Mounayer,Sebastian Rodriguez,Francisco Chinesta*

Main category: math.NA

TL;DR: A data-driven framework combining Rank Reduction Autoencoders (RRAEs) with neural networks for fast forward/inverse topology optimization, enabling efficient surrogate models for mechanical design.


<details>
  <summary>Details</summary>
Motivation: To address the computational expense of traditional topology optimization by developing fast surrogate models that can efficiently approximate relationships between optimized geometries and mechanical responses.

Method: Uses Rank Reduction Autoencoders (RRAEs) based on SVD to compress high-dimensional TO data into low-rank latent representations. Separate RRAEs are trained for geometry and different QoIs (scalar metrics, 1D stress fields, 2D von Mises distributions). Multilayer perceptrons then map between latent spaces for forward (geometry→response) and inverse (response→geometry) problems.

Result: The framework achieves accurate and computationally efficient surrogate models on a half MBB beam benchmark. Performance improves with richer QoIs, showing increasing robustness and fidelity. Enables generative mechanical design through latent-space exploration.

Conclusion: The proposed RRAE-based framework successfully enables fast forward/inverse analysis in topology optimization, providing a foundation for efficient surrogate modeling and generative mechanical design with potential applications in computational mechanics.

Abstract: This work presents a data-driven framework for fast forward and inverse analysis in topology optimization (TO) by combining Rank Reduction Autoencoders (RRAEs) with neural latent-space mappings. The methodology targets the efficient approximation of the relationship between optimized geometries and their corresponding mechanical responses or Quantity of Interest (QoI), with a particular focus on compliance-minimized linear elastic structures. High-dimensional TO results are first compressed using RRAEs, which encode the data into a low-rank approximation via Singular Value Decomposition (SVD), obtained in this sense the most important features that approximate the data. Separate RRAE models are trained for geometry and for different types of QoIs, including scalar metrics, one-dimensional stress fields, and full two-dimensional von Mises stress distributions. The resulting low-dimensional latent coefficients of the latent space are then related through multilayer perceptrons to address both direct problems -- predicting structural responses from geometry -- and inverse problems -- recovering geometries from prescribed performance targets. The proposed approach is demonstrated on a benchmark TO problem based on a half MBB beam, using datasets generated via density-based Solid Isotropic Material with Penalization (SIMP) optimization. Numerical results show that the framework enables accurate and computationally efficient surrogate models, with increasing robustness and fidelity as richer QoIs are considered. The methodology also provides a foundation for generative mechanical design by enabling the synthesis of new geometries and responses through latent-space exploration.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [20] [The metaplectic semigroup and its applications to time-frequency analysis and evolution operators](https://arxiv.org/abs/2601.22252)
*Gianluca Giacchi,Luigi Rodino,Davide Tramontana*

Main category: math.AP

TL;DR: Extends metaplectic theory to positive complex symplectic matrices using operator-theoretic approach, investigates semigroup structure, and applies to time-frequency analysis and parabolic equations.


<details>
  <summary>Details</summary>
Motivation: Existing literature focuses on propagators of quadratic evolution equations via Mehler formulas, but lacks systematic operator-theoretic analysis of the metaplectic semigroup for positive complex symplectic matrices beyond unitary settings.

Method: Operator-theoretic and symplectic approach adapting techniques from standard metaplectic group to broader framework, investigating generators, polar decomposition, and intertwining relations with complex conjugation and Wigner distribution.

Result: Provides deeper insight into metaplectic semigroup structure, characterizes time-frequency representations with prescribed properties, studies boundedness of propagators on modulation spaces, obtains operator norm estimates, and analyzes Wigner singularity propagation.

Conclusion: Systematic operator-theoretic framework extends metaplectic theory beyond differential problems, enabling new applications in time-frequency analysis and parabolic equations with complex quadratic Hamiltonians.

Abstract: We develop a systematic analysis of the metaplectic semigroup $\mathrm{Mp}_+(d,\mathbb{C})$ associated with positive complex symplectic matrices, a notion introduced almost simultaneously and independently by Hörmander, Brunet, Kramer, and Howe, thereby extending the classical metaplectic theory beyond the unitary setting.
  While the existing literature has largely focused on propagators of quadratic evolution equations, for which results are typically obtained via Mehler formulas, our approach is operator-theoretic and symplectic in spirit and adapts techniques from the standard metaplectic group $\mathrm{Mp}(d,\mathbb{R})$ to a substantially broader framework that is not driven by differential problems or particular propagators.
  This point of view provides deeper insight into the structure of the metaplectic semigroup, and allows us to investigate its generators, polar decomposition, and intertwining relations with complex conjugation and with the Wigner distribution. We then exploit these structural results to characterize, from a metaplectic perspective, classes of time-frequency representations satisfying prescribed structural properties. Finally, we discuss further implications for parabolic equations with complex quadratic Hamiltonians, we study the boundedness of their propagators on modulation spaces, we obtain estimates in time of their operator norms. Finally, we apply our theory to the study of propagation of Wigner singularities.

</details>


### [21] [A Generalized Analytical Heat Transfer Model for Enhanced Geothermal Systems: Capturing Fracture Interactions and Correcting Classical Optimistic Predictions](https://arxiv.org/abs/2601.22316)
*Nelson Barros-Galvis or Christine Ehlig-Economides or Cristi Darley Guevara*

Main category: math.AP

TL;DR: A new analytical heat transfer model for enhanced geothermal systems that accounts for fracture interactions, corrects optimistic bias in classical models, and is computationally efficient enough for spreadsheet implementation.


<details>
  <summary>Details</summary>
Motivation: Classical analytical models like Gringarten et al. 1975 overestimate thermal performance due to simplified assumptions, leading to unrealistic engineering decisions in geothermal design and feasibility studies.

Method: Developed a generalized analytical model based on Green's functions that explicitly captures thermal interactions between fractures while preserving analytical tractability. The solution avoids Laplace space transformations or numerical inversion algorithms.

Result: The model shows close agreement with numerical simulations (CMG STARS and Volsung software) in temperature evolution, including fracture interaction effects. It corrects optimistic bias of classical approaches and provides more reliable predictions of production temperature and energy recovery.

Conclusion: The proposed model bridges the gap between legacy analytical models and numerical/commercial tools, with direct implications for geothermal feasibility studies, well design, and power forecasting, while retaining analytical simplicity and practical applicability.

Abstract: Numerical analytical heat transfer models play a critical role in geothermal design and feasibility studies. Classical solutions, such as those proposed by Gringarten et al. 1975, rely on simplified assumptions and systematically overestimate thermal performance, which can lead to unrealistic engineering decisions.
  This study presents a generalized analytical model for enhanced geothermal systems that explicitly captures thermal interactions between fractures while preserving analytical tractability.
  The formulation is based on Greenś functions and reproduces realistic thermal behavior under conditions representative of fractured geothermal reservoirs. The resulting solution is computationally efficient and sufficiently simple to be implemented directly in standard spreadsheets, without requiring Laplace space transformations or numerical inversion algorithms.
  The model is validated against numerical simulations performed using CMG STARS and Volsung software, showing close agreement in temperature evolution, including the effects of interacting fractures. Compared with classical analytical approaches, the proposed model corrects optimistic bias and provides more reliable predictions of production temperature and energy recovery.
  These results have direct implications for geothermal feasibility studies, well design, and power forecasting, effectively bridging the gap between legacy analytical models and numerical or commercial engineering tools. Building on the analytical framework originally introduced by Gringarten et al. 1975, the proposed formulation generalizes classical heat transfer solutions to account for fracture interaction while retaining analytical simplicity and practical applicability.

</details>


### [22] [Local existence and nonexistence of solutions to the Hardy parabolic equation with general nonlinearity](https://arxiv.org/abs/2601.22520)
*Yo Tsusaka*

Main category: math.AP

TL;DR: Local existence and nonexistence results for Hardy parabolic equation with general nonlinearity, establishing optimal integrability conditions for initial data.


<details>
  <summary>Details</summary>
Motivation: To determine precise conditions on initial data that guarantee existence or nonexistence of local-in-time nonnegative solutions for Hardy parabolic equations with general nonlinearities.

Method: Supersolution method for proving existence results, establishing optimal integrability conditions on initial functions.

Result: Obtained optimal integrability conditions for initial data that determine when local-in-time nonnegative solutions exist or fail to exist.

Conclusion: The paper establishes sharp conditions for local existence of solutions to Hardy parabolic equations, with the supersolution method providing existence proofs under optimal integrability assumptions.

Abstract: In this paper, we consider the Cauchy problem for the Hardy parabolic equation with general nonlinearity and establish the local existence and nonexistence results. Our results provide the optimal integrability conditions on initial function for the existence of a local-in-time nonnegative solution. The proof of the existence result is based on the supersolution method.

</details>


### [23] [Transmission and Reflection coefficients for Schrödinger Operators with Truncated Periodic Potentials that support defect states](https://arxiv.org/abs/2601.22544)
*Joseph C. Stellman,Jeremy L. Marzuola*

Main category: math.AP

TL;DR: Study of scattering through truncated periodic potentials with perturbations supporting localized gap eigenstates, proving existence of zero reflection states near bound states and analyzing transmission/reflection behavior.


<details>
  <summary>Details</summary>
Motivation: To understand how scattering waves behave through truncated periodic potentials with perturbations that support localized gap eigenstates, particularly focusing on transmission resonances and their relationship to bound states.

Method: Mathematical analysis proving existence of zero reflection states (transmission resonances) in complex neighborhoods around positive bound states of model operators, comparing to previously found scattering resonances, and analyzing transmission/reflection coefficients near bound states.

Result: Proves existence of distinct zero reflection states near assumed bound states, establishes relationship between transmission resonances and scattering resonances, and provides analysis of transmission/reflection coefficient behavior near bound states.

Conclusion: The study provides theoretical understanding of scattering through perturbed truncated periodic potentials, demonstrating how transmission resonances emerge near bound states and offering analytical framework for studying transmission/reflection properties in such systems, with applications to both crystalline and harmonic oscillator cases.

Abstract: We consider scattering waves through truncated periodic potentials with perturbations that support localized gap eigenstates. In a small complex neighborhood around an assumed positive bound state of the model operator, we prove the existence of a distinct zero reflection state, or transmission resonance. We compare its location to a previously found scattering resonance and use the properties of solutions near these interesting points to analyze the behavior of transmission and reflection coefficients of scattering solutions near the assumed bound state. By example, we also discuss the truncated simple harmonic oscillator and compare the analysis to the crystalline case.

</details>


### [24] [Spectral properties and bound states of the Dirac equation on periodic quantum graphs](https://arxiv.org/abs/2601.22603)
*Zhipeng Yang,Ling Zhu*

Main category: math.AP

TL;DR: Variational approach for nonlinear Dirac equations on periodic quantum graphs yields existence and multiplicity of bound states.


<details>
  <summary>Details</summary>
Motivation: Study nonlinear Dirac equations on periodic quantum graphs to understand existence and multiplicity of bound states, which are important for quantum mechanics and condensed matter physics applications.

Method: Introduce Dirac operator on periodic graph with periodic potential, describe spectral decomposition, work in natural energy space, use variational approach with linking geometry and Cerami-type compactness modulo translations.

Result: Prove existence of at least one bound state for asymptotically linear or superquadratic nonlinearities; when nonlinearity is even, prove existence of infinitely many geometrically distinct bound states.

Conclusion: Variational methods successfully establish existence and multiplicity results for nonlinear Dirac equations on periodic quantum graphs, with translation-invariance playing crucial role in compactness.

Abstract: We investigate nonlinear Dirac equations on a periodic quantum graph $G$ and develop a variational approach to the existence and multiplicity of bound states. After introducing the Dirac operator on $G$ with a $\mathbb Z^{d}$-periodic potential, we describe its spectral decomposition and work in the natural energy space. Under asymptotically linear or superquadratic assumptions on the nonlinearity, we establish the required linking geometry and a Cerami-type compactness property modulo $\mathbb Z^{d}$-translations. As a consequence, we prove the existence of at least one bound state and, when the nonlinearity is even, infinitely many geometrically distinct bound states.

</details>


### [25] [Weighted estimates for Hodge-Maxwell systems](https://arxiv.org/abs/2601.22604)
*Rohit Mahato,Swarnendu Sil*

Main category: math.AP

TL;DR: The paper establishes boundary regularity estimates in weighted L^p spaces for weak solutions to Hodge systems with various boundary conditions, leading to solvability of Hodge-Maxwell systems and Hodge decomposition theorems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop regularity theory for Hodge systems in weighted L^p spaces with Muckenhoupt weights, which is important for understanding partial differential equations in geometric analysis and mathematical physics, particularly for Hodge-Maxwell systems that arise in electromagnetism and related fields.

Method: The authors avoid potential theory and representation formulas, instead using decay estimates in the spirit of the 'Campanato method' to establish weighted L^p estimates. They work with Muckenhoupt weights A_p and consider two types of boundary conditions: either ν∧ω and ν∧d*(Bω) or ν⌟Bω and ν⌟Adω prescribed on the boundary.

Result: The main results are up-to-the-boundary regularity estimates in weighted L^p spaces for weak solutions to Hodge systems. As consequences, the authors prove solvability of Hodge-Maxwell systems and derive Hodge decomposition theorems in weighted Lebesgue spaces.

Conclusion: The paper successfully establishes a comprehensive regularity theory for Hodge systems in weighted function spaces using Campanato-type methods, providing new tools for analyzing Hodge-Maxwell systems and extending Hodge decomposition to weighted settings.

Abstract: We establish up to the boundary regularity estimates in weighted $L^{p}$ spaces with Muckenhoupt weights $A_{p}$ for weak solutions to the Hodge systems
  \begin{align*}
  d^{\ast}\left(Adω\right) + B^{\intercal}dd^{\ast}\left(Bω\right) = λBω+ f \quad \text{ in } Ω
  \end{align*}
  with either $ν\wedge ω$ and $ν\wedge d^{\ast}\left(Bω\right)$ or $ν\lrcorner Bω$ and $ν\lrcorner Adω$ prescribed on $\partialΩ.$ As a consequence, we prove the solvability of Hodge-Maxwell systems and derive Hodge decomposition theorems in weighted Lebesgue spaces. Our proof avoids potential theory, does not rely on representation formulas and instead uses decay estimates in the spirit of `Campanato method' to establish weighted $L^{p}$ estimates.

</details>


### [26] [Simplicity of eigenvalues for elliptic problems with mixed Steklov-Robin boundary condition](https://arxiv.org/abs/2601.22829)
*Marco Ghimenti,Anna Maria Micheletti,Angela Pistoia*

Main category: math.AP

TL;DR: The paper proves that for generic domains, all eigenvalues of elliptic problems with mixed Steklov-Robin boundary conditions are simple.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral properties of elliptic problems with mixed Steklov-Robin boundary conditions and establish generic simplicity of eigenvalues.

Method: Domain perturbation techniques and analysis of operator transversality.

Result: For a generic domain, all eigenvalues of the studied elliptic problems are simple.

Conclusion: The spectral simplicity property holds generically for elliptic problems with mixed Steklov-Robin boundary conditions.

Abstract: This paper investigates the spectral properties of two classes of elliptic problems characterized by mixed Steklov-Robin boundary conditions. Our main objective is to prove that, for a generic domain, all the eigenvalues are simple. This result is established by employing domain perturbation techniques and analyzing the transversality of the associated operators.

</details>


### [27] [Unconditional well-posedness of the master equation for monotone mean field games of controls](https://arxiv.org/abs/2601.22845)
*Joe Jackson,Alpár R. Mészáros*

Main category: math.AP

TL;DR: First unconditional well-posedness for master equations in mean field games of controls, covering displacement/Lasry-Lions monotone data and small time horizons, without requiring a priori regularity on fixed-point mappings.


<details>
  <summary>Details</summary>
Motivation: Previous results on master equations for mean field games of controls required additional structural assumptions on fixed-point mappings arising from control interactions. The authors aim to establish unconditional well-posedness where all assumptions are imposed only at the Lagrangian and terminal cost level.

Method: Bottom-up approach using N-player Nash systems: 1) Show solutions of N-player systems are compact via uniform-in-N decay estimates for derivatives of value functions, 2) Prove subsequential limit points must be solutions to the master equation, 3) Build classical solution without relying on generalized method of characteristics.

Result: First unconditional well-posedness result for master equations in mean field games of controls, covering displacement monotone or Lasry-Lions monotone data, and small time horizons. Results allow for non-degenerate idiosyncratic Brownian noise and common noise with constant intensity.

Conclusion: The paper establishes a fundamental well-posedness result for master equations in mean field games of controls using a novel bottom-up approach, eliminating the need for additional structural assumptions on fixed-point mappings and providing a more natural framework for analyzing these games.

Abstract: We establish the first unconditional well-posedness result for the master equation associated with a general class of mean field games of controls. Our analysis covers games with displacement monotone or Lasry--Lions monotone data, as well as those with a small time horizon. By unconditional, we mean that all assumptions are imposed solely at the level of the Lagrangian and the terminal cost. In particular, we do not require any a priori regularity or structural assumptions on the additional fixed-point mappings arising from the control interactions; instead we show that these fixed-point mappings are well-behaved as a consequence of the regularity and the monotonicity of the data. Our approach is bottom-up in nature, unlike most previous results which rely on a generalized method of characteristics. In particular, we build a classical solution of the master equation by showing that the solutions of the corresponding $N$-player Nash systems are compact, in an appropriate sense, and that their subsequential limit points must be solutions to the master equation. Compactness is obtained via uniform-in-$N$ decay estimates for derivatives of the $N$-player value functions. The underlying games are driven by non-degenerate idiosyncratic Brownian noise, and our results allow for the presence of common noise with constant intensity.

</details>


### [28] [Existence of a solution of the TV Wasserstein gradient flow](https://arxiv.org/abs/2601.22847)
*Kexin Lin,Filippo Santambrogio*

Main category: math.AP

TL;DR: Existence of TV Wasserstein gradient flow solutions on flat torus for initial densities bounded away from zero, with preservation of bounds and BV norm decay rates.


<details>
  <summary>Details</summary>
Motivation: Generalize previous work by Carlier and Poon that only fully proved results in 1D and didn't handle non-BV initial densities. Extend existence theory for TV Wasserstein gradient flows to arbitrary dimensions with minimal regularity assumptions.

Method: Use approximated TV-JKO scheme that artificially imposes a lower bound on density, enabling construction of continuous-in-time solutions regular enough to propagate initial bounds and analyze BV norm decay.

Result: Prove existence of solutions for initial densities bounded from below and above by positive constants on flat torus in any dimension. Solutions preserve density bounds and show BV norm decay: t^{-1/3} as t→0 for non-BV initial data, t^{-1} as t→∞.

Conclusion: Successfully extend TV Wasserstein gradient flow theory to arbitrary dimensions with weak regularity assumptions, improving upon previous 1D-only results and handling non-BV initial data cases.

Abstract: On the flat torus in any dimension we prove existence of a solution to the TV Wasserstein gradient flow equation, only assuming that the initial density $ρ_0$ is bounded from below and above by strictly positive constants. This solution preserves upper and lower bounds of the densities, and shows a certain decay of the BV norm (of the order of $t^{-1/3}$ for $t\to 0$ -- if $ρ_0\notin BV$, otherwise the BV norm is of course bounded -- and of the order of $t^{-1}$ as $t\to\infty$). This generalizes a previous result by Carlier and Poon, who only gave a full proof in one dimension of space and did not consider the case $ρ_0\notin BV$.
  The main tool consists in considering an approximated TV-JKO scheme which artificially imposes a lower bound on the density and allows to find a continuous-in-time solution regular enough to prove that the lower bounds of the initial datum propagates in time, and study on this approximated equation the decay of the BV norm.

</details>


### [29] [Global Well-posedness of Strong Solutions to the Cauchy Problem of 2D Nonhomogeneous Navier-Stokes Equations with Density-Dependent Viscosity and Vacuum](https://arxiv.org/abs/2601.22877)
*Bing Yuan,Rong Zhang,Peng Zhou*

Main category: math.AP

TL;DR: Global existence of strong solutions for 2D nonhomogeneous Navier-Stokes equations with density-dependent viscosity, allowing arbitrarily large initial data and vacuum regions, plus large-time asymptotic behavior analysis.


<details>
  <summary>Details</summary>
Motivation: To study the Cauchy problem for modified 2D nonhomogeneous incompressible Navier-Stokes equations with density-dependent viscosity, addressing challenges with large initial data and vacuum regions where density vanishes.

Method: Fully utilizes the structure of the system to obtain key estimates of $\|\nabla ρ\|_{L_t^\infty L_x^q},q>2$ without smallness assumptions on initial data, enabling global existence proofs.

Result: Establishes global existence of strong solutions for both vacuum and nonvacuum far-field density cases, allows arbitrarily large initial data and vanishing initial density, and proves large-time asymptotic behavior of velocity and pressure gradients.

Conclusion: The paper successfully overcomes traditional smallness assumptions, providing global existence results for challenging scenarios in 2D nonhomogeneous Navier-Stokes equations with density-dependent viscosity, including vacuum cases and large initial data.

Abstract: This paper is concerned with the Cauchy problem for the modified two-dimensional (2D) nonhomogeneous incompressible Navier-Stokes equations with density-dependent viscosity. By fully using the structure of the system, we can obtain the key estimates of $\|\nabla ρ\|_{L_t^\infty L_x^q},q>2$ without any smallness asuumption on the initial data, and thus establish the global existence of the strong solutions with the far-field density being either vacuum or nonvacuum. Notably, the initial data can be arbitrarily large and the initial density is allowed to vanish. Furthermore, the large-time asymptotic behavior of the gradients of the velocity and the pressure is also established.

</details>


### [30] [Local Well-posedness and Blow-up for the Restricted Fourth-Order Prandtl Equation](https://arxiv.org/abs/2601.22940)
*Ik Hyun Choi*

Main category: math.AP

TL;DR: Local well-posedness and finite-time blow-up proved for a restricted fourth-order Prandtl equation on half-line with clamped boundary conditions.


<details>
  <summary>Details</summary>
Motivation: The equation arises from a two-dimensional fourth-order Prandtl system via ansatz reduction, featuring nonlinearity with nonlocal integral term. Understanding well-posedness and blow-up behavior is important for analyzing this higher-order boundary layer model.

Method: Use Duhamel fixed-point argument requiring uniform L¹ bounds for half-line biharmonic heat kernel. Establish uniform L¹ estimates for kernel and derivatives, show semigroup preserves spatial regularity under compatibility conditions using alternative representation via integration by parts.

Result: Proved local existence and uniqueness for restricted model, and constructed solutions that exhibit finite-time blow-up.

Conclusion: Successfully analyzed the restricted fourth-order Prandtl equation, establishing both local well-posedness and finite-time blow-up phenomena through careful kernel estimates and fixed-point methods.

Abstract: We prove local well-posedness and finite-time blow-up for a restricted fourth-order Prandtl equation posed on the half-line with clamped boundary conditions. The equation arises from a two-dimensional fourth-order Prandtl system via an ansatz reduction, and its nonlinearity involves a nonlocal integral term. To close a Duhamel fixed-point argument, we need uniform $L^1$ bounds for the associated half-line biharmonic heat kernel. We establish uniform $L^1$ estimates for the kernel and its derivatives, and we show that the semigroup preserves spatial regularity under appropriate compatibility conditions, using an alternative representation derived by integration by parts. These kernel estimates yield local existence and uniqueness for the restricted model and allow us to construct solutions that blow-up in finite time.

</details>


### [31] [Instability of two-dimensional Taylor-Green Vortices](https://arxiv.org/abs/2601.23040)
*Gonzalo Cao-Labora,Maria Colombo,Michele Dolce,Paolo Ventura*

Main category: math.AP

TL;DR: The paper develops a general criterion for characterizing unstable eigenvalues of linear Hamiltonian operators as zeros of a holomorphic function, and applies it to prove spectral instability of the Taylor-Green vortex in 2D ideal fluids.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical framework for analyzing spectral instability of steady states in Hamiltonian systems, particularly for fluid dynamics problems like the Taylor-Green vortex in ideal fluids.

Method: Develops a general criterion that characterizes unstable eigenvalues as zeros of a holomorphic function (determinant of finite-dimensional matrix). Applies this to Taylor-Green vortex by analyzing different invariant subspaces (odd/even perturbations) and combines analytical methods with rigorous computer-assisted arguments.

Result: Proves linear stability of odd perturbations (no unstable spectrum on real axis), detects real instabilities in rescaled vortices, and fully characterizes unstable spectrum in even function subspace using computer-assisted root finding.

Conclusion: The developed criterion successfully characterizes spectral instability of the Taylor-Green vortex, demonstrating both analytical stability results and computer-assisted detection of instabilities in different perturbation subspaces.

Abstract: For a wide class of linear Hamiltonian operators we develop a general criterion that characterizes the unstable eigenvalues as the zeros of a holomorphic function given by the determinant of a finite-dimensional matrix. We apply the latter result to prove the spectral instability of the Taylor-Green vortex in two-dimensional ideal fluids. The linearized Euler operator at this steady state possesses different invariant subspaces, within which we apply our criterion to rule out or detect instabilities. We show linear stability of odd perturbations, for which the unstable spectrum can appear only on the real axis. We exclude this possibility by applying our stability criterion. Real instabilities, instead, exist and can be detected with the same criterion if we consider suitable rescalings of the Taylor-Green vortex. In the subspace of functions even in both variables, the problem is reduced to finding a single complex root of our stability function. We successfully locate this value by combining our general criterion with a rigorous computer-assisted argument. As a consequence, we fully characterize the unstable spectrum of the Taylor-Green vortex.

</details>


### [32] [Existence of Traveling Waves in Infinite Range FPUT Lattices](https://arxiv.org/abs/2601.23091)
*Michael Herrmann,Karsten Matthies,Jan-Patrick Meyer*

Main category: math.AP

TL;DR: Existence of solitary waves in a lattice with long-range repulsive interactions, proven via constrained optimization yielding unimodal solutions.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of solitary wave solutions in systems with long-range repulsive interactions, which is mathematically challenging due to the non-local nature of the forces.

Method: Variational existence proof using constrained optimization techniques to find solitary wave solutions as critical points of an energy functional subject to constraints.

Result: Proved existence of a one-parameter family of unimodal solitary wave solutions and characterized asymptotic behavior of large, fast, high-energy waves.

Conclusion: Solitary waves can exist in lattices with purely repulsive long-range interactions, contrary to intuition, with specific mathematical properties and asymptotic behaviors.

Abstract: We prove the existence of solitary waves in a lattice where all particles interact with each other by pair-wise repulsive forces that decay with distance. The variational existence proof is based on constrained optimization and provides a one-parameter family of unimodal solutions. We also describe the asymptotic behavior of large, fast, high-energy waves.

</details>


### [33] [Nonlinear Schrödinger Equation with magnetic potential on metric graphs](https://arxiv.org/abs/2601.23115)
*Riccardo Adami,Nicolò Cangiotti,Ivan Gallo,David Spitzkopf*

Main category: math.AP

TL;DR: The paper proves that magnetic Schrödinger equations on metric graphs can be reduced to non-magnetic problems with effective repulsive potentials determined by Aharonov-Bohm flux, enabling extension of existence criteria and revealing mass-dependent phase transitions.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence of ground states for nonlinear magnetic Schrödinger equations on noncompact metric graphs, which are important in quantum mechanics and condensed matter physics for modeling systems with nontrivial topology and magnetic fields.

Method: Proves variational equivalence between magnetic Hamiltonian and non-magnetic operator with additional repulsive potentials supported on graph cycles, where the effective potential is strictly determined by Aharonov-Bohm flux through topological loops.

Result: Extends classical existence criteria to magnetic setting, characterizes ground state structure on tadpole graph revealing mass-dependent phase transition, shows ground states exist for small repulsion in intermediate mass regime, and demonstrates strong flux prevents ground state formation.

Conclusion: The reduction technique enables systematic study of magnetic ground states on metric graphs, revealing how Aharonov-Bohm flux creates effective repulsive potentials that control existence and properties of ground states, with applications to specific graph topologies.

Abstract: In this manuscript, we shall investigate the Nonlinear Magnetic Schrödinger Equation on noncompact metric graphs, focusing on the existence of ground states. We prove that the magnetic Hamiltonian is variationally equivalent to a non-magnetic operator with additional repulsive potentials supported on the graph's cycles. This effective potential is strictly determined by the Aharonov-Bohm flux through the topological loops. Leveraging this reduction, we extend classical existence criteria to the magnetic setting. As a key application, we characterize the ground state structure on the tadpole graph, revealing a mass-dependent phase transition. The ground states exist for sufficiently small repulsion in an intermediate regime of masses while sufficiently strong flux prevents the formation of ground states.

</details>


### [34] [Hyperbolic partial differential equations with complex characteristics on Fourier Lebesgue spaces](https://arxiv.org/abs/2601.23138)
*Duván Cardona,William Obeng-Denteh,Frederick Opoku*

Main category: math.AP

TL;DR: Establishes well-posedness for hyperbolic PDEs with complex characteristics on Fourier Lebesgue spaces using Fourier integral operators with complex-valued phase functions.


<details>
  <summary>Details</summary>
Motivation: To develop well-posedness theory for hyperbolic partial differential equations with complex characteristics in Fourier Lebesgue spaces, which requires extending harmonic analysis techniques to handle complex-valued phase functions.

Method: Uses harmonic analysis approach to study boundedness properties of Fourier integral operators with complex-valued phase functions on Fourier Lebesgue spaces, Besov spaces, and Triebel-Lizorkin spaces. These operators serve as propagators for the PDEs.

Result: Proves new boundedness results for Fourier integral operators under the spatial smooth factorization condition for their canonical relations, establishing well-posedness for the considered hyperbolic PDEs.

Conclusion: The harmonic analysis approach successfully establishes well-posedness for hyperbolic PDEs with complex characteristics in Fourier Lebesgue spaces through boundedness properties of Fourier integral operators with complex phase functions.

Abstract: The aim of this paper is to establish well-posedness properties for hyperbolic PDEs on Fourier Lebesgue spaces. We consider hyperbolic operators with complex characteristics. Since our approach comes from harmonic analysis, we establish boundedness properties of Fourier integral operators with complex-valued phase functions on Fourier Lebesgue spaces, Besov spaces and Triebel-Lizorkin spaces. Indeed, these classes of operators serve as propagators of the considered PDE problems. In terms of the boundedness properties, we prove new results in the case where the canonical relation of the operator is assumed to satisfy the {\it spatial smooth factorization condition}

</details>


### [35] [Non-uniformly elliptic variational problems on BV](https://arxiv.org/abs/2601.23195)
*Lisa Beck,Franz Gmeineder,Mathias Schäffner*

Main category: math.AP

TL;DR: The paper establishes W¹,¹-regularity and higher gradient integrability for relaxed minimizers of convex integral functionals on BV spaces, extending results to functionals with linear growth from below but not necessarily from above.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend regularity theory beyond classical examples like minimal surface integrands to more general convex integral functionals that only require linear growth from below, which typically exhibit non-uniformly degenerate elliptic behavior.

Method: The authors study relaxed minimizers of convex integral functionals on BV (bounded variation) spaces, focusing on functionals with linear growth from below but not necessarily from above, addressing the non-uniformly degenerate elliptic behavior that arises.

Result: The main results establish W¹,¹-regularity and higher gradient integrability for these relaxed minimizers, extending available bounds from the superlinear growth case in a sharp way.

Conclusion: The paper successfully extends regularity theory to a broader class of convex integral functionals with linear growth conditions, providing sharp bounds that generalize previous results from the superlinear growth case to include functionals with non-uniformly degenerate elliptic behavior.

Abstract: We establish $\mathrm{W}^{1,1}$-regularity and higher gradient integrability for relaxed minimizers of convex integral functionals on $\mathrm{BV}$. Unlike classical examples such as the minimal surface integrand, we only require linear growth from below but not necessarily from above. This typically comes with a non-uniformly degenerate elliptic behaviour, for which our results extend the presently available bounds from the superlinear growth case in a sharp way.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Batch Bayesian optimization of attosecond betatron pulses from laser wakefield acceleration](https://arxiv.org/abs/2601.22794)
*Dominika Maslarova,Albert Hansson,Mufei Luo,Vojtěch Horný,Julien Ferri,Istvan Pusztai,Tünde Fülöp*

Main category: physics.plasm-ph

TL;DR: Using Bayesian optimization to enhance attosecond betatron radiation from laser wakefield acceleration by optimizing plasma density spikes, achieving >10x power improvement.


<details>
  <summary>Details</summary>
Motivation: Laser wakefield acceleration produces femtosecond-scale broadband X-ray betatron radiation suitable for advanced imaging. Recent laser technology advances enable attosecond regimes where increased pulse energy would benefit practical applications.

Method: Numerical simulations combined with batch Bayesian optimization to efficiently explore multi-parameter space and identify optimal plasma density spike configurations.

Result: Identified regime where plasma density spike triggers high-charge electron beam generation, resulting in >10x improvement in on-axis time-averaged power within central time containing half radiated energy compared to reference case without density spike.

Conclusion: Bayesian optimization effectively enhances attosecond betatron radiation sources, demonstrating significant power improvements through optimized plasma density spike configurations.

Abstract: Laser wakefield acceleration can generate a femtosecond-scale broadband X-ray betatron radiation pulse from electrons accelerated by an intense laser pulse in a plasma. The micrometer-scale of the source makes wakefield betatron radiation well-suited for advanced imaging techniques, including diffraction and phase-contrast imaging. Recent progress in laser technology can expand these capabilities into the attosecond regime, where the practical applications would significantly benefit from the increased energy contained within the pulse. Here we use numerical simulations combined with batch Bayesian optimization to enhance the radiation produced by an attosecond betatron source. The method enables an efficient exploration of a multi-parameter space and identifies a regime in which a plasma density spike triggers the generation of a high-charge electron beam. This results in an improvement of more than one order of magnitude in the on-axis time-averaged power within the central time containing half of the radiated energy, compared to the reference case without the density spike.

</details>


### [37] [A predictive formula for the H-mode electron separatrix density: Bridging regression and physics-based models across C-Mod, AUG and JET tokamaks](https://arxiv.org/abs/2601.23140)
*D. Silvagni,O. Grover,A. Stagni,J. W. Hughes,M. A. Miller,B. Lomanowski,L. Balbinot,G. Ciraolo,W. Dekeyser,M. Dunne,L. Frassinetti,C. Giroud,T. Happel,I. Jepu,A. Kallenbach,A. Kirjasuo,A. Kuang,T. Luda,D. Moulton,O. Pan,C. Perez von Thun,T. Puetterich,G. Rubino,S. A. Silburn,H. J. Sun,D. Umezaki,H. Zohm,the ASDEX Upgrade team,JET contributors,the EUROfusion tokamak exploitation team*

Main category: physics.plasm-ph

TL;DR: Researchers developed a predictive formula for separatrix electron density in tokamaks by combining regression analysis of multi-machine data with theoretical two-point model, achieving accurate predictions across current and future devices.


<details>
  <summary>Details</summary>
Motivation: The separatrix electron density is crucial for balancing energy confinement, detachment achievement, and ELM suppression in tokamaks, affecting core-edge integration. Understanding what determines this key parameter is essential for predicting performance in current and future fusion devices.

Method: Created a database of H-mode separatrix density measurements from Alcator C-Mod, ASDEX Upgrade, and JET using consistent analysis methods. Derived regression scaling from engineering parameters, compared with two-point model predictions, and combined both approaches into a predictive formula.

Result: Found remarkable agreement between regression scaling and two-point model predictions. The combined formula estimates separatrix density within factor of 1.5 across three machines and provides projections for next-step devices (ITER, SPARC, DTT, JT-60SA, COMPASS-U) consistent with SOLPS simulations.

Conclusion: The study successfully developed a predictive formula for separatrix electron density that bridges empirical data and theoretical modeling, providing reliable projections for future fusion devices and advancing understanding of core-edge integration in tokamaks.

Abstract: The electron density at the separatrix ($n_{e,\mathrm{sep}}$) plays a central role in balancing energy confinement, detachment achievement, and ELM suppression in tokamaks, thereby influencing core-edge integration. To study what determines this key parameter, a database of H-mode separatrix density measurements from Alcator C-Mod, ASDEX Upgrade, and JET tokamaks has been assembled using a consistent analysis method across all devices. This dataset is used to derive a regression scaling expression based solely on engineering parameters, and the results are compared to predictions from the two-point model. The agreement found is remarkable: both the regression and model provide similar parameter dependencies and tokamak-specific multiplicative constants. Building on this agreement, a fully predictive formula that combines the regression dependencies and the two-point model multiplicative constant is proposed. This formula is able to estimate $n_{e,\mathrm{sep}}$ across the three machines within a factor of 1.5, and provides projections to next-step devices (ITER, SPARC, DTT, JT-60SA and COMPASS-U) that are in agreement with available SOLPS simulations.

</details>


### [38] [Time-Resolved Interferometric Measurements of Plasma Density Evolution in Laser-Driven Capacitor-Coil Targets](https://arxiv.org/abs/2601.23271)
*Yang Zhang,Ryo Omura,Rinya Akematsu,King Fai Farley Law,Brandon K. Russell,Geoffrey Pomraning,Kian Orr,Kai Kimura,Muhammad Fauzan Syahbana,Yuga Karaki,Hiroki Matsubara,Ryuya Yamada,Jinyuan Dun,Ryunosuke Takizawa,Yasunobu Arikawa,Tatiana Pikuz,Yuji Fukuda,Lan Gao,Hantao Ji,Shinsuke Fujioka*

Main category: physics.plasm-ph

TL;DR: Researchers used interferometry to measure plasma density evolution in laser-driven capacitor-coil targets, revealing two distinct plasma sources that load the coil region.


<details>
  <summary>Details</summary>
Motivation: While laser-driven capacitor-coil targets are widely used for generating strong magnetic fields in magnetized high-energy-density plasma experiments, there's limited direct, time-resolved measurements of the plasma density surrounding the coil, which can influence physical processes and interact with secondary targets or external plasmas.

Method: The researchers conducted interferometric measurements of plasma density evolution in laser-driven capacitor-coil targets irradiated by the University of Osaka LFEX laser, producing two-dimensional electron density maps.

Result: The measurements revealed two distinct plasma sources loading the coil region: plasma generated in the coil itself and plasma produced by laser ablation of the target plates.

Conclusion: These results provide quantitative information on plasma loading and evolution in capacitor-coil targets, which is directly relevant to the design and modeling of magnetized high-energy-density plasma experiments.

Abstract: Laser-driven capacitor-coil targets provide a compact platform for generating strong magnetic fields and are widely used in magnetized high-energy-density plasma experiments. In addition to magnetic-field generation, these targets also produce plasma in the coil region, which can influence the subject physical processes, interact with secondary targets or external plasmas in their applications. However, direct, time-resolved measurements of the plasma density surrounding the coil remain limited. Here, we report interferometric measurements of the plasma density evolution in laser-driven capacitor-coil targets irradiated by the University of Osaka LFEX laser. Two-dimensional electron density maps reveal two distinct plasma sources loading the coil region: plasma generated in the coil itself and plasma produced by laser ablation of the target plates. These results provide quantitative information on plasma loading and evolution in capacitor-coil targets and are directly relevant to the design and modeling of magnetized high-energy-density plasma experiments.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [39] [Synthesis of Monolayer Ice on a Hydrophobic Metal Surface](https://arxiv.org/abs/2601.22460)
*Qiaoxiao Zhao,Meiling Xu,Dong Li,Zhicheng Gao,Yudian Zhou,Wenbo Liu,Jingyan Chen,Peng Cheng,Sheng Meng,Kehui Wu,Yanchao Wang,Lan Chen,Baojie Feng*

Main category: cond-mat.mtrl-sci

TL;DR: Researchers synthesized a monolayer ice phase on hydrophobic Au(111) using low-energy-electron-assisted growth, challenging previous assumptions about water assembly on hydrophobic metals.


<details>
  <summary>Details</summary>
Motivation: Understanding water-metal interactions is crucial for catalysis, electrochemistry, and atmospheric science. While monolayer ice phases are known on hydrophilic surfaces, their formation on hydrophobic metals was considered thermodynamically unfavorable, with water typically forming amorphous films, 3D crystallites, or bilayer ice instead.

Method: Used low-energy-electron-assisted growth method on hydrophobic Au(111) surface. Combined experimental techniques including low-energy electron diffraction (LEED), angle-resolved photoemission spectroscopy (ARPES), and X-ray photoelectron spectroscopy (XPS) with first-principles calculations.

Result: Successfully synthesized a monolayer ice phase on hydrophobic Au(111). Experimental characterizations and calculations proved that the monolayer ice consists of intact water molecules, demonstrating that ordered 2D ice can be stabilized on inert substrates.

Conclusion: The low-energy-electron-assisted growth method provides a generalizable strategy for stabilizing ordered two-dimensional ice on hydrophobic substrates. This work offers new insights into water-low-energy electron interactions at hydrophobic interfaces and challenges previous assumptions about water assembly on hydrophobic metals.

Abstract: Understanding water-metal interactions is central to disciplines spanning catalysis, electrochemistry, and atmospheric science. Monolayer ice phases are well established on hydrophilic surfaces, where strong water-substrate interactions stabilize ordered hydrogen-bond networks. In contrast, their formation on hydrophobic metals has been deemed ther-modynamically unfavourable, with water typically assembling into amorphous films, three-dimensional crystallites, or interlocked bilayer ice. Here, we demonstrate the synthesis of a monolayer ice phase on the hydrophobic Au(111) surface using a low-energy-electron-assisted growth method. Combined experimental characterizations including low-energy electron diffraction, angle-resolved photoemission spectroscopy, and X-ray photoelectron spectroscopy, complemented by first-principles calculations, prove that the monolayer ice phase composes of intact water molecules. This approach provides a generalizable strategy for stabilizing ordered two-dimensional ice on inert substrates and offers new insight into the interplay between water and low-energy electrons at hydrophobic interfaces.

</details>


### [40] [Unlocking the Power of Orbital-Free Density Functional Theory to Explore the Electronic Structure Under Extreme Conditions](https://arxiv.org/abs/2601.23002)
*Cheng Ma,Qiang Xu,Zhenhao Zhang,Ke Wang,Ying Sun,Wenhui Mi,Zhandos A. Moldabekov,Tobias Dornheim,Jan Vorberger,Sebastian Schwalbe,Xuecheng Shao*

Main category: cond-mat.mtrl-sci

TL;DR: A new orbital-free DFT framework achieves KSDFT accuracy with 100x speedup for extreme condition simulations, validated against quantum Monte Carlo and experimental data.


<details>
  <summary>Details</summary>
Motivation: X-ray diagnostics enable probing extreme conditions (stellar interiors, fusion experiments), but KSDFT is too computationally expensive for routine use, while OFDFT lacks accuracy needed for electronic structure description.

Method: Developed a non-empirical Kohn-Sham-assisted orbital-free density functional framework that combines efficiency of OFDFT with accuracy of KSDFT for extreme conditions calculations.

Result: Achieves KSDFT-level accuracy for electron densities, structure factors, and equations of state with speedups up to several hundred times compared to KSDFT, validated against quantum Monte Carlo data and Rayleigh weight measurements.

Conclusion: The framework enables efficient high-accuracy simulations at extreme conditions, and demonstrates that quantum non-locality remains essential even at temperatures around 100 eV for dense hydrogen electronic structure.

Abstract: Recent advances in X-ray free-electron laser diagnostics have enabled direct probing of the electronic structure under extreme pressures and temperatures, such as those encountered in stellar interiors and inertial confinement fusion experiments, challenging theoretical models for interpreting experimental data. Kohn-Sham density functional theory (KSDFT) has been successfully applied to analyze experimental X-ray scattering measurements, but its high computational cost renders routine application impractical. Orbital-free DFT (OFDFT) is a substantially more efficient alternative, with computational cost scaling linearly with system size and a weak temperature dependence, yet it often lacks the accuracy required for electronic structure description. Overcoming this limitation, we present a non-empirical Kohn-Sham-assisted orbital-free density functional framework for calculations at extreme conditions, which enables efficient OFDFT simulations with KSDFT-level accuracy for electron densities, electron-ion structure factors, and equations of state across a broad range of conditions. Benchmark comparisons with quantum Monte Carlo data for dense hydrogen and validation against Rayleigh weight measurements of hot dense beryllium demonstrate the reliability of the framework and speedups of up to several hundred times compared with KSDFT. We further show that even at temperatures on the order of 100 eV, quantum non-locality remains essential for correctly describing the electronic structure of dense hydrogen.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [41] [Sculpting of Martian brain terrain reveals the drying of ancient Mars](https://arxiv.org/abs/2601.22606)
*Shenyi Zhang,Lei Zhang,Yutian Ke,Jinhai Zhang*

Main category: physics.geo-ph

TL;DR: Martian brain terrain formation involves two-stage process: initial freeze-thaw patterning requiring liquid water, followed by sublimation sculpting in dry conditions, providing evidence for Mars' climate transition from wet to hyper-arid.


<details>
  <summary>Details</summary>
Motivation: Martian brain terrain (MBT) resembles Earth's self-organized patterned ground, suggesting shared formation mechanisms. However, lack of quantitative descriptions and physical modeling limits understanding of thermal/aqueous conditions during MBT formation.

Method: Established specialized quantitative system to extract MBT morphological features in northern Arabia Terra, then employed numerical model to investigate formation mechanisms through simulation.

Result: Simulations accurately replicate MBT morphology with <10% deviation in key metrics, but show self-organized transport alone produces <0.5m relief, insufficient for MBT's 3.29±0.65m average relief. Sublimation sculpting explains discrepancy, indicating ~3m subsurface ice loss over ~3 million years.

Conclusion: MBT formation is multi-stage: initial freeze-thaw patterning (requiring liquid water) followed by sublimation sculpting (dry environment). This provides physical evidence for Mars' climate transition from wetter period to colder hyper-arid state.

Abstract: The Martian brain terrain (MBT), characterized by its unique brain-like morphology, is a potential geological archive for finding hints of paleoclimatic conditions during its formation period. The morphological similarity of MBT to self-organized patterned ground on Earth suggests a shared formation mechanism. However, the lack of quantitative descriptions and robust physical modeling of self-organized stone transport jointly limits the study of the thermal and aqueous conditions governing MBT's formation. Here we established a specialized quantitative system for extracting the morphological features of MBT, taking a typical region located in the northern Arabia Terra as an example, and then employed a numerical model to investigate its formation mechanisms. Our simulation results accurately replicate the observed morphology of MBT, matching its key geometric metrics with deviations $<10\%$. Crucially, however, we find that the self-organized transport can solely produce relief $<0.5$ m, insufficient to explain the formation of MBT with average relief of $3.29 \pm 0.65$ m. We attribute this discrepancy to sculpting driven by late-stage sublimation, constraining cumulative subsurface ice loss in this region to $\sim 3$ meters over the past $\sim 3$ Ma. These findings demonstrate that MBT's formation is a multi-stage process: initial patterning driven by freeze-thaw cycles (implying liquid water) followed by vertical sculpting via sublimation (requiring a dry environment). This evolution provides physical evidence for the transition of the ancient Martian climate from a wetter period to a colder hyper-arid state.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [42] [Parametric vector flows for registration fields in bounded domains with applications to nonlinear interpolation of shock-dominated flows](https://arxiv.org/abs/2601.22712)
*Jon Labatut,Jean-Baptiste Chapelier,Angelo Iollo,Tommaso Taddei*

Main category: physics.flu-dyn

TL;DR: A registration method for parametric model order reduction that aligns coherent structures (shocks, shear layers) across parameters using diffeomorphisms, combined with nonlinear interpolation for accurate shock-dominated fluid dynamics.


<details>
  <summary>Details</summary>
Motivation: To improve parametric model order reduction for fluid dynamics problems with moving coherent structures like shocks, which are difficult to approximate with linear subspaces due to their parameter-dependent positions.

Method: Develops an expectation-maximization procedure to find diffeomorphisms (vector flows of velocity fields) that align point clouds extracted from solution snapshots, combined with nonlinear interpolation for shock-dominated fields.

Result: Demonstrates effectiveness on 2D inviscid transonic flow past a NACA airfoil and 3D viscous transonic flow past an ONERA M6 wing, showing accurate interpolation of shock-dominated fluid dynamic fields.

Conclusion: The registration method combined with nonlinear interpolation enables accurate parametric MOR for fluid dynamics with moving shocks and coherent structures, overcoming limitations of linear subspace approximations.

Abstract: We present a registration procedure for parametric model order reduction (MOR) in two- and three-dimensional bounded domains. In the MOR framework, registration methods exploit solution snapshots to identify a parametric coordinate transformation that improves the approximation of the solution set through linear subspaces. For each training parameter, optimization-based (or variational) registration methods minimize a target function that measures the alignment of the coherent structures of interest (e.g., shocks, shear layers, cracks) for different parameter values, over a family of bijections of the computational domain $Ω$. We consider diffeomorphisms $Φ$ that are vector flows of given velocity fields $v$ with vanishing normal component on $\partial Ω$; we rely on a sensor to extract appropriate point clouds from the solution snapshots and we develop an expectation-maximization procedure to simultaneously solve the point cloud matching problem and to determine the velocity $v$ (and thus the bijection $Φ$); finally, we combine our registration method with the nonlinear interpolation technique of [Iollo, Taddei, J. Comput. Phys., 2022] to perform accurate interpolations of fluid dynamic fields in the presence of shocks. Numerical results for a two-dimensional inviscid transonic flow past a NACA airfoil and a three-dimensional viscous transonic flow past an ONERA M6 wing illustrate the many elements of the methodology and demonstrate the effectiveness of nonlinear interpolation for shock-dominated fields.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [43] [Adaptive Benign Overfitting (ABO): Overparameterized RLS for Online Learning in Non-stationary Time-series](https://arxiv.org/abs/2601.22200)
*Luis Ontaneda Mijares,Nick Firoozye*

Main category: q-fin.ST

TL;DR: QR-based exponentially weighted RLS algorithm enables stable online learning with benign overfitting in overparameterized models, achieving 20-40% speed improvements while maintaining accuracy comparable to kernel methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of conventional learning theory being challenged by overparameterized models exhibiting improved generalization beyond interpolation (benign overfitting), and to enable online adaptation under non-stationary conditions while preventing numerical divergence.

Method: Introduces Adaptive Benign Overfitting (ABO) extending recursive least-squares (RLS) with QR-based exponentially weighted RLS (QR-EWRLS) algorithm. Combines random Fourier feature mappings with forgetting-factor regularization, uses orthogonal-triangular updates for numerical stability, and prevents covariance-form RLS divergence while adapting to evolving data distributions.

Result: Experiments show bounded residuals and stable condition numbers while reproducing double-descent behavior. Applications to foreign exchange and electricity demand forecasting achieve accuracy comparable to baseline kernel methods with 20-40% speed improvements. The approach maintains adaptability under non-stationary conditions.

Conclusion: Provides a unified framework linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning system that combines numerical stability with the generalization benefits of overparameterized models.

Abstract: Overparameterized models have recently challenged conventional learning theory by exhibiting improved generalization beyond the interpolation limit, a phenomenon known as benign overfitting. This work introduces Adaptive Benign Overfitting (ABO), extending the recursive least-squares (RLS) framework to this regime through a numerically stable formulation based on orthogonal-triangular updates. A QR-based exponentially weighted RLS (QR-EWRLS) algorithm is introduced, combining random Fourier feature mappings with forgetting-factor regularization to enable online adaptation under non-stationary conditions. The orthogonal decomposition prevents the numerical divergence associated with covariance-form RLS while retaining adaptability to evolving data distributions. Experiments on nonlinear synthetic time series confirm that the proposed approach maintains bounded residuals and stable condition numbers while reproducing the double-descent behavior characteristic of overparameterized models. Applications to forecasting foreign exchange and electricity demand show that ABO is highly accurate (comparable to baseline kernel methods) while achieving speed improvements of between 20 and 40 percent. The results provide a unified view linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning framework.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [44] [High-Efficiency Hexagonal Nanowire MAPbI3 Perovskite Solar Cell with Broadband Light Trapping](https://arxiv.org/abs/2601.23191)
*Kawshik Nath,Bibekananda Nath,Ahmed Zubair*

Main category: physics.optics

TL;DR: Hexagonal nanowire perovskite solar cell achieves 24.2% efficiency through enhanced light absorption and polarization insensitivity.


<details>
  <summary>Details</summary>
Motivation: Perovskite solar cells show great promise for next-gen photovoltaics due to excellent light absorption and low-cost manufacturing, but further improvements in light management and charge collection are needed for higher performance.

Method: Designed hexagonal nanowire (HNW) perovskite solar cell structure with CH3NH3PbI3 material, optimized geometric parameters (diameter, period, fill ratio), and embedded SiO2 dielectric spheres in ITO layer to enhance photon confinement. Used FDTD method for optical analysis and solved coupled drift-diffusion and Poisson equations for electrical performance.

Result: Achieved polarization-independent broadband absorption, high optical short-circuit current density of 29.53 mA/cm², and power conversion efficiency of 24.2% through optimized light-matter interaction and carrier transport.

Conclusion: The HNW perovskite solar cell structure demonstrates strong potential for high-performance photovoltaic systems by effectively combining optical confinement with efficient carrier transport, making it suitable for scalable thin-film solar technologies.

Abstract: Perovskite solar cells (PSCs) have emerged as strong contenders for the next generation of photovoltaic (PV) technologies due to their exceptional light absorption properties, tunability, and affordability in manufacturing. Here, we presented an ingenious hexagonal nanowire (HNW)-based PSC that achieves broadband absorption, minimizes reflectance, and offers robust polarization insensitivity by improving light-matter interaction and increasing charge-collection efficiency. The rotational symmetry of the HNW configuration yielded polarization-independent absorbance under both TE and TM illumination across the visible and near-infrared spectra. The optimization of the geometrical parameters of CH3NH3PbI3-based HNW structure, including diameter, period, and fill ratio, offered a wide rangeof variations that influenced both optical properties and device performance. To further intensify photon confinement, a dielectric SiO2 sphere is partially embedded in the ITO layer, improving long-wavelength absorbance and increasing electron-hole pair generation near the active region. We analyzed the finite-difference time-domain (FDTD) method to examine the optical properties of our proposed structure. This study demonstrates that our proposed structure has achieved a higher generation rate, enhanced absorbance, and a higher optical short-circuit current density (Jsc) of 29.53 mA/cm2. Electrical performance is assessed by solving the coupled drift-diffusion and Poisson equations for the dynamics of carrier transport. The optimized HNW structure achieved a notable power conversion efficiency of 24.2%, highlighting a strong connection between optical confinement and effective carrier transport. These attributes render the proposed HNW PSC a viable option for high-performance PV systems and scalable thin-film solar technologies.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [45] [Leveraging Interactions for Efficient Swarm-Based Brownian Computing](https://arxiv.org/abs/2601.22874)
*Alessandro Pignedoli,Atreya Majumdar,Karin Everschor-Sitte*

Main category: cond-mat.stat-mech

TL;DR: Brownian quasiparticles with short-range attractive interactions form energy-efficient swarms that outperform non-interacting searchers in finding global optima in temperature-defined optimization landscapes.


<details>
  <summary>Details</summary>
Motivation: To develop energy-efficient unconventional computing platforms inspired by swarm intelligence, leveraging emergent cooperative behavior in physical systems for optimization tasks.

Method: Use thermally driven Brownian quasiparticles with short-range attractive interactions to form swarms. Define optimization tasks using spatially varying temperature landscapes. Coarse-grain dynamics onto sensor lattice to emulate experimental particle-tracking measurements.

Result: Interacting swarms reliably identify global optima and significantly outperform non-interacting searchers within specific regimes of interaction strength and swarm size. Swarms adapt robustly to time-evolving landscapes.

Conclusion: Interacting Brownian quasiparticles provide a physical platform for scalable, energy-efficient unconventional computing through emergent cooperative optimization behavior.

Abstract: Drawing inspiration from swarm intelligence, we show that short-range attractive interactions between thermally driven Brownian quasiparticles enable energy-efficient optimization. As quasiparticles can be generated directly within a material, the swarm size can be adjusted with minimal energy overhead. Using an optimization task defined by a spatially varying temperature landscape, we quantitatively show that interacting swarms reliably identify global optima and significantly outperform non-interacting searchers within a well-defined regime of interaction strength and swarm size. This improvement arises from emergent cooperative behavior, where local interactions guide the swarm toward high-quality solutions without central coordination. To link our physical model to experimental realizations, we coarse-grain the quasiparticle dynamics onto a sensor lattice and generate trajectories emulating particle-tracking measurements. We further show that the interacting swarm adapts robustly to landscapes that evolve over time. These findings establish interacting Brownian quasiparticles as a physical platform for scalable and energy-efficient unconventional computing.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [46] [Revisiting the energy-momentum squared gravity](https://arxiv.org/abs/2601.22333)
*Mihai Marciu*

Main category: gr-qc

TL;DR: Energy-momentum squared gravity theory revisited with second derivative corrections from thermodynamics, analyzed via linear stability theory to show cosmological compatibility with Universe expansion and matter-to-acceleration transitions.


<details>
  <summary>Details</summary>
Motivation: To revisit and extend energy-momentum squared gravity theory by incorporating second derivative terms of matter Lagrangian with respect to metric, motivated by thermodynamic considerations, and to analyze the physical implications of these corrections.

Method: Derived scalar tensor representation of energy-momentum squared gravity with new thermodynamic corrections, then analyzed physical implications using linear stability theory to examine cosmological system behavior.

Result: The cosmological system is compatible with Universe expansion for specific matter Lagrangians, explaining emergence of matter domination era and transition to late-time accelerated expansion approaching de-Sitter phenomenology.

Conclusion: The extended energy-momentum squared gravity theory with thermodynamic corrections provides a viable framework explaining cosmological evolution from matter domination to late-time acceleration, consistent with de-Sitter phenomenology.

Abstract: In this paper we have revisited the energy-momentum squared gravity theory, by taking into account the second derivative of the matter Lagrangian with respect to the metric, encapsulating relations originated from thermodynamical grounds. After obtaining the scalar tensor representation of the energy-momentum squared gravity with the new corrections, we have analyzed the physical implications by relying on the linear stability theory. The results show that the current cosmological system is compatible with the expansion of the Universe for some specific matter Lagrangians, explaining the emergence of matter domination era, approaching the late time accelerated expansion era close to the de-Sitter phenomenology.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [47] [Understanding the sign problem from an exact Path Integral Monte Carlo model of interacting harmonic fermions](https://arxiv.org/abs/2601.22559)
*Siu A. Chin*

Main category: cond-mat.str-el

TL;DR: Operator contraction identity for harmonic oscillator path integrals can be applied to fermions, creating an exactly solvable model for studying the sign problem in Path Integral Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: To develop an exactly solvable model for studying the fermion sign problem in Path Integral Monte Carlo simulations, allowing analytical understanding of how interactions affect the sign problem.

Method: Apply operator contraction identity (originally for harmonic oscillator) to fermions in any dimension, use fourth-order and variable-bead algorithms for numerical computations, compare with neural network results.

Result: Repulsive interactions shift sign problem to larger imaginary time, attractive to smaller time, but don't worsen it compared to non-interacting case. For closed-shell fermion numbers, sign problem disappears at large imaginary time. Successfully computed ground state energies of quantum dots with up to 110 electrons.

Conclusion: The operator contraction approach provides an exactly solvable framework for understanding fermion sign problems, revealing that interactions don't fundamentally worsen the sign problem and that closed-shell systems can avoid it at large imaginary times.

Abstract: This work shows that the recently discovered operator contraction identity for solving the discreet Path Integral of the harmonic oscillator can be applied equally to fermions in any dimension. This then yields an exactly solvable model for studying the sign problem where the Path Integral Monte Carlo energy at any time step for any number of fermions is known analytically, or can be computed numerically. It is found that repulsive/attractive pairwise interaction shifts the sign problem to larger/smaller imaginary time, but does not make it more severe than the non-interacting case. More surprisingly, for closed-shell number of fermions, the sign problem goes away at large imaginary time. Fourth-order and newly found variable-bead algorithms are used to compute ground state energies of quantum dots with up to 110 electrons and compared to results obtained by modern neural networks.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [48] [Dynamical stability of various convex graphical translators](https://arxiv.org/abs/2601.22368)
*Junyoung Park*

Main category: math.DG

TL;DR: Existence of longtime MCF solutions from continuous graphs over slabs, plus dynamical stability for various graphical translators.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous existence results for mean curvature flow starting from continuous initial data and to understand the stability properties of important special solutions (translators) in the flow.

Method: Two-part approach: 1) Prove existence of longtime solutions to mean curvature flow starting from graphs of continuous functions over slabs. 2) Establish dynamical stability results for graphical translators using analytical techniques.

Result: Successfully proved existence of longtime solutions from continuous initial data and established dynamical stability for grim reaper, 2D graphical translators, and asymptotically cylindrical translators.

Conclusion: The paper provides important existence and stability results for mean curvature flow, extending the theory to continuous initial data and establishing robustness properties for key translator solutions.

Abstract: In the first part of the paper, we prove the existence of longtime solution to mean curvature flow starting from a graph of a continuous function defined over a slab. Then, we establish dynamical stability results for various types of graphical translators to mean curvature flow, namely the grim reaper, two dimensional graphical translators, and asymptotically cylindrical translators.

</details>


### [49] [Sharp thresholds for Escobar and Gagliardo-Nirenberg functionals: the Escobar-Willmore mass, geometric selection, and compactness trichotomy](https://arxiv.org/abs/2601.22665)
*Mayukh Mukherjee,Utsab Sarkar*

Main category: math.DG

TL;DR: A unified framework for sharp threshold phenomena in boundary-critical variational problems on Riemannian manifolds, covering Escobar quotient and Gagliardo-Nirenberg inequalities, with geometric selection governed by mean curvature and Willmore-type anisotropy.


<details>
  <summary>Details</summary>
Motivation: To develop a quantitative framework for understanding sharp threshold phenomena in boundary-critical variational problems on compact Riemannian manifolds, particularly addressing the dichotomy between attainment (compactness) and bubbling (non-compactness) in critical Sobolev-type inequalities.

Method: Uses transfer-stability-reduction approach to obtain attainment-versus-bubbling alternatives, H¹-compactness, and finite-dimensional reductions. Geometric selection is governed by mean curvature H_g and a Willmore-type anisotropy from |ĪĪ|². Introduces renormalized boundary mass 𝔖_g involving Ricci curvature, scalar curvature, and traceless second fundamental form.

Result: Identifies threshold dichotomy: if first nonvanishing coefficient among {ρ_n^conf H_g, 𝔖_g, Θ_g} is negative somewhere, then C^*_Esc(M,g) < S_* and sequences are precompact. At threshold, blow-up concentrates where H_g is critical. In multi-bubble regime, dynamics governed by 𝒲_k produce k-bubble critical points. Resolves question about sharp constant divergence with small Dirichlet windows.

Conclusion: Provides a comprehensive framework for sharp threshold phenomena in boundary-critical problems, establishing precise geometric criteria for compactness versus bubbling, with applications to entropy inequalities, curvature-driven NLS ground states, and Euler characteristic recovery from Gagliardo-Nirenberg measurements.

Abstract: We develop a unified quantitative framework for sharp threshold phenomena in boundary-critical variational problems on compact Riemannian manifolds, covering the Escobar quotient and Gagliardo-Nirenberg inequalities. Via transfer-stability-reduction, we obtain attainment-versus-bubbling alternatives, $H^1$-compactness, and finite-dimensional reductions. Geometric selection is governed by mean curvature $H_g$ and a Willmore-type anisotropy from $|\mathring{\mathrm{II}}|^2$.
  At hemisphere threshold $S_\ast=C^*_{\mathrm{Esc}}(\mathbb S^n_+)$ for $n\ge5$ on $H_g\equiv0$, we identify a renormalized boundary mass $\mathfrak R_g=κ_1(n)\,\mathrm{Ric}_g(ν,ν)+κ_2(n)\,\mathrm{Scal}_{g|\partial M}+κ_3(n)\,|\mathring{\mathrm{II}}|^2$, $κ_3(n)<0$, yielding one-bubble expansions and energy-only estimators. Threshold dichotomy: if the first nonvanishing coefficient among $\{ρ_n^{\mathrm{conf}}H_g,\mathfrak R_g,Θ_g\}$ is negative somewhere, then $C^*_{\mathrm{Esc}}(M,g)<S_\ast$ and sequences are precompact. At threshold, blow-up concentrates where $H_g$ is critical; on $H_g\equiv0$, stationarity forces $\mathfrak R_g(p)=\nabla_\partial\mathfrak R_g(p)=0$. If $H_g$ is Morse and $\mathfrak R_g>0$ at all critical points, no bubbling occurs. In multi-bubble regime ($n\ge5$), dynamics governed by $\mathcal W_k=\sum_{i=1}^k\mathfrak R_g(x_i)$ produce $k$-bubble critical points at levels $k^{1/(n-1)}S_\ast$. In the degenerate case we obtain conformal hemispherical rigidity.
  The GN track yields analogous dichotomies and resolves a question of Christianson et al.: the sharp constant with small Dirichlet windows diverges at optimal capacitary rate, relating threshold to spectral/isoperimetric invariants. Applications include entropy inequalities for fast diffusion, curvature-driven NLS ground states, and (in $n=2$) Euler characteristic recovery from GN measurements.

</details>


### [50] [Prescribed $T$-curvature flow on the four-dimensional unit ball](https://arxiv.org/abs/2601.22934)
*Pak Tung Ho,Cheikh Birahim Ndiaye,Liming Sun,Heming Wang*

Main category: math.DG

TL;DR: Study of prescribed T-curvature problem on 4D unit ball using T-curvature flow, establishing existence results via Morse theory and proving exponential convergence of the flow.


<details>
  <summary>Details</summary>
Motivation: To solve the prescribed T-curvature problem on the 4-dimensional unit ball, which involves finding metrics with specific curvature properties, using a flow approach that provides both existence results and convergence properties.

Method: Combines Ache-Chang's inequality with Malchiodi-Struwe's Morse-theoretic approach to establish existence results under strong Morse-type inequalities at infinity. Uses T-curvature flow analysis starting from Q-flat minimal metrics conformal to Euclidean metric.

Result: Established existence results for prescribed T-curvature problem under strong Morse-type inequalities at infinity. Proved exponential convergence of T-curvature flow on 4D ball from Q-flat minimal metrics to extremal metrics of Ache-Chang's inequality.

Conclusion: The T-curvature flow approach successfully solves prescribed curvature problems on 4D balls, with both existence theorems and convergence results to known extremal metrics, connecting geometric analysis with Morse theory.

Abstract: In this paper, we study the prescribed $T$-curvature problem on the unit ball $\mathbb{B}^4$ of $\mathbb{R} ^4$ via the $T$-curvature flow approach. By combining Ache-Chang's inequality with the Morse-theoretic approach of Malchiodi-Struwe, we establish existence results under strong Morse-type inequalities at infinity. As a byproduct of our argument, we also prove the exponential convergence of the $T$-curvature flow on $\mathbb{B}^4$, starting from a $Q$-flat and minimal metric conformal to the standard Euclidean metric, to an extremal metric of Ache-Chang's inequality whose explicit expression was derived by Ndiaye-Sun.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Exact closed-form Gaussian moments of residual layers](https://arxiv.org/abs/2601.22307)
*Simon Kuang,Xinfan Lin*

Main category: cs.LG

TL;DR: Exact moment matching for Gaussian propagation through deep neural networks with various activation functions, achieving orders-of-magnitude improvements in KL divergence over alternatives.


<details>
  <summary>Details</summary>
Motivation: To address the longstanding gap in propagating mean and covariance of multivariate Gaussian distributions through deep neural networks using layer-by-layer moment matching, particularly for various activation functions in both feedforward and residual architectures.

Method: Derived exact moment matching for probit, GeLU, ReLU (as limit of GeLU), Heaviside (as limit of probit), and sine activation functions; applied to both feedforward and generalized residual layers using layer-by-layer Gaussian propagation.

Result: Achieved orders-of-magnitude improvements (up to millionfold) in KL divergence error on random networks, competitive statistical calibration for epistemic uncertainty inference on real data, and hundredfold improvements over state-of-the-art deterministic inference in variational Bayes networks.

Conclusion: The method provides exact moment matching for important activation functions, significantly outperforming existing alternatives in accuracy for uncertainty propagation through deep neural networks, with applications to both random networks and real-world inference tasks.

Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.

</details>


### [52] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: MSN-PINN is a physics-informed neural network that learns power-law scaling exponents as trainable parameters, enabling recovery of singular solution structures with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard neural networks fail to explicitly capture power-law scaling behavior near singularities, interfaces, and critical points, leaving important physical exponents implicit rather than learnable.

Method: Introduces physics-informed Müntz-Szász Networks (MSN-PINN) with power-law basis functions where scaling exponents are trainable parameters. Uses constraint-aware training to encode physical requirements like boundary condition compatibility.

Result: Achieves single-exponent recovery with 1-5% error under noise and sparse sampling. Recovers corner singularity exponents for 2D Laplace equation with 0.009% error, matches classical Kondrat'ev (1967) results, and achieves 100% success rate on 40-configuration wedge benchmark with 0.022% mean error.

Conclusion: MSN-PINN combines neural network expressiveness with asymptotic analysis interpretability, producing physically meaningful parameters and improving accuracy by three orders of magnitude over naive training through constraint-aware methods.

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [53] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: DDIS: A decoupled diffusion framework for inverse PDE problems that separates coefficient prior learning from physics modeling, achieving superior data efficiency and accuracy with limited supervision.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion posterior samplers for inverse PDE problems require substantial paired supervision and implicitly represent physics through joint coefficient-solution modeling, which suffers from guidance attenuation when training data is scarce.

Method: Decoupled Diffusion Inverse Solver (DDIS) uses two components: 1) an unconditional diffusion model learns the coefficient prior, and 2) a neural operator explicitly models the forward PDE for guidance. This enables Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing issues in Diffusion Posterior Sampling (DPS).

Result: DDIS achieves state-of-the-art performance under sparse observation, improving l₂ error by 11% and spectral error by 54% on average. With only 1% training data, DDIS maintains accuracy with 40% advantage in l₂ error compared to joint models.

Conclusion: The decoupled design enables superior data efficiency and effective physics-informed learning while avoiding guidance attenuation failure, making DDIS particularly effective for inverse PDE problems with limited training data.

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [54] [On two-dimensional Dirac operators with critical delta-shell interactions](https://arxiv.org/abs/2601.23053)
*William Borrelli,Pietro Carimati,Davide Fermi*

Main category: math.SP

TL;DR: 2D Dirac operators with singular interactions on lines/circles show critical points in essential spectrum - infinite multiplicity eigenvalue for lines vs. accumulation point of eigenvalues for circles.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties of Dirac operators with singular interactions in different geometries, particularly the nature of critical points in the essential spectrum that appear within the mass gap.

Method: Study two-dimensional Dirac operators with electrostatic and Lorentz-scalar singular interactions supported on straight lines and circles, analyzing spectral properties for critical interaction strengths.

Result: For straight line: critical point is eigenvalue of infinite multiplicity with detailed eigenfunction analysis. For circle: critical point is not an eigenvalue but accumulation point of double sequence of simple eigenvalues.

Conclusion: Different geometries yield fundamentally different spectral behaviors for critical singular interactions, suggesting conjectures about general smooth curves.

Abstract: We study two-dimensional Dirac operators with singular interactions of electrostatic and Lorentzscalar type, supported either on a straight line or a circle. For certain critical values of the interaction strengths, the essential spectrum of such operators comprises an isolated point lying within the mass gap. We clarify the nature of this point in both geometries. For the straight line model, this point is known to be an eigenvalue of infinite multiplicity, and we provide a detailed analysis of the corresponding eigenfunctions. By contrast, in the case of a circle, we show that the said point is not itself an eigenvalue, but rather an accumulation point of a double sequence of simple eigenvalues. In view of the high degree of symmetry of the configurations under analysis, this behavior is unexpected and our findings lead us to formulate some conjectures concerning critical singular interactions supported on generic smooth curves.

</details>
