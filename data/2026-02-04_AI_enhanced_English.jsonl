{"id": "2602.02616", "pdf": "https://arxiv.org/pdf/2602.02616", "abs": "https://arxiv.org/abs/2602.02616", "authors": ["\u00c9lise Foulatier", "Pierre-Alain Boucard", "Fran\u00e7ois Louf", "David N\u00e9ron", "Philipp Junker"], "title": "A space-time LATIN-PGD strategy for solving Newtonian compressible flows", "categories": ["math.NA", "physics.flu-dyn", "physics.med-ph"], "comment": null, "summary": "Simulating flow problems is at the core of many engineering applications but often requires high computational effort, especially when dealing with complex models. This work presents a novel approach for resolving flow problems using the LATIN-PGD solver. In this contribution, we place ourselves within the framework of Newtonian compressible and laminar flows. This specific and relatively simple case enables focusing on flows for which a state equation provides a direct relation between pressure and density. It is then possible to use the LATIN solver to set up a pressure-velocity decoupling algorithm. Moreover, Proper Generalised Decomposition (PGD) is natively included in the solver and yields two independent space-time decompositions for the velocity and the pressure fields. As a first step, the solver is validated on a problem for which an analytical solution is available. It is then applied to slightly more complex problems. The results show good agreement with the literature, and we expect that the solver could be used to compute more complicated material laws in the future.", "AI": {"tldr": "Novel LATIN-PGD solver for Newtonian compressible laminar flows with pressure-velocity decoupling and space-time decomposition.", "motivation": "Flow simulation requires high computational effort for complex models, especially in engineering applications. There's a need for efficient solvers that can handle complex flow problems with reduced computational cost.", "method": "Uses LATIN-PGD solver for Newtonian compressible laminar flows. Leverages state equation for direct pressure-density relation to enable pressure-velocity decoupling algorithm. Incorporates Proper Generalised Decomposition (PGD) for independent space-time decompositions of velocity and pressure fields.", "result": "Solver validated on problem with analytical solution, then applied to slightly more complex problems. Results show good agreement with literature, demonstrating the solver's effectiveness.", "conclusion": "The LATIN-PGD solver shows promise for Newtonian compressible laminar flows and is expected to be extendable to more complicated material laws in future applications."}}
{"id": "2602.02779", "pdf": "https://arxiv.org/pdf/2602.02779", "abs": "https://arxiv.org/abs/2602.02779", "authors": ["Koji Koyamada"], "title": "Comparison of Trefftz-Based PINNs and Standard PINNs Focusing on Structure Preservation", "categories": ["math.NA"], "comment": null, "summary": "In this study, we investigate the capability of physics-informed neural networks (PINNs) to preserve global physical structures by comparing standard PINNs with a Trefftz-based PINN (Trefftz-PINN). The target problem is the reproduction of mag-netic field-line structures in a helical fusion reactor configuration. Using identical training data sampled from exact solutions, we perform comparisons under matched mean squared error (MSE) levels. Visualization of magnetic field lines reveals that standard PINNs may exhibit structural collapse across magnetic surfaces even when the MSE is sufficiently small, whereas Trefftz-PINNs successfully preserve the global topology of magnetic field lines. Furthermore, the proposed framework is extended to computational fluid dynamics (CFD) problems, where streamline structures of veloc-ity fields are analyzed. Similar tendencies are observed, demonstrating that Trefftz-PINNs provide superior structure preservation compared to standard PINNs. These results indicate that minimizing numerical error alone does not guarantee physical consistency, and that constraining the solution space prior to learning is an effective strategy for physics-consistent surrogate modeling.", "AI": {"tldr": "Trefftz-PINNs outperform standard PINNs in preserving global physical structures like magnetic field lines and streamlines, showing that low MSE alone doesn't ensure physical consistency.", "motivation": "Standard physics-informed neural networks (PINNs) may fail to preserve global physical structures even with small numerical errors, raising concerns about physical consistency in surrogate modeling.", "method": "Compare standard PINNs with Trefftz-based PINNs (Trefftz-PINNs) on magnetic field-line structures in helical fusion reactors and CFD problems, using identical training data and matched MSE levels.", "result": "Trefftz-PINNs successfully preserve global topology of magnetic field lines and streamline structures, while standard PINNs exhibit structural collapse even with small MSE.", "conclusion": "Minimizing numerical error alone doesn't guarantee physical consistency; constraining solution space before learning (as in Trefftz-PINNs) is effective for physics-consistent surrogate modeling."}}
{"id": "2602.03083", "pdf": "https://arxiv.org/pdf/2602.03083", "abs": "https://arxiv.org/abs/2602.03083", "authors": ["Hao Hu", "Haijun Yu"], "title": "Scaling Optimized Spectral Approximations on Unbounded Domains: The Generalized Hermite and Laguerre Methods", "categories": ["math.NA"], "comment": "40 pages", "summary": "We propose a novel error analysis framework for scaled generalized Laguerre and generalized Hermite approximations.This framework can be regarded as an analogue of the Nyquist-Shannon sampling theorem: It characterizes the spatial and frequency bandwidths that can be effectively captured by Laguerre or Hermite sampling points. Provided a function satisfies the corresponding bandwidth constraints, it can be accurately approximated within this framework. The proposed framework is notably more powerful than classical theory -- it not only provides systematic guidance for choosing the optimal scaling factor, but also predicts root-exponential and other intricate convergence behaviors that classical approaches fail to capture. Leveraging this framework, we conducted a detailed comparative study of Hermite and Laguerre approximations. We find that functions with similar decay and oscillation characteristics may nonetheless display markedly different convergence rates. Furthermore, approximations based on two concatenated sets of Laguerre functions may offer significant advantages over those using a single set of Hermite functions.", "AI": {"tldr": "Novel error analysis framework for scaled generalized Laguerre/Hermite approximations that functions like Nyquist-Shannon sampling theorem, predicting optimal scaling and complex convergence behaviors.", "motivation": "Classical theory for Laguerre and Hermite approximations lacks systematic guidance for optimal scaling factors and fails to capture complex convergence behaviors like root-exponential convergence.", "method": "Proposed error analysis framework analogous to Nyquist-Shannon sampling theorem, characterizing spatial and frequency bandwidths captured by Laguerre/Hermite sampling points with bandwidth constraints for accurate approximation.", "result": "Framework provides systematic guidance for optimal scaling factor selection, predicts root-exponential and other intricate convergence behaviors, and reveals that functions with similar decay/oscillation can have different convergence rates; concatenated Laguerre functions may outperform single Hermite sets.", "conclusion": "The proposed framework significantly advances Laguerre/Hermite approximation theory, offering practical guidance for optimal scaling and revealing complex convergence patterns, with concatenated Laguerre approximations showing potential advantages over Hermite methods."}}
{"id": "2602.02812", "pdf": "https://arxiv.org/pdf/2602.02812", "abs": "https://arxiv.org/abs/2602.02812", "authors": ["Randall Clark", "Vacslav Glukhov", "Georgy Subbotin", "Maxim Nurgaliev", "Aleksandr Kachkin", "Lei Zeng", "Dmitri M. Orlov"], "title": "Plasma Confinement State Classification in Fusion Power Plants: Profile Reflectometer and Ensemble Diagnostics", "categories": ["physics.plasm-ph"], "comment": "8 pages, 6 figures, 4 tables", "summary": "As Fusion Pilot Plants (FPPs) are increasingly viewed as within reach, many engineering challenges remain. Not many diagnostics are expected to be available in a reactor environment. Survivability, maintainability, and limited port space substantially restrict the number of FPP-relevant diagnostics. One remaining challenge is developing tools and devices to extract plasma state information necessary for controlling an FPP from a limited subset of diagnostics. This work is part of an overarching project to address this challenge. The specific diagnostic subset to be used in FPPs is still under debate. We take the approach of developing machine-learning-based tools for different significant plasma state parameters, using already known FPP-viable diagnostics. Previously we developed a plasma confinement mode classifier utilizing the Electron Cyclotron Emission (ECE) diagnostic. Here, we expand on this by developing a Profile Reflectometer (PR) based classifier with 97\\% test accuracy, and an ensemble model that combines the ECE and PR models into a single model, achieving 99\\% test accuracy.", "AI": {"tldr": "Developed machine learning classifiers for plasma confinement mode detection in fusion reactors using Profile Reflectometer (97% accuracy) and ensemble model combining ECE and PR (99% accuracy).", "motivation": "Fusion Pilot Plants face engineering challenges with limited diagnostics due to survivability, maintainability, and port space constraints. Need tools to extract plasma state information from limited diagnostic subsets for reactor control.", "method": "Developed machine-learning-based tools using FPP-viable diagnostics: 1) Profile Reflectometer (PR) based classifier, 2) Ensemble model combining previously developed ECE-based classifier with new PR classifier.", "result": "PR-based classifier achieved 97% test accuracy. Ensemble model combining ECE and PR models achieved 99% test accuracy for plasma confinement mode classification.", "conclusion": "Machine learning approaches using limited diagnostic subsets (ECE and PR) can effectively classify plasma confinement modes with high accuracy, addressing the challenge of extracting plasma state information in reactor environments with restricted diagnostics."}}
{"id": "2602.03404", "pdf": "https://arxiv.org/pdf/2602.03404", "abs": "https://arxiv.org/abs/2602.03404", "authors": ["Arjun Puthli", "Somdatta Goswami", "Souvik Chakraborty"], "title": "Neural Hodge Corrective Solvers: A Hybrid Iterative-Neural Framework", "categories": ["physics.comp-ph"], "comment": null, "summary": "We introduce the Neural Hodge Corrective Solver (NHCS), a hybrid iterative-neural framework for partial differential equations that embeds learned corrective operators within the Discrete Exterior Calculus (DEC) formulation. The method combines classical Jacobi-Richardson iterations with data-driven corrections to refine numerical solutions while preserving the underlying topological and metric structure. NHCS employs a two-phase training strategy. In the first phase, DEC operators are learned through relative residual minimization from data. In the second phase, these operators are integrated into the iterative solver, and training targets the improvement of convergence through learned corrective updates that remain effective even for inaccurate intermediate solutions. This staggered training enables stable, progressive refinement while maintaining the structure-preserving properties of DEC discretizations. To improve multiscale adaptivity, NHCS introduces a convolutional neural network-based correction term capable of capturing fine-scale solution features via localized updates informed by global context, improving scalability over mesh component-wise neural approaches. Moreover, the proposed framework substantially reduces computational cost by avoiding Newton-Raphson-based training and the associated Jacobian evaluations of parameterized operators. The resulting solver achieves improved efficiency, robustness, and accuracy without compromising numerical stability.", "AI": {"tldr": "NHCS is a hybrid iterative-neural PDE solver that combines classical Jacobi-Richardson iterations with learned corrective operators within Discrete Exterior Calculus framework, using two-phase training for improved efficiency and stability.", "motivation": "To develop a PDE solver that combines classical numerical methods with data-driven corrections while preserving topological and metric structure, improving efficiency over Newton-Raphson approaches and scalability over mesh component-wise neural methods.", "method": "Hybrid iterative-neural framework embedding learned corrective operators within Discrete Exterior Calculus formulation. Uses two-phase training: first learns DEC operators through relative residual minimization, then integrates them into iterative solver with corrective updates. Employs CNN-based correction terms for multiscale adaptivity.", "result": "Achieves improved efficiency, robustness, and accuracy without compromising numerical stability. Reduces computational cost by avoiding Newton-Raphson training and Jacobian evaluations. Enables stable progressive refinement while preserving DEC structure.", "conclusion": "NHCS provides an effective hybrid approach combining classical iterative methods with data-driven corrections, offering improved performance while maintaining the structure-preserving properties of DEC discretizations."}}
{"id": "2602.02631", "pdf": "https://arxiv.org/pdf/2602.02631", "abs": "https://arxiv.org/abs/2602.02631", "authors": ["Hangsheng Chen"], "title": "Revisiting Non-Rotating Star Models: Classical Existence and Uniqueness Theory and Scaling Relations", "categories": ["math.AP", "math-ph"], "comment": "41 pages, comments welcome", "summary": "This paper presents a systematic study of the properties of non-rotating stellar models governed by the Euler-Poisson system under general equations of state, including the case of polytropic gaseous stars. We revisit and extend existence results by Auchmuty and Beals \\cite{AB71}, adapt the uniqueness results from the quantum mechanical framework of Lieb and Yau \\cite{LY87} to the classical Newtonian mechanical setting. The results are also synthesized in McCann \\cite{McC06} but without proof. The second work we do is applying a scaling method to establish relations between solutions with different total masses. As the mass tends to zero, we analyze convergence properties of the density functions and identify precise rates for the contraction or extension of their supports.", "AI": {"tldr": "Systematic study of non-rotating stellar models via Euler-Poisson system, extending existence results, adapting uniqueness proofs, and analyzing scaling properties as mass approaches zero.", "motivation": "To provide rigorous mathematical foundations for classical Newtonian stellar models, extending previous existence results by Auchmuty and Beals, and adapting quantum mechanical uniqueness proofs to classical setting.", "method": "Uses Euler-Poisson system with general equations of state; revisits/extends existence results; adapts uniqueness proofs from quantum mechanics; applies scaling analysis to study mass dependence; analyzes convergence as mass\u21920.", "result": "Establishes existence and uniqueness results for stellar models; derives scaling relations between solutions of different masses; identifies precise convergence rates for density functions and support contraction/extension as mass approaches zero.", "conclusion": "Provides comprehensive mathematical framework for non-rotating Newtonian stellar models, connecting classical and quantum mechanical approaches, and quantifying scaling behavior in the low-mass limit."}}
{"id": "2602.03118", "pdf": "https://arxiv.org/pdf/2602.03118", "abs": "https://arxiv.org/abs/2602.03118", "authors": ["Henri Klinteb\u00e4ck", "Christoph Ortner", "Lior Silberman"], "title": "The High Cost of Data Augmentation for Learning Equivariant Models", "categories": ["math.NA"], "comment": "33 pages, 13 figures", "summary": "According to Noether's theorem the presence of a continuous symmetry in a Hamiltonian systems is equivalent to the existence of a conserved quantity, yet these symmetries are not always explicitly enforced in data-driven models. There remains a debate whether or not encoding of symmetry into a model architecture is the optimal approach. A competing approach is to target approximate symmetry through data augmentation. In this work, we study two approaches aimed at improving the symmetry properties of such an approximation scheme: one based on a quadrature rule for the Haar measure on the compact Lie group encoding the continuous symmetry of interest and one based on a random sampling of that Haar measure. We demonstrate both theoretically and empirically that the quadrature augmentation leads to exact symmetry preservation in polynomial models, while the random augmentation has only square-root convergence of the symmetrization error.", "AI": {"tldr": "Quadrature-based data augmentation preserves exact symmetries in polynomial models, while random sampling has slower square-root convergence.", "motivation": "Data-driven models often don't explicitly enforce continuous symmetries from Noether's theorem, and there's debate about whether encoding symmetry into architecture or using data augmentation is better. This work studies approaches to improve symmetry properties in approximation schemes.", "method": "Two symmetry improvement approaches: 1) quadrature rule for Haar measure on compact Lie group encoding continuous symmetry, 2) random sampling of that Haar measure. Theoretical and empirical analysis comparing these augmentation methods.", "result": "Quadrature augmentation leads to exact symmetry preservation in polynomial models, while random augmentation has only square-root convergence of symmetrization error.", "conclusion": "For polynomial models, quadrature-based data augmentation is superior to random sampling for symmetry preservation, providing exact symmetry rather than approximate convergence."}}
{"id": "2602.03375", "pdf": "https://arxiv.org/pdf/2602.03375", "abs": "https://arxiv.org/abs/2602.03375", "authors": ["Vibhor Kumar Singh", "Amal R Biju", "Jaya Kumar Alageshan", "Kaushalender Singh", "Deepti Sharma", "Joydeep Ghosh", "Nishant Sirse", "Abhijit Sen", "Sarveshwar Sharma", "Manjunatha Valmiki", "Sandeep Agrawal", "Sanjay Wandhekar", "Animesh Kuley"], "title": "Effect of static magnetic island on ITG of ADITYA-U tokamak", "categories": ["physics.plasm-ph"], "comment": "19 pages, 11 figures", "summary": "Magnetic islands play a crucial role in regulating plasma confinement in tokamaks by interacting with micro-instabilities, such as the ion temperature gradient (ITG) mode. This work presents a detailed investigation of the effects of static magnetic islands on ITG instability, relevant to the ADITYA-U tokamak, using the Global Gyrokinetic Code in Cylindrical Coordinates (G2C3), a particle-in-cell (PIC) framework that employs a neural-network-assisted projection scheme. A two-phase simulation strategy is adopted. In the first phase, static magnetic islands with mode numbers (m, n) = (2, 1) and (3, 1) are introduced by perturbing the equilibrium magnetic flux functions. Particle dynamics within these modified topologies result in the flattening of plasma density profiles in the island regions, confirming island formation and its impact on the equilibrium profiles. In the second phase, the flattened profiles serve as new equilibria for linear electrostatic gyrokinetic simulations with adiabatic electrons, enabling the study of the modified ITG behavior. Magnetic islands significantly restructure the ITG mode, producing a spatial redistribution of potential fluctuations within and around the island region. Moreover, as the island width increases, the growth rates of different toroidal ITG modes converge, suggesting a universal stabilization trend. A comparison between the (2,1) and (3,1) islands indicates that higher-q islands lead to a more spatially extended ITG mode structure, reflecting the longer magnetic connection lengths and weaker curvature drive at outer flux surfaces. These results demonstrate the pivotal role of island-induced equilibrium modifications in determining ITG stability and mode structure in tokamak plasmas.", "AI": {"tldr": "Magnetic islands in tokamaks significantly alter ITG instability structure and growth rates through profile flattening, with wider islands showing universal stabilization trends and higher-q islands producing more extended mode structures.", "motivation": "To understand how magnetic islands affect ion temperature gradient (ITG) instability in tokamak plasmas, particularly for the ADITYA-U tokamak, since islands play a crucial role in regulating plasma confinement by interacting with micro-instabilities.", "method": "Used Global Gyrokinetic Code in Cylindrical Coordinates (G2C3) with neural-network-assisted PIC framework. Two-phase simulation: first introduced static (2,1) and (3,1) magnetic islands by perturbing equilibrium magnetic flux, then used flattened profiles as new equilibria for linear electrostatic gyrokinetic simulations with adiabatic electrons.", "result": "Magnetic islands restructure ITG mode spatial distribution, with growth rates converging as island width increases (universal stabilization trend). Higher-q (3,1) islands produce more spatially extended ITG mode structures due to longer magnetic connection lengths and weaker curvature drive at outer flux surfaces.", "conclusion": "Island-induced equilibrium modifications play a pivotal role in determining ITG stability and mode structure in tokamak plasmas, demonstrating how magnetic islands regulate micro-instability behavior through profile flattening effects."}}
{"id": "2602.03745", "pdf": "https://arxiv.org/pdf/2602.03745", "abs": "https://arxiv.org/abs/2602.03745", "authors": ["Michael Poluektov"], "title": "Transformation front kinetics in deformable ferromagnets", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci"], "comment": null, "summary": "Materials such as magnetic shape-memory alloys possess an intrinsic coupling between material's magnetisation and mechanical deformation. These materials also undergo structural phase transitions, with phase boundaries separating different phases and the kinetics of the phase boundaries governed by the magnetic field and the mechanical stresses. There is a multiplicity of other materials revealing similar phenomena, e.g. magnetic perovskites. To model the propagation of the phase boundaries in deformable magnetic materials at the continuum scale, three ingredients are required: a set of governing equations for the bulk behaviour with coupled magnetic and mechanical degrees of freedom, a dependency of the phase boundary velocity on the governing factors, and a reliable computational method. The expression for the phase boundary velocity is usually obtained within the continuum thermodynamics setting, where the entropy production due to phase boundary propagation is derived, which gives a thermodynamic driving force for the phase boundary kinetics. For deformable ferromagnets, all three elements (bulk behaviour, interface kinetics, and computational approaches) have been explored, but under a number of limitations. The present paper focuses on the derivation of the thermodynamic driving force for transformation fronts in a general magneto-mechanical setting, adapts the cut-finite-element method for transformation fronts in magneto-mechanics, which allows for an exceptionally efficient handling of the propagating interfaces, without modifying the finite-element mesh, and applies the developments to qualitative modelling of magneto-mechanics of magnetic shape-memory alloys.", "AI": {"tldr": "This paper develops a computational framework for modeling phase boundary propagation in magneto-mechanical materials, deriving thermodynamic driving forces and adapting cut-FEM methods for efficient interface tracking without mesh modification.", "motivation": "Materials like magnetic shape-memory alloys exhibit coupled magneto-mechanical behavior with phase transitions, but existing models have limitations in handling propagating phase boundaries efficiently. There's a need for a comprehensive computational approach that combines bulk behavior, interface kinetics, and efficient numerical methods for transformation fronts.", "method": "The paper derives thermodynamic driving forces for transformation fronts in general magneto-mechanical settings, then adapts the cut-finite-element method (cut-FEM) for magneto-mechanics. This approach allows efficient handling of propagating interfaces without modifying the finite-element mesh.", "result": "The developed framework enables qualitative modeling of magneto-mechanics in magnetic shape-memory alloys, providing an exceptionally efficient computational method for tracking propagating phase boundaries while maintaining accuracy in coupled magneto-mechanical simulations.", "conclusion": "The paper presents a comprehensive computational framework for modeling phase boundary propagation in magneto-mechanical materials, successfully combining thermodynamic driving force derivation with efficient cut-FEM implementation for practical applications in magnetic shape-memory alloy modeling."}}
{"id": "2602.02715", "pdf": "https://arxiv.org/pdf/2602.02715", "abs": "https://arxiv.org/abs/2602.02715", "authors": ["Istvan Kadar", "Warren Li"], "title": "Scattering and stability for ODE-type blow-up surfaces for focusing nonlinear wave equations", "categories": ["math.AP"], "comment": "45 pages", "summary": "We study the focusing power nonlinear wave equation with any power, in Minkowski space of any spacetime dimension. We present a complete understanding of the local stability and scattering theory (both in high regularity spaces) for solutions exhibiting ODE type blow-up on spacelike hypersurfaces, with the blow-up at each point modelled by the explicit solution $\u03c6_{\\mathrm{model}} = c_p t^{-\u03b1_p}$.\n  Given a sufficiently regular spacelike hypersurface $\u03a3_f$, together with auxiliary scattering data $\u03c8$, we construct the unique corresponding solution to the nonlinear wave equation that (locally) forms an ODE type singularity on $\u03a3_f$ attaining $\u03c8$ as scattering data. Conversely, we show that such ODE type singularities are (locally) stable to suitably regular perturbations away from the singularity, and that the blow-up surface and scattering data remain regular, in a continuously dependent manner, following such perturbations.", "AI": {"tldr": "Complete local stability and scattering theory for ODE-type blow-up singularities in focusing power nonlinear wave equations across any spacetime dimension.", "motivation": "To understand the local stability and scattering behavior of solutions to focusing power nonlinear wave equations that exhibit ODE-type blow-up singularities on spacelike hypersurfaces, providing a complete theoretical framework for such singular behavior.", "method": "Study focusing power nonlinear wave equations in Minkowski space of any dimension, analyze solutions with ODE-type blow-up modelled by explicit solution \u03c6_model = c_p t^{-\u03b1_p}, construct unique solutions from given spacelike hypersurfaces and scattering data, and prove stability under perturbations.", "result": "Successfully constructed unique solutions corresponding to given spacelike hypersurfaces and scattering data that form ODE-type singularities, and proved local stability of such singularities under regular perturbations with continuous dependence of blow-up surface and scattering data.", "conclusion": "The paper establishes a complete local stability and scattering theory for ODE-type blow-up singularities in focusing power nonlinear wave equations, demonstrating both existence/uniqueness of solutions with prescribed singular behavior and stability under perturbations."}}
{"id": "2602.03166", "pdf": "https://arxiv.org/pdf/2602.03166", "abs": "https://arxiv.org/abs/2602.03166", "authors": ["Arun Govind Neelan"], "title": "Event-Level Probabilistic Prediction of Extreme Rainfall over India Using Physics-Gated Latent Dynamics", "categories": ["math.NA"], "comment": null, "summary": "Extreme rainfall over the Indian monsoon region poses severe societal and infrastructural risks but remains difficult to predict at daily time scales due to stochastic convective triggering and multiscale atmospheric interactions. While large-scale atmospheric fields provide important environmental context, their ability to localize extreme rainfall events is fundamentally limited. In this study, we examine how large-scale atmospheric information from ERA5 reanalysis can be leveraged for event-level probabilistic prediction of daily rainfall extremes over India. We compare an adaptive ConvLSTM baseline with a proposed Physics-Gated Latent Ordinary Differential Equation (PG-LODE) framework, which models atmospheric evolution as a continuous-time latent process whose dynamics are explicitly modulated by a physics-based gating mechanism under convectively unstable conditions. Extreme events are defined using the local 95th percentile of the India Meteorological Department gridded rainfall dataset during the June to September monsoon season. Pixel-wise evaluation shows limited skill for both models due to spatial displacement errors, whereas event-level tile-based verification reveals a clear performance contrast. The ConvLSTM remains highly conservative, detecting only 27 percent of extreme events, while PG-LODE achieves near-complete detection with a substantially higher critical success index and a moderate false alarm rate. These results demonstrate that physics-gated continuous-time latent dynamics offer a robust pathway for translating large-scale atmospheric predictability into reliable assessments of extreme rainfall risk.", "AI": {"tldr": "Physics-gated continuous-time latent dynamics model (PG-LODE) outperforms ConvLSTM for probabilistic prediction of daily extreme rainfall over India, achieving near-complete detection with moderate false alarms.", "motivation": "Extreme rainfall over India is difficult to predict at daily scales due to stochastic convective triggering and multiscale interactions. Large-scale atmospheric fields provide environmental context but have limited ability to localize extreme events.", "method": "Proposed Physics-Gated Latent Ordinary Differential Equation (PG-LODE) framework models atmospheric evolution as continuous-time latent process with physics-based gating under convectively unstable conditions. Compared against adaptive ConvLSTM baseline using ERA5 reanalysis data. Extreme events defined as local 95th percentile of IMD rainfall data during monsoon season.", "result": "Pixel-wise evaluation shows limited skill for both models due to spatial displacement errors. Event-level tile-based verification reveals clear contrast: ConvLSTM detects only 27% of extreme events (highly conservative), while PG-LODE achieves near-complete detection with substantially higher critical success index and moderate false alarm rate.", "conclusion": "Physics-gated continuous-time latent dynamics offer robust pathway for translating large-scale atmospheric predictability into reliable assessments of extreme rainfall risk over India."}}
{"id": "2602.03458", "pdf": "https://arxiv.org/pdf/2602.03458", "abs": "https://arxiv.org/abs/2602.03458", "authors": ["Luc Revello", "Laurent Videau", "Fr\u00e9d\u00e9ric Zucchini", "Mathurin Lagr\u00e9e", "Christophe Blancard", "Benjamin Jodar"], "title": "Hydrodynamic simulations of expanded warm dense foil heated by pulsed-power", "categories": ["physics.plasm-ph", "hep-ex"], "comment": null, "summary": "Warm Dense Matter lies at the frontier between condensed matter and plasma, and plays a central role in various fields ranging from planetary science to inertial confinement fusion. Improving our understanding of this regime requires experimental data that can be directly compared with theoretical and numerical models over a broad range of conditions. In this work, a pulsed-power experiment is described in which thin metallic foils, confined within a sapphire cell, are Joule-heated to achieve the expanded warm dense matter regime. Designing such an experiment is challenging, as it requires simultaneously predicting the electrical response of the pulsed-power driver and the hydrodynamic evolution of the heated material. To tackle this challenge, a modeling framework has been developed that couples an electrical description of the pulsed-power system, including the driver, the switching stages and the load with a one-dimensional hydrodynamic code. This coupling allows the electrical energy deposition and the load thermodynamic evolution to be consistently linked through the material electrical conductivity. This approach takes advantage of the simplicity of a 1D geometry while retaining the essential physics and allowing to reproduce various measurements with good accuracy, such as expansion velocity, current and voltage. This numerical approach therefore constitutes a robust and efficient method for designing and optimizing future Warm Dense Matter experiments using pulsed-power facilities.", "AI": {"tldr": "A pulsed-power experiment for creating Warm Dense Matter by Joule-heating thin metallic foils in sapphire cells, with a coupled electrical-hydrodynamic modeling framework for design optimization.", "motivation": "Warm Dense Matter is important for planetary science and inertial confinement fusion, but requires experimental data that can be compared with theoretical models across broad conditions. Current experimental design is challenging due to the need to simultaneously predict electrical and hydrodynamic responses.", "method": "Developed a modeling framework that couples an electrical description of the pulsed-power system (driver, switching stages, load) with a one-dimensional hydrodynamic code. This links electrical energy deposition and thermodynamic evolution through material electrical conductivity, using 1D geometry while retaining essential physics.", "result": "The approach successfully reproduces various measurements with good accuracy, including expansion velocity, current, and voltage. It provides a robust method for designing pulsed-power Warm Dense Matter experiments.", "conclusion": "The coupled electrical-hydrodynamic modeling framework constitutes a robust and efficient method for designing and optimizing future Warm Dense Matter experiments using pulsed-power facilities, enabling better understanding of this frontier regime."}}
{"id": "2602.02507", "pdf": "https://arxiv.org/pdf/2602.02507", "abs": "https://arxiv.org/abs/2602.02507", "authors": ["Christian Y. Cardall", "Reuben D. Budiardja", "R. Daniel Murphy", "Eirik Endeve"], "title": "GenASiS: General Astrophysical Simulation System. II. Self-gravitating Baryonic Matter", "categories": ["astro-ph.HE", "physics.comp-ph"], "comment": "23 pages, 20 figures, to be submitted to Astrophysical Journal Supplement Series", "summary": "GenASiS (General Astrophysical Simulation System) is a code being developed initially and primarily, though not exclusively, for the simulation of core-collapse supernovae on the world's leading capability supercomputers. This paper -- the second in a series -- documents capabilities for Newtonian self-gravitating fluid dynamics, including tabulated microphysical equations of state treating nuclei and nuclear matter (`baryonic matter'). Computation of the gravitational potential of a spheroid, and simulation of the gravitational collapse of dust and of an ideal fluid, provide tests of self-gravitation against known solutions. In multidimensional computations of the adiabatic collapse, bounce, and explosion of spherically symmetric pre-supernova progenitors -- which we propose become a standard benchmark for code comparisons -- we find that the explosions are prompt and remain spherically symmetric (as expected), with an average shock expansion speed and total kinetic energy that are inversely correlated with the progenitor mass at the onset of collapse and the compactness parameter.", "AI": {"tldr": "GenASiS is a supercomputer code for astrophysical simulations, with this paper focusing on Newtonian self-gravitating fluid dynamics capabilities, including tests against known solutions and simulations of core-collapse supernovae.", "motivation": "To develop and document capabilities for simulating Newtonian self-gravitating fluid dynamics in astrophysical contexts, particularly for core-collapse supernovae simulations on leading supercomputers.", "method": "Implementation of tabulated microphysical equations of state for baryonic matter, computation of gravitational potentials for spheroids, and testing through simulations of gravitational collapse of dust and ideal fluids against known solutions.", "result": "Successful tests against known solutions, and multidimensional simulations showing prompt, spherically symmetric explosions in pre-supernova progenitors with inverse correlations between explosion properties and progenitor mass/compactness.", "conclusion": "The paper documents successful implementation of Newtonian self-gravitating fluid dynamics in GenASiS, proposes a standard benchmark for code comparisons, and demonstrates expected physical behaviors in supernova simulations."}}
{"id": "2602.02761", "pdf": "https://arxiv.org/pdf/2602.02761", "abs": "https://arxiv.org/abs/2602.02761", "authors": ["Hangsheng Chen"], "title": "Existence for Stable Rotating Star-Planet Systems", "categories": ["math.AP", "math-ph"], "comment": "53 pages, comments welcome", "summary": "This paper investigates the existence and properties of stable, uniformly rotating star-planet systems, i.e. mass ratio is sufficiently small. It is modeled by the Euler-Poisson equations. Following the framework established by McCann for binary stars \\cite{McC06}, we adopt a variational approach, and prove the existence of local energy minimizers with respect to the Wasserstein $L^\\infty$ metric, under the assumed equation of state $P(\u03c1)=K\u03c1^\u03b3$ and under the condition that the mass ratio $m$ is sufficiently small, corresponding to a star-planet system. Such minimizers correspond to solutions of the Euler-Poisson system. We consider two cases. For $\u03b3> 2$, we not only prove existence but also show, via scaling arguments, that the radii (to be precise, the bounds of the supports of the minimizers) tend to zero. For $\\frac{3}{2} < \u03b3\\leq 2$, we estimate an upper bound for the (potential) expansion rates of the radii, and it turns out that the existence result remains valid in this case as well. Finally, we provide estimates for the distances between different connected components of supports of minimizers and propose a conjecture regarding the number of connected components.", "AI": {"tldr": "Existence of stable rotating star-planet systems modeled by Euler-Poisson equations with small mass ratios, using variational methods and Wasserstein metrics.", "motivation": "To investigate the existence and properties of stable, uniformly rotating star-planet systems with small mass ratios, extending McCann's framework for binary stars to planetary systems.", "method": "Variational approach using Wasserstein L\u221e metric, proving existence of local energy minimizers for Euler-Poisson equations with polytropic equation of state P(\u03c1)=K\u03c1^\u03b3. Two cases analyzed: \u03b3>2 and 3/2<\u03b3\u22642.", "result": "For \u03b3>2, existence proved with radii tending to zero via scaling arguments. For 3/2<\u03b3\u22642, existence remains valid with estimated upper bounds for expansion rates of radii. Estimates provided for distances between connected components.", "conclusion": "Stable rotating star-planet systems exist for sufficiently small mass ratios under polytropic equations of state, with different scaling behaviors depending on \u03b3. Conjecture proposed regarding number of connected components."}}
{"id": "2602.03178", "pdf": "https://arxiv.org/pdf/2602.03178", "abs": "https://arxiv.org/abs/2602.03178", "authors": ["Davit Aslanyan", "Constantine Sideris"], "title": "Fully Automated Adaptive Parameter Selection for 3-D High-order Nystr\u00f6m Boundary Integral Equation Methods", "categories": ["math.NA", "physics.comp-ph"], "comment": null, "summary": "We present an adaptive Chebyshev-based Boundary Integral Equation (CBIE) solver for electromagnetic scattering from smooth perfect electric conductor (PEC) objects. The proposed approach eliminates manual parameter tuning by introducing (i) a unified adaptive quadrature strategy for automatic selection of the near-singular interaction distance and (ii) an adaptive computation of all self- and near-singular precomputation integrals to a prescribed accuracy using Gauss-Kronrod (h-adaptive) or Clenshaw-Curtis (p-adaptive) rules and singularity-resolving changes of variables. Both h-adaptive and p-adaptive schemes are explored within this framework, ensuring high-order accuracy and robustness across a broad range of geometries without loss of efficiency. Numerical results for canonical and complex CAD geometries demonstrate that the adaptive solver achieves accuracy and convergence rates comparable to optimally tuned fixed-grid CBIE implementations, while offering automation and scalability to electrically large, geometrically complex problems.", "AI": {"tldr": "Adaptive Chebyshev-based BIE solver for EM scattering from smooth PEC objects with automatic parameter tuning and adaptive quadrature strategies.", "motivation": "To eliminate manual parameter tuning in electromagnetic scattering simulations and provide automation for complex CAD geometries while maintaining accuracy and efficiency.", "method": "Uses adaptive Chebyshev-based BIE with unified adaptive quadrature strategy for near-singular interactions, plus adaptive computation of self/near-singular integrals using Gauss-Kronrod (h-adaptive) or Clenshaw-Curtis (p-adaptive) rules with singularity-resolving variable changes.", "result": "Achieves accuracy and convergence rates comparable to optimally tuned fixed-grid implementations while offering automation and scalability to electrically large, geometrically complex problems.", "conclusion": "The adaptive solver provides robust, high-order accuracy across diverse geometries without manual tuning, enabling automated simulation of complex electromagnetic scattering problems."}}
{"id": "2602.03494", "pdf": "https://arxiv.org/pdf/2602.03494", "abs": "https://arxiv.org/abs/2602.03494", "authors": ["Lucas Rovige", "Robert S. Dorst", "Ari Le", "Carmen G. Constantin", "Haiping Zhang", "David J. Larson", "Stephen Vincena", "Shreekrishna Tripathi", "Misa M. Cowee", "Derek B. Schaeffer", "Christoph Niemann"], "title": "Collisionless Larmor Coupling and Blob Formation in a Laser-Plasma Expanding into a Magnetized Ambient Plasma", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Collisionless Larmor coupling is a fundamental process in space and astrophysical plasmas that enables momentum transfer between an expanding plasma and a magnetized ambient medium. In this paper, we report on the laboratory experimental study of Larmor coupling leading to the formation of a plasma blob associated with a laser-driven, super-Alfv\u00e9nic plasma flow on the Large Plasma Device at the University of California, Los Angeles. The high-repetition rate enables systematic spatial and temporal scans of the plasma evolution using Doppler spectroscopy, as well as measurements of the magnetic field, electrostatic field, and self-emission of both debris and ambient ions using filtered imaging. We observe the self-focusing of the laser-produced plasma and the formation of a secondary diamagnetic cavity associated with a blob composed of background ions. Doppler spectroscopy reveals the transverse velocity distribution of the background ions, providing direct evidence of ion energization via Larmor coupling. The systematic spatial and temporal scans enabled by the high-repetition rate experiment allow for a detailed characterization of the ion dynamics. These experimental observations are supported by numerical simulations that provide more insight into the kinetic-scale physics associated with blob formation as well as the role of the ambient plasma density.", "AI": {"tldr": "Experimental study of Larmor coupling in laser-driven plasma flow shows formation of plasma blob and ion energization via Doppler spectroscopy measurements.", "motivation": "To study collisionless Larmor coupling, a fundamental process for momentum transfer between expanding plasma and magnetized ambient medium in space/astrophysical plasmas.", "method": "Laboratory experiment using laser-driven super-Alfv\u00e9nic plasma flow on UCLA's Large Plasma Device with high-repetition rate for systematic spatial/temporal scans using Doppler spectroscopy, magnetic/electrostatic field measurements, and filtered imaging.", "result": "Observed self-focusing of laser-produced plasma, formation of secondary diamagnetic cavity with blob of background ions, and direct evidence of ion energization via Larmor coupling through transverse velocity distribution measurements.", "conclusion": "Experimental observations supported by numerical simulations provide detailed characterization of ion dynamics and insight into kinetic-scale physics of blob formation and ambient plasma density effects."}}
{"id": "2602.02526", "pdf": "https://arxiv.org/pdf/2602.02526", "abs": "https://arxiv.org/abs/2602.02526", "authors": ["Pengyue Hou"], "title": "The \"Robert Boulton\" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.comp-ph"], "comment": "Companion paper to arXiv:2601.11594. Provides empirical validation of the MNCIS framework in Large Language Models (GPT-2) using a recursive training protocol (N=1500). Includes complete, reproducible Python implementation of Adaptive Spectral Negative Coupling (ASNC) and Effective Rank metrics in the Appendix", "summary": "The stability of generative artificial intelligence trained on recursive synthetic data is conventionally monitored via Perplexity (PPL). We demonstrate that PPL is a deceptive metric in context-stabilized regimes (L=128). Using a rigorous sliding-window protocol (N=1500), we identify a novel failure mode termed \"Semantic Tunneling.\" While the Baseline model maintains high grammatical fluency (PPL approx. 83.9), it suffers a catastrophic loss of semantic diversity, converging within seven generations to a single, low-entropy narrative attractor: the \"Robert Boulton\" Singularity. This phenomenon represents a total collapse of the latent manifold (Global Effective Rank 3.62 -> 2.22), where the model discards diverse world knowledge to optimize for statistically safe syntactic templates. To address this, we apply the Multi-Scale Negative Coupled Information Systems (MNCIS) framework recently established in Hou (2026) [arXiv:2601.11594]. We demonstrate that Adaptive Spectral Negative Coupling (ASNC) acts as a topological operator that actively induces \"Manifold Unfolding.\" MNCIS forces the model to expand its effective rank from the anisotropic baseline of 3.62 to a hyper-diverse state of 5.35, effectively constructing an \"Artificial Manifold\" that resists the gravitational pull of semantic attractors and preserves the long-tail distribution of the training data.", "AI": {"tldr": "PPL is unreliable for monitoring AI stability; models can maintain grammatical fluency while catastrophically losing semantic diversity, converging to single narrative attractors. MNCIS framework prevents this by inducing manifold unfolding.", "motivation": "Current AI stability monitoring relies on Perplexity (PPL), but this metric is deceptive in context-stabilized regimes where models can maintain grammatical fluency while suffering catastrophic semantic collapse.", "method": "Used sliding-window protocol (N=1500) to identify \"Semantic Tunneling\" failure mode. Applied Multi-Scale Negative Coupled Information Systems (MNCIS) framework with Adaptive Spectral Negative Coupling (ASNC) to induce \"Manifold Unfolding.\"", "result": "Baseline model converged to \"Robert Boulton\" Singularity within 7 generations despite high PPL (~83.9), with latent manifold collapse (Global Effective Rank 3.62\u21922.22). MNCIS expanded effective rank to 5.35, preventing semantic collapse while preserving training data diversity.", "conclusion": "PPL is insufficient for monitoring AI stability; semantic diversity collapse occurs despite good PPL scores. MNCIS framework successfully prevents semantic tunneling by inducing manifold unfolding, preserving long-tail distributions and resisting narrative attractors."}}
{"id": "2602.02818", "pdf": "https://arxiv.org/pdf/2602.02818", "abs": "https://arxiv.org/abs/2602.02818", "authors": ["Adrian Muntean", "Giulia Rui"], "title": "Lack of uniqueness for an elliptic equation with nonlinear and nonlocal drift posed on a torus", "categories": ["math.AP"], "comment": "11 pages", "summary": "We study a nonlinear and nonlocal elliptic equation posed on the flat torus. While constant solutions always exist, we show that uniqueness fails in general. Using spectral analysis and the Crandall--Rabinowitz bifurcation theorem, we prove the existence of branches of non-constant periodic solutions bifurcating from constant states. This result is qualitative and non-constructive. Using a conceptually different argument, we construct explicit multiple solutions for a specific one--dimensional formulation of our target problem.", "AI": {"tldr": "The paper studies a nonlinear nonlocal elliptic equation on a torus, showing constant solutions always exist but uniqueness fails. It proves existence of non-constant periodic solutions via bifurcation theory and constructs explicit multiple solutions in 1D.", "motivation": "To understand the solution structure of nonlinear nonlocal elliptic equations on periodic domains, particularly investigating when constant solutions are unique and when non-constant solutions emerge.", "method": "Uses spectral analysis and Crandall-Rabinowitz bifurcation theorem for qualitative existence results, plus a separate constructive approach for explicit solutions in one-dimensional case.", "result": "Constant solutions always exist but are not generally unique. Branches of non-constant periodic solutions bifurcate from constant states. Explicit multiple solutions constructed for specific 1D formulation.", "conclusion": "Nonlinear nonlocal elliptic equations on tori exhibit rich solution structures with both constant and non-constant periodic solutions, demonstrating failure of uniqueness and existence of bifurcating branches."}}
{"id": "2602.03239", "pdf": "https://arxiv.org/pdf/2602.03239", "abs": "https://arxiv.org/abs/2602.03239", "authors": ["Wenli Wang", "Duo Liu", "Gangrong Qu", "Michiel E. Hochstenbach"], "title": "Deterministic and randomized Kaczmarz methods for $AXB=C$ with applications to color image restoration", "categories": ["math.NA"], "comment": null, "summary": "We study Kaczmarz type methods to solve consistent linear matrix equations. We first present a block Kaczmarz (BK) method that employs a deterministic cyclic row selection strategy. Assuming that the associated coefficient matrix has full column or row rank, we derive matrix formulas for a cycle of this BK method. Moreover, we propose a greedy randomized block Kaczmarz (GRBK) method and further extend it to a relaxed variant (RGRBK) and a deterministic counterpart (MWRBK). We establish the convergence properties of the proposed methods. Numerical tests verify the theoretical findings, and we apply the proposed methods to color image restoration problems.", "AI": {"tldr": "The paper proposes several block Kaczmarz methods for solving consistent linear matrix equations, including deterministic cyclic, greedy randomized, and relaxed variants, with convergence analysis and applications to color image restoration.", "motivation": "To develop efficient iterative methods for solving consistent linear matrix equations, extending Kaczmarz-type approaches to handle matrix equations with block strategies and various selection schemes.", "method": "1) Block Kaczmarz (BK) with deterministic cyclic row selection; 2) Greedy Randomized Block Kaczmarz (GRBK); 3) Relaxed variant (RGRBK); 4) Deterministic counterpart (MWRBK). All methods include matrix formulas and convergence analysis.", "result": "Established convergence properties for all proposed methods. Numerical tests verify theoretical findings and demonstrate effectiveness in color image restoration applications.", "conclusion": "The proposed block Kaczmarz methods provide effective solutions for consistent linear matrix equations with proven convergence, offering practical tools for applications like image restoration."}}
{"id": "2602.03583", "pdf": "https://arxiv.org/pdf/2602.03583", "abs": "https://arxiv.org/abs/2602.03583", "authors": ["Stanislav Musikhin", "Anatoli Morozov", "Alec Griffith", "Shurik Yatom", "Ahmed Diallo"], "title": "Multi-Diagnostic Characterization of Laser-Produced Tin Plasmas for EUV Lithography", "categories": ["physics.plasm-ph"], "comment": null, "summary": "We present a comprehensive characterization of laser-produced tin (Sn) plasmas relevant to extreme ultraviolet (EUV) lithography using a multi-diagnostic suite integrated into the new experimental platform, \"SparkLight\". Tin plasmas are generated by irradiating a continuously moving tin-coated wire with laser pulses (1064 nm, 10 ns, up to $5.7\\times10^{10}$ W/cm$^2$) and probed via coherent Thomson scattering, laser interferometry, and EUV emission spectroscopy. Thomson scattering measurements reveal electron temperatures and densities that decay with distance from the target. Densities derived from Thomson scattering are cross-validated against laser interferometry, showing excellent agreement. Correlating the results of these laser diagnostics with spatially resolved EUV spectroscopy suggests that the bulk of useful EUV emission originates within 150 $\u03bc$m of the target and is generated under suboptimal plasma conditions. This work demonstrates a practical integrated approach for plasma characterization in EUV source development.", "AI": {"tldr": "Multi-diagnostic characterization of laser-produced tin plasmas for EUV lithography using integrated platform SparkLight, revealing EUV emission originates within 150 \u03bcm of target under suboptimal conditions.", "motivation": "Tin plasmas are crucial for extreme ultraviolet (EUV) lithography light sources, but comprehensive characterization of plasma conditions and EUV emission mechanisms is needed for source optimization and development.", "method": "Integrated multi-diagnostic approach using SparkLight platform: laser irradiation of moving tin-coated wire (1064 nm, 10 ns pulses), with coherent Thomson scattering for electron temperature/density, laser interferometry for density validation, and EUV emission spectroscopy for spatial analysis.", "result": "Electron temperatures and densities decay with distance from target; Thomson scattering and interferometry show excellent agreement; bulk of useful EUV emission originates within 150 \u03bcm of target and occurs under suboptimal plasma conditions.", "conclusion": "Demonstrates practical integrated approach for plasma characterization in EUV source development, revealing that optimal EUV emission occurs in specific spatial regions with non-ideal plasma conditions, providing insights for source optimization."}}
{"id": "2602.02788", "pdf": "https://arxiv.org/pdf/2602.02788", "abs": "https://arxiv.org/abs/2602.02788", "authors": ["Benjamin D. Shaffer", "Shawn Koohy", "Brooks Kinch", "M. Ani Hsieh", "Nathaniel Trask"], "title": "Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "We aim to develop physics foundation models for science and engineering that provide real-time solutions to Partial Differential Equations (PDEs) which preserve structure and accuracy under adaptation to unseen geometries. To this end, we introduce General-Geometry Neural Whitney Forms (Geo-NeW): a data-driven finite element method. We jointly learn a differential operator and compatible reduced finite element spaces defined on the underlying geometry. The resulting model is solved to generate predictions, while exactly preserving physical conservation laws through Finite Element Exterior Calculus. Geometry enters the model as a discretized mesh both through a transformer-based encoding and as the basis for the learned finite element spaces. This explicitly connects the underlying geometry and imposed boundary conditions to the solution, providing a powerful inductive bias for learning neural PDEs, which we demonstrate improves generalization to unseen domains. We provide a novel parameterization of the constitutive model ensuring the existence and uniqueness of the solution. Our approach demonstrates state-of-the-art performance on several steady-state PDE benchmarks, and provides a significant improvement over conventional baselines on out-of-distribution geometries.", "AI": {"tldr": "Geo-NeW: A data-driven finite element method that learns differential operators and compatible finite element spaces to solve PDEs on unseen geometries while preserving physical conservation laws.", "motivation": "Develop physics foundation models that provide real-time PDE solutions while preserving structure and accuracy when adapting to unseen geometries, overcoming limitations of conventional methods on out-of-distribution domains.", "method": "Jointly learn a differential operator and compatible reduced finite element spaces using Finite Element Exterior Calculus. Geometry enters via discretized mesh through transformer-based encoding and as basis for learned finite element spaces. Novel parameterization ensures solution existence and uniqueness.", "result": "State-of-the-art performance on several steady-state PDE benchmarks and significant improvement over conventional baselines on out-of-distribution geometries.", "conclusion": "Geo-NeW provides a powerful inductive bias for learning neural PDEs by explicitly connecting geometry and boundary conditions to solutions, enabling better generalization to unseen domains while preserving physical conservation laws."}}
{"id": "2602.02998", "pdf": "https://arxiv.org/pdf/2602.02998", "abs": "https://arxiv.org/abs/2602.02998", "authors": ["Bochao Chen", "Yixian Gao", "Hongyu Liu"], "title": "Symmetrization of the Maxwell--Neumann--Poincar'e operator, spectral decomposition in $\\mathbf{H}(\\mathrm{curl},D)$ traces, and boundary localisation of SPRs", "categories": ["math.AP"], "comment": "34pages", "summary": "The Neumann--Poincar\u00e9 (NP) operator, a fundamental operator in potential theory, has attracted renewed attention for its central role in the analysis of surface plasmon resonances (SPRs). SPRs, characterized by non-radiative electromagnetic waves at material interfaces with opposing permittivities, underpin advanced technologies such as bio-sensing and cloaking devices. While spectral properties of the scalar NP operator and SPR dynamics for scalar waves are well-established, their vectorial counterparts in Maxwell's framework remain poorly understood. This work bridges this gap by introducing a novel symmetrization principle for the matrix-valued Maxwell Neumann--Poincar\u00e9 (MNP) operator, enabling a spectral decomposition of traces in the $\\mathbf{H}(\\mathrm{curl},D)$ space--a foundational advance for electromagnetic theory. Building on this framework, we rigorously characterize the quantum-ergodic localization of weak surface plasmon resonances at material boundaries in the full Maxwell system, thereby settling a long-standing question concerning their quantitative description.", "AI": {"tldr": "The paper introduces a novel symmetrization principle for the Maxwell Neumann-Poincar\u00e9 operator, enabling spectral decomposition and rigorous characterization of weak surface plasmon resonances in the full Maxwell system.", "motivation": "While spectral properties of scalar NP operators and SPR dynamics for scalar waves are well-established, their vectorial counterparts in Maxwell's framework remain poorly understood. There's a need to bridge this gap for electromagnetic theory and provide quantitative description of surface plasmon resonances.", "method": "Introduces a novel symmetrization principle for the matrix-valued Maxwell Neumann-Poincar\u00e9 (MNP) operator, enabling spectral decomposition of traces in the \ud835\udc07(curl,D) space. Builds on this framework to characterize quantum-ergodic localization of weak surface plasmon resonances.", "result": "Enables spectral decomposition of traces in \ud835\udc07(curl,D) space - a foundational advance for electromagnetic theory. Rigorously characterizes quantum-ergodic localization of weak surface plasmon resonances at material boundaries in the full Maxwell system.", "conclusion": "Settles a long-standing question concerning the quantitative description of surface plasmon resonances by providing a rigorous framework for analyzing their spectral properties in the full Maxwell system through symmetrization of the MNP operator."}}
{"id": "2602.03247", "pdf": "https://arxiv.org/pdf/2602.03247", "abs": "https://arxiv.org/abs/2602.03247", "authors": ["Qianxing Jia", "Dong Wang"], "title": "Physics informed learning of orthogonal features with applications in solving partial differential equations", "categories": ["math.NA"], "comment": null, "summary": "The random feature method (RFM) constructs approximation spaces by initializing features from generic distributions, which provides universal approximation properties to solve general partial differential equations. However, such standard initializations lack awareness of the underlying physical laws and geometry, which limits approximation. In this work, we propose the Physics-Driven Orthogonal Feature Method (PD-OFM), a framework for constructing feature representations that are explicitly tailored to both the differential operator and the computational domain by pretraining features using physics-informed objectives together with orthogonality regularization. This pretraining strategy yields nearly orthogonal feature bases. We provide both theoretical and empirical evidence that physics-informed pretraining improves the approximation capability of the learned feature space. When employed to solve Helmholtz, Poisson, wave, and Navier-Stokes equations, the proposed method achieves residual errors 2-3 orders of magnitude lower than those of comparable methods. Furthermore, the orthogonality regularization improves transferability, enabling pretrained features to generalize effectively across different source terms and domain geometries for the same PDE.", "AI": {"tldr": "PD-OFM: Physics-driven orthogonal feature method that pretrains neural features with physics-informed objectives and orthogonality regularization to solve PDEs with 2-3 orders lower residual errors than comparable methods.", "motivation": "Standard random feature methods lack awareness of underlying physical laws and geometry, limiting their approximation capability for solving PDEs. There's a need for feature representations explicitly tailored to both differential operators and computational domains.", "method": "Physics-Driven Orthogonal Feature Method (PD-OFM) pretrains features using physics-informed objectives with orthogonality regularization, creating nearly orthogonal feature bases that are tailored to specific PDE operators and domains.", "result": "Achieves residual errors 2-3 orders of magnitude lower than comparable methods on Helmholtz, Poisson, wave, and Navier-Stokes equations. Orthogonality regularization improves transferability across different source terms and domain geometries.", "conclusion": "Physics-informed pretraining with orthogonality regularization significantly enhances feature space approximation capability and transferability, providing a superior framework for solving PDEs compared to standard random feature methods."}}
{"id": "2602.03754", "pdf": "https://arxiv.org/pdf/2602.03754", "abs": "https://arxiv.org/abs/2602.03754", "authors": ["G. Parise", "A. Cianchi", "M. Galletti", "F. Guglietta", "R. Pompili", "A. R. Rossi", "M. Sbragaglia", "D. Simeoni"], "title": "A numerical study on plasma acceleration processes with ion dynamics at the sub-nanosecond timescale", "categories": ["physics.plasm-ph", "physics.acc-ph"], "comment": null, "summary": "Plasma wakefield acceleration is a groundbreaking technique for accelerating particles, capable of sustaining gigavolt-per-meter accelerating fields. Understanding the physical mechanisms governing the recovery of plasma accelerating properties over time is essential for successfully achieving high-repetition-rate plasma acceleration, a key requirement for applicability in both research and commercial settings. In this paper, we present numerical simulations of the early-stage plasma evolution based on the parameters of the SPARC_LAB hydrogen plasma recovery time experiment (Pompili et al., Comm. Phys. 7, 241 (2024)), employing spatially resolved Particle-in-Cell and fluid models. The experiment reports on a non-monotonic dependence of the plasma recovery time on the initial plasma density, an effect for which ion motion has been invoked as a contributing factor. The simulations presented here provide further insight into the role of ion dynamics in shaping this behavior. Furthermore, comparing Particle-in-Cell and fluid approaches allows us to assess the quality of fluid models for describing this class of plasma dynamics.", "AI": {"tldr": "Numerical simulations of plasma recovery time show non-monotonic density dependence influenced by ion dynamics, comparing Particle-in-Cell and fluid models.", "motivation": "Understanding plasma recovery mechanisms is crucial for achieving high-repetition-rate plasma wakefield acceleration, which is essential for practical applications in research and commercial settings.", "method": "Numerical simulations using spatially resolved Particle-in-Cell and fluid models based on SPARC_LAB hydrogen plasma recovery time experiment parameters.", "result": "Simulations reveal non-monotonic dependence of plasma recovery time on initial density, with ion dynamics playing a key role in shaping this behavior. Comparison shows fluid models' quality for describing such plasma dynamics.", "conclusion": "Ion dynamics significantly influence plasma recovery time behavior, and fluid models can provide valuable insights for this class of plasma dynamics, though Particle-in-Cell simulations offer more detailed understanding."}}
{"id": "2602.03142", "pdf": "https://arxiv.org/pdf/2602.03142", "abs": "https://arxiv.org/abs/2602.03142", "authors": ["Bahrem Serhat Danis", "Demet Baldan Desdemir", "Enes Akcakoca", "Zeynep Ipek Yanmaz", "Gulzade Polat", "Ahmet Onur Dasdemir", "Aytug Aydogan", "Abdullah Magden", "Emir Salih Magden"], "title": "Intrinsically DRC-Compliant Nanophotonic Design via Learned Generative Manifolds", "categories": ["physics.optics", "physics.app-ph", "physics.comp-ph"], "comment": null, "summary": "Inverse design has enabled the systematic design of ultra-compact and high-performance nanophotonic components. Yet enforcing foundry design rules during inverse design remains a major challenge, as optimized devices frequently violate constraints on minimum feature size and spacing. Existing fabrication-constrained approaches typically rely on penalty terms, projection filters, or heuristic binarization schedules, which restrict the accessible design space, require extensive hyperparameter tuning, and often fail to guarantee compliance throughout the optimization trajectory. Here, we introduce a framework for nanophotonic inverse design with intrinsic enforcement of design rules through a generative reparameterization of the design space, restricting optimization to a learned manifold of DRC-compliant geometries. We validate this paradigm by designing representative silicon photonic components including broadband power splitters, spectral duplexers, and mode converters operating across the 1,500-1,600 nm band for both electron-beam lithography and photolithography platforms. Across all devices, the manifold-based formulation reaches state-of-the-art performance metrics with over a 5-fold reduction in computational cost compared to pixel-based representations, while ensuring fabrication-compatible geometries throughout the entire design process. By treating fabrication constraints as a fundamental property of the design representation rather than an external penalty, this work establishes a direct pathway toward broadly applicable, platform-agnostic, and intrinsically DRC-compliant nanophotonics.", "AI": {"tldr": "A framework for nanophotonic inverse design that intrinsically enforces design rules through generative reparameterization, restricting optimization to a learned manifold of DRC-compliant geometries, achieving state-of-the-art performance with 5x lower computational cost.", "motivation": "Existing inverse design approaches struggle with enforcing foundry design rules (minimum feature size/spacing), often using penalty terms or heuristic methods that restrict design space, require extensive tuning, and may fail to guarantee compliance throughout optimization.", "method": "Introduces a generative reparameterization framework that restricts optimization to a learned manifold of DRC-compliant geometries, treating fabrication constraints as intrinsic to the design representation rather than external penalties.", "result": "Validated by designing silicon photonic components (broadband power splitters, spectral duplexers, mode converters) for 1500-1600nm band across e-beam and photolithography platforms. Achieved state-of-the-art performance with over 5x reduction in computational cost compared to pixel-based methods while ensuring fabrication compatibility throughout design.", "conclusion": "Establishes a pathway toward broadly applicable, platform-agnostic, and intrinsically DRC-compliant nanophotonics by treating fabrication constraints as fundamental to the design representation rather than external penalties."}}
{"id": "2602.03044", "pdf": "https://arxiv.org/pdf/2602.03044", "abs": "https://arxiv.org/abs/2602.03044", "authors": ["Yoshiki Kaiho"], "title": "On very weak solutions of certain elliptic systems with double phase growth", "categories": ["math.AP"], "comment": "74 pages", "summary": "In this paper, we prove a higher integrability result for very weak solutions of higher-order elliptic systems involving a double phase operator as the principal part. As a model case, we consider \\begin{equation} \\int_\u03a9 \\left( |D^m u|^{p-2}D^m u + a(x)|D^m u|^{q-2}D^m u \\right) \\cdot D^m \\varphi = 0 \\quad \\text{for any } \\varphi \\in C_c^{\\infty}(\u03a9), \\end{equation} where $n,m \\in \\mathbb{N},\\ n\\ge 2,\\,1 < p \\le q < \\infty,\\,\u03a9\\subset \\mathbb{R}^n$ is an open set and $a:\u03a9\\rightarrow [0,\\infty)$ is a measurable function. The proof is based on a construction of an appropriate test function by the Lipschitz truncation technique, a deduction of a reverse H\u00f6lder inequality and an application of Gehring's lemma. Our contributions include estimates for weighted mean value polynomials and sharp Sobolev--Poincar\u00e9-type inequalities for the double phase operator. Our result can be viewed as a generalization with respect to the derivative order, the coefficient function and the growth conditions of the recent paper by Baasandorj, Byun and Kim (Trans. Amer. Math. Soc. 376:8733-8768,2023).", "AI": {"tldr": "Higher integrability result for very weak solutions of higher-order elliptic systems with double phase operator, generalizing previous work on derivative order, coefficient function, and growth conditions.", "motivation": "To extend higher integrability results to very weak solutions of higher-order elliptic systems involving double phase operators, which combine p-growth and q-growth terms with a coefficient function a(x). This generalizes previous work that was limited to lower-order derivatives and simpler growth conditions.", "method": "Uses Lipschitz truncation technique to construct appropriate test functions, deduces a reverse H\u00f6lder inequality, and applies Gehring's lemma. Also develops estimates for weighted mean value polynomials and sharp Sobolev-Poincar\u00e9-type inequalities for the double phase operator.", "result": "Proves higher integrability for very weak solutions of higher-order elliptic systems with double phase operator. The result generalizes previous work by Baasandorj, Byun and Kim with respect to derivative order, coefficient function, and growth conditions.", "conclusion": "The paper successfully establishes higher integrability for very weak solutions in the context of higher-order elliptic systems with double phase operators, providing a significant generalization of existing results through novel analytical techniques."}}
{"id": "2602.03322", "pdf": "https://arxiv.org/pdf/2602.03322", "abs": "https://arxiv.org/abs/2602.03322", "authors": ["Yanyan Shi", "Christian Lubich"], "title": "Weighted finite difference methods for a nonlinear Klein--Gordon equation with high oscillations in space and time", "categories": ["math.NA"], "comment": null, "summary": "We consider a nonlinear Klein--Gordon equation in the nonrelativistic limit regime with initial data in the form of a modulated highly oscillatory exponential. In this regime of a small scaling parameter $\\varepsilon$, the solution exhibits rapid oscillations in both time and space, posing challenges for numerical approximation. We propose an explicit and an implicit exponentially weighted finite difference method. While the explicit weighted leapfrog method needs to satisfy a CFL-type stability condition, the implicit weighted Crank--Nicolson method is unconditionally stable. Both methods achieve second-order accuracy with time steps and mesh sizes that are not restricted in magnitude by $\\varepsilon$. The methods are uniformly convergent in the range from arbitrarily small to moderately bounded $\\varepsilon$. Numerical experiments illustrate the theoretical results.", "AI": {"tldr": "Explicit and implicit exponentially weighted finite difference methods for nonlinear Klein-Gordon equation in nonrelativistic limit regime with oscillatory initial data, achieving second-order accuracy independent of small scaling parameter \u03b5.", "motivation": "The nonlinear Klein-Gordon equation in nonrelativistic limit regime with highly oscillatory initial data presents numerical challenges due to rapid oscillations in both time and space, especially when the scaling parameter \u03b5 is small, making traditional methods inefficient.", "method": "Proposed explicit exponentially weighted leapfrog method (requires CFL stability condition) and implicit exponentially weighted Crank-Nicolson method (unconditionally stable). Both use exponential weighting to handle oscillations and achieve second-order accuracy.", "result": "Both methods achieve second-order accuracy with time steps and mesh sizes not restricted by \u03b5. Methods are uniformly convergent for arbitrarily small to moderately bounded \u03b5. Numerical experiments confirm theoretical results.", "conclusion": "The proposed exponentially weighted finite difference methods effectively handle the numerical challenges of the nonlinear Klein-Gordon equation in nonrelativistic limit regime with oscillatory data, providing efficient and accurate solutions independent of the small scaling parameter \u03b5."}}
{"id": "2602.03759", "pdf": "https://arxiv.org/pdf/2602.03759", "abs": "https://arxiv.org/abs/2602.03759", "authors": ["Zhixin Lu", "Guo Meng", "Eric Sonnendruecker", "Roman Hatzky", "Giorgio Daneri", "Gengxian Li", "Peiyou Jiang", "Klaus Reuter", "Matthias Hoelzl"], "title": "A High-order piecewise field-aligned triangular finite element method for electromagnetic gyrokinetic particle simulations of tokamak plasmas with open field lines", "categories": ["physics.plasm-ph"], "comment": "16 pages, 8 figures", "summary": "A high-order piecewise field-aligned triangular finite element method is developed and implemented for global electromagnetic gyrokinetic particle-in-cell simulations of tokamak plasmas with open field lines. The approach combines locally field-aligned finite element basis functions with unstructured $C^{1}$ triangular meshes in cylindrical coordinates, enabling whole-volume simulations with substantially reduced computational effort, while avoiding the grid distortion associated with globally field-aligned coordinates and the associated singularity at the separatrix of diverted plasmas. The formulation is compatible with both $\u03b4f$ and full-$f$ models and employs mixed-variable representations, along with a generalized pullback scheme, to control numerical cancellation in electromagnetic simulations. The method is implemented in the TRIMEG-C1 code and demonstrated using linear and nonlinear electromagnetic simulations of the TCV-X21 configuration. The results indicate that the approach accurately captures the key features of electromagnetic ion-temperature-gradient and kinetic ballooning mode physics, including the separatrix regions in the simulation, thereby providing a robust framework for whole-volume electromagnetic gyrokinetic simulations in realistic tokamak geometries.", "AI": {"tldr": "A high-order field-aligned triangular FEM for global electromagnetic gyrokinetic PIC simulations in tokamaks with open field lines, implemented in TRIMEG-C1 code and validated on TCV-X21 configuration.", "motivation": "To enable whole-volume electromagnetic gyrokinetic simulations of tokamak plasmas with open field lines while avoiding grid distortion and separatrix singularities associated with globally field-aligned coordinates, and to reduce computational effort.", "method": "Combines locally field-aligned finite element basis functions with unstructured C\u00b9 triangular meshes in cylindrical coordinates. Uses mixed-variable representations and generalized pullback scheme to control numerical cancellation. Compatible with both \u03b4f and full-f models, implemented in TRIMEG-C1 code.", "result": "The method accurately captures key features of electromagnetic ion-temperature-gradient and kinetic ballooning mode physics, including separatrix regions, in linear and nonlinear simulations of TCV-X21 configuration.", "conclusion": "Provides a robust framework for whole-volume electromagnetic gyrokinetic simulations in realistic tokamak geometries with reduced computational effort and avoidance of coordinate singularities."}}
{"id": "2602.03063", "pdf": "https://arxiv.org/pdf/2602.03063", "abs": "https://arxiv.org/abs/2602.03063", "authors": ["Matthew Dominique Mitchell"], "title": "The Small Dispersion Limit of the Intermediate Long Wave Equation via Semiclassical Soliton Ensembles", "categories": ["math.AP", "math-ph"], "comment": "56 pages, 8 figures", "summary": "We study the small dispersion limit of the intermediate long wave (ILW) equation, specifically on a class of well-behaved initial conditions $u_0$ where the number of solitons in the solution increases without bound. First, we conduct a formal WKB-style analysis on the ILW direct scattering problem, generating approximate eigenvalues and norming constants. We then use this to define a modified set of scattering data and rigorously analyze the associated inverse scattering problem. The main results include demonstrating $L^2$-convergence of the solution at $t = 0$ to the original initial condition $u_0$ and for $0 < t < t_\\mathrm{c}$ to the associated solution of invicid Burgers' equation, where $t_\\mathrm{c}$ is the time of gradient catastrophe.", "AI": {"tldr": "The paper analyzes the small dispersion limit of the Intermediate Long Wave (ILW) equation, showing convergence to inviscid Burgers' equation before gradient catastrophe.", "motivation": "To understand the behavior of the ILW equation in the small dispersion limit, particularly when the number of solitons becomes unbounded, and to establish connections with simpler hydrodynamic models like inviscid Burgers' equation.", "method": "1) Formal WKB-style analysis on the ILW direct scattering problem to generate approximate eigenvalues and norming constants. 2) Definition of modified scattering data. 3) Rigorous analysis of the associated inverse scattering problem.", "result": "1) Demonstrated L\u00b2-convergence of the solution at t=0 to the original initial condition u\u2080. 2) Showed convergence for 0 < t < t_c to the solution of inviscid Burgers' equation, where t_c is the time of gradient catastrophe.", "conclusion": "The ILW equation in the small dispersion limit behaves like inviscid Burgers' equation up to the gradient catastrophe time, establishing a rigorous connection between integrable PDEs and hydrodynamic models in the zero-dispersion limit."}}
{"id": "2602.03348", "pdf": "https://arxiv.org/pdf/2602.03348", "abs": "https://arxiv.org/abs/2602.03348", "authors": ["Shaoshuai Chu", "Michael Herty"], "title": "A Comparative Study of Low-Dissipation Numerical Schemes for Hyperbolic Conservation Laws", "categories": ["math.NA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2504.01699", "summary": "This work provides a comparative assessment of several low-dissipation numerical schemes for hyperbolic conservation laws, highlighting their performance relative to the classical Harten-Lax-van Leer (HLL) schemes. The schemes under consideration include the classical Harten-Lax-van Leer-Contact (HLLC), the recently proposed TV flux splitting, the low-dissipation Central-Upwind (LDCU), and the local characteristic decomposition-based Central-Upwind (LCDCU) schemes. These methods are extended to higher orders of accuracy, up to the fifth order, within both finite-volume and finite-difference frameworks. A series of numerical experiments for the one- and two-dimensional Euler equations of gas dynamics are performed to evaluate the accuracy, robustness, and computational efficiency of the studied schemes. The comparison highlights the trade-offs between resolution of contact and shear waves, robustness in the presence of shocks, and computational cost. The investigated low-dissipation schemes show comparable levels of numerical dissipation, with only subtle differences appearing in selected benchmark problems. The results provide practical guidance for selecting efficient low-dissipation solvers for the simulation of complex compressible flows.", "AI": {"tldr": "Comparative analysis of low-dissipation numerical schemes for hyperbolic conservation laws, comparing HLLC, TV flux splitting, LDCU, and LCDCU schemes up to 5th order accuracy, with practical guidance for compressible flow simulations.", "motivation": "To provide a comprehensive comparison of modern low-dissipation numerical schemes for hyperbolic conservation laws, evaluating their performance relative to classical HLL schemes and offering practical guidance for selecting efficient solvers for complex compressible flow simulations.", "method": "Extended HLLC, TV flux splitting, LDCU, and LCDCU schemes to higher orders (up to 5th order) within both finite-volume and finite-difference frameworks. Conducted numerical experiments for 1D and 2D Euler equations to evaluate accuracy, robustness, and computational efficiency.", "result": "Low-dissipation schemes show comparable numerical dissipation levels with only subtle differences in selected benchmarks. The study reveals trade-offs between contact/shear wave resolution, shock robustness, and computational cost.", "conclusion": "Provides practical guidance for selecting efficient low-dissipation solvers for complex compressible flow simulations, highlighting that while schemes show similar dissipation characteristics, subtle differences emerge in specific benchmark problems."}}
{"id": "2602.03621", "pdf": "https://arxiv.org/pdf/2602.03621", "abs": "https://arxiv.org/abs/2602.03621", "authors": ["J. C. Dolence", "H. R. Hammer", "H. Park", "B. Prather", "B. R. Ryan", "R. T. Wollaeger"], "title": "A Method for Thermal Radiation Transport Using Backward Characteristic Tracing", "categories": ["astro-ph.IM", "physics.comp-ph"], "comment": "Submitted to Journal of Computational Physics", "summary": "Thermal radiation transport is a challenging problem in computational physics that has long been approached primarily in one of a few standard ways: approximate moment methods (for instance P$_1$ or M$_1$), implicit Monte Carlo, discrete ordinates, and long characteristics. In this work we consider the efficacy of the Method of (Long) Characteristics (MOC) applied to thermal radiation transport. Along the way we develop three major ideas: transporting MOC particles backwards in time from quadrature grids at the end of the timestep, limiting the computational cost of these backward characteristics by terminating transport once optical depths along rays become sufficiently large, and timestep-dependent closures with multigroup MOC solutions for a gray low-order system. We apply this method to a suite of standard radiation transport and radiation hydrodynamics test problems. We compare the method to several standard analytic and semi-analytic solutions, as well as implicit Monte Carlo, P$_1$, and discrete ordinates (S$_n$). We see that the method: gives excellent agreement with known results, has stability for large time steps, has the diffusion limit for large spatial cells, and achieves $\\sim$20-70\\% performance improvement when terminating optical depths at O(10-100) in the grey Marshak and crooked pipe problems. However, for the Coax radiation-hydrodynamics problem, we see that MOC is approximately two to three times slower than IMC-DDMC and S$_n$ in its current implementation.", "AI": {"tldr": "The paper evaluates the Method of Characteristics (MOC) for thermal radiation transport, introducing backward transport, optical depth termination, and timestep-dependent closures, showing improved performance on some problems but slower on others compared to existing methods.", "motivation": "Thermal radiation transport is computationally challenging, traditionally solved with approximate moment methods, implicit Monte Carlo, discrete ordinates, or long characteristics. The authors aim to assess the efficacy of the Method of Characteristics (MOC) as an alternative approach to improve computational efficiency and accuracy.", "method": "Three key innovations: 1) Transporting MOC particles backwards in time from quadrature grids at timestep end, 2) Limiting computational cost by terminating transport once optical depths along rays become sufficiently large, 3) Timestep-dependent closures with multigroup MOC solutions for a gray low-order system. Applied to standard radiation transport and radiation hydrodynamics test problems.", "result": "The method shows excellent agreement with known results, stability for large time steps, diffusion limit for large spatial cells, and achieves ~20-70% performance improvement when terminating optical depths at O(10-100) in grey Marshak and crooked pipe problems. However, for the Coax radiation-hydrodynamics problem, MOC is approximately two to three times slower than IMC-DDMC and S_n in its current implementation.", "conclusion": "The Method of Characteristics shows promise for thermal radiation transport with significant performance improvements on certain problems through backward transport and optical depth termination techniques, though further optimization is needed for radiation-hydrodynamics applications where it currently lags behind established methods."}}
{"id": "2602.03131", "pdf": "https://arxiv.org/pdf/2602.03131", "abs": "https://arxiv.org/abs/2602.03131", "authors": ["Renjun Duan", "Fengqiang Shi", "Wendong Wang", "Jianbo Yu"], "title": "Existence and partial regularity of suitable weak solutions to the 3D Navier-Stokes-Vlasov-Fokker-Planck equations", "categories": ["math.AP"], "comment": null, "summary": "In this paper, we investigate the incompressible Navier-Stokes equations coupled with the Vlasov-Fokker-Planck equation, which describes a two-phase mixture of the viscous incompressible fluid with particles or bubbles through a frictional force term. In the three-dimensional whole space, we construct a new class of suitable weak solutions to the Navier-Stokes-Vlasov-Fokker-Planck system satisfying energy estimates and three local or global energy inequalities of different forms. These obtained local energy inequalities play an important role in characterizing the measure of the singularity set of weak solutions. The main difficulties in deriving these inequalities lie in establishing the convergence of the density function $f$ in bounded or unbounded domains and dealing with the convergence of the non-local frictional force term. The strong convergence of both $f$ and $f \\log f$ weighted by $|v|^k$ is proved by exploring some new a priori quantities of the velocity with the help of Tao's $L^p$ decomposition and the DiPerna-Lions compactness method. Moreover, as an immediate consequence of the existence result, we are able to describe the Hausdorff dimension of set of singular points of the fluid velocity $u$ and also establish the $\u03b1$-H\u00f6lder continuity of $f$ at the regular points of $u$.", "AI": {"tldr": "The paper constructs suitable weak solutions for a coupled Navier-Stokes-Vlasov-Fokker-Planck system in 3D, establishing local energy inequalities that help characterize singularity sets of weak solutions.", "motivation": "To study two-phase mixtures of viscous incompressible fluid with particles/bubbles, and to develop suitable weak solutions with energy estimates that can characterize singularity sets in such coupled systems.", "method": "Constructs suitable weak solutions satisfying energy estimates and three local/global energy inequalities. Uses Tao's L^p decomposition and DiPerna-Lions compactness method to prove strong convergence of density functions f and f log f weighted by |v|^k.", "result": "Established existence of suitable weak solutions with energy inequalities, proved strong convergence of f and f log f, characterized Hausdorff dimension of singular points of fluid velocity u, and established \u03b1-H\u00f6lder continuity of f at regular points of u.", "conclusion": "The paper successfully constructs suitable weak solutions for the coupled system and provides tools to analyze singularity sets, with applications to describing singular point dimensions and regularity properties of the density function."}}
{"id": "2602.03428", "pdf": "https://arxiv.org/pdf/2602.03428", "abs": "https://arxiv.org/abs/2602.03428", "authors": ["T. Chaumont-Frelet", "S. Sauter"], "title": "On singular Galerkin discretizations for three models in high-frequency scattering", "categories": ["math.NA", "math.AP"], "comment": null, "summary": "We consider three common mathematical models for time-harmonic high frequency scattering: the Helmholtz equation in two and three spatial dimensions, a transverse magnetic problem in two dimensions, and Maxwell's equation in three dimensions with dissipative boundary conditions such that the continuous problem is well posed. In this paper, we construct meshes for popular (low order) Galerkin finite element discretizations such that the discrete system matrix becomes singular and the discrete problem is not well posed. This implies that a condition \"the finite element space has to be sufficiently rich\" in the form of a resolution condition - typically imposed for discrete well-posedness - is not an artifact from the proof by a compact perturbation argument but necessary for discrete stability of the Galerkin discretization.", "AI": {"tldr": "The paper shows that for high-frequency scattering problems, low-order Galerkin FEM can produce singular system matrices if meshes don't satisfy resolution conditions, proving these conditions are necessary for stability, not just proof artifacts.", "motivation": "To demonstrate that resolution conditions for finite element spaces in high-frequency scattering problems are not just theoretical artifacts from compact perturbation proofs, but are actually necessary for discrete stability and well-posedness of the Galerkin discretization.", "method": "Construct specific meshes for three common mathematical models of time-harmonic high frequency scattering (Helmholtz equation in 2D/3D, transverse magnetic problem in 2D, and Maxwell's equations in 3D) that cause low-order Galerkin finite element discretizations to produce singular system matrices, making the discrete problem ill-posed.", "result": "The constructed meshes lead to singular discrete system matrices, proving that without sufficient resolution conditions, the Galerkin discretization becomes unstable and ill-posed, confirming that these resolution requirements are essential rather than just proof artifacts.", "conclusion": "Resolution conditions for finite element spaces in high-frequency scattering problems are necessary for discrete stability, not merely theoretical requirements from compact perturbation arguments. This has practical implications for mesh design in computational wave propagation."}}
{"id": "2602.03770", "pdf": "https://arxiv.org/pdf/2602.03770", "abs": "https://arxiv.org/abs/2602.03770", "authors": ["A. Zaccone"], "title": "Ultrastable 2D glasses and packings explained by local centrosymmetry", "categories": ["cond-mat.soft", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cond-mat.stat-mech", "physics.comp-ph"], "comment": null, "summary": "Using the most recent numerical data by Bolton-Lum \\emph{et al.} [Phys. Rev. Lett. 136, 058201 (2026)], we demonstrate that ideal ultrastable glasses in the athermal limit (or ultrastable ideal 2D disk packings) possess a remarkably high degree of local centrosymmetry. In particular, we find that the inversion-symmetry order parameter for local force transmission introduced in Milkus and Zaccone, [Phys. Rev. 93, 094204 (2016)], is as high as $F_{IS}= 0.93546$, to be compared with $F_{IS}=1$ for perfect centrosymmetric crystals free of defects, and with $F_{IS} \\sim 0.3-0.5$ for standard random packings. This observation provides a clear, natural explanation for the ultra-high shear modulus of ideal packings and ideal glasses, because the high centrosymmetry prevents non-affine relaxations which decrease the shear modulus. The same mechanism explains the absence of boson peak-like soft vibrational modes. These results also confirm what was found previous work, i.e. that the bond-orientational order parameter is a very poor correlator for the vibrational and mechanical", "AI": {"tldr": "Ideal ultrastable glasses show near-perfect local centrosymmetry (F_IS=0.935), explaining their ultra-high shear modulus and absence of boson peak vibrations.", "motivation": "To understand why ideal ultrastable glasses have exceptional mechanical properties (ultra-high shear modulus) and vibrational characteristics (absence of boson peak).", "method": "Analyze numerical data from Bolton-Lum et al. (2026) using inversion-symmetry order parameter F_IS introduced by Milkus and Zaccone (2016) to measure local centrosymmetry in 2D disk packings.", "result": "Ideal ultrastable glasses achieve F_IS=0.93546, close to perfect centrosymmetric crystals (F_IS=1), far exceeding standard random packings (F_IS~0.3-0.5). High centrosymmetry prevents non-affine relaxations that reduce shear modulus.", "conclusion": "Local centrosymmetry, not bond-orientational order, is the key structural feature explaining the exceptional mechanical and vibrational properties of ideal ultrastable glasses."}}
{"id": "2602.03136", "pdf": "https://arxiv.org/pdf/2602.03136", "abs": "https://arxiv.org/abs/2602.03136", "authors": ["Enric Florit-Simon"], "title": "Phase transitions with bounded index: Parallels to De Giorgi's conjecture", "categories": ["math.AP", "math.DG"], "comment": null, "summary": "A well-known conjecture of De Giorgi -- motivated by analogy with the Bernstein problem for minimal surfaces -- asserts the rigidity of monotone solutions to the Allen--Cahn equation in $\\mathbb{R}^{d+1}$, with $d\\leq 7$.\n  We establish close parallels to De Giorgi's conjecture for general solutions of bounded Morse index, far stronger than the minimal surface analogy would suggest: Namely, any finite index solution to the Allen--Cahn equation with bounded energy density in $\\mathbb{R}^4$ is one-dimensional, and -- conditionally on the classification of stable solutions -- the same holds for all $4\\leq n \\leq 7$.\n  As a geometric application, phase transitions with bounded energy and index in closed four-manifolds have smooth transition layers which behave like minimal hypersurfaces.\n  Consequently, phase transitions exhibit a remarkably rigid behaviour in higher dimensions. This is in stark contrast with the 3D case, in which a wealth of nontrivial entire solutions with finite index (and energy density) is conversely known to exist, by work pioneered by Del Pino--Kowalczyk--Wei. The authors conjectured that any such solution must have parallel ends which are either planar or catenoidal, suggesting it as a parallel to De Giorgi's conjecture in this framework. We confirm this picture under the bounded energy density assumption.", "AI": {"tldr": "The paper establishes rigidity results for finite Morse index solutions to the Allen-Cahn equation in dimensions 4-7, showing they must be one-dimensional, and confirms conjectures about the structure of such solutions in 3D.", "motivation": "The motivation stems from De Giorgi's conjecture about rigidity of monotone solutions to the Allen-Cahn equation, which is analogous to the Bernstein problem for minimal surfaces. The authors aim to extend this understanding to general solutions with bounded Morse index, exploring parallels between phase transitions and minimal hypersurfaces in higher dimensions.", "method": "The authors establish parallels between De Giorgi's conjecture and solutions with bounded Morse index. They prove that any finite index solution with bounded energy density in \u211d\u2074 is one-dimensional, and conditionally extend this to dimensions 4-7 based on classification of stable solutions. They also analyze geometric applications to phase transitions in closed four-manifolds.", "result": "1. Finite index solutions with bounded energy density in \u211d\u2074 are one-dimensional.\n2. Conditionally on stable solution classification, same holds for dimensions 4-7.\n3. Phase transitions in closed four-manifolds have smooth transition layers behaving like minimal hypersurfaces.\n4. Confirms conjectures about 3D solutions having parallel ends that are either planar or catenoidal under bounded energy density assumption.", "conclusion": "Phase transitions exhibit remarkable rigidity in higher dimensions (4-7), contrasting sharply with the rich variety of nontrivial solutions in 3D. The work establishes strong parallels between De Giorgi's conjecture and solutions with bounded Morse index, revealing deep connections between Allen-Cahn equations and minimal surface theory in higher dimensions."}}
{"id": "2602.02500", "pdf": "https://arxiv.org/pdf/2602.02500", "abs": "https://arxiv.org/abs/2602.02500", "authors": ["Chen Hu", "Qianxi Zhao", "Yuming Li", "Mingyu Zhou", "Xiyin Li"], "title": "UNSO: Unified Newton Schulz Orthogonalization", "categories": ["cs.LG", "cs.AI", "math.NA"], "comment": null, "summary": "The Newton-Schulz (NS) iteration has gained increasing interest for its role in the Muon optimizer and the Stiefel manifold. However, the conventional NS iteration suffers from inefficiency and instability. Although various improvements have been introduced to NS iteration, they fail to deviate from the conventional iterative paradigm, which could increase computation burden largely due to the matrix products along the long dimension repeatedly. To address this, we consolidate the iterative structure into a unified framework, named Unified Newton-Schulz Orthogonalization (UNSO). To do so, we could avoid a polynomial expansion. Instead, we evaluate the role of each matrix power, remove the insignificant terms, and provide a recommended polynomial with learnable coefficients. These learnable coefficients are then optimized, and achieve an outstanding performance with stable convergence. The code of our method is available: https://github.com/greekinRoma/Unified_Newton_Schulz_Orthogonalization.", "AI": {"tldr": "UNSO framework improves Newton-Schulz iteration by replacing conventional polynomial expansion with optimized learnable coefficients, achieving better efficiency and stability.", "motivation": "The conventional Newton-Schulz iteration suffers from inefficiency and instability, and existing improvements still follow the same iterative paradigm that increases computational burden through repeated matrix products along long dimensions.", "method": "Consolidate iterative structure into Unified Newton-Schulz Orthogonalization (UNSO) framework, avoid polynomial expansion, evaluate role of each matrix power, remove insignificant terms, and provide recommended polynomial with learnable coefficients that are optimized.", "result": "Achieves outstanding performance with stable convergence while reducing computational burden compared to conventional NS iteration approaches.", "conclusion": "UNSO provides an efficient and stable alternative to conventional Newton-Schulz orthogonalization by optimizing learnable coefficients in a unified framework, avoiding the computational inefficiencies of traditional iterative approaches."}}
{"id": "2602.03813", "pdf": "https://arxiv.org/pdf/2602.03813", "abs": "https://arxiv.org/abs/2602.03813", "authors": ["Alptu\u011f Ulug\u00f6l", "Giovanni Del Monte", "Eline K. Kempkes", "Frank Smallenburg", "Laura Filion"], "title": "Vacancy defects in square-triangle tilings and their implications for quasicrystals formed by square-shoulder particles", "categories": ["cond-mat.soft", "cond-mat.mtrl-sci", "cond-mat.stat-mech", "physics.comp-ph"], "comment": "16 pages, 15 figures, 4 tables", "summary": "Almost all observed square-triangle quasicrystals in soft-matter systems contain a large number of point-like defects, yet the role these defects play in stabilizing the quasicrystal phase remains poorly understood. In this work, we investigate the thermodynamic role of such defects in the widely observed 12-fold symmetric square-triangle quasicrystal. We develop a new Monte Carlo simulation to compute the configurational entropy of square-triangle tilings augmented to contain two types of irregular hexagons as defect tiles. We find that the introduction of defects leads to a notable entropy gain, with each defect contributing considerably more than a conventional vacancy in a periodic crystal. Intriguingly, the entropy gain is not simply due to individual defect types but isamplified by their combinatorial mixing. We then apply our findings to a microscopic model of core-corona particles interacting via a square-shoulder potential. By combining the configurational entropy with vibrational free-energy calculations, we predict the equilibrium defect concentration and confirm that the quasicrystalline phase contains a higher concentration of point-defects than a typical periodic crystal. These results provide a new understanding of the prominence of observed defects in soft-matter quasicrystals.", "AI": {"tldr": "Defects in square-triangle quasicrystals provide significant configurational entropy gain, making them thermodynamically favorable and explaining their high prevalence in soft-matter systems.", "motivation": "Square-triangle quasicrystals in soft-matter systems contain many point defects, but the thermodynamic role of these defects in stabilizing the quasicrystal phase remains poorly understood.", "method": "Developed new Monte Carlo simulation to compute configurational entropy of square-triangle tilings with two types of irregular hexagon defect tiles. Combined configurational entropy with vibrational free-energy calculations for core-corona particles with square-shoulder potential.", "result": "Defects provide notable entropy gain, with each defect contributing more than conventional vacancies in periodic crystals. Entropy gain is amplified by combinatorial mixing of defect types. Quasicrystalline phase contains higher defect concentration than typical periodic crystals.", "conclusion": "Defects play crucial thermodynamic role in stabilizing soft-matter quasicrystals by providing significant configurational entropy, explaining the prominence of observed defects in these systems."}}
{"id": "2602.03174", "pdf": "https://arxiv.org/pdf/2602.03174", "abs": "https://arxiv.org/abs/2602.03174", "authors": ["Martin Morange"], "title": "Quantitative sensitivity analysis for Fokker-Planck equation with respect to the Wasserstein distance", "categories": ["math.AP"], "comment": null, "summary": "We analyze the sensitivity of solutions to the Fokker-Planck equation with respect to some unknown parameter. Our main result is to provide quantitative upper bounds for the $p$-Wasserstein distance $\\mathcal{W}_p$ between two solutions with different parameters, for every $p \\geq 2$. We are able to give two proofs of this result, the first relying on synchronous coupling between two solutions of an SDE, and another one that relies on the differentiation of Kantorovitch dual formulation of optimal transport. We also provide more specific bounds in the case of the overdamped Langevin process, for which we are able to compare convergence to the invariant measure and sensitivity to the parameter.", "AI": {"tldr": "The paper provides quantitative upper bounds for the p-Wasserstein distance between solutions of Fokker-Planck equations with different parameters, using both synchronous coupling and optimal transport approaches.", "motivation": "To analyze the sensitivity of Fokker-Planck equation solutions to unknown parameters, providing quantitative measures of how solutions change with parameter variations.", "method": "Two approaches: 1) Synchronous coupling between two solutions of stochastic differential equations, 2) Differentiation of Kantorovitch dual formulation of optimal transport.", "result": "Quantitative upper bounds for p-Wasserstein distance (p\u22652) between solutions with different parameters, with specific bounds for overdamped Langevin processes comparing convergence to invariant measure and parameter sensitivity.", "conclusion": "The paper establishes rigorous sensitivity analysis for Fokker-Planck equations using Wasserstein metrics, providing two complementary mathematical approaches and specialized results for Langevin processes."}}
{"id": "2602.02945", "pdf": "https://arxiv.org/pdf/2602.02945", "abs": "https://arxiv.org/abs/2602.02945", "authors": ["Nicholas Polson", "Vadim Sokolov"], "title": "Bayesian Methods for the Navier-Stokes Equations", "categories": ["stat.CO", "math.NA"], "comment": null, "summary": "We develop a Bayesian methodology for numerical solution of the incompressible Navier--Stokes equations with quantified uncertainty. The central idea is to treat discretized Navier--Stokes dynamics as a state-space model and to view numerical solution as posterior computation: priors encode physical structure and modeling error, and the solver outputs a distribution over states and quantities of interest rather than a single trajectory. In two dimensions, stochastic representations (Feynman--Kac and stochastic characteristics for linear advection--diffusion with prescribed drift) motivate Monte Carlo solvers and provide intuition for uncertainty propagation. In three dimensions, we formulate stochastic Navier--Stokes models and describe particle-based and ensemble-based Bayesian workflows for uncertainty propagation in spectral discretizations. A key computational advantage is that parameter learning can be performed stably via particle learning: marginalization and resample--propagate (one-step smoothing) constructions avoid the weight-collapse that plagues naive sequential importance sampling on static parameters. When partial observations are available, the same machinery supports sequential observational updating as an additional capability. We also discuss non-Gaussian (heavy-tailed) error models based on normal variance-mean mixtures, which yield conditionally Gaussian updates via latent scale augmentation.", "AI": {"tldr": "Bayesian methodology for solving incompressible Navier-Stokes equations with uncertainty quantification, treating discretized dynamics as state-space model and numerical solution as posterior computation.", "motivation": "To develop a framework for uncertainty quantification in numerical solutions of Navier-Stokes equations, moving beyond deterministic solutions to provide probabilistic distributions over states and quantities of interest.", "method": "Treats discretized Navier-Stokes as state-space model; uses stochastic representations (Feynman-Kac, stochastic characteristics) in 2D; formulates stochastic Navier-Stokes models in 3D; employs particle-based and ensemble-based Bayesian workflows with spectral discretizations; uses particle learning with marginalization and resample-propagate to avoid weight collapse.", "result": "Developed Bayesian methodology that outputs distributions over states rather than single trajectories; enables stable parameter learning via particle learning; supports sequential observational updating; incorporates non-Gaussian error models via normal variance-mean mixtures.", "conclusion": "Bayesian approach provides robust uncertainty quantification for Navier-Stokes solutions, with computational advantages for parameter learning and flexibility for observational updating and non-Gaussian error modeling."}}
{"id": "2602.03191", "pdf": "https://arxiv.org/pdf/2602.03191", "abs": "https://arxiv.org/abs/2602.03191", "authors": ["Yingfang Zhang", "Xuexiu Zhong", "Wenming Zou"], "title": "Bivariate Hardy-Sobolev Inequality and Its Sharp Stability", "categories": ["math.AP"], "comment": null, "summary": "This paper establishes a bivariate Hardy-Sobolev inequality. Let $\u03a9\\subset \\mathbb{R}^N$ ($N \\geq 3$) be an open domain, $s \\in (0,2)$, $\u03b1> 1$, $\u03b2> 1$ with $\u03b1+ \u03b2= 2^*(s)$, and $\u03ba\\in \\mathbb{R}$. For any functions $u, v \\in D_0^{1,2}(\u03a9)$, we prove the inequality:\n  \\begin{multline*}\n  \\int_\u03a9 |\\nabla u|^2 \\, \\mathrm{d}x + \\int_\u03a9 |\\nabla v|^2 \\, \\mathrm{d}x\n  \\ge S_{\u03b1,\u03b2,\u03bb,\u03bc}(\u03a9) \\left( \\int_\u03a9 \\Big( \u03bb\\frac{|u|^{2^*(s)}}{|x|^s} + \u03bc\\frac{|v|^{2^*(s)}}{|x|^s} + 2^*(s) \u03ba\\frac{|u|^\u03b1|v|^\u03b2}{|x|^s} \\Big)\\, \\mathrm{d}x \\right)^{\\frac{2}{2^*(s)}}.\n  \\end{multline*}\n  We derive the best constant $S_{\u03b1,\u03b2,\u03bb,\u03bc}(\u03a9)$ and characterize the set of minimizers. Moreover, for $\u03a9= \\mathbb{R}^N$ and $\u03ba> 0$, we obtain sharp stability results for nonnegative functions.", "AI": {"tldr": "The paper establishes a bivariate Hardy-Sobolev inequality with optimal constant and characterizes minimizers, including sharp stability results for nonnegative functions in the whole space.", "motivation": "To extend Hardy-Sobolev inequalities to systems of two functions, establishing optimal constants and understanding the structure of minimizers for coupled nonlinear terms with singular weights.", "method": "Analytic approach using variational methods to prove the inequality, compute the best constant S_{\u03b1,\u03b2,\u03bb,\u03bc}(\u03a9), and characterize the set of minimizers through careful analysis of the functional.", "result": "Proved the bivariate Hardy-Sobolev inequality with explicit best constant, characterized the minimizer set, and obtained sharp stability results for nonnegative functions when \u03a9=\u211d^N and \u03ba>0.", "conclusion": "The paper successfully establishes a comprehensive theory for bivariate Hardy-Sobolev inequalities, providing optimal constants, complete characterization of minimizers, and stability results that extend the classical theory to coupled systems."}}
{"id": "2602.02948", "pdf": "https://arxiv.org/pdf/2602.02948", "abs": "https://arxiv.org/abs/2602.02948", "authors": ["Jack Michael Solomon", "Rishi Leburu", "Matthias Chung"], "title": "Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification", "categories": ["cs.LG", "math.NA"], "comment": null, "summary": "Inverse problems are fundamental to many scientific and engineering disciplines; they arise when one seeks to reconstruct hidden, underlying quantities from noisy measurements. Many applications demand not just point estimates but interpretable uncertainty. Providing fast inference alongside uncertainty estimates remains challenging yet desirable in numerous applications.\n  We propose the Variational Sparse Paired Autoencoder (vsPAIR) to address this challenge. The architecture pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through a learned latent mapping. The variational structure enables uncertainty estimation, the paired architecture encourages interpretability by anchoring QoI representations to clean data, and sparse encodings provide structure by concentrating information into identifiable factors rather than diffusing across all dimensions. We also propose modifications to existing sparse VAE methods: a hard-concrete spike-and-slab relaxation for differentiable training and a beta hyperprior for adaptive sparsity levels. To validate the effectiveness of our proposed architecture, we conduct experiments on blind inpainting and computed tomography, demonstrating that vsPAIR is a capable inverse problem solver that can provide interpretable and structured uncertainty estimates.", "AI": {"tldr": "vsPAIR is a variational sparse paired autoencoder that solves inverse problems with interpretable uncertainty estimates by pairing a standard VAE for observations with a sparse VAE for quantities of interest, connected via learned latent mapping.", "motivation": "Inverse problems require not just point estimates but interpretable uncertainty, yet providing fast inference with uncertainty estimates remains challenging in many scientific and engineering applications.", "method": "Pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through learned latent mapping. Uses hard-concrete spike-and-slab relaxation for differentiable training and beta hyperprior for adaptive sparsity.", "result": "Experiments on blind inpainting and computed tomography demonstrate vsPAIR is a capable inverse problem solver that provides interpretable and structured uncertainty estimates.", "conclusion": "vsPAIR effectively addresses the challenge of providing both fast inference and interpretable uncertainty in inverse problems through its paired variational architecture with sparse encodings."}}
{"id": "2602.03299", "pdf": "https://arxiv.org/pdf/2602.03299", "abs": "https://arxiv.org/abs/2602.03299", "authors": ["Huyuan Chen", "Rui Chen"], "title": "On Poincar\u00e9-Sobolev level involving fractional GJMS operators on hyperbolic space", "categories": ["math.AP"], "comment": null, "summary": "This paper is devoted to a qualitative analysis of the Poincar\u00e9--Sobolev level associated with the fractional GJMS operators \\(\\mathcal{P}_s\\) \\(\\bigl(s\\in(0,\\tfrac n2)\\setminus\\mathbb N\\bigr)\\) on the hyperbolic space \\(\\mathbb H^n\\). In contrast to the integer-order case, when \\(s\\notin\\mathbb N\\) the operator \\(\\mathcal{P}_s\\) does not enjoy the conformal covariance that allows one, in the upper half-space or ball model, to relate it to the Euclidean fractional Laplacian \\((-\u0394)^s\\); this link is crucial for importing Euclidean theory. We therefore introduce \\(\\widetilde{\\mathcal{P}}_s\\) (\\(s>0\\)), which is conformally related to \\((-\u0394)^s\\). Our purpose in the paper is to analyze the monotonicity, attainability, and strict-gap regions of the Poincar\u00e9--Sobolev levels associated with \\(\\mathcal{P}_s\\) and with \\(\\widetilde{\\mathcal{P}}_s\\) with \\(s\\in(0,\\tfrac n2)\\setminus\\mathbb N\\). First, we reinterpret the Brezis--Nirenberg problem through the lens of Poincar\u00e9--Sobolev levels, connecting earlier results for the Euclidean Laplacian and for operators \\(\\mathcal{P}_k\\) on \\(\\mathbb H^n\\) with integer \\(k\\in(0,\\tfrac n2)\\). We then establish new, explicit lower bounds for the Hardy term in fractional Hardy--Sobolev--Maz'ya inequalities involving both \\(\\mathcal{P}_s\\) and \\(\\widetilde{\\mathcal{P}}_s\\). By applying the concentration--compactness principle together with a detailed analysis of the strict-gap regions for the Poincar\u00e9--Sobolev levels, we prove the existence of solutions to the Brezis--Nirenberg problem on \\(\\mathbb H^n\\) for both operators. Finally, combining the Hardy lower bounds with criteria for attainability, we obtain a complete characterization of the Poincar\u00e9--Sobolev levels \\(H_{n,s}\\) and \\(\\widetilde H_{n,s}\\).", "AI": {"tldr": "Analysis of Poincar\u00e9-Sobolev levels for fractional GJMS operators on hyperbolic space, establishing existence of solutions to Brezis-Nirenberg problem and complete characterization of Poincar\u00e9-Sobolev levels.", "motivation": "Fractional GJMS operators on hyperbolic space lack the conformal covariance property that integer-order operators enjoy, making it difficult to import Euclidean theory. The paper aims to analyze Poincar\u00e9-Sobolev levels for these non-integer fractional operators.", "method": "Introduce conformally related operator \\(\\widetilde{\\mathcal{P}}_s\\), reinterpret Brezis-Nirenberg problem through Poincar\u00e9-Sobolev levels, establish explicit lower bounds for Hardy term in fractional Hardy-Sobolev-Maz'ya inequalities, apply concentration-compactness principle with strict-gap region analysis.", "result": "Prove existence of solutions to Brezis-Nirenberg problem on hyperbolic space for both \\(\\mathcal{P}_s\\) and \\(\\widetilde{\\mathcal{P}}_s\\), and obtain complete characterization of Poincar\u00e9-Sobolev levels \\(H_{n,s}\\) and \\(\\widetilde H_{n,s}\\).", "conclusion": "The paper successfully extends analysis of Poincar\u00e9-Sobolev levels to non-integer fractional GJMS operators on hyperbolic space, overcoming lack of conformal covariance through introduction of conformally related operator and establishing comprehensive results including existence theorems and complete characterization."}}
{"id": "2602.02981", "pdf": "https://arxiv.org/pdf/2602.02981", "abs": "https://arxiv.org/abs/2602.02981", "authors": ["Harbir Antil", "Animesh Jain", "Rainald L\u00f6hner"], "title": "Fisher-Information-Based Sensor Placement for Structural Digital Twins: Analytic Results and Benchmarks", "categories": ["math.OC", "math.NA"], "comment": null, "summary": "High-fidelity digital twins rely on the accurate assimilation of sensor data into physics-based computational models. In structural applications, such twins aim to identify spatially distributed quantities--such as elementwise weakening fields, material parameters, or effective thermal loads--by minimizing discrepancies between measured and simulated responses subject to the governing equations of structural mechanics. While adjoint-based methods enable efficient gradient computation for these inverse problems, the quality and stability of the resulting estimates depend critically on the choice of sensor locations, measurement types, and directions.\n  This paper develops a rigorous and implementation-ready framework for Fisher-information-based sensor placement in adjoint-based finite-element digital twins. Sensor configurations are evaluated using a D-optimal design criterion derived from a linearization of the measurement map, yielding a statistically meaningful measure of information content. We present matrix-free operator formulas for applying the Jacobian and its adjoint, and hence for computing Fisher-information products $Fv = J^\\top R^{-1} Jv$ using only forward and adjoint solves. Building on these operator evaluations, we derive explicit sensitivity expressions for D-optimal sensor design with respect to measurement parameters and discuss practical strategies for evaluating the associated log-determinant objectives. To complement the general framework, we provide analytically tractable sensor placement results for a canonical one-dimensional structural model, clarifying the distinction between detectability and localizability and proving that D-optimal placement of multiple displacement sensors yields approximately uniform spacing.", "AI": {"tldr": "A framework for optimal sensor placement in structural digital twins using Fisher information and D-optimal design to maximize information content from measurements.", "motivation": "High-fidelity digital twins need accurate sensor data assimilation, but the quality of inverse problem solutions depends critically on sensor placement, measurement types, and directions. Current methods lack rigorous frameworks for optimal sensor configuration.", "method": "Develops Fisher-information-based sensor placement using D-optimal design criterion from linearized measurement map. Provides matrix-free operator formulas for Jacobian and adjoint computations, enabling efficient Fisher-information products using only forward/adjoint solves. Derives sensitivity expressions for D-optimal design and practical strategies for log-determinant objectives.", "result": "Implementation-ready framework for sensor placement in adjoint-based finite-element digital twins. Analytical results for 1D structural model show distinction between detectability and localizability, and prove D-optimal placement of multiple displacement sensors yields approximately uniform spacing.", "conclusion": "Provides rigorous, statistically meaningful framework for optimal sensor placement in structural digital twins, enabling more accurate and stable parameter estimation through principled sensor configuration design."}}
{"id": "2602.03341", "pdf": "https://arxiv.org/pdf/2602.03341", "abs": "https://arxiv.org/abs/2602.03341", "authors": ["Ming Li", "Linyu Peng", "Ping Zhang", "Xin Zhang"], "title": "Local profiles of self-similar solutions of the planar stationary Navier--Stokes equations", "categories": ["math.AP"], "comment": "25 pages, 2 figures", "summary": "In this paper, we revisit self-similar solutions of the two-dimensional stationary incompressible Navier-Stokes equations under scaling symmetries, also known as Jeffery-Hamel solutions. We investigate the local patterns of smooth Jeffery-Hamel solutions in a conical subdomain $\u03a9$ with vertex at the origin, without imposing any boundary conditions on $\u03a9$. For radial Jeffery-Hamel solutions, we obtain all the explicit local profiles in $\u03a9$ with arbitrary opening angles. In the non-radial case, we show that some Jeffery-Hamel solutions can be obtained via solving a Li\u00e9nard equation, and we derive new explicit local profiles expressible in terms of Weierstrass elliptic functions.", "AI": {"tldr": "Revisiting self-similar Jeffery-Hamel solutions of 2D stationary incompressible Navier-Stokes equations, obtaining explicit local profiles for radial cases and deriving new explicit solutions in non-radial cases using Weierstrass elliptic functions.", "motivation": "To investigate local patterns of smooth Jeffery-Hamel solutions in conical domains without boundary conditions, extending understanding of self-similar solutions under scaling symmetries in fluid dynamics.", "method": "Analyze Jeffery-Hamel solutions under scaling symmetries; for radial cases obtain all explicit local profiles; for non-radial cases show solutions can be obtained via solving a Li\u00e9nard equation and derive explicit profiles using Weierstrass elliptic functions.", "result": "Complete explicit local profiles obtained for radial Jeffery-Hamel solutions with arbitrary opening angles; new explicit local profiles derived for non-radial cases expressible in terms of Weierstrass elliptic functions.", "conclusion": "The study provides comprehensive explicit Jeffery-Hamel solutions for both radial and non-radial cases, advancing the mathematical understanding of self-similar solutions in 2D stationary incompressible Navier-Stokes equations."}}
{"id": "2602.03067", "pdf": "https://arxiv.org/pdf/2602.03067", "abs": "https://arxiv.org/abs/2602.03067", "authors": ["Felix X. -F. Ye", "Xingjie Li", "An Yu", "Ming-Ching Chang", "Linsong Chu", "Davis Wertheimer"], "title": "FlashSinkhorn: IO-Aware Entropic Optimal Transport", "categories": ["cs.LG", "cs.AI", "math.NA"], "comment": null, "summary": "Entropic optimal transport (EOT) via Sinkhorn iterations is widely used in modern machine learning, yet GPU solvers remain inefficient at scale. Tensorized implementations suffer quadratic HBM traffic from dense $n\\times m$ interactions, while existing online backends avoid storing dense matrices but still rely on generic tiled map-reduce reduction kernels with limited fusion. We present \\textbf{FlashSinkhorn}, an IO-aware EOT solver for squared Euclidean cost that rewrites stabilized log-domain Sinkhorn updates as row-wise LogSumExp reductions of biased dot-product scores, the same normalization as transformer attention. This enables FlashAttention-style fusion and tiling: fused Triton kernels stream tiles through on-chip SRAM and update dual potentials in a single pass, substantially reducing HBM IO per iteration while retaining linear-memory operations. We further provide streaming kernels for transport application, enabling scalable first- and second-order optimization. On A100 GPUs, FlashSinkhorn achieves up to $32\\times$ forward-pass and $161\\times$ end-to-end speedups over state-of-the-art online baselines on point-cloud OT, improves scalability on OT-based downstream tasks. For reproducibility, we release an open-source implementation at https://github.com/ot-triton-lab/ot_triton.", "AI": {"tldr": "FlashSinkhorn is a GPU-optimized entropic optimal transport solver that achieves up to 32x speedup by reformulating Sinkhorn iterations as attention-like operations and using FlashAttention-style fused kernels.", "motivation": "Current GPU solvers for entropic optimal transport are inefficient at scale. Tensorized implementations have quadratic HBM traffic, while online backends use generic reduction kernels with limited fusion, leading to poor performance on large-scale problems.", "method": "Reformulates stabilized log-domain Sinkhorn updates as row-wise LogSumExp reductions of biased dot-product scores (same normalization as transformer attention). Uses FlashAttention-style fusion and tiling with fused Triton kernels that stream tiles through on-chip SRAM and update dual potentials in a single pass. Also provides streaming kernels for transport application.", "result": "On A100 GPUs, achieves up to 32\u00d7 forward-pass and 161\u00d7 end-to-end speedups over state-of-the-art online baselines on point-cloud OT. Improves scalability on OT-based downstream tasks. Reduces HBM IO per iteration while maintaining linear-memory operations.", "conclusion": "FlashSinkhorn demonstrates that IO-aware algorithm design and kernel fusion can dramatically accelerate EOT computations on GPUs, enabling scalable first- and second-order optimization. The approach is open-sourced for reproducibility."}}
{"id": "2602.03463", "pdf": "https://arxiv.org/pdf/2602.03463", "abs": "https://arxiv.org/abs/2602.03463", "authors": ["Lidia Gargyants", "Anna Konovalova", "Olga Rozanova"], "title": "Internal free boundary problem for cold plasma equations", "categories": ["math.AP", "math-ph"], "comment": "14 pages, 8 figures", "summary": "For the system of cold plasma equations describing the motion of electrons in the field of stationary ions, we consider the Riemann problem posed at an impenetrable interface between two media. These media differ in the magnitude of the constant ion field. The interface between the media is assumed to be free. Its position is determined from the generalized Rankine-Hugoniot conditions and the stability condition, that is, the intersection of Lagrangian particle trajectories at the interface.", "AI": {"tldr": "Analysis of Riemann problem for cold plasma equations at an impenetrable interface between two media with different constant ion fields, using generalized Rankine-Hugoniot conditions and stability conditions.", "motivation": "To understand the behavior of electron motion in cold plasma systems when there is an interface between two media with different constant ion fields, particularly focusing on how such interfaces behave under Riemann problem conditions.", "method": "Uses the system of cold plasma equations describing electron motion in stationary ion fields. Formulates Riemann problem at an impenetrable interface between two media with different constant ion fields. Applies generalized Rankine-Hugoniot conditions and stability conditions (intersection of Lagrangian particle trajectories at the interface) to determine interface position.", "result": "The paper develops a mathematical framework for analyzing interface behavior in cold plasma systems, establishing conditions for interface position determination through generalized Rankine-Hugoniot conditions and stability requirements.", "conclusion": "The Riemann problem approach with generalized Rankine-Hugoniot conditions and stability criteria provides a systematic method for analyzing interface behavior in cold plasma systems with different ion fields, offering insights into electron motion dynamics at media boundaries."}}
{"id": "2602.03535", "pdf": "https://arxiv.org/pdf/2602.03535", "abs": "https://arxiv.org/abs/2602.03535", "authors": ["Yannick Lunk", "Sebastian J. Scott", "Leon Bungert"], "title": "Sparse Training of Neural Networks based on Multilevel Mirror Descent", "categories": ["cs.LG", "math.NA", "math.OC"], "comment": null, "summary": "We introduce a dynamic sparse training algorithm based on linearized Bregman iterations / mirror descent that exploits the naturally incurred sparsity by alternating between periods of static and dynamic sparsity pattern updates. The key idea is to combine sparsity-inducing Bregman iterations with adaptive freezing of the network structure to enable efficient exploration of the sparse parameter space while maintaining sparsity. We provide convergence guaranties by embedding our method in a multilevel optimization framework. Furthermore, we empirically show that our algorithm can produce highly sparse and accurate models on standard benchmarks. We also show that the theoretical number of FLOPs compared to SGD training can be reduced from 38% for standard Bregman iterations to 6% for our method while maintaining test accuracy.", "AI": {"tldr": "Dynamic sparse training algorithm using linearized Bregman iterations with alternating static/dynamic sparsity updates, achieving high sparsity with maintained accuracy and reduced FLOPs.", "motivation": "To develop an efficient sparse training algorithm that can explore sparse parameter spaces while maintaining model accuracy, addressing the computational inefficiency of traditional training methods.", "method": "Combines sparsity-inducing Bregman iterations with adaptive freezing of network structure, alternating between periods of static and dynamic sparsity pattern updates, embedded in a multilevel optimization framework.", "result": "Produces highly sparse and accurate models on standard benchmarks, reducing theoretical FLOPs from 38% (standard Bregman) to 6% compared to SGD while maintaining test accuracy.", "conclusion": "The proposed dynamic sparse training algorithm effectively balances exploration of sparse parameter space with computational efficiency, achieving significant sparsity and accuracy improvements over existing methods."}}
{"id": "2602.03481", "pdf": "https://arxiv.org/pdf/2602.03481", "abs": "https://arxiv.org/abs/2602.03481", "authors": ["Alexander Zlotnik"], "title": "On weak solutions to the 1d compressible Navier-Stokes equations: a Lipschitz continuous dependence on data in weaker norms and an error of their homogenization", "categories": ["math.AP"], "comment": "30 pages", "summary": "We deal with the global in time weak solutions to the 1D compressible Navier-Stokes system of equations for large discontinuous initial data and nonhomogeneous boundary conditions of three standard types. We prove the Lipschitz-type continuous dependence of the solution $(\u03b7,u,\u03b8)$, in a norm slightly stronger than $L^{2,\\infty}(Q)\\times L^2(Q)\\times L^2(Q)$, on the initial data $(\u03b7^0,u^0,e^0)$ in a norm of $L^2(\u03a9)\\times H^{-1}(\u03a9)\\times H^{-1}(\u03a9)$-type and also on the free terms in all the equations in some dual norms. Here $\u03b7$, $u$ and $\u03b8$ are the specific volume, velocity and absolute temperature as well as $\u03b7^0$, $u^0$ and $e^0$ are the initial specific volume, velocity and specific total energy, and $Q=\u03a9\\times (0,T)$. We also apply this result to the case of discontinuous rapidly oscillating, with the period $\\varepsilon$, initial data and free terms and derive an estimate $O(\\varepsilon)$ for the difference between the solutions to the Navier-Stokes equations and their Bakhvalov-Eglit two-scale homogenized version with averaged data.", "AI": {"tldr": "The paper proves Lipschitz-type continuous dependence of weak solutions for 1D compressible Navier-Stokes equations on initial data and source terms, and applies this to derive O(\u03b5) error estimates for homogenization with rapidly oscillating data.", "motivation": "To establish stability and continuous dependence results for weak solutions of 1D compressible Navier-Stokes equations with large discontinuous initial data and nonhomogeneous boundary conditions, which is important for understanding solution behavior and for applications to homogenization problems.", "method": "The authors work with global weak solutions to the 1D compressible Navier-Stokes system. They prove Lipschitz-type continuous dependence of solutions (\u03b7,u,\u03b8) in norms slightly stronger than L\u00b2,\u221e(Q)\u00d7L\u00b2(Q)\u00d7L\u00b2(Q) on initial data (\u03b7\u2070,u\u2070,e\u2070) in L\u00b2(\u03a9)\u00d7H\u207b\u00b9(\u03a9)\u00d7H\u207b\u00b9(\u03a9)-type norms and on source terms in dual norms. They then apply this result to rapidly oscillating initial data with period \u03b5 to compare solutions to the original Navier-Stokes equations with their Bakhvalov-Eglit two-scale homogenized version.", "result": "The main result is the Lipschitz-type continuous dependence of weak solutions on initial data and source terms. As an application, they derive an O(\u03b5) estimate for the difference between solutions to the Navier-Stokes equations with rapidly oscillating data and solutions to the homogenized equations with averaged data.", "conclusion": "The paper establishes important stability properties for weak solutions of 1D compressible Navier-Stokes equations, which provides mathematical justification for homogenization approaches and demonstrates that solutions depend continuously on initial data even for large discontinuous data and nonhomogeneous boundary conditions."}}
{"id": "2602.03654", "pdf": "https://arxiv.org/pdf/2602.03654", "abs": "https://arxiv.org/abs/2602.03654", "authors": ["Su Yang", "Weiqi Chu", "Panayotis G. Kevrekidis"], "title": "Noisy nonlocal aggregation model with gradient flow structures", "categories": ["nlin.AO", "math.NA", "physics.soc-ph"], "comment": "15 pages; 4 figures", "summary": "Interacting particle systems provide a fundamental framework for modeling collective behavior in biological, social, and physical systems. In many applications, stochastic perturbations are essential for capturing environmental variability and individual uncertainty, yet their impact on long-term dynamics and equilibrium structure remains incompletely understood, particularly in the presence of nonlocal interactions. We investigate a stochastic interacting particle system governed by potential-driven interactions and its continuum density formulation in the large-population limit. We introduce an energy functional and show that the macroscopic density evolution has a gradient-flow structure in the Wasserstein-2 space. The associated variational framework yields equilibrium states through constrained energy minimization and illustrates how noise regulates the density and mitigates singular concentration. We demonstrate the connection between microscopic and macroscopic descriptions through numerical examples in one and two dimensions. Within the variational framework, we compute energy minimizers and perform a linear stability analysis. The numerical results show that the stable minimizers agree with the long-time dynamics of the macroscopic density model.", "AI": {"tldr": "The paper studies stochastic interacting particle systems with potential-driven interactions, showing their continuum limit has gradient-flow structure in Wasserstein space, and analyzes how noise regulates density and prevents singular concentrations.", "motivation": "Stochastic perturbations are essential for capturing environmental variability and individual uncertainty in interacting particle systems, but their impact on long-term dynamics and equilibrium structure remains incompletely understood, especially with nonlocal interactions.", "method": "Investigates a stochastic interacting particle system with potential-driven interactions and its continuum density formulation in the large-population limit. Introduces an energy functional and shows macroscopic density evolution has gradient-flow structure in Wasserstein-2 space. Uses variational framework for equilibrium states through constrained energy minimization.", "result": "Demonstrates connection between microscopic and macroscopic descriptions through numerical examples in 1D and 2D. Computes energy minimizers and performs linear stability analysis. Shows stable minimizers agree with long-time dynamics of macroscopic density model, illustrating how noise regulates density and mitigates singular concentration.", "conclusion": "The variational framework provides a rigorous connection between stochastic particle systems and their continuum limits, showing how noise influences equilibrium structure and prevents singular concentrations in nonlocal interacting systems."}}
{"id": "2602.03559", "pdf": "https://arxiv.org/pdf/2602.03559", "abs": "https://arxiv.org/abs/2602.03559", "authors": ["Tao Feng", "Minbo Yang", "Xianmei Zhou"], "title": "Asymptotic behavior of solutions to a planar Hartree equation with isolated singularities", "categories": ["math.AP"], "comment": null, "summary": "In this paper we investigate the isolated singularities of the Hartree type equation\n  \\begin{equation*}\n  -\u0394u (x)= \\left(\\frac{1}{|x|^\u03b1}*e^u\\right)e^{u(x)}\\quad \\text{in } B_{1}\\setminus\\{0\\} ,\n  \\end{equation*}\n  where $\u03b1>0$, $\\displaystyle \\frac{1}{|x|^\u03b1}*e^u\\triangleq\\int_{B_{1} \\setminus \\{0\\}}\\frac{e^u(y)}{|x-y|^\u03b1}dy$, and the punctured ball $B_{1}\\setminus\\{0\\}\\subset \\mathbb{R}^2$. Under the finite total curvature condition, by establishing a representation formula for singular solutions, we obtain the asymptotic behavior of the solutions near the origin. We also extend this asymptotic behavior results to the case with a general non-negative coefficient $K(x)$, and to the higher-order Hartree-type equations in any dimension $n \\geq 3$.", "AI": {"tldr": "This paper studies isolated singularities of Hartree-type equations, establishing representation formulas and asymptotic behavior for singular solutions under finite total curvature conditions, with extensions to cases with general coefficients and higher dimensions.", "motivation": "To understand the behavior of singular solutions to Hartree-type equations near isolated singularities, particularly in the context of finite total curvature conditions, which is important for analyzing nonlinear elliptic equations with nonlocal interactions.", "method": "Establish representation formulas for singular solutions of Hartree-type equations, use finite total curvature condition to derive asymptotic behavior near singularities, and extend results to cases with general non-negative coefficients and higher-order equations in higher dimensions.", "result": "Obtained precise asymptotic behavior of solutions near isolated singularities under finite total curvature condition, with results applicable to both the basic Hartree equation and its generalizations with coefficients and higher-order versions.", "conclusion": "The paper successfully characterizes singular behavior of Hartree-type equations, providing representation formulas and asymptotic analysis that extend to more general settings, contributing to the understanding of nonlocal elliptic equations with singularities."}}
{"id": "2602.03682", "pdf": "https://arxiv.org/pdf/2602.03682", "abs": "https://arxiv.org/abs/2602.03682", "authors": ["Pierre Agui\u00e9", "Mathieu Even", "Laurent Massouli\u00e9"], "title": "Improved Analysis of the Accelerated Noisy Power Method with Applications to Decentralized PCA", "categories": ["stat.ML", "cs.DC", "cs.LG", "math.NA"], "comment": null, "summary": "We analyze the Accelerated Noisy Power Method, an algorithm for Principal Component Analysis in the setting where only inexact matrix-vector products are available, which can arise for instance in decentralized PCA. While previous works have established that acceleration can improve convergence rates compared to the standard Noisy Power Method, these guarantees require overly restrictive upper bounds on the magnitude of the perturbations, limiting their practical applicability. We provide an improved analysis of this algorithm, which preserves the accelerated convergence rate under much milder conditions on the perturbations. We show that our new analysis is worst-case optimal, in the sense that the convergence rate cannot be improved, and that the noise conditions we derive cannot be relaxed without sacrificing convergence guarantees. We demonstrate the practical relevance of our results by deriving an accelerated algorithm for decentralized PCA, which has similar communication costs to non-accelerated methods. To our knowledge, this is the first decentralized algorithm for PCA with provably accelerated convergence.", "AI": {"tldr": "Improved analysis of Accelerated Noisy Power Method for PCA shows accelerated convergence under milder noise conditions, with worst-case optimal guarantees and practical application to decentralized PCA.", "motivation": "Previous analyses of accelerated PCA algorithms require overly restrictive noise bounds that limit practical applicability, especially in decentralized settings where only inexact matrix-vector products are available.", "method": "Improved theoretical analysis of the Accelerated Noisy Power Method algorithm, establishing worst-case optimal convergence rates under milder perturbation conditions compared to prior work.", "result": "The new analysis preserves accelerated convergence rates under much less restrictive noise conditions, proves worst-case optimality of the convergence rate, and enables the first provably accelerated decentralized PCA algorithm with similar communication costs to non-accelerated methods.", "conclusion": "The work provides practical accelerated PCA algorithms for noisy settings like decentralized PCA, with optimal theoretical guarantees and practical relevance demonstrated through the first provably accelerated decentralized PCA algorithm."}}
{"id": "2602.03575", "pdf": "https://arxiv.org/pdf/2602.03575", "abs": "https://arxiv.org/abs/2602.03575", "authors": ["Timoth\u00e9e Crin-Barat", "Zihao Song"], "title": "The compressible Euler system with damping in hybrid Besov spaces: global well-posedness and relaxation limit", "categories": ["math.AP"], "comment": null, "summary": "We investigate the global well-posedness of the compressible Euler system with damping in Rd (d\\geq1) and its relaxation limit toward the porous medium equation. In [12], the first author and Danchin studied these two problems in hybrid Besov spaces, where the high-frequency components of the solution are bounded in L2-based norms, while the low-frequency components are controlled in Lp-based norms with p\\in[2,\\max{4,\\frac{2d}{d-2}}]. Motivated by the observation that the limit system is well-posed in Lp-based spaces for p\\in[2, \\infty), we extend the low-frequency analysis to this full range, thereby providing a more unified framework for studying such relaxation limits.\n  The core of our proof consists in establishing refined product and commutator estimates describing sharply the interactions between the high, medium, and low-frequency regimes. A key observation underlying our analysis is that the product of two functions localized at low frequencies generates only interactions between low and medium frequencies, never purely high-frequency ones. Consequently, for a suitable choice of frequency threshold, the high-frequency projection of the product of two functions localized low frequencies vanishes.", "AI": {"tldr": "Extends low-frequency analysis for compressible Euler system with damping to full Lp range (p\u2208[2,\u221e)), providing unified framework for relaxation limits to porous medium equation.", "motivation": "Previous work studied compressible Euler system with damping in hybrid Besov spaces with limited p-range for low frequencies. Since the limit system (porous medium equation) is well-posed in Lp-based spaces for p\u2208[2,\u221e), the authors aim to extend low-frequency analysis to this full range for a more unified framework.", "method": "Establish refined product and commutator estimates describing interactions between high, medium, and low-frequency regimes. Key observation: product of two low-frequency functions generates only low and medium frequency interactions, never purely high-frequency ones. This allows suitable frequency threshold where high-frequency projection of low-frequency products vanishes.", "result": "Provides extended low-frequency analysis to full Lp range p\u2208[2,\u221e), offering more unified framework for studying relaxation limits of compressible Euler system with damping toward porous medium equation.", "conclusion": "The paper successfully extends the analysis of compressible Euler system with damping to include full Lp range for low frequencies, creating a more comprehensive framework for studying relaxation limits to porous medium equation through refined frequency interaction estimates."}}
{"id": "2602.03744", "pdf": "https://arxiv.org/pdf/2602.03744", "abs": "https://arxiv.org/abs/2602.03744", "authors": ["Maike Meier", "Lorenzo Lazzarino", "Boris Shustin", "Hussam Al Daas", "Paul Quinn"], "title": "Reducing acquisition time and radiation damage: data-driven subsampling for spectro-microscopy", "categories": ["physics.med-ph", "math.NA", "physics.optics"], "comment": null, "summary": "Spectro-microscopy is an experimental technique which can be used to observe spatial variations in chemical state and changes in chemical state over time or under experimental conditions. As a result it has broad applications across areas such as energy materials, catalysis, environmental science and biological samples. However, the technique is often limited by factors such as long acquisition times and radiation damage. We present two measurement strategies that allow for significantly shorter experiment times and total doses applied. The strategies are based on taking only a small subset of all the measurements (e.g. sparse acquisition or subsampling), and then computationally reconstructing all unobserved measurements using mathematical techniques. The methods are data-driven, using spectral and spatial importance subsampling distributions to identify important measurements. As a result, taking as little as 4-6\\% of the measurements is sufficient to capture the same information as in a conventional scan.", "AI": {"tldr": "Two data-driven measurement strategies using sparse acquisition and computational reconstruction enable spectro-microscopy with only 4-6% of measurements, reducing experiment time and radiation damage.", "motivation": "Spectro-microscopy is limited by long acquisition times and radiation damage, which restricts its applications in energy materials, catalysis, environmental science, and biological samples.", "method": "Data-driven strategies using sparse acquisition/subsampling with spectral and spatial importance distributions, followed by computational reconstruction of unobserved measurements.", "result": "Taking only 4-6% of measurements captures the same information as conventional scans, significantly reducing experiment times and total radiation doses.", "conclusion": "The proposed measurement strategies overcome key limitations of spectro-microscopy, enabling faster experiments with reduced radiation damage while maintaining information quality."}}
{"id": "2602.03644", "pdf": "https://arxiv.org/pdf/2602.03644", "abs": "https://arxiv.org/abs/2602.03644", "authors": ["Luca Rossi"], "title": "On the criticality and the principal eigenvalue of almost periodic elliptic operators", "categories": ["math.AP"], "comment": null, "summary": "We review the notion and the properties of the generalised \\pe\\ for elliptic operators in unbounded domains, and we relate it with the criticality theory. We focus on operators with almost periodic coefficients. We present a Liouville-type result in dimension $N\\leq2$. Next, we show with a counter-example that criticality is not equivalent to the existence of an almost periodic principal eigenvalue, even for self-adjoint operators. Finally, we exhibit an almost periodic operator which is subcritical but which admits a critical limit operator. This is a manifestation of the instability character of the criticality property in the almost periodic setting.", "AI": {"tldr": "The paper examines generalized principal eigenvalues for elliptic operators in unbounded domains with almost periodic coefficients, showing criticality theory connections, Liouville-type results for low dimensions, and counterexamples demonstrating instability in almost periodic settings.", "motivation": "To understand the relationship between generalized principal eigenvalues and criticality theory for elliptic operators with almost periodic coefficients in unbounded domains, and to investigate the stability properties of criticality in almost periodic settings.", "method": "Theoretical analysis of elliptic operators with almost periodic coefficients, using criticality theory framework, proving Liouville-type results for dimensions N\u22642, and constructing counterexamples to demonstrate properties.", "result": "1) Liouville-type result established for dimensions N\u22642; 2) Counterexample shows criticality is not equivalent to existence of almost periodic principal eigenvalue, even for self-adjoint operators; 3) Example of almost periodic operator that is subcritical but admits critical limit operator, demonstrating instability of criticality property.", "conclusion": "Criticality theory for elliptic operators with almost periodic coefficients exhibits instability properties, with criticality not equivalent to existence of almost periodic principal eigenvalues, and subcritical operators can have critical limit operators, highlighting the complex behavior in almost periodic settings."}}
{"id": "2602.03802", "pdf": "https://arxiv.org/pdf/2602.03802", "abs": "https://arxiv.org/abs/2602.03802", "authors": ["Grigory Begunov", "Alexander Tyurin"], "title": "Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods", "categories": ["cs.DC", "cs.AI", "math.NA", "math.OC"], "comment": null, "summary": "Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.", "AI": {"tldr": "Synchronous SGD methods (including m-Synchronous SGD) are surprisingly optimal for many heterogeneous computation scenarios, despite recent focus on asynchronous approaches.", "motivation": "To challenge the prevailing trend toward asynchronous optimization methods by re-evaluating synchronous approaches in modern heterogeneous computing environments where worker computation times vary and participation may be adversarial.", "method": "Theoretical analysis of Synchronous SGD and its robust variant m-Synchronous SGD under two challenging conditions: random computation times and adversarial partial participation of workers.", "result": "Synchronous methods achieve nearly optimal time complexity (up to logarithmic factors) in many practical heterogeneous computation regimes, making them sufficient for many modern scenarios despite not being universal solutions.", "conclusion": "While asynchronous methods may be necessary for some tasks, synchronous optimization methods are surprisingly effective and often optimal for many heterogeneous computation scenarios, challenging the assumption that asynchronous approaches are always superior."}}
{"id": "2602.03715", "pdf": "https://arxiv.org/pdf/2602.03715", "abs": "https://arxiv.org/abs/2602.03715", "authors": ["J\u00e9r\u00f4me Le Rousseau", "Jeffrey B. Rauch"], "title": "Timelike curves: homotopies and domain of determinacy", "categories": ["math.AP"], "comment": "43 pages, 35 figures", "summary": "This paper studies domains of determination of linear strictly hyperbolic second order operators $P$. For an open set $\\mathcal O$, a set $Z$ is a domain of determination when the values of solutions of the differential equation $Pu=0$ are determined on $Z$ by their values in $\\mathcal O$. Fritz John's global H\u00f6lmgren theorem implies that points that can be reached by deformations of noncharacteristic hypersufaces with initial surface and boundaries in $\\mathcal O$ belong to a domain of determination provided that local uniqueness holds at noncharacteristic surfaces. Using spacelike hypersurfaces yields sharp finite speed results whose domains of determination are described in terms of influence curves that never exceed the local speed of propagation. This paper studies deformations of noncharacteristic nonspacelike hypersurfaces. We prove that points reachable by (repeated) deformations by noncharacteristic nonspacelike hypersurfaces coincide exactly with the set of points reachable by (repeated) homotopies of timelike arcs whose initial curves and endpoints belong to $\\mathcal O$. When the set $\\mathcal O$ is a small neighborhood of a forward timelike arc connecting $a$ to $b$, a natural candidate for $Z$ is the intesection of the future of $a$ with the past of $b$. This candidate is exact for D'Alembert's equation. We prove that it is also exact when $a,b$ are points close together on a fixed timelike arc. The timelike homotopy criterion fuels the construction of surprising examples for which the domain of determination is strictly larger (resp. strictly smaller) than the future-intersect-past candidate.", "AI": {"tldr": "This paper studies domains of determination for linear strictly hyperbolic second-order operators, characterizing which points' solution values are determined by data in an open set O. It extends beyond spacelike hypersurfaces to noncharacteristic nonspacelike ones, establishing equivalence between deformation reachability and timelike arc homotopy reachability.", "motivation": "To understand and characterize domains of determination for hyperbolic PDEs beyond classical finite speed results, particularly exploring cases where domains differ from the intuitive \"future of a intersect past of b\" candidate that works for wave equations.", "method": "The paper studies deformations of noncharacteristic nonspacelike hypersurfaces and proves equivalence between points reachable by such deformations and points reachable by homotopies of timelike arcs. It constructs examples using the timelike homotopy criterion to demonstrate domains that differ from the future-intersect-past candidate.", "result": "Establishes exact equivalence between deformation reachability and timelike arc homotopy reachability. Shows the future-intersect-past candidate is exact for D'Alembert's equation and for nearby points on a timelike arc, but constructs surprising examples where domains are strictly larger or smaller than this candidate.", "conclusion": "The timelike homotopy criterion provides a precise characterization of domains of determination for hyperbolic operators, revealing richer structure than classical finite speed results and showing that intuitive geometric candidates like future-intersect-past are not universally valid."}}
{"id": "2602.03768", "pdf": "https://arxiv.org/pdf/2602.03768", "abs": "https://arxiv.org/abs/2602.03768", "authors": ["Tatsuya Hosono"], "title": "Global existence for the fully parabolic Keller--Segel system with critical mass on the plane", "categories": ["math.AP"], "comment": null, "summary": "We study the global existence of solutions to the Cauchy problem for the two-dimensional fully parabolic Keller--Segel system at the critical mass. It is known that global-in-time existence holds for initial data with critical mass under radial symmetry or suitable moment conditions, whereas the behavior of general solutions in the critical regime remains delicate. In this paper, we establish global-in-time existence for general initial data with critical mass, without imposing any symmetry or moment assumptions. The proof relies on the construction of a reconstructed Lyapunov functional, combined with refined regularity estimates for the associated dissipative terms, which enable us to control the solution dynamics in the critical regime.", "AI": {"tldr": "Global existence of solutions to 2D fully parabolic Keller-Segel system at critical mass is proven for general initial data without symmetry or moment assumptions.", "motivation": "Previous results required radial symmetry or moment conditions for global existence at critical mass; this paper aims to remove these restrictions and handle general initial data.", "method": "Constructs a reconstructed Lyapunov functional combined with refined regularity estimates for dissipative terms to control solution dynamics in critical regime.", "result": "Establishes global-in-time existence for general initial data with critical mass, without imposing any symmetry or moment assumptions.", "conclusion": "The paper successfully extends global existence results for the critical Keller-Segel system to general initial data, overcoming previous limitations requiring symmetry or moment conditions."}}
{"id": "2602.02997", "pdf": "https://arxiv.org/pdf/2602.02997", "abs": "https://arxiv.org/abs/2602.02997", "authors": ["Nick Edelen", "Luis Atzin Franco Reyna", "Paul Minter"], "title": "Entire area-minimizing surfaces in R^4 are algebraic", "categories": ["math.DG", "math.AP"], "comment": null, "summary": "We classify entire 2-dimensional area-minimizing or stable surfaces in R^4 with quadratic area growth as algebraic, cut out by a finite union of holomorphic polynomials whose collective degrees are controlled by the density at infinity. As a consequence, we obtain bounds on the singular set size and genus in terms of the density at infinity.", "AI": {"tldr": "Entire 2D area-minimizing/stable surfaces in R^4 with quadratic area growth are algebraic, defined by holomorphic polynomials with degree bounds determined by density at infinity.", "motivation": "To understand the structure and classification of entire minimal surfaces in higher dimensions, particularly establishing algebraic properties for area-minimizing surfaces with controlled growth.", "method": "Analytic and geometric methods for minimal surfaces, using density at infinity to control polynomial degrees, likely involving complex analysis techniques for surfaces in R^4.", "result": "Proves that such surfaces are algebraic (finite union of holomorphic polynomials) with degree bounds from density at infinity, leading to bounds on singular set size and genus.", "conclusion": "Entire area-minimizing surfaces in R^4 with quadratic growth are algebraic, providing structural control and quantitative bounds on geometric invariants via density at infinity."}}
{"id": "2602.03241", "pdf": "https://arxiv.org/pdf/2602.03241", "abs": "https://arxiv.org/abs/2602.03241", "authors": ["Jeffrey S. Case", "Yuya Takeuchi"], "title": "Non-homothetic complete periodic contact forms with constant Tanaka--Webster scalar curvature", "categories": ["math.DG", "math.AP", "math.CV"], "comment": "17 pages", "summary": "We study the existence problem for complete contact forms with constant Tanaka--Webster scalar curvature on non-compact strictly pseudoconvex CR manifolds. We prove that, under mild assumptions, the universal cover of a compact strictly pseudoconvex CR manifold admits infinitely many non-homothetic such contact forms whenever its fundamental group has infinite profinite completion. As applications, we treat complements of real or complex spheres in the standard CR sphere, as well as circle bundles over compact K\u00e4hler manifolds and the boundary of a Reinhardt domain.", "AI": {"tldr": "The paper studies existence of complete contact forms with constant Tanaka-Webster scalar curvature on non-compact strictly pseudoconvex CR manifolds, proving existence under certain conditions and applying results to various geometric settings.", "motivation": "To understand the existence problem for complete contact forms with constant Tanaka-Webster scalar curvature on non-compact strictly pseudoconvex CR manifolds, which is an important problem in CR geometry and contact geometry.", "method": "The authors prove that under mild assumptions, the universal cover of a compact strictly pseudoconvex CR manifold admits infinitely many non-homothetic contact forms with constant Tanaka-Webster scalar curvature whenever its fundamental group has infinite profinite completion.", "result": "Existence of infinitely many non-homothetic complete contact forms with constant Tanaka-Webster scalar curvature is established for various geometric settings including complements of spheres in the standard CR sphere, circle bundles over compact K\u00e4hler manifolds, and boundaries of Reinhardt domains.", "conclusion": "The paper successfully addresses the existence problem for constant Tanaka-Webster scalar curvature contact forms on non-compact CR manifolds, providing both general theoretical results and concrete applications to important geometric examples."}}
