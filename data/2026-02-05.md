<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 23]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [nucl-th](#nucl-th) [Total: 1]
- [cond-mat.quant-gas](#cond-mat.quant-gas) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Efficient Explicit Taylor ODE Integrators with Symbolic-Numeric Computing](https://arxiv.org/abs/2602.04086)
*Songchen Tan,Oscar Smith,Christopher Rackauckas*

Main category: math.NA

TL;DR: New Julia-based Taylor series ODE solver using compiler-enhanced automatic differentiation outperforms traditional Runge-Kutta methods with adaptive time/order control.


<details>
  <summary>Details</summary>
Motivation: Taylor series methods show renewed potential for solving non-stiff ODEs due to advances in compiler-enhanced techniques for computing high-order derivatives, offering an alternative to traditional explicit Runge-Kutta methods.

Method: Developed a Julia implementation with: (1) general-purpose higher-order automatic differentiation engine with low overhead, (2) combined symbolic-numeric approach to generate code for recursively computing Taylor polynomial of ODE solution, and (3) comprehensive adaptive time and order algorithm.

Result: The implementation achieves better runtime performance than standard explicit Runge-Kutta methods while maintaining transparent compiler-based tooling that requires no interface changes from users. The adaptive algorithm makes it efficient across diverse dynamics.

Conclusion: For compiler-compatible codes, these Taylor series integrators are more efficient and robust than traditional explicit Runge-Kutta methods, offering a versatile solution for non-stiff ODE problems.

Abstract: Taylor series methods show a newfound promise for the solution of non-stiff ordinary differential equations (ODEs) given the rise of new compiler-enhanced techniques for calculating high order derivatives. In this paper we detail a new Julia-based implementation that
  has two important techniques: (1) a general purpose higher-order automatic differentiation engine for derivative evaluation with low overhead; (2) a combined symbolic-numeric approach to generate code for recursively computing the Taylor polynomial of the ODE solution. We demonstrate that the resulting software's compiler-based tooling is transparent to the user, requiring no changes from interfaces required to use standard explicit Runge-Kutta methods, while achieving better run time performance. In addition, we also developed a comprehensive adaptive time and order algorithm that uses different step size and polynomial degree across the integration period, which makes this implementation more efficient and versatile in a broad range of dynamics. We show that for codes compatible with compiler transformations, these integrators are more efficient and robust than the traditionally used explicit Runge-Kutta methods.

</details>


### [2] [A frequency-domain method to inverse moving source problem with unknown radiating moment](https://arxiv.org/abs/2602.04207)
*Guanqiu Ma,Hongxia Guo,Guanghui Hu*

Main category: math.NA

TL;DR: Multi-frequency factorization method for imaging time-dependent sources using far-field data from two opposite directions to recover spatial support and excitation instants.


<details>
  <summary>Details</summary>
Motivation: To develop an effective method for imaging time-dependent sources that can recover both the spatial support (where the source is located) and the temporal excitation instants (when it activates) from limited far-field measurement data.

Method: Uses a multi-frequency factorization approach with far-field data from two opposite directions. Constructs indicator functions defined over spatial and temporal sampling variables. Establishes computational criterion to characterize pulse moments and the narrowest strip enclosing source support perpendicular to observation direction.

Result: Method permits recovery of Θ-convex support domain from sparse observation directions. Uniqueness in determining convex hull of support and excitation instants is established using all observation directions. Effectiveness demonstrated through comprehensive 2D and 3D numerical simulations.

Conclusion: The proposed multi-frequency factorization method provides an effective and feasible approach for imaging time-dependent sources, enabling recovery of both spatial support and temporal excitation characteristics from limited far-field measurements.

Abstract: This paper introduces a multi-frequency factorization method for imaging a time-dependent source, specifically to recover its spatial support and the associated excitation instants. Using far-field data from two opposite directions, we establish a computational criterion that characterizes both the unknown pulse moments and the narrowest strip (perpendicular to the direction) enclosing the source support. Central to our inversion scheme is the construction of indicator functions, defined pointwise over the spatial and temporal sampling variables. The proposed inversion scheme permits the recovery of the $Θ$-convex support domain from far-field data at sparse observation directions. Uniqueness in determining the convex hull of the support and the excitation instants-using all observation directions-is also established as a direct consequence of the factorization method. The effectiveness and feasibility of the approach are examined through comprehensive numerical simulations in two and three dimensions.

</details>


### [3] [Towards $C^0$ finite element methods for fourth-order elliptic equation. Part I: general boundary conditions](https://arxiv.org/abs/2602.04235)
*Xihao Zhang,Hengguang Li,Nianyu Yi,Peimeng Yin*

Main category: math.NA

TL;DR: A C^0 finite element method for biharmonic equations on polygonal domains with general boundary conditions, using a modified mixed formulation that decomposes into multiple Poisson equations based on domain geometry and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To develop effective C^0 finite element methods for fourth-order elliptic equations (specifically biharmonic equations) on polygonal domains that can handle general boundary conditions including Navier, Neumann, and mixed conditions, overcoming limitations of naive mixed formulations.

Method: Proposes a modified mixed formulation that decomposes the biharmonic equation into a system of Poisson equations, where the number of equations depends on both the largest interior angle and boundary conditions on adjacent sides. Develops C^0 finite element algorithms and establishes rigorous error estimates.

Result: The proposed approach guarantees convergence to the true solution for arbitrary polygonal domains and general boundary conditions, unlike the naive mixed formulation which only involves two Poisson problems. Numerical experiments demonstrate well-posedness and effectiveness.

Conclusion: The modified mixed formulation provides a robust C^0 finite element method for biharmonic equations on polygonal domains that works for various boundary conditions, with proven convergence properties and practical effectiveness demonstrated through numerical experiments.

Abstract: This paper is part of a series developing $C^0$ finite element methods for fourth-order elliptic equations on polygonal domains. Here, we investigate how boundary conditions influence the design of effective $C^0$ schemes, specifically focusing on equations without lower-order terms, namely the biharmonic equation. We propose a modified mixed formulation that decomposes the problem into a system of Poisson equations, where the number of equations depends on both the largest interior angle and the boundary conditions on its two adjacent sides. In contrast to the naive mixed formulation, which involves only two Poisson problems, the proposed approach guarantees convergence to the true solution for arbitrary polygonal domains and general boundary conditions, including Navier, Neumann, and mixed boundary conditions. $C^0$ finite element algorithms are developed, rigorous error estimates are established, and numerical experiments are presented to demonstrate the well-posedness and effectiveness of the proposed method.

</details>


### [4] [Balancing Inexactness in Mixed Precision Matrix Computations](https://arxiv.org/abs/2602.04348)
*Erin Claire Carson*

Main category: math.NA

TL;DR: The paper discusses how to safely exploit mixed precision computation in scientific codes by balancing numerical errors with other sources of inexactness to improve performance without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Emerging high-performance architectures increasingly support multiple precisions and number formats. Computational scientists want to determine where they can safely use mixed precision computation to improve performance, particularly in cases where other sources of inexactness (like discretization error, measurement error, or algorithmic approximation error) already exist.

Method: Analyzing the interaction between different sources of inexactness to determine how precisions of various computations should be chosen to "balance" errors. The approach examines cases where low precision is natural due to existing error sources.

Result: The paper presents recent examples demonstrating the potential for using mixed precision in numerical linear algebra and matrix computations, showing how performance can be improved without noticeable accuracy degradation.

Conclusion: Mixed precision computation can be effectively used in scientific computing by strategically balancing numerical errors with other sources of inexactness, particularly in numerical linear algebra and matrix computations, leading to performance improvements without compromising accuracy.

Abstract: Support for arithmetic in multiple precisions and number formats is becoming increasingly common in emerging high-performance architectures. From a computational scientist's perspective, our goal is to determine how and where we can safely exploit mixed precision computation in our codes to improve performance. One case where the use of low precision is natural, common in computational science, is when there are already other significant sources of ``inexactness'' present, e.g., discretization error, measurement error, or algorithmic approximation error. In such instances, analyzing the interaction of these different sources of inexactness can give insight into how the precisions of various computations should be chosen in order to ``balance'' the errors, potentially improving performance without a noticeable decrease in accuracy. We present a few recent examples of this approach which demonstrate the potential for the use of mixed precision in numerical linear algebra and matrix computations.

</details>


### [5] [On the pure traction problem of linear elasticity: a regularized formulation and its robust approximation](https://arxiv.org/abs/2602.04359)
*Ahsan Kaleem,Cristian Gebhardt,Ignacio Romero*

Main category: math.NA

TL;DR: A new finite element method for pure traction elasticity problems using regularization to handle solution uniqueness issues and loading incompatibilities.


<details>
  <summary>Details</summary>
Motivation: Pure traction problems in elasticity have non-unique solutions (unique only up to rigid body motions), and existing finite element approaches require complex boundary conditions or global constraints that are neither simple nor computationally efficient.

Method: Proposes a regularized form of the pure traction problem with unique solutions that converge to the minimal norm solution. Uses a predictor-corrector finite element formulation to handle loading incompatibilities when domain approximation makes loading non-equilibrated.

Result: The regularized approach provides unique solutions that converge to the original minimal norm solution, can be approximated with standard finite elements without additional degrees of freedom, and handles loading incompatibilities effectively.

Conclusion: The proposed method overcomes limitations of existing approaches by providing a simple, computationally efficient solution for pure traction elasticity problems that handles both solution uniqueness and loading compatibility issues.

Abstract: The pure traction problem of elasticity appears frequently in engineering applications, and its complexity stems from the fact that its solution is unique only up to (infinitesimal) rigid body motions. When finite elements are employed to approximate this problem, one solution is typically singled out by applying carefully selected boundary conditions on the discrete model or by imposing global constraints on the deformation. However, neither of these strategies is both simple and computationally efficient. In this work, we propose a new approach to solving the pure traction problem that overcomes existing limitations. Our method builds on a regularized form of the problem whose solution is shown to be unique, converges to the original solution of minimal norm, and can be approximated with finite elements in a straightforward way, without additional degrees of freedom. Additionally, we analyze the situation in which the approximation of the solution domain renders the loading of the discretized problem non-equilibrated, making the problem ill-posed. In this case, we propose a regularized predictor--corrector finite element formulation that handles the incompatibilities of the loading, providing a solution that converges to that of the original Neumann problem as the mesh size and the regularizing parameter tend to zero. Numerical examples illustrate the effectiveness of the proposed approach for representative problems in mechanics where pure traction boundary conditions appear.

</details>


### [6] [A Priori and A Posteriori Error Identities for Vectorial Problems via Convex Duality](https://arxiv.org/abs/2602.04368)
*P. A. Gazca-Orozco,A. Kaltenbach*

Main category: math.NA

TL;DR: Extends convex duality error analysis from scalar to vectorial problems (Stokes/Navier-Lamé) with inhomogeneous mixed boundary conditions, deriving quasi-optimal error estimates for Crouzeix-Raviart discretization.


<details>
  <summary>Details</summary>
Motivation: Previous convex duality methods have been successful for scalar problems but haven't been extended to vectorial settings. The paper aims to apply these techniques to incompressible Stokes and Navier-Lamé equations with more realistic boundary conditions and loads.

Method: Uses convex duality principles and compatibility properties of Crouzeix-Raviart and Raviart-Thomas finite elements. Extends the framework to handle inhomogeneous mixed boundary conditions and loads in topological dual spaces.

Result: Derives error identities and estimates that enable quasi-optimal error estimates for Crouzeix-Raviart discretization with minimal regularity assumptions and no data oscillation terms.

Conclusion: Successfully extends convex duality error analysis to vectorial problems, providing rigorous error estimates for practical finite element discretizations of Stokes and Navier-Lamé equations under realistic conditions.

Abstract: Convex duality has been leveraged in recent years to derive a posteriori error estimates and identities for a wide range of non-linear and non-smooth scalar problems. By employing remarkable compatibility properties of the Crouzeix-Raviart and Raviart-Thomas elements, optimal convergence of non-conforming discretisations and flux reconstruction formulas have also been established. This paper aims to extend these results to the vectorial setting, focusing on the archetypical problems of incompressible Stokes and Navier-Lamé. Moreover, unlike most previous results, we consider inhomogeneous mixed boundary conditions and loads in the topological dual of the energy space. At the discrete level, we derive error identities and estimates that enable to prove quasi-optimal error estimates for a Crouzeix-Raviart discretisation with minimal regularity assumptions and no data oscillation terms.

</details>


### [7] [Randomized Projection Operators onto Piecewise Polynomial Spaces](https://arxiv.org/abs/2602.04490)
*Johannes Storn*

Main category: math.NA

TL;DR: Computable projection operators onto piecewise polynomial spaces using sampling and discrete least-squares approximations, with optimal approximation properties in L² and H⁻¹ spaces.


<details>
  <summary>Details</summary>
Motivation: To develop practical projection operators for piecewise polynomial spaces that can handle incomplete or rough data while maintaining optimal approximation properties, enabling effective finite element discretizations.

Method: Define projection operators via sampling and discrete least-squares polynomial approximations onto piecewise polynomial spaces.

Result: The resulting mappings exhibit (almost) optimal approximation properties in L² and H⁻¹ spaces, and serve as effective smoothers for incomplete or rough data.

Conclusion: These computable projection operators yield finite element discretizations with optimal convergence rates, providing a practical tool for numerical analysis with non-smooth data.

Abstract: We introduce computable projection operators onto piecewise polynomial spaces, defined via sampling and discrete least-squares polynomial approximations. The resulting mappings exhibit (almost) optimal approximation properties in $L^2$ and $H^{-1}$. As smoothers for incomplete or rough data, they yield computable finite element discretizations with optimal convergence rates.

</details>


### [8] [Domain decomposition methods and preconditioning strategies using generalized locally Toepltiz tools: proposals, analysis, and numerical validation](https://arxiv.org/abs/2602.04603)
*Abdessadek Rifqui,Ahmed Ratnani,Stefano Serra-Capizzano*

Main category: math.NA

TL;DR: Spectral analysis of additive and multiplicative Schwarz domain decomposition methods using GLT theory to study convergence behavior and transmission operators, with focus on restricted variants for parallel efficiency.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous spectral analysis of classical Schwarz preconditioning methods, understand their convergence behavior, and examine the effect of transmission operators within domain decomposition techniques.

Method: Employ generalized locally Toeplitz (GLT) sequence theory to analyze asymptotic spectral distribution of discretized operators from Schwarz iterations. Derive explicit GLT symbols for convergence factors of both additive and multiplicative Schwarz methods.

Result: Established unified framework for understanding spectral evolution with mesh refinement and overlap size. Derived explicit GLT symbols for convergence factors, enabling prediction of efficiency for block Jacobi/Gauss-Seidel and block additive/multiplicative Schwarz preconditioners.

Conclusion: GLT-based spectral approach provides systematic understanding of Schwarz methods' convergence behavior, deepens theoretical foundations, and establishes tools for analyzing future restricted/hybrid Schwarz variants through symbolic spectral analysis.

Abstract: In the current work we present a spectral analysis of the additive and multiplicative Schwarz methods within the framework of domain decomposition techniques, by investigating the spectral properties of these classical Schwarz preconditioning matrix-sequences, with emphasis on their convergence behavior and on the effect of transmission operators. In particular, after a general presentation of various options, we focus on restricted variants of the Schwarz methods aimed at improving parallel efficiency, while preserving their convergence features. In order to rigorously describe and analyze the convergence behavior, we employ the theory of generalized locally Toeplitz (GLT) sequences, which provides a robust framework for studying the asymptotic spectral distribution of the discretized operators arising from Schwarz iterations. By associating each operator sequence with the appropriate GLT symbol, we derive explicit expressions for the GLT symbols of the convergence factors, for both additive and multiplicative Schwarz methods. The GLT-based spectral approach offers a unified and systematic understanding of how the spectrum evolves with mesh refinement and overlap size (in the algebraic case). Our analysis not only deepens the theoretical understanding of classical Schwarz methods, but also establishes a foundation for examining future restricted or hybrid Schwarz variants using symbolic spectral tools. These results enable the prediction of the remarkable efficiency of block Jacobi/Gauss--Seidel and block additive/multiplicative Schwarz preconditioners for GLT sequences, as further illustrated through a wide choice of numerical experiments.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [Using wave packet decompositions to construct function spaces: a user guide](https://arxiv.org/abs/2602.03952)
*Pierre Portal*

Main category: math.AP

TL;DR: Survey paper on constructing function spaces for harmonic analysis of PDEs using wave packet decompositions as a unifying framework, with a new construction for Schrödinger operators Δ-V.


<details>
  <summary>Details</summary>
Motivation: To provide a unified conceptual framework for understanding various function spaces used in harmonic analysis of PDEs, showing how they can be systematically constructed through wave packet decompositions rather than as isolated examples.

Method: Frames function spaces as retracts of simple function spaces over phase space using projections associated with wave packet decompositions. Provides a "user guide" for choosing appropriate wave packet decompositions for specific PDEs and constructing corresponding function spaces.

Result: Surveys classical and recent constructions of function spaces, presents a unified framework, and introduces a new construction specifically adapted to Schrödinger operators of the form Δ-V for V≥0 (from upcoming joint work).

Conclusion: The wave packet decomposition approach provides a powerful, systematic method for constructing appropriate function spaces for studying PDEs, with the choice of decomposition tailored to the specific operator being studied.

Abstract: We survey the construction of a range of function spaces used in harmonic analysis of PDE, including classical results as well as recent developments. We frame these constructions in a common conceptual framework, where these function spaces arise as retracts of simple function spaces over phase space, through a projection associated with a wave packet decomposition. Finding appropriate function spaces to study a given PDE then consists in choosing a relevant wave packet decomposition. We provide a user guide to making such choices, and constructing the corresponding function spaces. This is done mostly by surveying recent constructions, but we also include a new construction, adapted to Schrödinger operators of the form $Δ- V$ for $V \geq 0$, as a sneak peek into upcoming joint work with Dorothee Frey, Andrew Morris, and Adam Sikora.

</details>


### [10] [A note on exterior stability of finite time singularity formation for nonlinear wave equations](https://arxiv.org/abs/2602.03963)
*Istvan Kadar,Lionor Kehrberger*

Main category: math.AP

TL;DR: The paper studies stability of spacetime regions outside wave singularity formations, establishing existence of solutions up to the Cauchy horizon for wave maps and power nonlinear wave equations with characteristic initial data.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of spacetime regions exterior to singularity formations in nonlinear wave equations, particularly whether solutions can be extended all the way to the Cauchy horizon given appropriate initial data.

Method: Uses characteristic initial data on backwards lightcone converging to singular background solution, applies coordinate transformation and scattering results from [KK25] that work with scaling-critical potentials. For wave maps, focuses on corotational symmetry class but sketches generalization.

Result: Establishes existence of solutions in region {t+r∈(0,v₁), t-r∈(-1,0)} for suitably small v₁, reaching the Cauchy horizon, under conjectural assumptions about initial data regularity inside the lightcone.

Conclusion: The exterior regions of Type I and Type II singularities for these wave equations can be stable, with solutions existing up to the Cauchy horizon given appropriate characteristic initial data, though full understanding depends on unproven interior behavior.

Abstract: We study the stability of the exterior of Type I and Type II singularity formation for the wave maps equation in $\mathbb{R}^{d+1}$ with $d\geq2$ and the power nonlinear wave equation in $\mathbb{R}^{d+1}$ with $d\geq3$: Given characteristic initial data on the backwards lightcone of the singularity $\mathcal{C}=\{t+r=0\}$ converging to the singular background solution along with suitable data on an outgoing cone, we establish existence in a region $\{t+r\in(0,v_1),t-r\in(-1,0)\}$ for some suitably small $v_1$, i.e. all the way to the Cauchy horizon. Our result hinges on a particular set of assumptions on the regularity properties of these initial data and is therefore conjectural on the behaviour inside the lightcone.
  The proof goes via a suitable change of coordinates and an application of the scattering result of [KK25], which, in particular, also applies to scaling-critical potentials.
  In the case of the wave maps equation, we only provide the proof in the corotational symmetry class, but we also sketch how to lift this restriction.

</details>


### [11] [Hotspot formation driven by temperature-dependent coefficients in one-dimensional thermoviscoelasticity](https://arxiv.org/abs/2602.03997)
*Michael Winkler*

Main category: math.AP

TL;DR: Paper studies thermoviscoelastic system with temperature-dependent parameters, discovers finite-time blow-up in temperature.


<details>
  <summary>Details</summary>
Motivation: To understand blow-up phenomena in generalized thermoviscoelastic systems with temperature-dependent viscosities and elastic stiffnesses, extending classical Kelvin-Voigt models.

Method: Analysis of two-component evolution system generalizing 1D thermoviscoelastic dynamics, with assumptions on growth of temperature-dependent parameters and initial data.

Result: Discovery of finite-time blow-up with respect to L^∞ norm in the temperature variable under suitable assumptions.

Conclusion: Temperature-dependent parameters in thermoviscoelastic systems can lead to finite-time blow-up in temperature, revealing important mathematical properties of these generalized models.

Abstract: This manuscript is concerned with a two-component evolution system generalizing the classical model for one-dimensional thermoviscoelastic dynamics in Kelvin-Voigt materials in the presence of temperature-dependent viscosities and elastic stiffnesses. Under suitable assumptions on the growth of these ingredients and on the initial data, the occurrence of finite-time blow-up with respect to the $L^\infty$ norm in the temperature variable is discovered.

</details>


### [12] [Large-data global solutions to a quasilinear model for viscuos acoustic wave propagation in a non-isothermal setting](https://arxiv.org/abs/2602.04001)
*Felix Meyer,Michael Winkler*

Main category: math.AP

TL;DR: The paper analyzes a thermoacoustic model with temperature-dependent elasticity, proving global existence of classical solutions under certain conditions on the elasticity function, and establishing convergence to equilibrium under additional parameter constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the global behavior of thermoacoustic models where mechanical energy converts to heat during wave propagation, particularly addressing whether solutions blow up in finite time or remain globally defined and converge to equilibrium.

Method: Mathematical analysis of a coupled PDE system describing acoustic wave propagation with temperature-dependent elastic parameters. Uses energy methods and functional analysis to prove global existence of classical solutions under specific conditions on the elasticity function γ, and establishes large-time stabilization using additional parameter constraints.

Result: 1) Under conditions γ>0, γ'≥0, and D·(γ+D)·γ'' + 2γγ'² ≤ 0, the problem admits globally defined classical solutions for all suitably regular initial data. 2) With additional constraint a|Ω|² ≤ π²γ(0)/(1+√(1+γ(0)/D)), all solutions stabilize toward spatially homogeneous equilibrium in the large time limit.

Conclusion: The paper provides sufficient conditions for global existence and convergence to equilibrium in thermoacoustic models with temperature-dependent elasticity, complementing previous results showing potential finite-time blowup for certain elasticity functions.

Abstract: The manuscript considers the model for conversion of mechanical energy into heat during acoustic wave propagation in the presence of temperature-dependent elastic parameters, as given by \[
  \left\{ \begin{array}{l}
  u_{tt} = (γ(Θ) u_{xt})_x + a (γ(Θ) u_x)_x, \\[1mm]
  Θ_t = DΘ_{xx} + γ(Θ) u_{xt}^2.
  \end{array} \right.
  \qquad \qquad (\star) \] It is firstly shown that when considered along with no-flux boundary conditions in an open bounded real interval $Ω$, under the assumption that $γ\in C^2([0,\infty))$ is such that $γ>0$ and $γ'\ge 0$ on $[0,\infty)$ as well as \[
  D\cdot (γ+D) \cdot γ'' + 2γγ'^2 \le 0
  \qquad \mbox{on } [0,\infty), \] for all suitably regular initial data this problem admits a globally defined classical solution. This complements recent findings in the literature, according to which ($\star$) may admit solutions blowing up in finite time whenever $γ$ is positive and nondecreasing on $[0,\infty)$ with $\int_0^\infty \frac{dξ}{γ(ξ)} < \infty$.
  Apart from that, it is found that if the additional assumption \[
  a|Ω|^2 \le \frac{π^2 γ(0)}{1+\sqrt{1+\frac{γ(0)}{D}}} \] is satisfied, the all these solutions stabilize toward some spatially homogeneous equilibrium in the large time limit.

</details>


### [13] [Local strong solutions in a quasilinear Moore-Gibson-Thompson type model for thermoviscoelastic evolution in a standard linear solid](https://arxiv.org/abs/2602.04005)
*Leander Claes,Michael Winkler*

Main category: math.AP

TL;DR: Local existence and uniqueness of solutions for a coupled PDE system modeling heat generation during acoustic wave propagation in viscoelastic media.


<details>
  <summary>Details</summary>
Motivation: The paper studies a simplified model for heat generation during acoustic wave propagation in one-dimensional viscoelastic media of standard linear solid type, addressing the mathematical analysis of the coupled thermo-mechanical system.

Method: The authors analyze a system of coupled PDEs: a third-order wave equation for displacement u coupled with a heat equation for temperature Θ. They work under assumptions D>0, α≥0, with smooth positive coefficients γ, γ̂, and non-negative Γ. They establish local existence and uniqueness in a strong solvability framework for a Neumann problem with regular initial data.

Result: The main result is a local existence and uniqueness theorem for solutions to the coupled system in a suitable strong solvability framework for the associated Neumann problem with sufficiently regular initial data.

Conclusion: The paper provides a rigorous mathematical foundation for the simplified thermo-acoustic model, establishing well-posedness under physically reasonable assumptions about the material parameters and initial conditions.

Abstract: This manuscript is concerned with the evolution system \[
  \left\{ \begin{array}{l}
  u_{ttt} + αu_{tt} = \big(γ(Θ) u_{xt}\big)_x + \big( \widehatγ(Θ) u_x\big)_x,
  Θ_t = D Θ_{xx} + Γ(Θ) u_{xt}^2,
  \end{array} \right. \] which arises as a simplified model for heat generation during acoustic wave propagation in a one-dimensional viscoelastic medium of standard linear solid type.
  Under the assumptions that $D>0$ and $α\ge 0$, and that $γ, \widehatγ$ and $Γ$ are sufficiently smooth with $γ>0, \widehatγ>0$ and $Γ\ge 0$ on $[0,\infty)$, for suitably regular initial data a statement on local existence and uniqueness of solutions in an associated Neumann problem is derived in a suitable framework of strong solvability.

</details>


### [14] [On a Mathematical Model Describing Chemotherapeutic Drug Treatment for Tumor Cells](https://arxiv.org/abs/2602.04025)
*Xiaoqin Liu,Hong-Ming Yin*

Main category: math.AP

TL;DR: The paper studies a semilinear parabolic PDE system modeling normal cells, tumor cells, immune cells, and chemotherapy with Allee effects, analyzing global existence, uniqueness, spatiotemporal dynamics, and optimal therapy scheduling.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a more realistic mathematical model of tumor-immune-chemotherapy interactions by incorporating strong Allee effects in normal and tumor tissue dynamics, which better captures biological phenomena like critical population thresholds for survival.

Method: Extends previous PDE model with Allee effects, establishes global existence/uniqueness of nonnegative weak solutions with L-infinity bounds, investigates spatiotemporal dynamics using implicit Crank Nicolson Backward Euler (CNBE) scheme, simulates in 2D heterogeneous tissue with multiple tumor peaks.

Result: Without treatment: rapid tumor invasion. With pulsed chemotherapy: significant tumor suppression. For fixed total dose: concentrated pulses produce stronger early tumor knockdown, while more frequent gentler pulses achieve comparable tumor control at invasive front while better preserving normal tissue over 4 weeks.

Conclusion: The model successfully captures tumor-immune-chemotherapy dynamics with Allee effects, revealing important trade-offs in therapy scheduling - concentrated pulses for early knockdown vs. gentler frequent pulses for better normal tissue preservation and long-term control.

Abstract: In this paper, we study a semilinear parabolic PDE system which describes the interaction of normal cells, tumor cells, immune cells, with a chemotherapeutic drug. The model extends the previous model with incorporating strong Allee affects in the normal-tissue and tumor dynamics. Under mild assumptions, we establish global-in-time existence and uniqueness of nonnegative weak solutions and derive L-infinity bounds for all time. We then investigate spatiotemporal dynamics of the model and therapy scheduling using an implicit Crank Nicolson Backward Euler (CNBE) scheme. Simulations in a heterogeneous two-dimensional space-dimensional tissue region with three tumor peaks indicate rapid tumor invasion without treatment and significant tumor suppression under pulsed chemotherapeutic treatment. Moreover, in a fixed total dose delivered within the treatment cycle, while keeping each injection duration fixed, concentrated pulses produce stronger early knock-down of tumor density, while more frequent but gentler pulses achieve comparable control of the tumor invasive front while better preserving normal tissue over a four-week period.

</details>


### [15] [The Existence, uniqueness, and regularity of weak solutions for a thermodynamically consistent two-phase flow model in porous media](https://arxiv.org/abs/2602.04175)
*Huangxin Chen,Jisheng Kou,Haitao Leng,Shuyu Sun,Hai Zhao*

Main category: math.AP

TL;DR: Existence, uniqueness, and regularity proofs for weak solutions of a thermodynamically consistent two-phase flow model in porous media using discrete approximations and analytical techniques.


<details>
  <summary>Details</summary>
Motivation: Thermodynamically consistent models for two-phase flow in porous media are important but require rigorous mathematical analysis to establish solution properties like existence, uniqueness, and regularity.

Method: Introduce fully implicit time semi-discrete and fully discrete approximations, use zeros of vector field theorem for discrete existence, apply weak convergence and energy stability for semi-discrete and continuous existence, use Grönwall inequality for uniqueness, and apply elliptic PDE regularity theory.

Result: Proved existence, uniqueness, and regularity of weak solutions for the thermodynamically consistent two-phase flow model with complete Neumann boundary conditions.

Conclusion: The paper provides rigorous mathematical foundations for a recent thermodynamically consistent two-phase flow model, establishing its well-posedness through systematic approximation and analytical techniques.

Abstract: Thermodynamically consistent models for two-phase flow in porous media have attracted significant attention in recent years. In this paper, we prove the existence, uniqueness and regularity of the weak solution to such a recent model proposed in [25,35]. To this end, firstly, we introduce a fully implicit time semi-discrete approximation and a fully discrete approximation for an appropriate weak formulation of the thermodynamically consistent model. Next, by using the zeros of a vector field theorem, we prove the existence of the weak solution for the fully discrete approximation. Then the existence of weak solutions for the fully implicit time semi-discrete approximation and the weak formulation of the model are derived by the weak convergence technique and the energy stability estimate. Subsequently, by the Gr{\" o}nwall inequality, we prove the uniqueness result under the smoothness assumption on the chemical potential. Finally, combined with the regularity theory of elliptic partial differential equations (PDE), the regularity of the weak solution for the model with complete Neumann boundary conditions is established.

</details>


### [16] [Existence and Spatial Decay of Forced Waves for the Fisher-KPP Equation with a Degenerate Shifting Environment](https://arxiv.org/abs/2602.04180)
*Zhibao Tang,Shi-Liang Wu,Yaping Wu*

Main category: math.AP

TL;DR: The paper studies forced waves in heterogeneous Fisher-KPP equations with moving environments, classifying solutions by their spatial decay rates and establishing existence, uniqueness, and multiplicity results based on integrability conditions.


<details>
  <summary>Details</summary>
Motivation: To understand forced waves in Fisher-KPP models with degenerate moving environments, addressing open problems about existence, uniqueness, multiplicity, and spatial decay rates when the environment decays at infinity.

Method: Uses ODE asymptotic analysis to classify local positive solutions near infinity, examining exponential vs. non-exponential decay solutions based on integrability conditions of $\mathrm{e}^{-\frac{1}{c}\int_{z_0}^z a(s)ds}$.

Result: For $c\in(0,2\sqrtα)$, unique exponentially decaying forced wave exists; it's either unique or minimal depending on integrability. In super-critical case, infinitely many non-exponentially decaying forced waves exist for any $c>0$, with maximal wave not in $L^1$.

Conclusion: Provides nearly complete answers to open problems about forced waves in Fisher-KPP models with degenerate moving environments, establishing comprehensive theory for existence, multiplicity, and spatial decay rates based on environmental decay conditions.

Abstract: This paper studies forced waves for the heterogeneous Fisher-KPP equation $u_t = u_{xx} + u(a(x-ct)-u)$, where $c>0$ and $a(z)>0$ satisfies $a(-\infty)=α>0=a(+\infty)$, $a'(z)\le0$ ($z\gg1$). Using ODE asymptotic analysis, we classify all local positive solutions near $z=+\infty$. Exponential decay solutions always exist; non-exponential decay solutions exist if and only if $\mathrm{e}^{-\frac{1}{c}\int_{z_0}^z a(s)ds}\in L^1$ (or equivalently, when $a(z)$ decays slower than a critical algebraic rate).
  We establish a complete existence, multiplicity and spatial decay theory for forced waves. For each $c\in(0,2\sqrtα)$, there exists a unique exponentially decaying forced wave. This wave is either the unique forced wave or the minimal forced wave, depending on the integrability condition. In the super-critical case $\mathrm{e}^{-\frac{1}{c}\int_{z_0}^z a(s)ds}\in L^1$, for any $c>0$ there exist infinitely many non-exponentially decaying forced waves. The maximal wave is not in $L^1$, and for nearly all such $a(z)$ we establish the existence, multiplicity and precise decay of these waves.
  These results provide nearly complete answers to open problems concerning the existence, uniqueness, multiplicity and spatial decay rates of forced waves in Fisher-KPP models with degenerate moving environments.

</details>


### [17] [Nonexistence of pure bubble type II blow-up solutions to energy critical wave equation in the 3D radial case](https://arxiv.org/abs/2602.04205)
*Ruipeng Shen*

Main category: math.AP

TL;DR: No pure bubble type II blow-up solutions exist for 3D radial focusing energy-critical wave equation - free wave component is always present in soliton resolution.


<details>
  <summary>Details</summary>
Motivation: To investigate the structure of type II blow-up solutions for the focusing energy-critical wave equation in 3D radial case, specifically examining whether pure bubble solutions (without free wave component) can exist according to the soliton resolution conjecture.

Method: Mathematical analysis of the focusing energy-critical wave equation in 3D radial case, building on the verified soliton resolution conjecture for radial case. The approach involves proving the non-existence of pure bubble type II blow-up solutions through rigorous mathematical arguments.

Result: The paper proves that there does not exist any pure bubble type II blow-up solutions. The free wave part is never zero in the soliton resolution of type II blow-up solutions, regardless of the number of bubbles present.

Conclusion: Type II blow-up solutions for the 3D radial focusing energy-critical wave equation always contain a non-zero free wave component in their soliton resolution decomposition, meaning pure bubble solutions cannot exist.

Abstract: In this work we consider the focusing, energy-critical wave equation in 3D radial case. According to the soliton resolution conjecture, which has been verified in the radial case, any type II blow-up solution decomposes into a superposition of several decoupled grounds states, a free wave and a small error, as time tends to the blow-up time. We prove that there does not exist any pure bubble type II blow up solutions. In other words, the free wave part is never zero in the soliton resolution of type II blow-up solutions, regardless of the bubble number.

</details>


### [18] [Global Regularity for Non-resistive or Non-viscous MHD System on the Torus](https://arxiv.org/abs/2602.04293)
*Quansen Jiu,Yaowei Xie,Zhihong Yan*

Main category: math.AP

TL;DR: Global well-posedness for incompressible MHD with either no magnetic diffusivity or no fluid viscosity, assuming initial magnetic fields are close to background field e_n. Novel energy estimates and commutator estimates developed, with improved regularity requirements and decay/growth bounds established.


<details>
  <summary>Details</summary>
Motivation: To establish global well-posedness for MHD systems with missing dissipation (either no magnetic diffusivity or no fluid viscosity), which are challenging due to potential blow-up mechanisms. The research aims to show that magnetic fields near background fields can provide enhanced dissipation and suppress blow-up.

Method: Developed novel time-weighted energy estimates and commutator estimates involving Riesz transforms in negative Sobolev spaces. Handled two distinct dissipation cases under different initial symmetry assumptions. Used Eulerian coordinates framework.

Result: 1) For 3D non-resistive case: improved regularity requirement from H^11 to H^(9/2+). 2) Established precise decay rates and growth bounds for velocity and magnetic field derivatives. 3) For 3D non-viscous case: first nonlinear stability result near background field e_3, contrasting with recent Euler blow-up results.

Conclusion: Magnetic fields near background fields provide enhanced dissipation and suppress potential blow-up mechanisms in non-viscous MHD systems under certain symmetry assumptions. The analysis shows fundamental differences between pure Euler equations and MHD with background magnetic fields.

Abstract: In this paper, we establish the global well-posedness of the incompressible magnetohydrodynamics (MHD) system on $n-$dimensional $(n\geq 2)$ periodic boxes with either no magnetic diffusivity (non-resistive case) or no fluid viscosity (non-viscous case) under assumption that initial magnetic fields are sufficiently close to the background magnetic field ${\bf e}_n=(0,\cdots,0,1)$. In Eulerian coordinates, we develop novel time-weighted energy estimates and commutator estimates involving Riesz transforms in negative Sobolev spaces to handle two distinct dissipation cases under different initial symmetry assumptions. The analysis becomes much more difficult and delicate in three- or higher-dimensional cases. In particular, for the three-dimensional and non-resistive case, compared with the regularity requirement proposed by Pan, Zhou and Zhu {\it [Arch. Ration. Mech. Anal. 2018]}, our result relaxes it from $H^{11}(\mathbb{T}^3)$ to $H^{\frac{9}{2}+}(\mathbb{T}^3)$. And we further establish precise decay rates and growth bounds for both $u(t)$ and $\partial_n(u(t),b(t))$ in Sobolev norms. For the three-dimensional and non-viscous case, we prove the first nonlinear stability result near the background field $\mathbf{e}_3 = (0,0,1)$. This sharply contrasts with the recent blow-up results on the 3D incompressible Euler equations by Elgindi {\it [Ann. Math. 2021]}, Chen-Hou {\it [Commun. Math. Phys. 2021]} and by Chen-Hou {\it [arXiv:2210.07191]}. Our results show that, under certain symmetry assumptions, magnetic fields near the background field provide enhanced dissipations and suppress potential blow-up mechanisms in non-viscous MHD system.

</details>


### [19] [On the Cauchy problem to the axially-symmetric solutions to the Navier-Stokes equations](https://arxiv.org/abs/2602.04377)
*Wiesław J. Grygierzec,Wojciech M. Zajączkowski*

Main category: math.AP

TL;DR: The paper proves global existence of regular solutions to the axisymmetric Navier-Stokes equations by establishing a global a priori estimate using separate analysis near and far from the axis of symmetry.


<details>
  <summary>Details</summary>
Motivation: To establish global existence of regular solutions for the axisymmetric Navier-Stokes equations, which is a long-standing open problem in fluid dynamics. The motivation is to overcome the singularity issues near the axis of symmetry that typically arise in axisymmetric formulations.

Method: The approach examines the Navier-Stokes equations separately near and far from the axis of symmetry. Near the axis, it uses energy estimates, L∞-estimates for swirl, H² and H³ estimates for the modified stream function, and expansions by Liu-Wang. Far from the axis, the estimates are simpler. The method establishes a global a priori estimate for vorticity components, then uses this to derive higher regularity estimates.

Result: The main result is the existence of global regular solutions to the Cauchy problem for axisymmetric Navier-Stokes equations. This is achieved through two key estimates: a global a priori estimate for vorticity components in energy norm, and a higher regularity estimate for velocity and pressure in Sobolev spaces.

Conclusion: The paper successfully proves global existence of regular solutions for axisymmetric Navier-Stokes equations by establishing crucial a priori estimates that overcome the singularity challenges near the axis of symmetry, combining local solution existence with global regularity estimates.

Abstract: We consider the Cauchy problem to the axisymmetric Navier-Stokes equations. To prove an existence of global regular solutions we examine the Navier-Stokes equations near the axis of symmetry and far from it separately. We derive only a global a priori estimate. To show it near the axis of symmetry we need the energy estimate, $L_\infty$-estimate for swirl, $H^2$ and $H^3$ estimates for the modified stream function (stream function divided by radius) and also expansions of velocity and modified stream function found by Liu-Wang. The estimate for solutions far from the axis of symmetry follows easily. Hence, having so regular solutions that Liu-Wang expansions hold we have the global a priori estimate $(Ω=\mathbb{R}^3)$ $$ \|ω_{r/r} \|_{V(Ω^t)} + \|ω_{\varphi/r}\|_{V(Ω^t)}\leφ({\rm data}),\ \ t<\infty, \qquad(*) $$ where $ω_r$ is the radiar component of vorticity, $ω_\varphi$ the angular, $V(Ω^t)$ is the energy norm. Estimate $(*)$ can be treated as an a priori estimate derived on sufficiently regular solutions. Increasing regularity of solutions $(*)$ we derive the estimate $$\eqalign{ &\|v\|_{W_3^{3,3/2}(Ω^t)}+\|\nabla p\|_{W_3^{1,1/2}(Ω^t)}\cr &\leφ(φ({\rm data}),\|f\|_{W_3^{1,1/2}(Ω^t)},\|v(0)\|_{W_3^{3-2/3}(Ω)}),\cr} \qquad(**) $$ where $φ$ is an increasing positive function. The estimate is proved on the local solution. Estimate $(**)$ plus existence of local solutions imply the existence of global regular solutions to the Cauchy problem.

</details>


### [20] [Derivation of the Boltzmann equation from hard-sphere dynamics (after Y. Deng, Z. Hani, and X. Ma)](https://arxiv.org/abs/2602.04407)
*Thierry Bodineau,Isabelle Gallagher,Laure Saint-Raymond,Sergio Simonella*

Main category: math.AP

TL;DR: This paper explains key elements of the proof by Deng, Hani, and Ma that extends Lanford's convergence result from short to arbitrarily large time intervals for hard sphere systems converging to Boltzmann equation solutions.


<details>
  <summary>Details</summary>
Motivation: Lanford's classical result showed that the empirical measure of hard spheres converges to the Boltzmann equation solution, but only for short time intervals. The motivation is to extend this convergence to arbitrarily large time intervals where the Boltzmann equation has regular solutions.

Method: The paper explains elements of Deng, Hani, and Ma's proof, which likely involves controlling correlations induced by particle collisions over long time scales, managing the BBGKY hierarchy, and handling the weak density scaling Nε²=1.

Result: The referenced work successfully obtained convergence of the empirical measure to Boltzmann equation solutions for arbitrarily large time intervals, extending Lanford's short-time result to the full time domain where the Boltzmann equation has regular solutions.

Conclusion: The convergence of microscopic hard sphere systems to the Boltzmann equation holds not just for short times but for arbitrarily long time intervals, provided the Boltzmann equation has regular solutions, representing a significant extension of Lanford's classical result.

Abstract: Consider a microscopic system of $N$ hard spheres that are initially independent (modulo the exclusion condition on particle positions) and identically distributed in $\mathbb{R}^3$. When the number $N$ of particles goes to infinity and the diameter $\varepsilon$ of the particles goes to zero, and under the weak density assumption $N\varepsilon^2=1$, it has been known since the work of Lanford that the empirical measure for the particles converges to the solution of the Boltzmann equation in a short time interval. In particular, the particles remain dynamically independent, in this limit and in the short time interval where the correlations induced by their collisions remain under control. In a recent work, Y. Deng, Z. Hani and X. Ma successfully obtained the same convergence result in arbitrary large time; more precisely, the convergence result holds for any time interval on which the Boltzmann equation has a regular solution. In this note, we explain a few elements of their proof.

</details>


### [21] [Interpretation of stochastic primitive equations with relaxed hydrostatic assumption as a higher order approximation of 3D stochastic Navier-Stokes](https://arxiv.org/abs/2602.04422)
*Arnaud Debussche,Étienne Mémin,Antoine Moneyron*

Main category: math.AP

TL;DR: Stochastic 3D Navier-Stokes to primitive equations convergence analysis, including weak/strong regimes and non-hydrostatic effects via martingale terms.


<details>
  <summary>Details</summary>
Motivation: To investigate the convergence relationship between stochastic 3D Navier-Stokes equations and primitive equations, and to explore the impact of relaxing the hydrostatic assumption within the stochastic framework.

Method: Analysis of convergence regimes (weak/strong corresponding to rigid-lid/fully periodic BCs), introduction of martingale terms to capture non-hydrostatic effects via specific asymptotic scaling, and regularization using suitable filters for divergence-free noises.

Result: Demonstrated convergence of stochastic 3D Navier-Stokes solutions to primitive equations counterparts, developed a generalized hydrostatic model that captures non-hydrostatic effects while remaining within primitive equations formalism, and showed well-posedness under suitable assumptions.

Conclusion: The stochastic framework enables a higher-order approximation of 3D Navier-Stokes equations through a generalized hydrostatic model that incorporates non-hydrostatic effects via martingale terms, providing a bridge between full 3D dynamics and primitive equations.

Abstract: In this paper, we investigate the convergence of solutions of a stochastic representation of the three-dimensional Navier-Stokes equations to those of their primitive equations counterpart. Our analysis covers both weak and strong convergence regimes, corresponding respectively to rigid-lid and "fully periodic" boundary conditions. Furthermore, we explore the impact of relaxing the hydrostatic assumption in the stochastic primitive equations by retaining martingale terms as deviations from hydrostatic equilibrium. This modified model, obtained through a specific asymptotic scaling accessible only within the stochastic framework, captures non-hydrostatic effects while remaining within the primitive equations formalism. The resulting generalized hydrostatic model has been shown to be well-posed when the additional terms are regularized using a suitable filter for divergence-free noises under suitable assumptions. Within this setting, we demonstrate that the model provides a higher-order approximation of the 3D Navier-Stokes equations for appropriately scaled noises.

</details>


### [22] [The monotonicity method for the inverse elastic scattering on unbounded domains](https://arxiv.org/abs/2602.04453)
*Bastian Harrach,Jianli Xiang*

Main category: math.AP

TL;DR: Extends monotonicity method to time-harmonic inverse scattering for Navier equation, using far field operator to characterize inhomogeneities in Lamé parameters and density.


<details>
  <summary>Details</summary>
Motivation: To develop a method for characterizing penetrable and inhomogeneous scattering objects in elastic wave inverse scattering problems, particularly for identifying material property variations in Lamé parameters and density from far field measurements.

Method: Develops a monotonicity relation for the far field operator in Navier equation scattering, then combines this with the method of localized potentials to extend the monotonicity method for support characterization.

Result: Establishes a theoretical framework to characterize the support of inhomogeneities in Lamé parameters and density using the far field operator, extending existing monotonicity methods to elastic wave scattering problems.

Conclusion: The extended monotonicity method provides a rigorous approach for identifying material inhomogeneities in elastic media from far field scattering data, advancing inverse scattering techniques for Navier equations.

Abstract: We discuss a time-harmonic inverse scattering problem for the Navier equation with compactly supported penetrable and possibly inhomogeneous scattering objects in an unbounded homogeneous background medium, and we develop a monotonicity relation for the far field operator that maps superpositions of incident plane waves to the far field patterns of the corresponding scattered waves. Combining the monotonicity relation with the method of localized potentials, we extend the so called monotonicity method to characterize the support of inhomogeneities in the Lamé parameters and the density in terms of the far field operator.

</details>


### [23] [Local well-posedness of strong solutions to the compressible Navier-Stokes equations with degenerate viscosities and far field vacuum in 3D exterior domains](https://arxiv.org/abs/2602.04597)
*Jiaxu Li,Boqiang Lü,Bing Yuan*

Main category: math.AP

TL;DR: Local well-posedness of strong solutions for isentropic compressible Navier-Stokes system in 3D exterior domains with Navier-slip boundary conditions, where density approaches vacuum at far-field and viscosities are power functions of density (ρ^δ, 0<δ<1).


<details>
  <summary>Details</summary>
Motivation: To establish local well-posedness for compressible Navier-Stokes system in challenging scenarios: exterior domains with boundary conditions, far-field vacuum, and density-dependent viscosities. The problem combines multiple mathematical difficulties that need simultaneous handling.

Method: Develop mathematical framework to handle boundary terms and far-field vacuum simultaneously. The method makes the selection of viscosity exponent δ independent of the gas coefficient γ, which is a key technical advancement.

Result: Proves local well-posedness of strong solutions for the isentropic compressible Navier-Stokes system in 3D exterior domains with Navier-slip boundary conditions, when initial density approaches vacuum at far-field and viscosities follow power law ρ^δ (0<δ<1).

Conclusion: The paper successfully establishes local existence of strong solutions for a challenging compressible flow problem, with a method that simultaneously handles boundary effects and vacuum conditions while decoupling viscosity exponent selection from gas coefficient.

Abstract: The isentropic compressible Navier-Stokes system subject to the Navier-slip boundary conditions is considered in a general three-dimensional exterior domain. For the density approaches far-field vacuum initially and the viscosities are power functions of the density(ρ^δ with 0 < δ< 1), the local well-posedness of strong solutions is established in this paper. In particular, the method we adopt can not only simultaneously handle the difficulties caused by boundary terms and far-field vacuum, but also make the selection of δ independent of the gas coefficient γ.

</details>


### [24] [Polygons and multi-product of eigenfunctions](https://arxiv.org/abs/2602.04664)
*Emmett L. Wyman,Yakun Xi,Yi Zhang*

Main category: math.AP

TL;DR: The paper analyzes inner products of Laplace-Beltrami eigenfunctions on compact Riemannian manifolds, showing that after frequency averaging, the main concentration is determined by geometric configurations of polygons with eigenfrequencies as side lengths.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of inner products of eigenfunctions (which appear in nonlinear PDEs and quantum chaos) and their relationship to geometric constraints imposed by the eigenfrequencies.

Method: Study the inner product of k+1 eigenfunctions after mild averaging in frequency variables, connecting it to the measure of configurations of (k+1)-gons whose side lengths correspond to the eigenfrequencies.

Result: The main ℓ²-concentration of the averaged inner product is determined by polygon configurations, with only a rapidly vanishing proportion occurring when the frequencies cannot form valid (k+1)-gons.

Conclusion: Geometric polygon constraints strongly govern the structure of eigenfunction inner products, revealing fundamental connections between spectral geometry and nonlinear interactions.

Abstract: Let $M$ be a compact Riemannian manifold without boundary, with $L^2$-normalized Laplace-Beltrami eigenfunctions $\{e_j\}_j$, which satisfy $Δ_g e_j = -λ_j^2 e_j$. We study the following inner product of eigenfunctions \[
  \langle e_{i_1} e_{i_2} \ldots e_{i_k}, e_{i_{k+1}} \rangle = \int e_{i_1} e_{i_2}\ldots e_{i_k} \overline{e_{i_{k+1}}} \, dV. \] We show that, after a mild averaging in the frequency variables, the main $\ell^2$-concentration of this inner product is determined by the measure of a set of configurations of $(k+1)$-gons whose side lengths are the frequencies $λ_{i_1}, λ_{i_2}, \dots, λ_{i_{k+1}}$. We prove that a rapidly vanishing proportion of this mass lies in the regime where $λ_{i_1}, λ_{i_2}, \dots, λ_{i_{k+1}}$ cannot occur as the side lengths of any $(k+1)$-gon.

</details>


### [25] [Intrinsic Ultracontractivity for a class of Schroedinger Semigroups in $L^{2}(\mathbb{R}^{n})$ by Logarithmic Sobolev inequalities](https://arxiv.org/abs/2602.04685)
*Christoph Schwerdt,Alexander Mill,Dirk Hundertmark*

Main category: math.AP

TL;DR: The paper establishes growth conditions on Schrödinger potentials that yield Rosen inequalities for ground states, which in turn provide Logarithmic Sobolev inequalities enabling proof of intrinsic ultracontractivity for Schrödinger semigroups.


<details>
  <summary>Details</summary>
Motivation: To establish conditions under which Schrödinger operators with potentials satisfy intrinsic ultracontractivity, which is important for understanding the long-time behavior and smoothing properties of associated semigroups.

Method: Two-part approach: First, derive Rosen inequalities for ground states by solving radial Schrödinger inequalities using Agmon's comparison principle and Young's inequality for increasing functions. Second, use these inequalities to obtain Logarithmic Sobolev inequalities, then apply classic methods involving weighted Sobolev spaces and weighted Schrödinger semigroups to prove intrinsic ultracontractivity.

Result: The paper proves that under appropriate growth conditions on the potential q, the Schrödinger semigroup e^{-tH} is intrinsically ultracontractive, meaning its kernel decays pointwise like the ground state φ for all t>0.

Conclusion: The growth condition on potentials ensures Rosen inequalities hold, which yield Logarithmic Sobolev inequalities, ultimately proving intrinsic ultracontractivity of Schrödinger semigroups - connecting potential growth properties to semigroup smoothing behavior.

Abstract: In the first part of this article we present a growth condition on the potential $q$ in the Schrödinger operator $H=-Δ+ q(x)$ in $\mathrm{L}^{2}\left( \mathbb{R}^{n} \right)$ that implies Rosen inequalities for the ground state $\varphi$ of $H$, i.e.
  $\forall \varepsilon > 0 \exists γ(\varepsilon) > 0 \ : \ - \ln\left( \varphi(x) \right) \leq \varepsilon q(x) + γ(\varepsilon)$.
  While these inequalities are not particularly interesting in themselves, they offer Logarithmic Sobolev inequalities which are absolutely essential to prove an intrinsic ultracontractivity of the associated Schrödinger semigroup $\mathrm{e}^{-tH}$, i.e.
  $\forall t>0 \exists C_{t} > 0 \ : \ \left| \mathrm{e}^{-tH} u (x) \right| \ \leq \ C_{t} \varphi(x) \| u \|_{2}$
  holds for every $u \in \mathrm{L}^{2}\left( \mathbb{R}^{n} \right)$ almost everywhere in $\mathbb{R}^{n}$ which we prove in the second part of this article. For proving Rosen inequalities we focus on solving a radial Schrödinger inequality and use Agmon's version of the comparison principle and Young's inequality for increasing functions. We follow the classic method proving intrinsic ultracontractivity of $\mathrm{e}^{-tH}$ by using weighted Sobolev function spaces, weighted Schrödinger semigroups and Logarithmic Sobolev inequalities.

</details>


### [26] [Semilinear wave equations with time-dependent coefficients](https://arxiv.org/abs/2602.04727)
*Nenad Antonić,Matko Grbac*

Main category: math.AP

TL;DR: Existence and uniqueness results for semilinear wave equations with time-space dependent coefficients and sign-condition nonlinearities, with practical applications.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for semilinear wave equations with variable coefficients, which are important in physics and engineering applications where material properties vary in both time and space.

Method: Abstract mathematical analysis using functional analysis and PDE theory to prove existence of strong and weak solutions, with uniqueness under more restrictive assumptions. The approach handles continuous nonlinearities satisfying sign conditions.

Result: Proved existence of both strong and weak solutions for semilinear wave equations with time-space dependent coefficients. Established uniqueness under slightly stronger conditions. Demonstrated practical applicability through concrete examples.

Conclusion: The paper provides complete existence theory for semilinear wave equations with variable coefficients, offering both theoretical results and practical validation through examples, advancing the mathematical understanding of these important PDEs.

Abstract: We prove the existence of strong and weak solutions to the semilinear wave equation with coefficients depending both on time and space variables, with continuous nonlinearity satisfying the sign condition. The uniqueness is proven under slightly more restrictive assumptions. Furthermore, the results obtained in abstract setting are illustrated on practical examples.

</details>


### [27] [Intrinsic Ultracontractivity for a class of Schroedinger Semigroups in $\mathrm{L}^{2}\left( \mathbb{R}^{n} \right)$ using Log-Sobolev-inequalities and duality arguments](https://arxiv.org/abs/2602.04738)
*Christoph Schwerdt,Ilham Ouelddris*

Main category: math.AP

TL;DR: The paper establishes conditions for weighted Schrödinger semigroups to map between weighted Lebesgue spaces using logarithmic Sobolev inequalities and intrinsic ultracontractivity.


<details>
  <summary>Details</summary>
Motivation: To characterize when weighted Schrödinger semigroups exhibit continuous mapping properties between different weighted Lebesgue function spaces, which has implications for the regularity and smoothing properties of solutions to Schrödinger equations.

Method: Uses logarithmic Sobolev inequalities for the Schrödinger operator H = -Δ + q(x) with its strictly positive ground state φ, and leverages the self-adjointness of e^{-tH} in L²(ℝⁿ) to establish intrinsic ultracontractivity properties.

Result: Identifies a class of potentials q: ℝⁿ → (0,∞) that ensures the weighted semigroup φ^{-1}e^{-tH}φ continuously maps L_μ¹(ℝⁿ) into L_μ²(ℝⁿ) for all t>0, and proves an intrinsic ultracontractivity bound: |e^{-tH}u(x)| ≤ C_t φ(x)‖u‖₂ for all u∈L²(ℝⁿ).

Conclusion: Logarithmic Sobolev inequalities combined with intrinsic ultracontractivity provide a powerful framework for establishing mapping properties of weighted Schrödinger semigroups between weighted Lebesgue spaces, with applications to the regularity theory of Schrödinger equations.

Abstract: We present a class of potentials $q \colon \mathbb{R}^{n} \to (0,\infty)$ that implies the weighted Schrödinger semigroup $\varphi^{-1}\mathrm{e}^{-tH}\varphi$ to map a weighted Lebesgue function space $\mathrm{L}_μ^{1}(\mathbb{R}^{n})$ into a weighted Lebesgue function space $\mathrm{L}_μ^{2}(\mathbb{R}^{n})$ continously at every time $t>0$ by Logarithmic Sobolev inequalities for $H=-Δ+ q(x)$ with it's strictly positive ground state $\varphi \colon \mathbb{R}^{n} \to (0,\infty)$. We use the self-adjointness of $\mathrm{e}^{-tH}$ in $\mathrm{L}^{2}(\mathbb{R}^{n})$ to infer an intrinsic ultracontractivity, i.e.
  $\forall t>0 \ \exists C_{t} > 0 \ : \ \left| \mathrm{e}^{-tH} u (x) \right| \ \leq \ C_{t} \varphi(x) \| u \|_{2}$
  for every $u \in \mathrm{L}^{2}\left( \mathbb{R}^{n} \right)$ almost everywhere in $\mathbb{R}^{n}$.

</details>


### [28] [Uniqueness of sign-changing solutions to Trudinger's equation](https://arxiv.org/abs/2602.04748)
*Riku Anttila,Peter Lindqvist,Mikko Parviainen*

Main category: math.AP

TL;DR: Uniqueness established for sign-changing solutions to Trudinger's parabolic equation with time-dependent C² Dirichlet boundary data.


<details>
  <summary>Details</summary>
Motivation: To address the uniqueness problem for sign-changing solutions to Trudinger's parabolic equation, particularly when dealing with time-dependent boundary conditions, which is important for understanding solution behavior and well-posedness of such PDEs.

Method: Mathematical analysis of Trudinger's parabolic equation, likely using PDE techniques, maximum principles, energy methods, or comparison principles to prove uniqueness for sign-changing solutions with time-dependent C² Dirichlet boundary data.

Result: Successfully established uniqueness for sign-changing solutions to Trudinger's parabolic equation with time-dependent C² Dirichlet boundary data.

Conclusion: The paper provides a rigorous uniqueness result for sign-changing solutions to Trudinger's parabolic equation with time-dependent boundary conditions, contributing to the understanding of solution properties for this class of parabolic equations.

Abstract: We establish uniqueness for sign-changing solutions to Trudinger's parabolic equation with time dependent $C^2$ Dirichlet boundary data.

</details>


### [29] [Blow-up Solutions for General Toda Systems on Riemann Surfaces](https://arxiv.org/abs/2602.04777)
*Zhengni Hu,Miaomiao Zhu*

Main category: math.AP

TL;DR: The paper constructs bubbling solutions for Toda systems with Neumann boundary conditions on k-symmetric Riemann surfaces, showing existence of asymmetric blow-up solutions for SU(3) Toda system with blow-up points at symmetric centers.


<details>
  <summary>Details</summary>
Motivation: To study general Toda systems with homogeneous Neumann boundary conditions on Riemann surfaces and understand the formation of bubbling solutions, particularly asymmetric blow-up patterns in symmetric geometric settings.

Method: Uses singular perturbation methods and finite-dimensional reduction technique, assuming the Riemann surface satisfies a k-symmetric condition to construct families of bubbling solutions with distinct concentration rates.

Result: Establishes existence of asymmetric blow-up solutions for the SU(3) Toda system, with blow-up points precisely located at the k-symmetric centers of the Riemann surface.

Conclusion: The geometric symmetry of the Riemann surface (k-symmetry) plays a crucial role in determining both the existence and precise location of asymmetric blow-up solutions for Toda systems with Neumann boundary conditions.

Abstract: In this paper, we study general Toda systems with homogeneous Neumann boundary conditions on Riemann surfaces. Assuming the surface satisfies the ``$k$-symmetric'' condition, we construct a family of bubbling solutions using singular perturbation methods, where the concentration rates of different components occur in distinct orders. In particular, we establish the existence of asymmetric blow-up solutions for the $SU(3)$ Toda system. Furthermore, the blow-up points are precisely located at the ``$k$-symmetric'' centers of the surface.
  Keywords: Toda system, Neumann boundary condition, Blow-up solutions, $k$-symmetry, Finite-dimensional reduction

</details>


### [30] [Blow-up solutions for mean field equations with non-quantized singularities on Riemann surfaces with boundary](https://arxiv.org/abs/2602.04790)
*Mohameden Ahmedou,Zhengni Hu,Miaomiao Zhu*

Main category: math.AP

TL;DR: The paper constructs blow-up solutions for singular mean field equations on Riemann surfaces with boundary using Lyapunov-Schmidt reduction, focusing on non-quantized singular regimes including purely singular and mixed singular-regular blow-up cases.


<details>
  <summary>Details</summary>
Motivation: To understand blow-up phenomena in singular mean field equations with Neumann boundary conditions on Riemann surfaces with boundary, particularly in non-quantized singular regimes where parameters approach resonant values.

Method: Lyapunov-Schmidt reduction under suitable stability assumptions to construct blow-up solutions, handling both purely singular and mixed singular-regular blow-up cases.

Result: Successful construction of blow-up solutions in non-quantized singular regimes, including purely singular and mixed singular-regular blow-up cases with parameters approaching resonant values.

Conclusion: The Lyapunov-Schmidt reduction method effectively constructs blow-up solutions for singular mean field equations on Riemann surfaces with boundary, advancing understanding of blow-up phenomena in non-quantized singular regimes.

Abstract: We study mean field equations with singular sources on a compact Riemann surface with boundary $(Σ,g)$, subject to homogeneous Neumann boundary conditions:
  \[
  -Δ_g v
  = ρ\left( \frac{V e^{v}}{\int_ΣV e^{v}\, d v_g}
  - \frac{1}{|Σ|_g}\right)
  - \sum_{ξ\in Q} \frac{\varrho(ξ)}{2}γ(ξ)
  \left(δ_ξ- \dfrac{1}{|Σ|_g}\right)
  \text{in }Σ; \qquad
  \partial_{ν_g} v = 0
  \text{ on }\partialΣ.
  \]
  Here, $V$ is a smooth positive function, $ρ$ is a non-negative parameter, $Q\subsetΣ$ is a finite set of prescribed singular points, and the singular weights satisfy $γ(ξ)\in(-1,+\infty)\setminus(\mathbb{N}\cup\{0\})$. The coefficients are given by $\varrho(ξ)=8π$ for $ξ\inΣ\setminus\partialΣ$ and $\varrho(ξ)=4π$ for $ξ\in\partialΣ$.
  We construct blow-up solutions in the non-quantized singular regime, including purely singular and mixed singular-regular blow-up cases, with parameters approaching resonant values. The construction is achieved via a Lyapunov-Schmidt reduction under suitable stability assumptions.
  Key words: Singular mean field equations, Blow-up phenomena, Lyapunov-Schmidt reduction, Riemann surfaces with boundary

</details>


### [31] [Reconstruction of potential and damping coefficients in a semi-linear wave equation](https://arxiv.org/abs/2602.04822)
*Rahul Bhardwaj,Mandeep Kumar,Manmohan Vashisth*

Main category: math.AP

TL;DR: Inverse problem for semi-linear wave equation: reconstruct damping coefficient, linear potential, and nonlinear potential from Dirichlet-to-Neumann map using higher-order linearization method.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop mathematical methods for reconstructing multiple physical parameters (damping coefficient, linear potential, nonlinear potential) in semi-linear wave equations from boundary measurements, which has applications in various fields including medical imaging, geophysics, and material science.

Method: Higher-order linearization method is employed. The key technical step involves establishing the existence of suitable asymptotic solutions, which are crucial for reconstructing the nonlinear potential. The paper also includes a detailed study of the corresponding forward problem.

Result: The paper demonstrates that the damping coefficient, linear potential, and nonlinear potential can be reconstructed from the Dirichlet-to-Neumann map for semi-linear wave equations in dimensions n ≥ 2.

Conclusion: The higher-order linearization method provides an effective approach for solving inverse problems for semi-linear wave equations, allowing simultaneous reconstruction of multiple parameters from boundary measurements.

Abstract: In this article, we investigate an inverse problem for a semi-linear wave equation posed in $\mathbb{R}^{n+1}$, with $n \geq 2$. Our primary objective is to reconstruct the damping coefficient, the linear potential, and the nonlinear potential from the associated Dirichlet-to-Neumann map. The analysis is based on a higher-order linearization method. As a key step, we establish the existence of suitable asymptotic solutions, crucial for reconstructing the nonlinear potential. In addition, we also provide a detailed study of the corresponding forward problem.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [32] [Numerical study of loss of hyperbolicity using a cold plasma model](https://arxiv.org/abs/2602.03859)
*Evgeniy V. Chizhonkov,Olga S. Rozanova*

Main category: physics.comp-ph

TL;DR: A new implicit method for solving 1D cold plasma equations with electron-ion collisions that handles non-hyperbolic systems when collision coefficient depends on electron density.


<details>
  <summary>Details</summary>
Motivation: When collision coefficient ν depends on electron density N, the plasma system loses hyperbolicity, creating computational difficulties. Existing methods struggle with this type change, especially for threshold cases where smoothness can be lost.

Method: Proposes a new implicit solution method in Euler variables that works for both nonrelativistic and relativistic cases. The method is tested specifically for the threshold case of linear dependence ν(N)=ν₁+ν₀N where smoothness loss can occur.

Result: The computational experiments fully agree with available theoretical results, demonstrating the method's effectiveness in handling the non-hyperbolic system and threshold cases.

Conclusion: The proposed implicit method successfully overcomes computational difficulties arising from loss of hyperbolicity when collision coefficient depends on electron density, providing reliable solutions for both relativistic and nonrelativistic plasma systems.

Abstract: We study a one-dimensional system of cold plasma equations taking into account electron-ion collisions in both relativistic and nonrelativistic cases. It is known that for a constant collision coefficient $ν$, the solution to the Cauchy problem for such a system can lose smoothness. However, if the dependence of $ν$ on the electron density $N$ is more than linear, then the solution remains globally smooth for any initial data. However, the appearance of the dependence $ν(N)$ leads to a change in the type of the system, it loses hyperbolicity, which leads to computational problems. In this paper, we propose a new implicit solution method in Euler variables that overcomes these difficulties. It can be used in both nonrelativistic and relativistic cases and is tested for the threshold case of a linear dependence $ν(N)=ν_1+ν_0 N$, when smoothness can still be lost. The computational experiments carried out are in full agreement with the available theoretical results.

</details>


### [33] [Topology- and Geometry-Exact Coupling for Incompressible Fluids and Thin Deformables](https://arxiv.org/abs/2602.03988)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: A topology-preserving discretization method for coupling incompressible fluids with thin deformable structures that guarantees leakproofness by maintaining fluid domain connectivity.


<details>
  <summary>Details</summary>
Motivation: To achieve robust two-way coupling between fluids and thin deformable structures without fluid leakage through solids, while allowing flow wherever continuous paths exist through the fluid domain.

Method: Uses a stitching algorithm applied to a clipped Voronoi diagram generated from Lagrangian fluid particles to maintain path connectivity around obstacles. Discretizes pressure projection equations on this conforming mesh to enforce velocity boundary conditions at fluid-solid interfaces and apply pressure forces directly on solid boundaries.

Result: The method prevents fluid leakage through solids while permitting flow through continuous paths, enabling sharp two-way coupling between phases without artificial sealing or leakage artifacts.

Conclusion: The approach effectively handles diverse scenarios including flows around thin membranes, complex geometries with narrow passages, and deformable structures immersed in liquid, demonstrating robust two-way coupling.

Abstract: We introduce a topology-preserving discretization for coupling incompressible fluids with thin deformable structures, achieving guaranteed leakproofness through preservation of fluid domain connectivity. Our approach leverages a stitching algorithm applied to a clipped Voronoi diagram generated from Lagrangian fluid particles, in order to maintain path connectivity around obstacles. This geometric discretization naturally conforms to arbitrarily thin structures, enabling boundary conditions to be enforced exactly at fluid-solid interfaces. By discretizing the pressure projection equations on this conforming mesh, we can enforce velocity boundary conditions at the interface for the fluid while applying pressure forces directly on the solid boundary, enabling sharp two-way coupling between phases. The resulting method prevents fluid leakage through solids while permitting flow wherever a continuous path exists through the fluid domain. We demonstrate the effectiveness of our approach on diverse scenarios including flows around thin membranes, complex geometries with narrow passages, and deformable structures immersed in liquid, showcasing robust two-way coupling without artificial sealing or leakage artifacts.

</details>


### [34] [At the Top of the Mountain, the World can Look Boltzmann-Like: Sampling Dynamics of Noisy Double-Well Systems](https://arxiv.org/abs/2602.04014)
*Abir Hasan,Nikhil Shukla*

Main category: physics.comp-ph

TL;DR: The paper shows that diverse physical systems with double-well potentials can serve as universal p-bits (probabilistic bits) for probabilistic computing, as their stochastic dynamics near barrier tops follow a canonical tanh-like response independent of specific potential details.


<details>
  <summary>Details</summary>
Motivation: To identify a fundamental hardware primitive (p-bit) for probabilistic computing, analogous to the transistor for digital computation, by understanding the universal stochastic dynamics of double-well energy systems.

Method: Using topological framework from Morse theory and singularity theory to show all smooth, even double-well potentials reduce near saddle points to a canonical quartic normal form. Analytical derivations and numerical simulations across multiple representative systems.

Result: Discovered that in the regime near barrier tops, the interplay of noise, synaptic bias, and potential curvature produces a topologically robust short-time evolution with tanh-like response, enabling Boltzmann-like sampling largely independent of potential shape details.

Conclusion: Provides a unifying foundation for assessing and engineering various physical platforms (oscillators, bistable latches, magnetic devices) as p-bits for synchronous stochastic sampling and probabilistic computation.

Abstract: The success of the transistor as the cornerstone of digital computation motivates analogous efforts to identify an equivalent hardware primitive, the probabilistic bit or p-bit, for the emerging paradigm of probabilistic computing. Here, we uncover a fundamental ubiquity in the stochastic dynamics of double well energy systems when initialized near the barrier top. Using a topological framework grounded in Morse theory and singularity theory, we make use of the result that all smooth, even double well potentials reduce near the saddle point to a canonical quartic normal form. Within this regime, the interplay of noise, synaptic bias, and potential curvature produces a topologically robust short time evolution characterized by a tanh like response. This enables Boltzmann like sampling that is largely independent of the detailed shape of the potential, apart from its effective temperature scaling. Analytical derivations and numerical simulations across multiple representative systems corroborate this behavior. Our work provides a unifying foundation for assessing and engineering a broad class of physical platforms, including oscillators, bistable latches, and magnetic devices, as p-bits operating within a synchronous framework for stochastic sampling and probabilistic computation.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [Eigenmodes in an ultra-relativistic ultra-magnetized pair QED-plasma](https://arxiv.org/abs/2602.04065)
*Ryan T. Low,Mikhail V. Medvedev*

Main category: physics.plasm-ph

TL;DR: Ultra-relativistic QED plasmas in super-strong magnetic fields show reduced plasma frequency cutoff (leading to transparency) and temperature-independent refractive index modification.


<details>
  <summary>Details</summary>
Motivation: Study ultra-relativistic QED plasmas with magnetic fields approaching/exceeding Schwinger field (B_Q ≈ 4×10^13 gauss), relevant for laser-plasma experiments and astrophysical observations of neutron stars/magnetars.

Method: Investigate joint modification of normal plasma modes in ultra-relativistic electron-positron plasmas (both charge neutral and non-neutral) by super-strong magnetic field and plasma relativistic temperature.

Result: Most substantial modification is reduction of plasma frequency cutoff, resulting in relativistic and field-induced transparency. Also observe temperature-independent modification of electromagnetic wave refractive index, coinciding with cold QED plasma behavior.

Conclusion: Super-strong magnetic fields significantly modify plasma properties in ultra-relativistic QED plasmas, particularly reducing plasma frequency cutoff (enabling transparency) and altering refractive index in temperature-independent manner.

Abstract: Ultra-relativistic quantum-electrodynamic (QED) plasmas, characterized by magnetic field strengths approaching and even exceeding the Schwinger field of approximately $B_{Q} \approx 4 \times 10^{13}$ gauss, hold significant interest for laser-plasma experiments and astrophysical observations of neutron stars and magnetars. In this study, we investigate the joint modification of normal plasma modes in ultra-relativistic electron-positron plasmas, both charge neutral and non-neutral, by the super-strong magnetic field and plasma relativistic temperature. Our analysis shows that the most substantial modification concerns the reduction of the plasma frequency cutoff, resulting in relativistic and field-induced transparency. Additionally, we observe a temperature-independent modification of the index of refraction of electromagnetic waves, which coincides with the behavior observed in a cold QED plasma.

</details>


### [36] [Improved Fluid Modeling of Space Debris Generated Ion-Acoustic Precursor Solitons](https://arxiv.org/abs/2602.04338)
*Ajaz Mir,Abhijit Sen,Pintu Bandyopadhyay,Sanat Tiwari,Chris Crabtree,Gurudas Ganguli*

Main category: physics.plasm-ph

TL;DR: This paper reexamines ion-acoustic precursor soliton excitation by supersonic charged debris, incorporating dynamic charging and surface impermeability effects that were previously overlooked.


<details>
  <summary>Details</summary>
Motivation: Previous studies of ion-acoustic precursor solitons excited by supersonic charged debris objects overlooked two important physical factors: the dynamic charging of the debris object and the impermeable nature of its surface. These omissions may have affected the accuracy of predictions about soliton generation and propagation.

Method: 1) Used an enhanced 1D fluid-Poisson model with dynamic charging where source charge is treated as a dynamical variable solved self-consistently with plasma equations. 2) Developed a 2D fluid model to simulate interaction between an electrostatically biased, impenetrable object and flowing plasma, comparing infinite wall vs finite object geometries.

Result: 1) Dynamic charging does not hinder soliton generation or evolution compared to fixed-charge models. 2) Impermeable infinite wall geometry disconnects upstream/downstream plasma, forming a sheath without solitons. 3) Finite object geometry allows plasma flow around it, restores upstream-downstream connectivity, and naturally generates precursor solitons.

Conclusion: The impermeability of debris surfaces plays a crucial role in soliton generation, while dynamic charging has minimal impact. Soliton formation requires finite object geometry that allows plasma flow around the object, maintaining upstream-downstream connectivity.

Abstract: This study reexamines the excitation of ion-acoustic precursor solitons by a supersonically moving charged debris object, incorporating two previously overlooked physical factors: the dynamic charging of the debris and the impermeable nature of its surface. The influence of charging dynamics is explored using an enhanced one-dimensional fluid-Poisson model, where the source charge is treated as a dynamical variable and solved self-consistently alongside the core plasma equations. By comparing these results with prior fixed-charge models, we evaluate the effects on soliton onset and propagation, finding that charging dynamics does not hinder soliton generation or evolution. To assess the impact of the impermeability of debris surface, a two-dimensional fluid model simulates the interaction between an electrostatically biased, impenetrable object and a flowing plasma. Modeling the object as an infinite wall disconnects the upstream and downstream plasma regions, forming a sheath without solitons -- consistent with earlier fluid and particle-in-cell simulations. However, replacing the wall with a finite object enables plasma flow around it, restoring upstream-downstream connectivity and naturally generating precursor solitons.

</details>


### [37] [Nonlinear Dynamical Friction from the Doppler-Shifted Equilibrium Memory Kernel](https://arxiv.org/abs/2602.04545)
*N. R. Sree Harsha,Zhenyuan Yu,Chuang Ren,Virginia Billings,Michael Huang*

Main category: physics.plasm-ph

TL;DR: A statistical mechanics framework using Generalized Langevin Equation (GLE) to model non-equilibrium transport coefficients from equilibrium simulations via Fluctuation-Dissipation Theorem.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient method for predicting non-equilibrium transport properties from first-principles equilibrium simulations, avoiding the need for expensive non-equilibrium simulations.

Method: Use Generalized Langevin Equation (GLE) formalism with kernel obtained via Fluctuation-Dissipation Theorem from stochastic force autocorrelation measured in thermal equilibrium state. Apply to test particle drag in uniform plasma.

Result: GLE captures non-Markovian phenomena including effective mass renormalization and oscillatory relaxation. Chandrasekhar stopping power formula emerges as Markovian limit. Predictions validated by Particle-in-Cell simulations confirming oscillatory memory kernel structure.

Conclusion: Establishes practical method for predicting non-equilibrium transport properties from equilibrium simulations, providing efficient path to model complex transport problems.

Abstract: We present a statistical mechanics framework for modeling non-equilibrium transport coefficients using the Generalized Langevin Equation (GLE). We show that the kernel, obtained via the Fluctuation-Dissipation Theorem (FDT) from the stochastic force autocorrelation measured in a thermal equilibrium state, is sufficient to model the dynamics of the system in a Non-Equilibrium Steady State (NESS). This approach provides a computationally efficient path to modeling complex transport problems. We apply this framework to the canonical problem of test particle drag in a uniform plasma. The GLE formalism is shown to naturally capture non-Markovian phenomena through the moments of the kernel, including an effective mass renormalization and oscillatory relaxation. We demonstrate that the standard Chandrasekhar stopping power formula arises naturally as the Markovian limit of this equilibrium memory kernel. These theoretical predictions are quantitatively validated by direct Particle-in-Cell simulations, which confirm the predicted oscillatory structure of the memory kernel. This work thus establishes a practical method for predicting non-equilibrium transport properties from first-principles equilibrium simulations.

</details>


### [38] [Physics-Informed Neural Compression of High-Dimensional Plasma Data](https://arxiv.org/abs/2602.04758)
*Gianluca Galletti,Gerald Gutenbrunner Sandeep S. Cranganore,William Hornsby,Lorenzo Zanisi,Naomi Carey,Stanislas Pamela,Johannes Brandstetter,Fabian Paischer*

Main category: physics.plasm-ph

TL;DR: Physics-Informed Neural Compression (PINC) achieves extreme compression ratios (70,000-120,000x) for gyrokinetic plasma turbulence simulations while preserving essential physical fidelity, solving storage bottlenecks for high-fidelity scientific data.


<details>
  <summary>Details</summary>
Motivation: High-fidelity scientific simulations like gyrokinetic plasma turbulence generate massive data volumes (tens of terabytes), creating storage and analysis bottlenecks that force researchers to discard valuable information. Existing compression techniques fail to preserve essential physical quantities needed for scientific analysis.

Method: Developed Physics-Informed Neural Compression (PINC) with physics-informed losses tailored to gyrokinetics, incorporating a spatiotemporal evaluation pipeline that accounts for structural phenomena and multi-scale transient fluctuations to assess physical fidelity. Combined with entropy coding for further compression.

Result: PINC achieves extreme compression ratios of over 70,000x, and with entropy coding reaches 120,000x. Unlike conventional compression methods, PINC preserves both spatial mode structure and temporal turbulence characteristics essential for scientific analysis.

Conclusion: PINC provides a viable and scalable solution to prohibitive storage demands of gyrokinetic simulations, enabling previously infeasible post-hoc analyses while maintaining physical fidelity through physics-informed compression approaches.

Abstract: High-fidelity scientific simulations are now producing unprecedented amounts of data, creating a storage and analysis bottleneck. A single simulation can generate tremendous data volumes, often forcing researchers to discard valuable information. A prime example of this is plasma turbulence described by the gyrokinetic equations: nonlinear, multiscale, and 5D in phase space. It constitutes one of the most computationally demanding frontiers of modern science, with runs taking weeks and yielding tens of terabytes of data dumps.The increasing storage demands underscore the importance of compression. However, reconstructed snapshots do not necessarily preserve essential physical quantities. We present a spatiotemporal evaluation pipeline, accounting for structural phenomena and multi-scale transient fluctuations to assess the degree of physical fidelity. Indeed, we find that various compression techniques lack preservation of both spatial mode structure and temporal turbulence characteristics. Therefore, we explore Physics-Informed Neural Compression (PINC), which incorporates physics-informed losses tailored to gyrokinetics and enables extreme compressions ratios of over 70,000x. Entropy coding on top of PINC further pushes it to 120,000x. This direction provides a viable and scalable solution to the prohibitive storage demands of gyrokinetics, enabling post-hoc analyses that were previously infeasible.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Generative Neural Operators through Diffusion Last Layer](https://arxiv.org/abs/2602.04139)
*Sungwon Park,Anthony Zhou,Hongjoong Kim,Amir Barati Farimani*

Main category: cs.LG

TL;DR: A lightweight probabilistic head called diffusion last layer (DLL) that attaches to neural operators for uncertainty quantification in stochastic systems.


<details>
  <summary>Details</summary>
Motivation: Neural operators lack principled uncertainty quantification for stochastic systems, which is essential for reliable deployment in scientific computing applications.

Method: DLL is a simple add-on probabilistic head that parameterizes conditional output distributions directly in function space using low-rank Karhunen-Loève expansion, leveraging smoothness and low-dimensional structure of PDE solution distributions.

Result: DLL improves generalization and uncertainty-aware prediction across stochastic PDE operator learning benchmarks, and enhances rollout stability with meaningful epistemic uncertainty estimates even in deterministic long-horizon settings.

Conclusion: DLL provides an effective, lightweight approach for uncertainty quantification in neural operators, addressing a critical gap for reliable deployment in stochastic scientific computing systems.

Abstract: Neural operators have emerged as a powerful paradigm for learning discretization-invariant function-to-function mappings in scientific computing. However, many practical systems are inherently stochastic, making principled uncertainty quantification essential for reliable deployment. To address this, we introduce a simple add-on, the diffusion last layer (DLL), a lightweight probabilistic head that can be attached to arbitrary neural operator backbones to model predictive uncertainty. Motivated by the relative smoothness and low-dimensional structure often exhibited by PDE solution distributions, DLL parameterizes the conditional output distribution directly in function space through a low-rank Karhunen-Loève expansion, enabling efficient and expressive uncertainty modeling. Across stochastic PDE operator learning benchmarks, DLL improves generalization and uncertainty-aware prediction. Moreover, even in deterministic long-horizon rollout settings, DLL enhances rollout stability and provides meaningful estimates of epistemic uncertainty for backbone neural operators.

</details>


### [40] [A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models](https://arxiv.org/abs/2602.04082)
*Yicheng Zou,Samuel Lanthaler,Hossein Salahshoor*

Main category: cs.LG

TL;DR: Probabilistic neural operator using score-based diffusion outperforms deterministic methods for high-frequency wave PDEs like Helmholtz, providing robust predictions with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Deterministic neural operators struggle with high-frequency wave phenomena due to input-to-output sensitivity and spectral bias, which blurs oscillations. A probabilistic approach is needed for robust approximation in challenging high-frequency regimes.

Method: Developed a probabilistic framework using score-based conditional diffusion operator. Performed stability analysis of the Helmholtz operator and conducted numerical experiments across wide frequency ranges, benchmarking against other data-driven and ML approaches.

Result: The probabilistic neural operator consistently produces robust predictions with lowest errors in L², H¹, and energy norms. Unlike deterministic approaches, it captures uncertainties in input sound speed maps propagated to solution fields.

Conclusion: Probabilistic operator learning is a principled and effective approach for solving complex PDEs like Helmholtz in challenging high-frequency regimes, offering both accuracy and uncertainty quantification.

Abstract: Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.

</details>


### [41] [From Dead Neurons to Deep Approximators: Deep Bernstein Networks as a Provable Alternative to Residual Layers](https://arxiv.org/abs/2602.04264)
*Ibrahim Albool,Malak Gamal El-Din,Salma Elmalaki,Yasser Shoukry*

Main category: cs.LG

TL;DR: Deep Bernstein Networks use Bernstein polynomials as activation functions to create residual-free architectures that prevent vanishing gradients and achieve exponential approximation error decay with depth.


<details>
  <summary>Details</summary>
Motivation: Residual connections are standard for mitigating vanishing gradients but impose structural constraints and don't address inefficiencies of piecewise linear activations. The paper aims to develop residual-free architectures with better trainability and representation power.

Method: Proposes Deep Bernstein Networks using Bernstein polynomials as activation functions. Provides theoretical foundation: 1) Derives lower bound on local derivative to prevent gradient stagnation, 2) Shows approximation error decays exponentially with depth for Bernstein-based networks.

Result: Bernstein activations reduce "dead" neurons from 90% in standard networks to less than 5%, outperforming ReLU, Leaky ReLU, SeLU, and GeLU. Achieves high-performance training without skip-connections on HIGGS and MNIST datasets.

Conclusion: Bernstein activations provide superior mechanism for function approximation and signal flow, offering principled path toward deep, residual-free architectures with enhanced expressive capacity.

Abstract: Residual connections are the de facto standard for mitigating vanishing gradients, yet they impose structural constraints and fail to address the inherent inefficiencies of piecewise linear activations. We show that Deep Bernstein Networks (which utilizes Bernstein polynomials as activation functions) can act as residual-free architecture while simultaneously optimize trainability and representation power. We provide a two-fold theoretical foundation for our approach. First, we derive a theoretical lower bound on the local derivative, proving it remains strictly bounded away from zero. This directly addresses the root cause of gradient stagnation; empirically, our architecture reduces ``dead'' neurons from 90\% in standard deep networks to less than 5\%, outperforming ReLU, Leaky ReLU, SeLU, and GeLU. Second, we establish that the approximation error for Bernstein-based networks decays exponentially with depth, a significant improvement over the polynomial rates of ReLU-based architectures. By unifying these results, we demonstrate that Bernstein activations provide a superior mechanism for function approximation and signal flow. Our experiments on HIGGS and MNIST confirm that Deep Bernstein Networks achieve high-performance training without skip-connections, offering a principled path toward deep, residual-free architectures with enhanced expressive capacity.

</details>


### [42] [Maximum-Volume Nonnegative Matrix Factorization](https://arxiv.org/abs/2602.04795)
*Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.LG

TL;DR: MaxVol NMF maximizes volume of H instead of minimizing volume of W, leading to sparser decompositions and avoiding rank-deficient solutions, with solutions corresponding to clustering columns of X.


<details>
  <summary>Details</summary>
Motivation: Standard MinVol NMF can generate rank-deficient solutions and lacks effectiveness in extracting sparse decompositions. The dual approach of maximizing volume of H instead of minimizing volume of W offers better performance for sparse decomposition and avoids rank deficiency issues.

Method: Proposes Maximum-volume NMF (MaxVol NMF) that maximizes the volume of H rather than minimizing volume of W. Introduces two algorithms to solve MaxVol NMF and a normalized variant that bridges standard NMF and orthogonal NMF.

Result: MaxVol NMF is identifiable under same conditions as MinVol NMF in noiseless case but behaves differently with noise. Solutions with largest volume correspond to clustering columns of X in disjoint clusters, while MinVol NMF solutions with smallest volume are rank deficient. Normalized variant shows better performance than both MinVol and MaxVol NMF.

Conclusion: MaxVol NMF provides an effective alternative to MinVol NMF for obtaining interpretable and unique NMF solutions, particularly for sparse decomposition tasks and avoiding rank deficiency, with applications demonstrated in hyperspectral unmixing.

Abstract: Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [43] [Atomistic and data-driven insights into the local slip resistances in random refractory multi-principal element alloys](https://arxiv.org/abs/2602.04827)
*Wu-Rong Jian,Arjun S. Kulathuvayal,Hanfeng Zhai,Anshu Raj,Xiaohu Yao,Yanqing Su,Shuozhi Xu,Irene J. Beyerlein*

Main category: cond-mat.mtrl-sci

TL;DR: Atomistic simulations and machine learning reveal how composition affects dislocation slip resistance in refractory multi-principal element alloys, enabling predictive yield stress modeling and alloy design guidance.


<details>
  <summary>Details</summary>
Motivation: Refractory multi-principal element alloys (RMPEAs) show exceptional high-temperature strength, but their complex compositions make it difficult to understand the fundamental mechanisms of plastic deformation at the dislocation level.

Method: Used atomistic simulations to determine local slip resistances of edge and screw dislocations on primary BCC slip planes in 12 equal-molar RMPEAs. Applied machine learning to uncover relationships between slip resistance and material properties. Developed a thermally activated, dislocation-based model to predict macroscopic yield stress.

Result: Increasing hexagonal close-packed elements above 50% reduces unstable stacking fault energy, ideal shear strength, and screw dislocation slip resistance. Higher elastic anisotropy lowers these quantities, while lattice distortion modifies relative slip resistances between dislocation characters and slip systems. Elastic constants and lattice distortion are identified as dominant factors controlling slip resistance.

Conclusion: The developed framework accurately predicts tensile yield stress in BCC RMPEAs and provides guidance for alloy design by revealing how specific compositional features affect dislocation behavior and mechanical properties.

Abstract: Refractory multi-principal element alloys (RMPEAs) have attracted growing interest for their exceptional high-temperature strength, yet their complex compositions hinder a mechanistic understanding of plastic deformation. Here, we perform atomistic simulations to determine local slip resistances (LSRs) of edge and screw dislocations on primary BCC slip planes in 12 equal-molar RMPEAs. Machine learning is employed to uncover relationships between LSR and underlying material properties, enabling systematic assessment of compositional effects on dislocation behavior. Based on these insights, we develop a thermally activated, dislocation-based model to predict macroscopic yield stress. We find that increasing the fraction of hexagonal close-packed elements above 50% significantly reduces unstable stacking fault energy, ideal shear strength, and screw LSR across all slip planes. Higher elastic anisotropy further lowers these quantities, while lattice distortion modifies relative slip resistances between dislocation characters and slip systems. By combining an autoencoder with a random forest model, we identify elastic constants and lattice distortion as the dominant factors controlling LSR. The resulting framework accurately predicts tensile yield stress in BCC RMPEAs and provides guidance for alloy design.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [44] [Caffarelli-Kohn-Nirenberg Inequalities in Weak Lebesgue Spaces](https://arxiv.org/abs/2602.04601)
*Dinghuai Wang*

Main category: math.CA

TL;DR: Weak-type Caffarelli-Kohn-Nirenberg inequalities derived via harmonic analysis, valid at critical parameters where classical versions fail, including weak-type Hardy inequalities in critical dimension d=p.


<details>
  <summary>Details</summary>
Motivation: Classical Caffarelli-Kohn-Nirenberg inequalities fail at critical parameter values, creating a gap in the theory. The paper aims to establish weak-type versions that remain valid even at these critical endpoints.

Method: Employ harmonic analysis techniques to derive weak-type inequalities. The methods are flexible enough to handle homogeneous, non-homogeneous and anisotropic weights, providing a unified approach.

Result: Successfully derived weak-type Caffarelli-Kohn-Nirenberg inequalities that hold even at critical parameter values. As a corollary, obtained weak-type Hardy inequalities that remain valid in the critical dimension d=p.

Conclusion: The developed harmonic analysis methods provide a unified framework for handling endpoint cases in interpolation theory, extending the validity of Caffarelli-Kohn-Nirenberg inequalities to critical parameters where classical versions fail.

Abstract: By employing harmonic analysis techniques, we derive weak-type Caffarelli-Kohn-Nirenberg inequalities under natural parameter conditions. A key feature of these weak-type versions is that they remain valid even at critical parameter values where the classical inequalities fail. As an important corollary, we obtain weak-type Hardy inequalities that hold true even in the critical dimension \(d = p\). The methods developed here are sufficiently flexible to handle homogeneous, non-homogeneous and anisotropic weights, providing a unified approach to various endpoint cases in interpolation theory.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [45] [Turbulence teaches equivariance to neural networks](https://arxiv.org/abs/2602.04695)
*Ryley McConkey,Julia Balla,Jeremiah Bailey,Ali Backour,Elyssa Hofgard,Tommi Jaakkola,Abigail Bodner,Tess Smidt*

Main category: physics.flu-dyn

TL;DR: Turbulence's rotational nature acts as implicit data augmentation for learned mappings between Navier-Stokes quantities, improving generalization through statistical symmetry sampling.


<details>
  <summary>Details</summary>
Motivation: To understand how turbulence's rotational structure affects learned mappings between flow quantities and how statistical symmetry impacts generalization in machine learning applications for turbulence.

Method: Train super-resolution models at different wall-normal locations in channel flow where anisotropy varies naturally, then test generalization on new coordinate frames and flow conditions.

Result: Coordinate-frame generalization is crucial; turbulence's rotational structure embeds Navier-Stokes symmetries into learned mappings, with stronger effects in more isotropic datasets and larger datasets.

Conclusion: Turbulence provides implicit data augmentation through its rotational structure, making learned mappings more generalizable across different flow conditions and coordinate frames, with scale-dependent effects consistent with turbulence theory.

Abstract: We investigate how the rotational nature of turbulence affects learned mappings between quantities governed by the Navier-Stokes equations. By varying the degree of anisotropy in a turbulence dataset, we explore how statistical symmetry affects these mappings. To do this, we train super-resolution models at different wall-normal locations in a channel flow, where anisotropy varies naturally, and test their generalization. By evaluating the learned mappings on new coordinate frames and new flow conditions, we find that coordinate-frame generalization is a key part of the generalization problem. Turbulent flows naturally present a wide range of local orientations, so respecting the symmetries of the Navier-Stokes equations improves generalization to new flows. Importantly, turbulence's rotational structure can embed these symmetries into learned mappings -- an effect that strengthens with isotropy and dataset size. This is because a more isotropic dataset samples a wider range of orientations, more fully covering the rotational symmetries of the Navier-Stokes equations. The dependence on isotropy means equivariance error is also scale-dependent, consistent with Kolmogorov's hypothesis. Therefore, turbulence provides its own data augmentation (we term this implicit data augmentation). We expect this effect to apply broadly to learned mappings between tensorial flow quantities, making it relevant to most machine learning applications in turbulence.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [46] [The matrix-vector complexity of $Ax=b$](https://arxiv.org/abs/2602.04842)
*Michał Dereziński,Ethan N. Epperly,Raphael A. Meyer*

Main category: cs.DS

TL;DR: The paper establishes fundamental lower bounds on matrix-vector product complexity for solving linear systems, showing Ω(κ log(1/ε)) products needed for two-sided algorithms and n products for one-sided algorithms, matching known upper bounds.


<details>
  <summary>Details</summary>
Motivation: Matrix-vector algorithms (especially Krylov subspace methods) are considered state-of-the-art for large linear systems, but their fundamental computational limits weren't rigorously established. The paper aims to provide theoretical lower bounds to understand the inherent limitations of these algorithms.

Method: The authors establish worst-case lower bounds through theoretical analysis. For two-sided algorithms (access to both matrix and transpose), they prove Ω(κ log(1/ε)) matrix-vector products are necessary. For one-sided algorithms (no transpose access), they prove n products are needed even for perfectly conditioned n×n systems.

Result: Two main results: 1) Two-sided algorithms require Ω(κ log(1/ε)) matrix-vector products, matching conjugate gradient upper bounds. 2) One-sided algorithms require n matrix-vector products for n×n systems. Both results include explicit constants matching known upper bounds within factor of 4.

Conclusion: The paper provides rigorous lower bounds that demonstrate fundamental limitations of matrix-vector algorithms and confirm the optimality of widely used Krylov subspace methods like conjugate gradient, establishing their theoretical efficiency.

Abstract: Matrix-vector algorithms, particularly Krylov subspace methods, are widely viewed as the most effective algorithms for solving large systems of linear equations. This paper establishes lower bounds on the worst-case number of matrix-vector products needed by such an algorithm to approximately solve a general linear system. The first main result is that, for a matrix-vector algorithm which can perform products with both a matrix and its transpose, $Ω(κ\log(1/\varepsilon))$ matrix-vector products are necessary to solve a linear system with condition number $κ$ to accuracy $\varepsilon$, matching an upper bound for conjugate gradient on the normal equations. The second main result is that one-sided algorithms, which lack access to the transpose, must use $n$ matrix-vector products to solve an $n \times n$ linear system, even when the problem is perfectly conditioned. Both main results include explicit constants that match known upper bounds up to a factor of four. These results rigorously demonstrate the limitations of matrix-vector algorithms and confirm the optimality of widely used Krylov subspace algorithms.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [47] [UGA-SSMRPT2 -- A Multireference Perturbation Theory Predicting Accurate Electronic Excitation Energies in Diverse Molecular Systems](https://arxiv.org/abs/2602.04420)
*Shamik Chanda,Pratyush Bhattacharjya,Avijit Sen,Sangita Sen*

Main category: physics.chem-ph

TL;DR: UGA-SSMRPT2, a spin-free perturbative method, is shown to be accurate and computationally inexpensive for computing excitation energies across various excited state types, outperforming popular MRPT2 methods while requiring smaller active spaces.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that UGA-SSMRPT2, originally successful for dissociation curves, can also serve as an accurate and computationally efficient framework for computing excitation energies across diverse excited state types.

Method: UGA-SSMRPT2 (Unitary Group Adapted State-Specific Multireference Perturbation Theory) - a spin-free perturbative analogue of Mukerjee's State-Specific Multireference Coupled Cluster Theory, using state-specific formulation to avoid intruder-state problems.

Result: UGA-SSMRPT2 achieves near-chemical accuracy (within 0.20 eV) for most excited states compared to EOM-CCSD and theoretical best estimates. It outperforms popular MRPT2 methods like NEVPT2, CASPT2, and MCQDPT while typically requiring smaller active spaces.

Conclusion: UGA-SSMRPT2 is proposed as a robust and scalable approach for modeling challenging electronic excited states, offering accurate excitation energies without empirical parameters and with computational efficiency.

Abstract: UGA-SSMRPT2, the spin-free perturbative analogue of Mukerjee's State-Specific Multireference Coupled Cluster Theory (MkMRCC) is known to be successful for size-extensive and intruder-free construction of dissociation curves. This work demonstrates that UGA-SSMRPT2 is also an accurate and computationally inexpensive framework for computing excitation energies. The method achieves near-chemical accuracy for the vast majority of $π\to π^*$, $n \to π^*$, charge-transfer, valence-Rydberg and Rydberg excited states commonly used for benchmarking electronic structure theories for excited states. Our results demonstrate that UGA-SSMRPT2 excitation energies lie within 0.20 eV of EOM-CCSD and/or well-established theoretical best estimates often surpassing the popular MRPT2 approaches like NEVPT2, CASPT2, and MCQDPT while typically requiring smaller active spaces. Its state-specific formulation circumvents the well-known intruder-state problem and eliminates the need for empirical parameters such as IPEA shifts in CASPT2. This work proposes UGA-SSMRPT2 as a robust, and scalable approach for modeling challenging electronic excited states.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [48] [Backend-agnostic Julia framework for 3D modeling and inversion of gravity data](https://arxiv.org/abs/2602.03857)
*Nimatullah,Pankaj K Mishra,Jochen Kamm,Anand Singh*

Main category: physics.geo-ph

TL;DR: A Julia-based GPU-accelerated framework for 3D gravity modeling and inversion that uses data-space formulation to reduce dimensionality, enabling efficient large-scale computations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address computational complexity, ill-posedness, and non-uniqueness in gravity inversion while enabling large-scale 3D modeling with improved efficiency through modern computing approaches.

Method: Data-space inversion formulation to reduce problem dimensionality, backend-agnostic kernel abstraction for CPU/GPU execution, implicit model constraints via depth-weighted sensitivity, and validation with synthetic and field data.

Result: Significant runtime reductions on NVIDIA GPUs (especially for large datasets up to 3.3 million prisms), accurate reconstruction of complex structures, and geologically coherent density models consistent with independent constraints.

Conclusion: GPU-enabled computing in Julia transforms large-scale gravity inversion workflows, providing an efficient, extensible, and accurate computational solution for high-resolution geophysical studies.

Abstract: This paper presents a high-performance framework for three-dimensional gravity modeling and inversion implemented in Julia, addressing key challenges in geophysical modeling such as computational complexity, ill-posedness, and the non-uniqueness inherent to gravity inversion. The framework adopts a data-space inversion formulation to reduce the dimensionality of the problem, leading to significantly lower memory requirements and improved computational efficiency while maintaining inversion accuracy. Forward modeling and inversion operators are implemented within a backend-agnostic kernel abstraction, enabling execution on both multicore CPUs and GPU accelerators from a single code base. Performance analyses conducted on NVIDIA CUDA GPUs demonstrate substantial reductions in runtime relative to CPU execution, particularly for large-scale datasets involving up to approximately 3.3 million rectangular prisms, highlighting the scalability of the proposed approach. The inversion incorporates implicit model constraints through the data-space formulation and depth-weighted sensitivity, which mitigate depth-related amplitude decay and yield geologically coherent, high-resolution subsurface density models. Validation using synthetic models confirms the ability of the framework to accurately reconstruct complex subsurface structures such as vertical and dipping dykes. Application to field gravity data further demonstrates the robustness and practical utility of the GPU-accelerated framework, with the recovered models showing strong consistency with independent geological constraints and prior interpretations. Overall, this work underscores the potential of GPU-enabled computing in Julia to transform large-scale gravity inversion workflows, providing an efficient, extensible, and accurate computational solution for high-resolution geophysical studies.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [49] [Nonlinear Saturation of the Acoustic Resonant Drag Instability](https://arxiv.org/abs/2602.04710)
*Ben Y. Israeli,Jonathan Squire,Eric Moseley,Amitava Bhattacharjee*

Main category: astro-ph.GA

TL;DR: Acoustic RDI simulations reveal nonlinear saturation via balance between instability growth and turbulent eddy turnover, showing anisotropic outer forcing range and isotropic inertial range.


<details>
  <summary>Details</summary>
Motivation: Resonant drag instabilities (RDIs) are important in astrophysics but their nonlinear evolution is poorly understood, requiring study of the simplest case (acoustic RDI) to establish framework for nonlinear theory.

Method: Simulations of acoustic RDI (simplest case) where sound waves in gas are amplified by interaction with supersonically streaming dust particles.

Result: Nonlinear growth and saturation characterized by balance between instability growth timescales and turbulent eddy turnover timescales, showing anisotropic outer forcing range and isotropic turbulent inertial range at smaller scales.

Conclusion: Provides model for nonlinear growth and saturation of acoustic RDI, establishing framework for studying nonlinear behavior of RDIs more broadly.

Abstract: Resonant drag instabilities (RDIs) are a novel type of dust/fluid instability relevant to a diverse range of astrophysical environments. They are driven by a resonant interaction between streaming dust and waves in a background medium, which results in dust density fluctuations and amplification of the waves. This broad class of instabilities includes recently-proposed modes incorporating acoustic and magnetohydrodynamic waves, as well as the well-studied disk streaming instability. As the study of RDIs is at an early stage, their evolution beyond the linear regime is not well understood. In order to make inroads into the nonlinear theory of RDIs, we performed simulations of the simplest case, the acoustic RDI, in which sound waves in a gas are amplified by interaction with supersonically streaming dust. This particular instability is of interest both due its potential relevance in various poorly ionized environments, and due to its resemblance to the fast magnetosonic RDI. We find that the nonlinear growth and saturation of the instability are characterized by a balance between time scales of instability growth and turbulent eddy turnover. The simulations demonstrate a saturated state possessing an anisotropic outer forcing range in which this balance is maintained, and suggest the presence of an isotropic turbulent inertial range below this scale. By presenting a model for the nonlinear growth and saturated state of the acoustic RDI, this work provides a framework for further study of the nonlinear behavior of this and other RDIs.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [50] ["$H=W$" in infinite dimensions](https://arxiv.org/abs/2602.04136)
*Zhouzhe Wang,Jiayang Yu,Xu Zhang*

Main category: math.FA

TL;DR: The paper proves that smooth cylinder functions are dense in Sobolev spaces on infinite-dimensional Hilbert spaces, establishing that H^{m,p}(O) = W^{m,p}(O) for open subsets O of ℓ² with suitable boundaries.


<details>
  <summary>Details</summary>
Motivation: In finite dimensions, Sobolev spaces H^{m,p} and W^{m,p} are equivalent, but this equivalence is difficult to establish in infinite dimensions due to the lack of translation-invariant measures. The paper aims to extend these classical results to infinite-dimensional settings.

Method: The authors develop infinite-dimensional analogues of classical techniques: truncation, boundary straightening, and partition of unity. They prove density of smooth cylinder functions in W^{m,p}(O) and establish sharp Schatten p-norm type estimates for higher-order derivatives of the Gross convolution, particularly for p=2.

Result: Main result: H^{m,p}(O) = W^{m,p}(O) for any m ∈ ℕ, p ∈ [1, ∞), and open subset O of ℓ² with suitable boundary. Additionally, the Schatten p-norm estimates for Gross convolution derivatives are shown to be sharp for p=2.

Conclusion: The paper successfully extends classical Sobolev space equivalence results to infinite-dimensional Hilbert spaces, overcoming fundamental measure-theoretic obstacles through novel adaptations of finite-dimensional techniques.

Abstract: It is well known that $H^{m,p}(Ω) = W^{m,p}(Ω)$ holds for any $m, n \in \mathbb{N}$, $p \in [1, \infty)$, and open subset $Ω$ of $\mathbb{R}^n$. Due to the essential difficulty that there exists no nontrivial translation-invariant measure in infinite dimensions, it is hard to obtain its infinite-dimensional counterparts. In this paper, using infinite-dimensional analogues of the classical techniques of truncation, boundary straightening, and partition of unity, we prove that smooth cylinder functions are dense in $W^{m,p}(O)$. Consequently, $H^{m,p}(O) = W^{m,p}(O)$ holds for any $m \in \mathbb{N}$, $p \in [1, \infty)$, and open subset $O$ of $\ell^2$ with a suitable boundary. Moreover, in the key step of compact truncation, we also prove that the Schatten $p$-norm type estimates for the higher-order derivatives of the Gross convolution are sharp for $p = 2$.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [51] [Global Convergence of the Gursky-Malchiodi $Q$-curvature Flow](https://arxiv.org/abs/2602.04267)
*Liuwei Gong,Sanghoon Lee,Juncheng Wei*

Main category: math.DG

TL;DR: The paper proves global convergence of Gursky-Malchiodi's non-local conformal flow for arbitrary initial energy under positivity assumptions, using a non-local Łojasiewicz-Simon inequality and geometric bubble analysis.


<details>
  <summary>Details</summary>
Motivation: Gursky and Malchiodi's non-local conformal flow only had sequential convergence results for small energy initial metrics. The authors aim to extend this to global convergence for arbitrary initial energy, addressing the limitations of previous work.

Method: 1) Establish a non-local version of the Łojasiewicz-Simon inequality for the Paneitz-Sobolev quotient along the flow. 2) Construct test bubbles and estimate their Paneitz-Sobolev quotients using geometric methods inspired by Brendle's Yamabe flow work. 3) Develop systematic geometric proofs to handle algebraic complexity of Q-curvature and Paneitz operator. 4) Derive stability inequality using higher-order Koiso-Bochner formula from recent work.

Result: Proves global convergence of the non-local conformal flow for arbitrary initial energy under positivity assumptions (positive scalar curvature and Q-curvature), overcoming the small energy limitation of previous results.

Conclusion: The paper successfully extends Gursky-Malchiodi's flow theory to arbitrary energy levels through geometric analysis techniques, establishing a complete convergence theory for the constant Q-curvature problem in dimensions n≥5.

Abstract: In their seminal work, Gursky and Malchiodi introduced a non-local conformal flow in dimensions $n \geq 5$ to resolve the constant $Q$-curvature problem. They proved sequential convergence of the flow for initial metrics with positive scalar curvature and $Q$-curvature, provided the energy was sufficiently small.
  In this paper, we prove the global convergence of the flow for arbitrary initial energy under the same positivity assumptions by establishing a non-local version of the Łojasiewicz-Simon inequality for the Paneitz-Sobolev quotient along the flow.
  We construct test bubbles and estimate their Paneitz-Sobolev quotients, a strategy that was carried out in the celebrated work of Brendle in the context of the Yamabe flow. We develop a more geometric and systematic proof that addresses the algebraic and computational complexity inherent in the $Q$-curvature and the Paneitz operator. Along the way, we derive a stability inequality for the Paneitz-Sobolev quotient using a higher-order Koiso-Bochner formula established in recent work of Bahuaud, Guenther, Isenberg, and Mazzeo.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [52] [Exterior complex scaling enables physics-informed neural networks for quantum scattering](https://arxiv.org/abs/2602.04553)
*Jin Lei*

Main category: nucl-th

TL;DR: PINNs + exterior complex scaling enables accurate quantum scattering solutions for nuclear and heavy-ion systems with high precision.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with quantum scattering problems due to oscillatory, non-decaying wave functions. Need method to handle scattering boundary conditions for nuclear physics applications.

Method: Combine PINNs with exterior complex scaling (ECS) to transform scattering waves into exponentially decaying forms. Use driven-equation formulation with source term confined to real axis, avoiding complex continuation of nuclear potentials.

Result: Successfully solved nucleon-nucleus scattering (n+⁴⁰Ca at 20 MeV) with 21 partial waves achieving phase shift accuracy Δδ < 0.1° for most channels. Also demonstrated on heavy-ion scattering (⁶Li+²⁰⁸Pb at 40 MeV) with 41 partial waves and strong Coulomb effects.

Conclusion: This work establishes foundation for extending PINNs to inverse problems, optical potential fitting, coupled-channel reactions, and few-body scattering where traditional methods face exponential scaling challenges.

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving differential equations, yet their application to quantum scattering problems has been hindered by the oscillatory, non-decaying nature of scattering wave functions. In this work, I demonstrate that exterior complex scaling (ECS) transforms scattering boundary conditions into exponentially decaying waves suitable for neural network solutions, enabling PINNs to solve nuclear scattering problems for the first time. I develop a driven-equation formulation where the source term is confined to the real axis, avoiding the need to analytically continue nuclear potentials into the complex plane. The method is validated on nucleon-nucleus scattering (n+$^{40}$Ca at $E_{\text{lab}}=20$~MeV) with 21 partial waves, achieving phase shift accuracy of $Δδ< 0.1^\circ$ for most channels when compared to conventional solvers. I further demonstrate the approach on heavy-ion scattering ($^6$Li+$^{208}$Pb at 40~MeV) with 41 partial waves and strong Coulomb effects. This work establishes the foundation for extending PINNs to inverse problems where end-to-end differentiability enables direct fitting of optical potential parameters, coupled-channel reactions, and few-body scattering where traditional grid methods face exponential scaling.

</details>


<div id='cond-mat.quant-gas'></div>

# cond-mat.quant-gas [[Back]](#toc)

### [53] [Density Modulations of Zero Sound](https://arxiv.org/abs/2602.04612)
*Leonardo Pisani*

Main category: cond-mat.quant-gas

TL;DR: Study of density modulation in interacting Fermi gas caused by moving impurity at zero temperature, focusing on zero sound excitation above threshold speed.


<details>
  <summary>Details</summary>
Motivation: To understand how density modulation propagates in strongly interacting Fermi gases when an impurity moves through it, particularly examining the role of collective zero sound mode versus particle-hole excitations.

Method: Semi-analytic evaluation of density modulation in interacting Fermi gas at zero temperature, analyzing impurity motion above zero sound threshold, with investigation of how results depend on interaction potential features (strength, range, shape).

Result: For strong enough interactions, density modulation propagates via zero sound excitation when impurity speed exceeds zero sound threshold; researchers can assess the extent of zero sound contribution relative to incoherent particle-hole background.

Conclusion: The propagation of density modulation in strongly interacting Fermi gases depends critically on impurity speed relative to zero sound threshold, with results strongly influenced by the specific features of the gas interaction potential.

Abstract: We study the density modulation of an interacting Fermi gas caused by the uniform motion of an impurity at zero temperature. For strong enough interaction among Fermi atoms, the modulation propagates thanks to the excitation of the collective zero sound mode if the impurity speed is above the zero sound threshold. We are able to assess, via a semi-analytic evaluation, the extent of the zero sound contribution to the density oscillation over and above the incoherent background of particle-hole excitations. Given the strong dependence of the results on the features of the gas interaction potential, we also analyze how they vary depending on its strength, range and shape.

</details>
