<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 27]
- [math.AP](#math.AP) [Total: 38]
- [physics.comp-ph](#physics.comp-ph) [Total: 7]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [math.DG](#math.DG) [Total: 3]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [math.PR](#math.PR) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Stability and Convergence of Modal Approximations in Coupled Thermoelastic Systems: Theory and Simulation](https://arxiv.org/abs/2602.07224)
*I. Essadeq,S. Nafiri,S. Benjelloun,A. E. Fettouh*

Main category: math.NA

TL;DR: The paper analyzes thermoelastic systems using spectral methods to derive uniform polynomial decay rates for semigroups, with numerical experiments showing how initial data regularity affects decay rates.


<details>
  <summary>Details</summary>
Motivation: To understand the decay behavior of both strongly and weakly coupled thermoelastic systems, examining how spectral structure relates to energy dissipation, and investigating the practical implications of initial data regularity on observed decay rates.

Method: Employ spectral analysis techniques and establish uniform resolvent estimates to derive uniform polynomial decay rates for associated semigroups. Use modal approximations in energy analysis and complement theoretical results with numerical experiments comparing smooth versus nonsmooth initial data.

Result: Derived uniform polynomial decay rates for thermoelastic semigroups under suitable boundary conditions. Numerical experiments revealed that the regularity of initial data (smooth vs. nonsmooth) significantly affects observed decay rates, providing insight into the interplay between spectral structure and energy dissipation.

Conclusion: The study successfully connects theoretical spectral analysis with practical numerical observations, demonstrating that initial data regularity plays a crucial role in the decay behavior of thermoelastic systems, with implications for understanding energy dissipation mechanisms in coupled systems.

Abstract: In this work, we review and analyze both the theoretical and numerical aspects of strongly and weakly coupled thermoelastic systems. By employing spectral analysis techniques and establishing uniform resolvent estimates, we derive uniform polynomial decay rates for the associated semigroups under a suitable class of boundary conditions. Particular attention is paid to the role of modal approximations in energy analysis. The theoretical results are complemented by numerical experiments that illustrate how the regularity of initial data, smooth versus nonsmooth, affects the observed decay rates, providing deeper insight into the interplay between spectral structure and energy dissipation.

</details>


### [2] [Scalable Preconditioners for the Pseudo-4D DFN Lithium-ion Battery Model](https://arxiv.org/abs/2602.07225)
*Thomas Roy,Nicholas W. Brady,Giovanna Bucci,Nicholas R. Cross,Victoria M. Ehlinger,Tiras Y. Lin,Hanyu Li,Marcus A. Worsley*

Main category: math.NA

TL;DR: Block-structured preconditioning strategies enable scalable solution of pseudo-4D Doyle-Fuller-Newman battery models with 3D electrode architectures and particle-scale diffusion.


<details>
  <summary>Details</summary>
Motivation: The pseudo-4D DFN model enables predictive simulation of lithium-ion batteries with 3D electrode architectures, but leads to large, nonlinear systems with strong coupling across multiple physical scales, posing significant challenges for scalable numerical solution.

Method: Block-structured preconditioning strategies that exploit mathematical properties of the coupled system, employing multigrid techniques for electrode-level operators and localized solvers for particle-scale diffusion.

Result: Comprehensive scalability studies across various geometries show the methods deliver efficient convergence and enable solution of battery models with hundreds of millions of degrees of freedom on large-scale parallel hardware.

Conclusion: The proposed preconditioning strategies provide robust, scalable numerical methods for solving complex pseudo-4D battery models, enabling high-fidelity simulation of advanced battery architectures.

Abstract: The pseudo-4D Doyle-Fuller-Newman (DFN) model enables predictive simulation of lithium-ion batteries with three-dimensional electrode architectures and particle-scale diffusion, extending the standard pseudo-2D (P2D) formulation to fully resolve cell geometry. This leads to large, nonlinear systems with strong coupling across multiple physical scales, posing significant challenges for scalable numerical solution. We introduce block-structured preconditioning strategies that exploit the mathematical properties of the coupled system, employing multigrid techniques for electrode-level operators and localized solvers for particle-scale diffusion. Comprehensive scalability studies are performed across a range of geometries, including homogeneous and heterogeneous cubic cells, flattened jelly-roll configurations, and triply periodic minimal surface electrodes, to assess solver robustness and parallel scalability. The proposed methods consistently deliver efficient convergence and enable the solution of battery models with hundreds of millions of degrees of freedom on large-scale parallel hardware.

</details>


### [3] [A Unifying Framework for Doubling Algorithms](https://arxiv.org/abs/2602.07250)
*Changli Liu,Tiexiang Li,Jungong Xue,Ren-Cang Li,Wen-Wei Lin*

Main category: math.NA

TL;DR: A new Q-doubling algorithm is proposed that generalizes existing doubling algorithms for solving nonlinear matrix equations, removing the restrictive requirement that eigenspace basis matrices must take particular forms, and demonstrating superior robustness.


<details>
  <summary>Details</summary>
Motivation: Existing doubling algorithms for nonlinear matrix equations require that the eigenspace basis matrix takes one of two particular forms, which cannot be guaranteed in general. This limitation restricts the applicability and robustness of current methods.

Method: Proposes a new Q-doubling algorithm that generalizes existing doubling algorithms. The method does not require the basis matrix to take particular forms, making it more flexible. It includes existing doubling algorithms as special cases.

Result: Numerical experiments show that the Q-doubling algorithm demonstrates superior robustness compared to existing doubling algorithms when applied to solve eigenvalue problems.

Conclusion: The Q-doubling algorithm overcomes the limitations of existing doubling algorithms by removing restrictive form requirements for basis matrices, providing a more robust and generalizable approach for solving nonlinear matrix equations from engineering applications.

Abstract: The existing doubling algorithms have been proven efficient for several important nonlinear matrix equations arising from real-world engineering applications. In a nutshell, the algorithms iteratively compute a basis matrix, in one of the two particular forms, for the eigenspace of some matrix pencil associated with its eigenvalues in certain complex region such as the left-half plane or the open unit disk, and their success critically depends on that the interested eigenspace do have a basis matrix taking one of the two particular forms. However, that requirement in general cannot be guaranteed. In this paper, a new doubling algorithm, called the $Q$-doubling algorithm, is proposed. It includes the existing doubling algorithms as special cases and does not require that the basis matrix takes one of the particular forms. An application of the $Q$-doubling algorithm to solve eigenvalue problems is investigated with numerical experiments that demonstrate its superior robustness to the existing doubling algorithms.

</details>


### [4] [Multifidelity sensor placement in Bayesian state estimation problems](https://arxiv.org/abs/2602.07269)
*Gabriela Ramon,Geena Sarnoski,Vasishta Tumuluri,Hugo Díaz,Arvind K. Saibaba*

Main category: math.NA

TL;DR: Novel greedy algorithms for budget-constrained multifidelity sensor placement using D-optimality criterion and column subset selection connections.


<details>
  <summary>Details</summary>
Motivation: Need optimal sensor placement for Bayesian state estimation when sensors vary in cost and fidelity, requiring budget-constrained multifidelity experimental design.

Method: 1) Greedy approach with computational efficiency improved via Sherman-Morrison rank-one updates; 2) Iterative algorithm that greedily optimizes sensor fidelity for each feasible allocation subject to previous choices.

Result: Methods evaluated on benchmark state estimation problems (sea surface temperature, flow around cylinder) show improved performance over random designs.

Conclusion: Proposed algorithms are novel for cost-constrained multifidelity sensor placement and demonstrate empirical effectiveness on practical state estimation problems.

Abstract: We study optimal sensor placement for Bayesian state estimation problems in which sensors vary in cost and fidelity, resulting in a budget-constrained multifidelity optimal experimental design problem. Sensor placement optimality is quantified using the D-optimality criterion, and the problem is approached by leveraging connections with the column subset selection problem in numerical linear algebra. We implement a greedy approach for this problem, whose computational efficiency we improve using rank-one updates via the Sherman-Morrison formula. We additionally present an iterative algorithm that, for each feasible allocation of sensors, greedily optimizes over each sensor fidelity subject to previous sensor choices, repeating this process until a termination criterion is satisfied. To the best of our knowledge, these algorithms are novel in the context of cost constrained multifidelity sensor placement. We evaluate our methods on several benchmark state estimation problems, including reconstructions of sea surface temperature and flow around a cylinder, and empirically demonstrate improved performance over random designs.

</details>


### [5] [Convergence of a Low-Rank Strang Splitting for Stiff Matrix Differential Equations](https://arxiv.org/abs/2602.07437)
*Carmen Scalone,Nicola Guglielmi*

Main category: math.NA

TL;DR: Second-order Strang splitting method for stiff Sylvester-type matrix ODEs, combining exact stiff linear part with DLR for nonlinear part, proven second-order convergence.


<details>
  <summary>Details</summary>
Motivation: Need efficient numerical methods for stiff matrix differential equations with Sylvester structure, where stiffness poses challenges for standard integration methods.

Method: Strang splitting separates stiff linear part (solved exactly via matrix exponentials) from nonlinear part (integrated by second-order dynamical low-rank scheme).

Result: Rigorous convergence proof shows second-order accuracy under suitable assumptions; numerical experiments confirm theory and demonstrate robustness/efficiency.

Conclusion: Proposed splitting method effectively handles stiffness in Sylvester-type matrix ODEs with proven second-order convergence and practical efficiency.

Abstract: We propose and analyze a second-order Strang splitting method for a class of stiff matrix differential equations with Sylvester-type structure. The method splits the dynamics into a stiff linear part, treated exactly via matrix exponentials, and a nonlinear part, integrated by a second-order dynamical low-rank (DLR) scheme. Our main contribution is a rigorous convergence proof showing that, under suitable assumptions, the overall scheme achieves second-order accuracy. Numerical experiments confirm the theoretical results and demonstrate the robustness and efficiency of the proposed method.

</details>


### [6] [Learned Finite Element-based Regularization of the Inverse Problem in Electrocardiographic Imaging](https://arxiv.org/abs/2602.07466)
*Manuel Haas,Thomas Grandits,Thomas Pinetz,Thomas Beiert,Simone Pezzuto,Alexander Effland*

Main category: math.NA

TL;DR: Space-time regularization framework for ECGI that combines spatial regularization with learned temporal Fields-of-Experts prior to improve cardiac electrical activity reconstruction from body-surface potentials.


<details>
  <summary>Details</summary>
Motivation: ECGI's inverse problem is severely ill-posed and requires robust regularization. Classical approaches primarily use spatial smoothing, but the temporal structure of cardiac dynamics remains underexploited despite its physiological relevance.

Method: Introduces a space-time regularization framework that couples spatial regularization with a learned temporal Fields-of-Experts (FoE) prior to capture complex spatiotemporal activation patterns. Derives finite element discretization on unstructured cardiac surface meshes, proves Mosco-convergence, and develops a scalable optimization algorithm capable of handling the FoE term.

Result: Numerical experiments on synthetic epicardial data demonstrate improved denoising and inverse reconstructions compared to handcrafted spatiotemporal methods, yielding solutions that are both robust to noise and physiologically plausible.

Conclusion: The proposed space-time regularization framework with learned temporal FoE prior effectively captures cardiac spatiotemporal dynamics, providing more robust and physiologically plausible ECGI reconstructions than traditional methods.

Abstract: Electrocardiographic imaging (ECGI) seeks to reconstruct cardiac electrical activity from body-surface potentials noninvasively. However, the associated inverse problem is severely ill-posed and requires robust regularization. While classical approaches primarily employ spatial smoothing, the temporal structure of cardiac dynamics remains underexploited despite its physiological relevance. We introduce a space-time regularization framework that couples spatial regularization with a learned temporal Fields-of-Experts (FoE) prior to capture complex spatiotemporal activation patterns. We derive a finite element discretization on unstructured cardiac surface meshes, prove Mosco-convergence, and develop a scalable optimization algorithm capable of handling the FoE term. Numerical experiments on synthetic epicardial data demonstrate improved denoising and inverse reconstructions compared to handcrafted spatiotemporal methods, yielding solutions that are both robust to noise and physiologically plausible.

</details>


### [7] [Stability and error analysis of fully discrete original energy-dissipative and length-preserving scheme for the Landau-Lifshitz-Gilbert equation](https://arxiv.org/abs/2602.07571)
*Binghong Li,Xiaoli Li,Cheng Wang,Jiang Yang*

Main category: math.NA

TL;DR: A linear finite difference scheme using projection method for LLG equation that preserves both manifold constraint and unconditional energy dissipation, with proven optimal convergence rate.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LLG equation have limitations: normalized tangent plane method is computationally expensive, while projection method compromises energy dissipation and lacks rigorous error analysis.

Method: Constructed a linear, fully discrete finite difference scheme based on projection method that preserves |m|=1 constraint and unconditional energy dissipation; used equivalent weak form analysis for error estimates.

Result: Developed first linear algorithm that simultaneously achieves: (i) point-wise length preservation, (ii) unconditional original energy dissipation, (iii) theoretical convergence analysis with optimal rate error estimate.

Conclusion: The proposed projection-based finite difference scheme successfully addresses computational efficiency while maintaining key physical properties of LLG equation, with rigorous mathematical foundation.

Abstract: The Landau-Lifshitz-Gilbert (LLG) equation, regarded as a gradient flow with manifold constraint, is the fundamental model describing magnetization dynamics in ferromagnetic materials. It is well known that the normalized tangent plane method is able to simultaneously achieve the non-convex manifold constraint and original energy dissipation. However, the associated computational cost of this numerical approach is exceedingly high. By contrast, the projection method is more straightforward to implement, while it often compromises the inherent energy dissipative property of the continuous model, and the error analysis turns out to be even more challenging. In this work, we first construct a linear and fully discrete finite difference numerical scheme, based on the projection method for the LLG equation, which is capable of simultaneously preserving the non-convex manifold constraint \(|\mathbf{m}| = 1\) and an unconditional original energy dissipation. In the error analysis, the classical theoretical technique becomes ineffective, due to the presence of the nonlinear Laplacian term, which in turn poses a significant challenge. To overcome this subtle difficulty, we carefully rewrite the numerical method in an equivalent weak form, in which a point-wise length preserving feature of the numerical solution plays an essential role. As a result of these estimates in the reformulated weak form, an optimal convergence rate could be theoretically established. In our knowledge, this numerical method is the first linear algorithm that preserves the following combined theoretical properties: (i) point-wise length preservation, (ii) unconditional original energy dissipation, (iii) a theoretical justification of convergence analysis and optimal rate error estimate.

</details>


### [8] [An Efficient and Robust Projection Enhanced Interpolation Based Tensor Train Decomposition](https://arxiv.org/abs/2602.07653)
*Daniel Hayes,Jing-Mei Qiu,Tianyi Shi*

Main category: math.NA

TL;DR: Proposes projection-enhanced interpolation algorithms to improve accuracy of skeletonized tensor-train approximations while maintaining low computational cost.


<details>
  <summary>Details</summary>
Motivation: Skeletonized low-rank tensor approximations (like tensor-train format) suffer from accuracy degeneracy, nonrobustness, and high computation costs, despite being useful for interpretability in data science.

Method: Develops projection-enhanced interpolation algorithms as postprocessing to existing interpolative decompositions. Uses oversampling of non-skeleton data to include more information and selects subsets of pivots for faster projections.

Result: Significant accuracy improvement over original skeletonized TT approximations demonstrated through extensive numerical experiments on up to 10D synthetic datasets, including kernel-generated tensors and Maxwellian distribution functions from kinetic theory.

Conclusion: The proposed family of algorithms effectively enhances skeletonized tensor-train approximations by improving accuracy while keeping computational complexity low and using limited computational resources.

Abstract: The tensor-train (TT) format is a data-sparse tensor representation commonly used in high dimensional data approximations. In order to represent data with interpretability in data science, researchers develop data-centric skeletonized low rank approximations. However, these methods might still suffer from accuracy degeneracy, nonrobustness, and high computation costs. In this paper, given existing skeletonized TT approximations, we propose a family of projection enhanced interpolation based algorithms to further improve approximation accuracy while keeping low computational complexity. We do this as a postprocessing step to existing interpolative decompositions, via oversampling data not in skeletons to include more information and selecting subsets of pivots for faster projections. We illustrate the performances of our proposed methods with extensive numerical experiments. These include up to 10D synthetic datasets such as tensors generated from kernel functions, and tensors constructed from Maxwellian distribution functions that arise in kinetic theory. Our results demonstrate significant accuracy improvement over original skeletonized TT approximations, while using limited amount of computational resources.

</details>


### [9] [High-Resolution Solvers for 3D Helmholtz Scattering Problems Using PFFT and Eigenvector-Based Preconditioning](https://arxiv.org/abs/2602.07711)
*Yury Gryazin,Ron Gonzales,Xiaoye Sherry Li*

Main category: math.NA

TL;DR: Efficient Krylov solver for 3D Helmholtz equation using high-order compact schemes with novel low-order preconditioners (EigT and PFFT) to reduce pollution error and improve convergence.


<details>
  <summary>Details</summary>
Motivation: To solve large, ill-conditioned linear systems from 3D Helmholtz equations with non-constant coefficients and absorbing boundaries while reducing numerical dispersion and pollution error that plague high-resolution schemes.

Method: Combine fourth- and sixth-order compact finite-difference schemes with preconditioned GMRES using two novel low-order preconditioners: one based on eigenvector transformation (EigT) and another on partial Fast Fourier Transform (PFFT), both derived from lower-order approximations of the original problem.

Result: The method significantly softens the strict points-per-wavelength requirement, reduces pollution error, and demonstrates efficient performance for realistic problem sizes and parameters through comprehensive numerical experiments.

Conclusion: The proposed low-order preconditioning strategy effectively accelerates convergence for high-resolution Helmholtz solvers, with both EigT and PFFT preconditioners showing strong performance for practical 3D problems with absorbing boundaries.

Abstract: This paper presents an efficient Krylov subspace iterative solver for the three-dimensional (3D) Helmholtz equation with non-constant coefficients and absorbing boundary conditions, combining high-resolution compact schemes with low-order preconditioners. To mitigate numerical dispersion and reduce pollution error, we employ fourth- and sixth-order compact finite-difference schemes, thereby significantly softening the strict points-per-wavelength requirement. The resulting large, ill-conditioned linear systems are solved using a preconditioned GMRES method. The key innovation lies in the construction of the preconditioner: we introduce two highly efficient direct solvers - one based on a low-dimensional eigenvector transformation (EigT) and another on a partial Fast Fourier Transform (PFFT) algorithm - both derived from a lower-order approximation of the original problem that incorporates the absorbing boundary conditions. The motivation and efficacy of this lower-order preconditioning strategy for high-resolution schemes are analyzed through model problems, providing insight into the convergence rate. The theoretical analysis is validated by a comprehensive set of numerical experiments, demonstrating the method's performance for realistic problem sizes and parameters.

</details>


### [10] [Certified surface approximations using the interval Krawczyk test](https://arxiv.org/abs/2602.07718)
*Michael Burr,Jonathan D. Hauenstein,Kisun Lee*

Main category: math.NA

TL;DR: Algorithm for certified surface approximation using generalized Krawczyk test for non-square systems and higher-dimensional varieties


<details>
  <summary>Details</summary>
Motivation: The Krawczyk test is limited to square systems of analytic equations, but many practical problems involve non-square systems and higher-dimensional varieties that need certified approximations.

Method: Generalize the Krawczyk test (based on interval arithmetic) to handle non-square systems and higher-dimensional varieties, enabling certified approximation of surfaces.

Result: Developed a prototype implementation and demonstrated its effectiveness on several examples, showing the technique can handle cases beyond traditional square systems.

Conclusion: The generalized Krawczyk test successfully extends certified approximation capabilities to non-square systems and higher-dimensional varieties, providing a practical tool for surface approximation with mathematical guarantees.

Abstract: We propose an algorithm to construct a certified approximation of a surface by generalizing the Krawczyk test. The Krawczyk test is based on interval arithmetic, and confirms the existence and uniqueness of a solution to a square system of analytic equations in a region. By generalizing this test, we extend the reach of this technique to non-square systems and higher-dimensional varieties. We provide a prototype implementation and illustrate its use on several examples.

</details>


### [11] [Data-Driven Discovery of Sign-Indefinite Artificial Viscosity for Linear Convection -- A Space-Time Reconvolution Perspective](https://arxiv.org/abs/2602.07733)
*Arun Govind Neelan*

Main category: math.NA

TL;DR: Data-driven discovery shows that optimal artificial viscosity for stabilizing convection equations can be locally negative near extrema, challenging classical views that require positive viscosity everywhere.


<details>
  <summary>Details</summary>
Motivation: Traditional understanding of artificial viscosity as strictly positive spatial regularization is challenged by data-driven optimization results showing locally negative viscosity can produce stable, accurate solutions.

Method: Use automatic differentiation and gradient-based optimization to learn artificial viscosity fields for unstable FTCS discretization of linear convection equation, without imposing sign constraints, minimizing error relative to exact solution.

Result: Optimized viscosity consistently becomes locally negative near extrema while maintaining stability and near-exact solutions, contradicting classical modified equation analysis that predicts strictly positive effective viscosity.

Conclusion: Artificial viscosity should be reinterpreted as a space-time closure mechanism where entropy stability constrains integrated dissipation budget rather than pointwise positivity, allowing locally negative viscosity to compensate dispersive errors as a numerical reconvolution operator.

Abstract: Artificial viscosity is traditionally interpreted as a positive, spatially acting regularization introduced to stabilize numerical discretizations of hyperbolic conservation laws. In this work, we report a data-driven discovery that motivates a reinterpretation of this classical view. We consider the linear convection equation discretized using an unstable FTCS scheme augmented with a learnable artificial viscosity. Using automatic differentiation and gradient-based optimization, the viscosity field is inferred by minimizing the error with respect to the exact solution, without imposing any sign constraints. The optimized viscosity consistently becomes locally negative near extrema, while the numerical solution remains stable and nearly exact. This behavior is not readily explained within classical modified equation analysis and Lax-Wendroff-type arguments, which predict a strictly positive effective viscosity. To resolve this apparent contradiction, we reinterpret artificial viscosity as a space-time closure that compensates unresolved truncation errors while enforcing entropy stability through global dissipation balance rather than pointwise positivity. Within this framework, the Lax-Wendroff scheme corresponds to a degenerate projection in which temporal truncation errors are eliminated and reintroduced as spatial diffusion. We show that entropy stability constrains the integrated dissipation budget rather than the pointwise sign of spatial viscosity. As a result, locally negative viscosity naturally emerges as a numerical reconvolution operator that compensates for dispersive truncation errors. Negative viscosity is therefore not an unphysical diffusion process, but a scheme- and grid-dependent correction mechanism.

</details>


### [12] [Fast Jacobi Spectral Methods and Closure Approximations for the Homogeneous FENE Model of Complex Fluids](https://arxiv.org/abs/2602.07752)
*Runkai Feng,Jie Shen,Haijun Yu*

Main category: math.NA

TL;DR: Developed fast spectral methods for FENE dumbbell model with rigorous stability proofs, and compared closure approximations including a novel neural network implementation.


<details>
  <summary>Details</summary>
Motivation: The FENE dumbbell model is important for complex fluids but direct simulation is computationally challenging due to high dimensionality and singularity near boundary.

Method: Two fast Jacobi-Spherical Harmonic spectral methods with weighted variational formulation to resolve singularity; BDF2 time marching with proved energy stability; comparison of closure approximations (FENE-P, FENE-QE, and novel FENE-QE-NN neural network).

Result: Methods achieve spectral convergence and efficiency, providing reliable reference solutions; FENE-QE-NN shows superior accuracy and efficiency in extensional and shear flows compared to traditional approaches.

Conclusion: The proposed spectral methods and neural network-enhanced closure approximation offer optimal trade-off between accuracy and efficiency for FENE model simulations.

Abstract: The Finitely Extensible Nonlinear Elastic (FENE) dumbbell model is a widely used mathematical model for complex fluids. Direct simulation of the FENE Fokker--Planck equation is computationally challenging due to high dimensionality and singularity of its potential. In this paper, we develop two fast Jacobi-Spherical Harmonic spectral methods for the spatially homogeneous FENE Fokker--Planck equation. These methods effectively resolve the singularity near the boundary by combining properly designed Jacobi polynomials with a weighted variational formulation. A semi-implicit backward differentiation formula of second-order (BDF2) is employed for time marching, and its energy stability is rigorously proved. The resulting linear algebraic system possesses a sparse structure and can be efficiently solved. Numerical results verify the spectral convergence and efficiency of the direct spectral solvers, establishing them as a reliable tool for generating reference solutions for challenging benchmark problems. Furthermore, to achieve an optimal trade-off between accuracy and efficiency, we compare several closure approximation models, including the industry workhorse Peterlin approximation (FENE-P), the quasi-equilibrium approximation (FENE-QE), and a novel neural network implementation for FENE-QE proposed in this paper (FENE-QE-NN). Numerical experiments in extensional and shear flows demonstrate the superior accuracy and efficiency of the proposed methods compared to traditional approaches.

</details>


### [13] [Data Completion for Electrical Impedance Tomography by Conditional Diffusion Models](https://arxiv.org/abs/2602.07813)
*Ke Chen,Haizhao Yang,Chugang Yi*

Main category: math.NA

TL;DR: Diffusion-based generative model completes undersampled EIT measurements, enabling comparable reconstructions with only 1% of data vs 30% needed by matrix completion.


<details>
  <summary>Details</summary>
Motivation: Data scarcity in Electrical Impedance Tomography (EIT) degrades conductivity reconstructions due to undersampled Dirichlet-to-Neumann measurements.

Method: Train conditional diffusion model to learn DtN data distribution and infer full measurements from partial observations; use as preprocessing step with existing EIT solvers.

Result: Diffusion completion enables reconstructions comparable to full data baseline using only 1% of measurements, vs 30% needed by matrix completion; theoretical bounds on distributional discrepancy.

Conclusion: Diffusion-based measurement completion effectively addresses EIT data scarcity, achieving high-quality reconstructions with minimal measurements and supporting flexible source-receiver configurations.

Abstract: Data scarcity is a fundamental barrier in Electrical Impedance Tomography (EIT), as undersampled Dirichlet-to-Neumann (DtN) measurements can substantially degrade conductivity reconstructions. We address this bottleneck by completing partially observed DtN measurements using a diffusion based generative model. Specifically, we train a conditional diffusion model to learn the distribution of DtN data and to infer full measurement vectors given partial observations. Our approach supports flexible source receiver configurations and can be used as a plug in preprocessing step with off the shelf EIT solvers. Under mild assumptions on the polygon conductivity class, we derive nonasymptotic end to end bounds on the distributional discrepancy between the completed and ground truth DtN measurements. In numerical experiments, we couple the proposed diffusion completion procedure with a deep learning based inverse solver and compare its performance against the same solver with full measurement data. The results show that diffusion completion enables reconstructions comparable to the full data baseline while using only 1% of the measurements. In contrast, standard baselines such as matrix completion require 30% of the measurements to achieve similar reconstruction quality.

</details>


### [14] [Field conserving adaptive mesh refinement (AMR) scheme on massively parallel adaptive octree meshes](https://arxiv.org/abs/2602.07817)
*Kumar Saurabh,Makrand A. Khanwale,Masado Ishii,Hari Sundar,Baskar Ganapathysubramanian*

Main category: math.NA

TL;DR: A field-conserving coarsening operator for AMR that maintains discrete global conservation during mesh coarsening via quadrature-based conservation enforcement and L² projection.


<details>
  <summary>Details</summary>
Motivation: Standard injection-based coarsening in AMR for continuous Galerkin discretizations introduces systematic drift in conserved quantities during long-horizon simulations, especially for variational discretizations with continuous basis functions.

Method: Proposes a scalable field-conserving coarsening operator that: 1) computes field-conserving coarse-element values at quadrature points, 2) recovers coarse nodal degrees of freedom via L² projection (mass-matrix solve) to control L₂ error, and 3) works for parallel, octree-based AMR.

Result: Evaluated on mass-conserving phase-field models (Cahn-Hilliard and Cahn-Hilliard-Navier-Stokes systems), showing improved conservation error and solution quality compared to standard injection, with reasonable computational cost.

Conclusion: The proposed method provides a simple, scalable solution to the conservation problem in AMR coarsening, maintaining discrete global conservation while controlling approximation error for continuous Galerkin discretizations.

Abstract: Adaptive mesh refinement (AMR) is widely used to efficiently resolve localized features in time-dependent partial differential equations (PDEs) by selectively refining and coarsening the mesh. However, in long-horizon simulations, repeated intergrid interpolations can introduce systematic drift in conserved quantities, especially for variational discretizations with continuous basis functions. While interpolation from parent-to-child during refinement in continuous Galerkin (CG) discretizations is naturally conservative, the standard injection-based child-to-parent coarsening interpolation is generally not.
  We propose a simple, scalable field-conserving coarsening operator for parallel, octree-based AMR. The method enforces discrete global conservation during coarsening by first computing field conserving coarse-element values at quadrature points and then recovering coarse nodal degrees of freedom via an $L^2$ projection (mass-matrix solve), which simultaneously controls the $L_2$ error. We evaluate the approach on mass-conserving phase-field models, including the Cahn--Hilliard and Cahn--Hilliard--Navier--Stokes systems, and compare against injection in terms of conservation error, solution quality, and computational cost.

</details>


### [15] [Deep Energy Method with Large Language Model assistance: an open-source Streamlit-based platform for solving variational PDEs](https://arxiv.org/abs/2602.07838)
*Yizheng Wang,Cosmin Anitescu,Mohammad Sadegh Eshaghi,Xiaoying Zhuang,Timon Rabczuk,Yinghua Liu*

Main category: math.NA

TL;DR: LM-DEM is an open-source platform that combines large language models with the deep energy method to solve variational PDEs, featuring natural language geometry generation and a user-friendly interface.


<details>
  <summary>Details</summary>
Motivation: Energy-form PINNs (DEM) have advantages over strong-form PINNs but lack dedicated, user-friendly software. Traditional geometry preprocessing is burdensome, creating barriers for practitioners.

Method: LM-DEM integrates LLMs for geometry modeling from natural language/images, uses deep energy method for PDE solving, supports built-in problems (Poisson, elasticity, hyperelasticity), and allows user-defined energy functionals similar to Abaqus UMAT.

Result: An open-source Streamlit-based platform with web accessibility that reduces geometry preprocessing burden and lowers adoption barriers for energy-form PINNs in computational mechanics.

Conclusion: LM-DEM successfully addresses the software gap for energy-form PINNs by combining LLM-assisted geometry modeling with DEM solving, making variational PDE problems more accessible to practitioners and beginners.

Abstract: Physics-informed neural networks (PINNs) in energy form, also known as the deep energy method (DEM), offer advantages over strong-form PINNs such as lower-order derivatives and fewer hyperparameters, yet dedicated and user-friendly software for energy-form PINNs remains scarce. To address this gap, we present \textbf{LM-DEM} (Large-Model-assisted Deep Energy Method), an open-source, Streamlit-based platform for solving variational partial differential equations (PDEs) in computational mechanics. LM-DEM integrates large language models (LLMs) for geometry modeling: users can generate Gmsh-compatible geometries directly from natural language descriptions or images, significantly reducing the burden of traditional geometry preprocessing. The solution process is driven by the deep energy method, while finite element solutions can be obtained in parallel. The framework supports built-in problems including Poisson, screened Poisson, linear elasticity, and hyperelasticity in two and three dimensions, as well as user-defined energy functionals analogous to the \texttt{UMAT} interface in Abaqus. The source code is available at https://github.com/yizheng-wang/LMDEM, and a web-based version is accessible at https://ai4m.llmdem.com. LM-DEM aims to lower the barrier for practitioners and beginners to adopt energy-form PINNs for variational PDE problems.

</details>


### [16] [Inhomogeneous Priors for Bayesian Inverse Problems](https://arxiv.org/abs/2602.07856)
*Babak Maboudi Afkham,Tomas Soto,Mirza Karamehmedovic,Lassi Roininen*

Main category: math.NA

TL;DR: The paper introduces a new class of inhomogeneous priors for Bayesian inverse problems that can capture spatial inhomogeneity, unlike traditional Gaussian priors which assume global homogeneity.


<details>
  <summary>Details</summary>
Motivation: Many inverse problems involve spatially inhomogeneous unknown quantities (like defects or varying material properties), making uncertainty quantification challenging. Traditional Gaussian priors (like Whittle-Matern models) impose globally homogeneous assumptions that limit their ability to capture such structure in large-scale settings.

Method: Introduces a new class of inhomogeneous priors defined via convolution with white noise, yielding nonstationary Whittle-Matern-type random fields with rigorous mathematical construction. These priors fit within existing Bayesian well-posedness theory and enable efficient sampling by reducing prior realizations to solving a pseudo-differential equation, with developed numerical schemes having quantified approximation error.

Result: Numerical experiments in one-dimensional denoising and two-dimensional limited-angle X-ray tomography demonstrate significant improvements in reconstruction quality and uncertainty quantification, particularly in data-limited scenarios.

Conclusion: The proposed inhomogeneous priors provide a principled framework for Bayesian inverse problems with spatially varying unknowns, overcoming limitations of traditional homogeneous priors while maintaining mathematical rigor and computational efficiency.

Abstract: Many inverse problems arising in engineering and applied sciences involve unknown quantities with pronounced spatial inhomogeneity, such as localized defects or spatially varying material properties, making reliable uncertainty quantification particularly challenging. While Bayesian inverse problem methodologies provide a principled framework for assessing reconstruction reliability, commonly used Gaussian priors, such as Whittle-Matern models, impose globally homogeneous assumptions that limit their ability to capture such structure in large-scale settings. We introduce a new class of inhomogeneous priors defined via convolution with white noise, yielding nonstationary Whittle-Matern-type random fields with a rigorous mathematical construction. These priors fit naturally within existing Bayesian well-posedness theory and enable efficient sampling by reducing prior realizations to the solution of a pseudo-differential equation, for which we develop numerical schemes with quantified approximation error. Numerical experiments in one-dimensional denoising and two-dimensional limited-angle X-ray tomography demonstrate significant improvements in reconstruction quality and uncertainty quantification, particularly in data-limited scenarios.

</details>


### [17] [Characteristic Sweeps and Source Iteration for Charged-Particle Transport with Continuous Slowing-Down and Angular Scattering](https://arxiv.org/abs/2602.07857)
*Ben S. Ashby,Alex Lukyanov,Tristan Pryer*

Main category: math.NA

TL;DR: Semi-analytic deterministic framework for charged-particle transport with continuous slowing-down and angular scattering, using method-of-characteristics integration and fixed-point source iteration.


<details>
  <summary>Details</summary>
Motivation: Develop a rigorous computational framework for charged-particle transport that handles continuous energy loss (slowing-down) and angular scattering simultaneously, with applications to proton and carbon-ion therapy where accurate dose deposition (Bragg peaks) is critical.

Method: Method-of-characteristics integration for directed transport and energy advection, creating explicit directional sweeps via characteristic maps. Scattering handled through fixed-point source iteration with lagged angular gain. Formulated variationally in transport graph space adapted to charged particle drift.

Result: Established coercivity and boundedness of transport bilinear form, proved contraction of source iteration under subcriticality condition, derived rigorous a posteriori error bound. Numerical validation shows characteristic sweeps match exact ballistic benchmark, fixed-point convergence under forward-peaked scattering, and carbon-ion simulations demonstrate Bragg peak localization and distal tail formation from secondary fragments.

Conclusion: The framework provides a rigorous, efficient deterministic method for charged-particle transport with continuous slowing-down, validated for proton and carbon-ion applications, with proven mathematical properties and practical stopping criteria for iterative solution.

Abstract: We develop a semi-analytic deterministic framework for charged-particle transport with continuous slowing-down in energy and angular scattering. Directed transport and energy advection are treated by method-of-characteristics integration, yielding explicit directional sweeps defined by characteristic maps and inflow data. Scattering is incorporated through a fixed-point (source-iteration) scheme in which the angular gain is lagged, yielding a sequence of decoupled directional solves coupled only through angular sums.
  The method is formulated variationally in a transport graph space adapted to the charged particle drift. Under standard monotonicity and positivity assumptions on the stopping power and boundedness assumptions on cross sections, we establish coercivity and boundedness of the transport bilinear form, prove contraction of the source iteration under a subcriticality condition and derive a rigorous a posteriori bound for the iteration error, providing an efficient stopping criterion.
  We further analyse an elastic discrete-ordinates approximation, including conservation properties and a decomposition of angular error into quadrature, cone truncation and finite iteration effects. Numerical experiments for proton transport validate the characteristic sweep against an exact ballistic benchmark and demonstrate the predicted fixed-point convergence under forward-peaked scattering. Carbon-ion simulations with tabulated stopping powers and a reduced multi-species coupling illustrate Bragg peak localisation and distal tail formation driven by secondary charged fragments.

</details>


### [18] [A quantum-inspired multi-level tensor-train monolithic space-time method for nonlinear PDEs](https://arxiv.org/abs/2602.07945)
*N. R. Rapaka,R. Peddinti,E. Tiunov,N. J. Faraj,A. N. Alkhooori,L. Aolita,Y. Addad,M. K. Riahi*

Main category: math.NA

TL;DR: A multilevel tensor-train framework for solving nonlinear PDEs in space-time that overcomes convergence issues of single-level approaches through coarse-to-fine refinement and low-rank prolongation.


<details>
  <summary>Details</summary>
Motivation: Existing space-time TT solvers lack systematic comparisons with classical methods, have limited error analysis, and struggle with convergence in strongly nonlinear, stiff, or advection-dominated regimes due to poor initial guesses and ill-conditioned Jacobians.

Method: A coarse-to-fine multilevel strategy fully embedded within TT format, where each level refines spatial/temporal resolutions while transferring TT solutions through low-rank prolongation operators. Uses adaptive-rank DMRG algorithm for residuals, Jacobians, and transfer operators.

Result: Multilevel TT consistently converges where single-level space-time Newton iterations fail. In advection-dominated scenarios, it achieves high accuracy with significantly reduced computational cost compared to single-level TT.

Conclusion: The multilevel TT approach provides robust convergence for diverse nonlinear PDEs (Fisher-KPP, Burgers, sine-Gordon, KdV) across diffusive, convective, and dispersive regimes, enabling high-fidelity simulations where previous methods struggled.

Abstract: We propose a multilevel tensor-train (TT) framework for solving nonlinear partial differential equations (PDEs) in a global space-time formulation. While space-time TT solvers have demonstrated significant potential for compressed high-dimensional simulations, the literature contains few systematic comparisons with classical time-stepping methods, limited error convergence analyses, and little quantitative assessment of the impact of TT rounding on numerical accuracy. Likewise, existing studies fail to demonstrate performance across a diverse set of PDEs and parameter ranges. In practice, monolithic Newton iterations may stagnate or fail to converge in strongly nonlinear, stiff, or advection-dominated regimes, where poor initial guesses and severely ill-conditioned space-time Jacobians hinder robust convergence. We overcome this limitation by introducing a coarse-to-fine multilevel strategy fully embedded within the TT format. Each level refines both spatial and temporal resolutions while transferring the TT solution through low-rank prolongation operators, providing robust initializations for successive Newton solves. Residuals, Jacobians, and transfer operators are represented directly in TT and solved with the adaptive-rank DMRG algorithm. Numerical experiments for a selection of nonlinear PDEs including Fisher-KPP, viscous Burgers, sine-Gordon, and KdV cover diffusive, convective, and dispersive dynamics, demonstrating that the multilevel TT approach consistently converges where single-level space-time Newton iterations fail. In dynamic, advection-dominated (nonlinear) scenarios, multilevel TT surpasses single-level TT, achieving high accuracy with significantly reduced computational cost, specifically when high-fidelity numerical simulation is required.

</details>


### [19] [Optimality Conditions for Rational Minimax Approximations: Bridging Ruttan's Criteria to Dual-Based Methods](https://arxiv.org/abs/2602.07862)
*Lei-Hong Zhang*

Main category: math.NA

TL;DR: The paper extends Ruttan's optimality conditions for rational minimax approximations, showing his sufficient condition becomes necessary with minimal extreme points, connects these conditions to the d-Lawson method, and enables continuum minimax computation via discrete methods.


<details>
  <summary>Details</summary>
Motivation: To bridge theoretical optimality conditions with practical computation for rational minimax approximations, addressing limitations in existing optimality criteria and enabling efficient computation of continuum approximations through discrete methods.

Method: Develops extended second-order optimality criteria for discrete rational minimax approximations, analyzes relationships between Ruttan's conditions and the d-Lawson method, and establishes connections between continuum and discrete approximations through boundary point selection.

Result: Shows Ruttan's sufficient condition becomes necessary with minimal extreme points, proves strong duality in d-Lawson ensures satisfaction of both Ruttan's and Kolmogorov's criteria, and demonstrates continuum approximants can be captured via discrete approximations at properly chosen boundary points.

Conclusion: Theoretical foundations for rational minimax approximations are unified with computational practice, enabling efficient computation of continuum minimax approximants using discrete methods while establishing fundamental connections between various optimality criteria.

Abstract: This paper presents a theoretical discussion on Ruttan's optimality conditions for rational minimax approximations in discrete and continuum settings, integrating analytical foundations with computational practice. We develop extended second-order optimality criteria for the discrete case, demonstrating that Ruttan's sufficient condition for global solutions [Ruttan, {Constr. Approx.}, 1 (1985), 287-296] becomes necessary when the number of extreme points is minimal. Our analysis further uncovers fundamental relationships between these conditions and the dual-based {d-Lawson} method [L.-H. Zhang et al., {Math. Comp.}, 94 (2025), 2457-2494], proving that strong duality in {d-Lawson} ensures simultaneous satisfaction of both Ruttan's and Kolmogorov's criteria. Additionally, we show that minimax approximants on a continuum satisfying Ruttan's sufficient global optimality can be captured through discrete minimax approximations at properly chosen boundary points, thereby enabling efficient computation of minimax approximants on a continuum using discrete methods.

</details>


### [20] [Finite Element Convergence Analysis For Wave Equations With Time-Dependent Coefficients](https://arxiv.org/abs/2602.07990)
*Oussama Al Jarroudi,Marcus J. Grote*

Main category: math.NA

TL;DR: Error estimates for finite element approximations of second-order hyperbolic PDEs with spatio-temporally varying coefficients, proving optimal convergence rates in energy norm using time-dependent Ritz projection.


<details>
  <summary>Details</summary>
Motivation: To develop rigorous error analysis for finite element methods applied to hyperbolic PDEs with coefficients that vary in both space and time, which is important for modeling wave propagation in complex time-varying media.

Method: Introduces a time-dependent Ritz-like projection to analyze semi-discrete Galerkin finite element solutions, proving optimal convergence rates in energy norm for second-order hyperbolic PDEs with spatio-temporally varying coefficients.

Result: Proves optimal rates of convergence in energy norm for the semi-discrete finite element approximations. Numerical experiments confirm theoretical convergence rates and demonstrate localized wave field enhancement in time-modulated subwavelength resonator chains.

Conclusion: The proposed time-dependent Ritz projection enables rigorous error analysis for hyperbolic PDEs with time-varying coefficients, with applications to wave propagation in time-modulated metamaterials and resonators.

Abstract: Error estimates are proved for finite element approximations to the solution of second-order hyperbolic partial differential equations with coefficients varying in both space and time. Optimal rates of convergence in the energy norm are proved for the semi-discrete Galerkin finite element solution by introducing a time-dependent Ritz-like projection. Numerical experiments corroborate the rates of convergence and illustrate the localized wave field enhancement in a chain of time-modulated subwavelength resonators.

</details>


### [21] [Invariant-domain preserving IMEX schemes for the nonequilibrium Gray Radiation-Hydrodynamics equations Part I](https://arxiv.org/abs/2602.08291)
*Jean-Luc Guermond,Eric J. Tovar*

Main category: math.NA

TL;DR: Implicit-explicit invariant-domain preserving approximation for nonequilibrium gray radiation-hydrodynamics equations using novel three-subsystem split.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for radiation-hydrodynamics that preserves physical invariants (invariant-domain preserving) while handling the challenging coupling between hyperbolic and parabolic components.

Method: Novel split of equations into three elementary subsystems (two hyperbolic, one parabolic) with implicit-explicit time discretization. First-order accurate in space and time with invariant-domain preserving properties.

Result: Proved method is consistent, conservative, invariant-domain preserving, and first-order accurate. Numerical tests show convergence as expected.

Conclusion: The method provides a foundation for higher-order accurate approximations in future work (Part II). The approach successfully balances accuracy with preservation of physical invariants.

Abstract: In this work we introduce an implicit-explicit invariant-domain
  preserving approximation of the nonequilibrium gray
  radiation-hydrodynamics equations. A time and space approximation
  of the system is proposed using a novel split of the equations
  composed of three elementary subsystems, two hyperbolic and one
  parabolic. The approximation thus realized is proved to be
  consistent, conservative, invariant-domain preserving, and
  first-order accurate. The proposed method is a stepping stone for
  achieving higher-order accuracy in space and time in the forthcoming
  second part of this work. The method is numerically illustrated and
  shown to converge as advertised. This paper is dedicated to the memory
  of Peter Lax.

</details>


### [22] [A numerical study for tempered time-fractional advection-dispersion equation on graded meshes](https://arxiv.org/abs/2602.08325)
*Liangcai Huang,Lin Li,Shujuan Lü*

Main category: math.NA

TL;DR: A second-order accurate SOE-based time-stepping scheme for tempered time-fractional advection-dispersion equations using graded meshes to handle initial singularity, with reduced storage and computational costs compared to classical L1 schemes.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for tempered time-fractional advection-dispersion equations that addresses the weak initial-time singularity while reducing the high storage and computational costs of existing methods like the classical L1 scheme.

Method: Uses sum-of-exponentials (SOE) approximation for the fractional derivative convolution kernel, graded temporal meshes to handle initial singularity, half-time-level temporal discretization coupled with finite difference in space.

Result: Achieves second-order global convergence while reducing storage from O(MN) to O(MN_exp) and computational complexity from O(MN^2) to O(MN N_exp), with rigorous analysis of solvability, stability, and accuracy.

Conclusion: The SOE-based method provides an efficient alternative to classical L1 schemes for tempered time-fractional equations, maintaining accuracy while significantly reducing computational and storage requirements.

Abstract: In this paper, we develop a second-order accurate time-stepping scheme for the tempered time-fractional advection-dispersion equation based on a sum-of-exponentials (SOE) approximation to the convolution kernel involved in the fractional derivative. To effectively resolve the weak initial-time singularity at t=0, graded temporal meshes are employed. A fully discrete scheme is constructed by coupling the proposed half-time-level temporal discretization with a finite difference method in space. Compared with the classical L1 scheme, the proposed SOE-based method achieves the same global convergence order while reducing both storage requirements and computational cost. Specifically, the storage demand is reduced from O(MN) to O(MN_exp), and the computational complexity is lowered from O(MN^2) to O(MN N_exp), where M and N denote the numbers of spatial and temporal grid points, respectively, and N_exp is the number of exponential terms used in the SOE approximation. The unique solvability, stability and accuracy of the resulting scheme are rigorously analyzed. Several numerical results are presented to confirm the sharpness of the error analysis and to demonstrate the efficiency of the proposed method.

</details>


### [23] [On the Existence of Steady States for Blended Gas Flow with Non-Constant Compressibility Factor on Networks](https://arxiv.org/abs/2602.08481)
*Simone Göttlich,Michael Schuster,Alena Ulke*

Main category: math.NA

TL;DR: Existence of steady-state solutions for hydrogen-natural gas mixtures in pipeline networks with composition-dependent compressibility factors.


<details>
  <summary>Details</summary>
Motivation: Study hydrogen-natural gas mixtures in pipeline networks to understand how varying gas composition affects flow dynamics and pressure distribution, particularly important for energy transition scenarios where hydrogen is blended with natural gas.

Method: Model flow using isothermal Euler equations with composition-dependent compressibility factor. Use implicit representation of pressure profiles and continuity argument to handle discontinuous dependence of gas composition on flow direction. Analyze networks containing compressor stations.

Result: Prove existence of steady-state solutions for broad class of compressibility models on networks with compressor stations. Numerical examples show influence of different compressibility models on resulting states.

Conclusion: Theoretical framework established for analyzing hydrogen-natural gas mixtures in pipeline networks, with existence results that accommodate realistic compressibility models and network components like compressor stations.

Abstract: In this paper, we study hydrogen-natural gas mixtures transported through pipeline networks. The flow is modeled by the isothermal Euler equations with a pressure law involving a non-constant, composition-dependent compressibility factor. For a broad class of such compressibility models, we prove the existence of steady-state solutions on networks containing compressor stations. The analysis is based on an implicit representation of the pressure profiles and a continuity argument that overcomes the discontinuous dependence of the gas composition on the flow direction. Numerical examples illustrate the influence of different compressibility models on the resulting states.

</details>


### [24] [Do physics-informed neural networks (PINNs) need to be deep? Shallow PINNs using the Levenberg-Marquardt algorithm](https://arxiv.org/abs/2602.08515)
*Muhammad Luthfi Shahab,Imam Mukhlash,Hadi Susanto*

Main category: math.NA

TL;DR: Shallow PINNs with Levenberg-Marquardt optimization outperform BFGS for solving nonlinear PDE forward/inverse problems using only 2 hidden layers.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and accurate solutions for nonlinear PDE forward and inverse problems using shallow physics-informed neural networks with improved optimization methods.

Method: Reformulate PINNs as nonlinear systems, employ Levenberg-Marquardt algorithm for optimization, derive analytical expressions for neural network derivatives to compute Jacobian matrix efficiently.

Result: LM significantly outperforms BFGS in convergence speed, accuracy, and final loss values across Burgers, Schrödinger, Allen-Cahn, and 3D Bratu equations using shallow networks with only 2 hidden layers.

Conclusion: Shallow PINNs combined with efficient second-order optimization methods (like LM) provide accurate and computationally efficient solutions for a wide class of PDE forward and inverse problems.

Abstract: This work investigates the use of shallow physics-informed neural networks (PINNs) for solving forward and inverse problems of nonlinear partial differential equations (PDEs). By reformulating PINNs as nonlinear systems, the Levenberg-Marquardt (LM) algorithm is employed to efficiently optimize the network parameters. Analytical expressions for the neural network derivatives with respect to the input variables are derived, enabling accurate and efficient computation of the Jacobian matrix required by LM. The proposed approach is tested on several benchmark problems, including the Burgers, Schrödinger, Allen-Cahn, and three-dimensional Bratu equations. Numerical results demonstrate that LM significantly outperforms BFGS in terms of convergence speed, accuracy, and final loss values, even when using shallow network architectures with only two hidden layers. These findings indicate that, for a wide class of PDEs, shallow PINNs combined with efficient second-order optimization methods can provide accurate and computationally efficient solutions for both forward and inverse problems.

</details>


### [25] [Comparison of Structure Preserving Schemes for the Cahn-Hilliard-Navier-Stokes Equations with Degenerate Mobility and Adaptive Mesh Refinement](https://arxiv.org/abs/2602.08639)
*Jimmy Kornelije Gunnarsson,Robert Klöfkorn*

Main category: math.NA

TL;DR: Comparison of decoupled implicit-explicit DG methods for Cahn-Hilliard-Navier-Stokes system with adaptive meshing, focusing on structure-preserving properties like mass conservation and bound preservation.


<details>
  <summary>Details</summary>
Motivation: To develop and compare structure-preserving numerical methods for CHNS systems that maintain important physical properties like mass conservation and bound preservation, which are crucial for accurate interface tracking in multi-phase flows.

Method: Decoupled implicit-explicit formulations based on Discontinuous Galerkin methodology for phase-field, combined with standard continuous Galerkin for fluid flow. Uses adaptive conforming grids with finer meshes at interfaces and coarser meshes in pure phases.

Result: Comparison of presented methods against existing schemes in terms of bound preservation, mass conservation, and energy dissipation for various test cases including rising droplet problem.

Conclusion: The paper presents and evaluates structure-preserving DG methods for CHNS systems with adaptive meshing, demonstrating their performance on benchmark problems while maintaining important physical properties.

Abstract: The Cahn-Hilliard-Navier-Stokes (CHNS) system utilizes a diffusive phase-field for interface tracking of multi-phase fluid flows. Recently structure preserving methods for CHNS have moved into focus to construct numerical schemes that, for example, are mass conservative or obey initial bounds of the phase-field variable. In this work decoupled implicit-explicit formulations based on the Discontinuous Galerkin (DG) methodology are considered and compared to existing schemes from the literature.
  For the fluid flow a standard continuous Galerkin approach is applied. An adaptive conforming grid is utilized to further draw computational focus on the interface regions, while coarser meshes are utilized around pure phases. All presented methods are compared against each other in terms of bound preservation, mass conservation, and energy dissipation for different examples found in the literature, including a classical rising droplet problem.

</details>


### [26] [Review of thermodynamic structures and structure-preserving discretisations of Cahn--Hilliard-type models](https://arxiv.org/abs/2602.08791)
*Aaron Brunk,Marco F. P. ten Eikelder,Marvin Fritz,Dennis Höhn,Dennis Trautwein*

Main category: math.NA

TL;DR: Review paper analyzing thermodynamic structures of Cahn-Hilliard models and their numerical discretization strategies that preserve conservation laws and energy dissipation.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive survey of the thermodynamic foundations of Cahn-Hilliard models and their extensions, bridging theoretical structures with practical numerical implementation.

Method: Review and comparative analysis of thermodynamic structures, focusing on free energy functionals, dissipation mechanisms, variational principles, and their translation to numerical discretization strategies.

Result: Systematic comparison of structural properties showing how different Cahn-Hilliard models encode conservation laws and energy dissipation, with representative numerical strategies that preserve these properties.

Conclusion: The paper provides a framework for understanding thermodynamic structures in Cahn-Hilliard models and offers practical guidance for developing numerical methods that balance accuracy, efficiency, and structure preservation in large-scale simulations.

Abstract: The Cahn-Hilliard equation and extensions, notably the Cahn-Hilliard-Darcy and Cahn-Hilliard-Navier-Stokes systems, provide widely used frameworks for coupling interfacial thermodynamics with flow. This review surveys the thermodynamic structures underlying these models, focusing on the formulation of free energy functionals, dissipation mechanisms, and variational principles. We compare structural properties, emphasizing how these models encode conservation laws and energy dissipation. A central theme is the translation of these thermodynamic structures into numerical practice by providing representative discretisation strategies that aim to preserve mass conservation, stability, and energy decay. Particular attention is paid to the trade-offs between accuracy, efficiency, and structure preservation in large-scale simulations.

</details>


### [27] [Convergence Analysis for the Recovery of the Friction Threshold in a Scalar Tresca Model](https://arxiv.org/abs/2602.08967)
*Erik Burman,Marvin Knöller,Lauri Oksanen,Andreas Rupp*

Main category: math.NA

TL;DR: Inverse problem for recovering unknown friction threshold in elliptic PDE with Tresca friction boundary condition using finite element method and iterative algorithm.


<details>
  <summary>Details</summary>
Motivation: Need to recover unknown friction threshold parameter in elliptic PDE with friction-type boundary conditions, which is important for practical applications where boundary parameters are unknown but partial solution measurements are available.

Method: Iterative computational method using piecewise linear finite elements, assuming friction threshold lies in finite dimensional space with known basis, known right-hand sides, and solution measurements on small open subset.

Result: Algorithm converges in second order to approximate function a_h, and a_h converges in second order in mesh size h to true friction threshold, confirmed by numerical simulations.

Conclusion: Proposed iterative finite element method effectively recovers unknown friction threshold with second-order convergence rates, validated by numerical experiments.

Abstract: We consider a scalar valued elliptic partial differential equation on a sufficiently smooth domain $Ω$, subject to a regularized Tresca friction-type boundary condition on a subset $Γ$ of $\partial Ω$. The friction threshold, a positive function appearing in this boundary condition, is assumed to be unknown and serves as the coefficient to be recovered in our inverse problem. Assuming that (i) the friction threshold lies in a finite dimensional space with known basis functions, (ii) the right hand sides of the partial differential equation are known, and (iii) the solution to the partial differential equation on some small open subset $ω\subset Ω$ is available, we develop an iterative computational method for the recovery of the friction threshold. This algorithm is simple to implement and is based on piecewise linear finite elements. We show that the proposed algorithm converges in second order to a function $a_h$ and, moreover, that $a_h$ converges in second order in the finite element's mesh size $h$ to the true (unknown) friction threshold. We highlight our theoretical results by simulations that confirm our rates numerically.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [28] [Global existence and uniqueness of weak solutions for the MHD equations with large $L^3$-initial values](https://arxiv.org/abs/2602.06979)
*Baishun Lai,Ge Tang,Ziying Xu*

Main category: math.AP

TL;DR: Global weak solution theory for MHD system with large L³-initial data using Leray approximation and perturbation methods, providing alternative proof for Navier-Stokes case.


<details>
  <summary>Details</summary>
Motivation: The slip boundary condition on magnetic field makes Leray-Schauder fixed-point theorem invalid for MHD system with large L³-initial data, requiring new approach.

Method: Uses Leray's approximation technique and perturbation theory to construct global weak solutions for Cauchy problem of MHD equations with large L³-initial data.

Result: Obtains global weak solution for MHD system with large L³-initial data, provides simple alternative self-contained proof for Navier-Stokes weak L³-solution theory, and establishes uniqueness under certain restrictions.

Conclusion: The Leray approximation and perturbation approach successfully addresses the limitations of Leray-Schauder fixed-point theorem for MHD systems with slip boundary conditions, yielding global weak solutions with uniqueness under specific conditions.

Abstract: This paper is concerned with the weak solution theory for the MHD system with large $L^3$-initial data. Due to the fact that the natural boundary condition on the magnetic field $H$ is the slip boundary condition, the Leray-Schauder fixed-point theorem, which have used to investigate the weak solution theory of the Navier-Stokes system, becomes invalid. To address such difficulty, we will invoke the Leray's approximation technique and the perturbation theory to seek a global weak solution to the Cauchy problem for MHD equations with large $L^3$-initial data. Our strategy provides a simple alternative (self-contained) proof of weak $L^3$-solution theory of incompressible Navier-Stokes system. Moreover, this weak solution is unique under some restrictions.

</details>


### [29] [Semiclassical localization of Schrödinger's eigenfunctions](https://arxiv.org/abs/2602.07128)
*Sébastien Campagne*

Main category: math.AP

TL;DR: Extends microlocalization bounds for semiclassical Schrödinger eigenfunctions on closed Riemann surfaces from smooth to bounded potentials, improving exponential weight from h^{-4/3} to h^{-1}log(h)^2.


<details>
  <summary>Details</summary>
Motivation: Classical results on eigenfunction concentration typically require smooth potentials, but many applications involve non-smooth potentials. The paper aims to extend quantitative bounds to the more general case of merely bounded potentials.

Method: Uses recent approach to Landis conjecture developed by Logunov, Malinnikova, Nadirashvili and Nazarov (2025) to establish quantitative bounds on eigenfunction concentration.

Result: Obtains explicit exponential bound for L^2-norm of eigenfunctions on entire surface in terms of L^2-norm on arbitrary open subset with exponential weight Ch^{-1}log(h)^2, improving previous h^{-4/3} bound for non-smooth potentials.

Conclusion: Successfully extends microlocalization results to bounded potentials with improved quantitative bounds, demonstrating applicability of recent Landis conjecture techniques to semiclassical analysis problems.

Abstract: This article addresses the microlocalization of eigenfunctions for the semiclassical Schrödinger operator $-h^2Δ+V$ on closed Riemann surfaces with real bounded potentials. Our primary aim is to establish quantitative bounds on the spatial concentration of these eigenfunctions, extending classical results, typically restricted to smooth potentials, to the more general case where the potential is merely bounded. Our main result provides an explicit exponential bound for the $L^2$-norm of eigenfunctions on the entire surface in terms of their $L^2$-norm on an arbitrary open subset with an exponential weight of $Ch^{-1}\log(h)^2$. This bound improves upon previous estimates for non-smooth potentials that was an exponential weight of $Ch^{-4/3}$. Our proof is based on a recent approach of the Landis conjecture develop by Logunov, Malinnikova, Nadirashvili and Nazarov (2025).

</details>


### [30] [Pointwise Hadamard variational formula for the fractional Laplacian](https://arxiv.org/abs/2602.07214)
*Sidy M. Djitte,Franck Sueur*

Main category: math.AP

TL;DR: The paper establishes pointwise formulas for shape derivatives of solutions to fractional Laplacian Dirichlet problems, extending classical Hadamard formulas to the fractional case.


<details>
  <summary>Details</summary>
Motivation: To extend the well-known Hadamard variational formula for the standard Laplacian to the fractional Laplacian case, providing tools for shape optimization problems involving non-local operators.

Method: Uses PDE techniques in the spirit of Ushikoshi and Kozono-Ushikoshi, considering two cases: right-hand side as Dirac delta distribution or Lipschitz function, and proving shape differentiability with formulas involving boundary integrals and fractional Neumann traces.

Result: Proves that solutions are shape differentiable in every direction and derives explicit pointwise formulas for the shape derivative involving boundary integrals and fractional Neumann traces.

Conclusion: Successfully extends Hadamard variational formulas to fractional Laplacian, providing fundamental tools for shape optimization with non-local operators and opening possibilities for further applications in fractional calculus.

Abstract: We establish pointwise formulas for the shape derivative of solutions to the Dirichlet problem associated with the fractional Laplacian. Specifically, we consider the equation $(-Δ)^s u = h$ in $Ω$ and $u=0$ in $Ω^c$, where the right-hand side $h$ is either a Dirac delta distribution or a Lipschitz function. In both cases, we prove that the corresponding solution is shape differentiable in every direction and we derive a formula for the pointwise value of its shape derivative. These formulas involve integral on the domain's boundary and fractional Neumann's traces. This extends to the case of the fractional Laplacian the well-known Hadamard variational formula for the standard Laplacian. Our argument is in the spirit of \cite{Ushikoshi, Kozono-Ushikoshi} and is based on PDEs techniques.

</details>


### [31] [A Brezis and Peletier type result for the fractional Robin function](https://arxiv.org/abs/2602.07221)
*Sidy M. Djitte,Franck Sueur*

Main category: math.AP

TL;DR: This paper studies fractional Laplacian operators and establishes representation formulas for derivatives, proves Lipschitz continuity for overdetermined problems, derives Pohozaev-type identities, and applies results to critical points of fractional Robin functions.


<details>
  <summary>Details</summary>
Motivation: To extend classical results for the Laplacian operator to the fractional setting, particularly establishing representation formulas for partial derivatives, studying overdetermined problems, and deriving Pohozaev-type identities for fractional operators.

Method: The authors develop representation formulas for partial derivatives of solutions in terms of the normal derivative u/δ^s, analyze overdetermined problems with boundary conditions, prove Pohozaev-type identities for Green functions, and extend classical results to fractional operators.

Result: Established representation formulas for partial derivatives, proved global Lipschitz continuity for solutions to overdetermined problems when 2s>1, derived Pohozaev-type identity for Green function, obtained gradient formula for Robin function, and applied results to nondegeneracy of critical points in symmetric domains.

Conclusion: The paper successfully extends classical Laplacian results to fractional operators, providing important analytical tools for studying fractional PDEs, with applications to critical point analysis and symmetry properties in fractional settings.

Abstract: This paper is devoted to the Laplacian operator of fractional order $s\in (0,1)$ in several dimensions. We consider the equation $(-Δ)^su=f(x,u)$ in $Ω$, $u=0$ in $Ω^c$ and establish a representation formula for partial derivatives of solutions in terms of the normal derivative $u/δ^s$. As a consequence, we prove that solutions to the overdetermined problem $(-Δ)^su=f(x,u)$ in $Ω$, $u=0$ in $Ω^c$, and $u/δ^s=0$ on $\partialΩ$ are globally Lipschitz continuous provided that $2s>1$. We also prove a Pohozaev-type identity for the Green function and, in particular, obtain a formula for the gradient of the Robin function, which extends to the fractional setting some results obtained by Brézis and Peletier in \cite{Bresiz} in the classical case of the Laplacian. Finally, an application to the nondegeneracy of critical points of the fractional Robin function in symmetric domains is discussed.

</details>


### [32] [Non-homogeneous boundary value problems for second-order degenerate hyperbolic equations and their application](https://arxiv.org/abs/2602.07271)
*Donghui Yang,Jie Zhong*

Main category: math.AP

TL;DR: The paper develops a solution theory for degenerate hyperbolic equations with non-homogeneous Dirichlet boundary conditions, establishing existence, regularity, and approximate controllability in weighted Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: Classical approaches to hyperbolic equations with boundary inputs don't directly apply to degenerate elliptic operators. There's a need to extend solution theory to handle degeneracy, non-homogeneous boundary conditions, and develop controllability methods for such systems.

Method: Constructs a Dirichlet map for degenerate elliptic operators, develops solution theory in weighted Sobolev spaces, establishes energy estimates, and extends the Hilbert Uniqueness Method to prove approximate controllability for degenerate wave equations.

Result: Establishes existence and regularity of weak solutions under mild assumptions on degenerate weights, proves well-posedness for low-regularity boundary inputs, and derives an approximate controllability criterion that generalizes to higher-dimensional degenerate waves.

Conclusion: The framework successfully extends classical hyperbolic equation theory to degenerate settings, accommodates various degenerate wave models, but leaves the unique continuation/observability issue as an open problem for future research.

Abstract: We study second-order hyperbolic equations with degenerate elliptic operators and non-homogeneous Dirichlet boundary inputs. We establish existence and regularity of weak solutions in weighted Sobolev spaces under mild assumptions on the degenerate weight. A Dirichlet map is constructed for the degenerate elliptic operator, leading to a solution theory that extends classical approaches to the degenerate setting. In particular, we derive energy estimates and well-posedness for boundary inputs of low regularity (in appropriate trace spaces), even though the classical Dirichlet-to-Neumann framework is not directly applicable in the degenerate setting. As an application, we prove an approximate controllability criterion, which generalizes the Hilbert Uniqueness Method to degenerate wave equations. Our framework accommodates higher-dimensional degenerate waves, non-homogeneous boundary conditions, and weighted functional analysis. We also illustrate how our criterion connects to higher-dimensional Grushin equations and waves with single-point degeneracy, and we highlight the remaining unique continuation/observability issue as an open problem.

</details>


### [33] [On two-dimensional steady compactly supported Euler flows with constant vorticity](https://arxiv.org/abs/2602.07407)
*Changfeng Gui,Jun Wang,Wen Yang,Yong Zhang*

Main category: math.AP

TL;DR: Construct local solution curves for 2D steady compactly supported incompressible Euler equations with free boundaries and constant vorticity, focusing on perturbations near annular flows rather than laminar flows.


<details>
  <summary>Details</summary>
Motivation: Most existing studies on 2D steady water waves focus on perturbations near laminar flows, but this work distinguishes itself by examining perturbations near annular flows, which correspond to different physical configurations and mathematical problems.

Method: Consider three classes of steady Euler flows with compact support corresponding to different overdetermined elliptic problems. Use shape derivatives and local bifurcation theory to establish flexibility results, discuss rigidity results, and apply implicit function theorem to demonstrate stability of annular flows under Neumann boundary condition perturbations.

Result: Established flexibility results (existence of nontrivial admissible domains) for each class, discussed corresponding rigidity results, and demonstrated stability of standard annular flows under Neumann boundary condition perturbations.

Conclusion: The work provides novel insights into elliptic overdetermined problems and successfully constructs local solution curves for annular flow perturbations in 2D steady Euler equations with free boundaries, distinguishing from traditional laminar flow approaches.

Abstract: In this paper, we mainly construct local solution curves for the two-dimensional steady compactly supported incompressible Euler equations with free boundaries and constant vorticity. Our work is distinguished from most existing studies on two-dimensional steady water waves by its focus on perturbations near annular flows, rather than laminar flows. More precisely, we consider three classes of steady Euler flows with compact support, corresponding to partially overdetermined, two-phase overdetermined, and overdetermined elliptic problems. The primary contribution of our work is threefold. For each class, we first establish the flexibility result (i.e., the existence of nontrivial admissible domains) via shape derivatives and local bifurcation theory. Second, we give and discuss the corresponding rigidity result respectively. Third, we apply the implicit function theorem to demonstrate the stability of standard annular flows under perturbations of the Neumann boundary condition. Our results also offer novel insights into the theory of elliptic overdetermined problems.

</details>


### [34] [A priori estimates for general elliptic and parabolic boundary value problems over irregular domains](https://arxiv.org/abs/2602.07485)
*Maria R. Lancia,Alejandro Vélez-Santiago*

Main category: math.AP

TL;DR: Unified framework for solving local and nonlocal elliptic/parabolic boundary value problems on irregular domains with various boundary conditions and unbounded coefficients.


<details>
  <summary>Details</summary>
Motivation: To address the need for solving diverse boundary value problems (local and nonlocal, elliptic and parabolic) over irregular real-world regions that arise in applications like heat transfer, electrical conductivity, probability theory, medical diffusion, and oceanography.

Method: Develop a unified approach treating local/nonlocal Neumann, Robin, and Wentzell boundary conditions simultaneously. Establish solvability and regularity results for stationary/time-dependent heat equations with general differential operators having unbounded measurable coefficients. Present concrete examples of irregular domains, Wentzell-type boundary conditions, and nonlocal maps.

Result: Established solvability and global regularity results for both stationary and time-dependent heat equations. Developed a priori estimates for multiple differential equations under various situations applicable to real-world irregular domains.

Conclusion: The unified framework successfully handles diverse boundary value problems on irregular domains, providing mathematical foundations for numerous practical applications across different scientific fields.

Abstract: We investigate the realization of a myriad of general local and nonlocal inhomogeneous elliptic and parabolic boundary value problems over classes of irregular regions. We present a unified approach in which either local or nonlocal Neumann, Robin, and Wentzell boundary value problems are treated simultaneously. We establish solvability and global regularity results for both the stationary and time-dependent heat equations governed by general differential operators with unbounded measurable coefficients and various boundary conditions at once, first on a general framework, and then by presenting concrete important examples of irregular domains, Wentzell-type boundary conditions, and nonlocal maps. As a consequence, we develop a priori estimates for multiple differential equations under various situations, which are tied to a large number of applications performed over real world regions, such heat transfer, electrical conductivity, stable-like processes (probability theory), diffusion of medical sprays in the bronchial trees, and oceanography (among many others).

</details>


### [35] [Well-posedness of Generalized Fractional Singular Burgers equation driven by $|D|^{\frac{1}{2}}ξ$](https://arxiv.org/abs/2602.07492)
*Shuolin Zhang,Zhaonan Luo,Zhaoyang Yin*

Main category: math.AP

TL;DR: The paper studies generalized solutions for Fractional Singular Burgers equations driven by fractional noise, establishes a framework for Generalized Fractional Singular Burgers equations (GFSB), proves local well-posedness, and shows GFSB solutions can serve as generalized solutions for the original equation when γ > 3/2.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous mathematical framework for studying generalized solutions of Fractional Singular Burgers equations driven by fractional noise, addressing the analytical challenges posed by singular terms and establishing proper solution concepts.

Method: Establishes a framework for Generalized Fractional Singular Burgers equations (GFSB), proves local well-posedness of GFSB, and demonstrates that GFSB solutions can serve as generalized solutions for the original Fractional Singular Burgers equation under certain parameter conditions.

Result: Proves local well-posedness of the Generalized Fractional Singular Burgers equation and shows that its solutions can be generalized solutions of the original Fractional Singular Burgers equation when γ > 3/2.

Conclusion: The paper successfully develops a mathematical framework for generalized solutions of Fractional Singular Burgers equations, establishing both local well-posedness and the connection between GFSB solutions and generalized solutions of the original equation for sufficiently large γ.

Abstract: In this paper, we study the generalized solution of Fractional Singular Burgers equation driving by $\vert D\vert^{\frac{1}{2}}ξ$. We establish a framework to describe the equations satisfied by generalized solutions, termed the Generalized Fractional Singular Burgers equation(GFSB), and prove its local well-posedness. Finally, we prove that the solution of GFSB can be the generalized solution of Fractional Singular Burgers equation for $γ>\frac{3}{2}$.

</details>


### [36] [Normalized Standing Waves for the Focusing Inhomogeneous Schrödinger Equation with Spatially Growing Nonlinearity](https://arxiv.org/abs/2602.07505)
*Mohamed Majdoub,Tarek Saanouni*

Main category: math.AP

TL;DR: The paper studies stability and instability of ground state standing waves for focusing inhomogeneous nonlinear Schrödinger equations with spatially growing nonlinearities (|x|^b term).


<details>
  <summary>Details</summary>
Motivation: Standard compactness arguments fail for spatially growing nonlinearities, creating new mathematical difficulties. The paper aims to extend classical stability/instability theory to these inhomogeneous cases.

Method: Variational approach on Nehari manifold to characterize ground states; constrained energy minimization in radial energy space for L^2-subcritical regime; analysis of orbital stability and finite-time blow-up.

Result: In L^2-subcritical regime: existence of normalized ground states via constrained minimization, orbital stability. In L^2-critical/supercritical regimes: ground states are strongly unstable with finite-time blow-up.

Conclusion: The paper successfully extends classical stability/instability theory to inhomogeneous nonlinear Schrödinger equations with spatially growing nonlinearities, resolving difficulties from standard compactness arguments.

Abstract: We study the focusing inhomogeneous nonlinear Schrödinger equation $$ i\partial_t u + Δu = -|x|^b |u|^{p-1}u ,\quad (t,x)\in (0,\infty)\times\mathbb{R}^N, $$ with $b>0$ and $p>1$. Due to the spatial growth of the nonlinearity, standard compactness arguments do not apply and new difficulties arise.
  We first characterize ground state standing waves via a variational approach on the Nehari manifold and we establish some sharp stability and instability properties. In the $L^2$-subcritical regime, we prove the existence of normalized ground states by solving a constrained energy minimization problem in the radial energy space, and we show that the resulting set of minimizers is orbitally stable under the flow. In contrast, in the $L^2$-critical and supercritical regimes, ground state standing waves are shown to be strongly unstable by finite-time blow-up.
  Our results extend classical stability and instability theory for nonlinear Schrödinger equations to the case of spatially growing inhomogeneous nonlinearities.

</details>


### [37] [The Dirichlet problem as the boundary of the Poisson problem: A sharp approximation result](https://arxiv.org/abs/2602.07560)
*Mihalis Mourgoglou,Bruno Poggi*

Main category: math.AP

TL;DR: The paper characterizes the dual space of functions with bounded non-tangential maximal operator and shows a novel approximation result connecting Dirichlet and Poisson problems for elliptic operators.


<details>
  <summary>Details</summary>
Motivation: The paper aims to answer a question by Hytönen and Rosén about characterizing the dual space of functions whose Kenig-Pipher modified non-tangential maximal operator lies in L^p. Additionally, inspired by recent characterizations of L^p-solvability of Dirichlet problems, the authors seek to establish a connection between solution spaces of Dirichlet and Poisson problems for elliptic operators.

Method: The authors work on bounded domains with corkscrew condition and Ahlfors regular boundary. They use functional analysis techniques to characterize the dual space (N_{2,p})* as a direct sum of a Carleson space C_{2,p'} and L^{p'}(∂Ω). They then apply this characterization to show an approximation result connecting solution spaces of Dirichlet and Poisson problems for elliptic operators with real bounded measurable coefficients.

Result: The main results are: 1) (N_{2,p})* = C_{2,p'} ⊕ L^{p'}(∂Ω), and L^{p'}(∂Ω) = ∂^{weak-*}C_{2,p'}/C_{2,p'}. 2) For elliptic operators L = -div A∇, the solution space to the Dirichlet problem with L^p boundary data lies on the weak-* boundary in N_{2,p} of the solution space to the Poisson problem with F ∈ C_{2,p}, provided the Dirichlet problem is solvable.

Conclusion: The paper successfully answers Hytönen and Rosén's question about dual space characterization and establishes a sharp, novel approximation result connecting Dirichlet and Poisson problems. The results are new even for the Laplacian on the unit ball and have implications for understanding the structure of solution spaces for elliptic PDEs.

Abstract: On a bounded domain $Ω\subset\mathbb R^{n+1}$, $n\geq2$, satisfying the corkscrew condition and with Ahlfors regular boundary, we characterize the dual space to the space ${\bf N}_{2,p}$ of functions $u$ whose Kenig-Pipher modified non-tangential maximal operator $\mathcal N_2(u)$ lies in $L^p(\partialΩ)$, $p\in(1,\infty)$. We find that \[ ({\bf N}_{2,p})^*={\bf C}_{2,p'}\oplus L^{p'}(\partialΩ),\qquad\text{and that}\qquad L^{p'}(\partialΩ)=\partial^{\operatorname{weak}-*}{\bf C}_{2,p'}\,/\,{\bf C}_{2,p'}, \] where ${\bf C}_{2,p'}$ is a certain $L^{p'}$-Carleson space and $p'$ is the Hölder conjugate of $p$. This answers a question considered by Hytönen and Rosén.
  Inspired by this result and the recently understood characterizations of the $L^p$-solvability of the Dirichlet problem in terms of the Poisson problem by Mourgoglou, Poggi, and Tolsa, we show a novel approximation result: for an arbitrary elliptic operator $L=-\operatorname{div} A\nabla$ with a not necessarily symmetric matrix $A$ of real bounded measurable coefficients, the solution space to the Dirichlet problem with data in $L^p(\partialΩ)$ \[
  \left\{\begin{aligned}-\operatorname{div} A\nabla u&=0,\quad&\text{in }&Ω,\\u&=g,\quad&\text{on }&\partialΩ,\end{aligned}\right. \]
  lies on the weak-$*$ boundary in ${\bf N}_{2,p}$ of the solution space to the Poisson problem \[ \left\{\begin{aligned}-\operatorname{div} A\nabla w&=-\operatorname{div} F,\qquad&\text{in }&Ω,\\ w&=0,\qquad&\text{on }&\partialΩ,\end{aligned}\right. \] with $F\in{\bf C}_{2,p}$, provided that the Dirichlet problem for $L$ with data in $L^p(\partialΩ)$ is solvable in $Ω$. This approximation result is sharp and new even for the Laplacian and on the unit ball.

</details>


### [38] [A Weighted Regularity Criterion for Suitable Weak Solutions of Incompressible Non-Newtonian Fluids](https://arxiv.org/abs/2602.07567)
*Jae-Myoung Kim*

Main category: math.AP

TL;DR: Paper establishes regularity criterion for non-Newtonian fluids in 3D using weighted gradient of velocity field based on Caffarelli-Kohn-Nirenberg inequality.


<details>
  <summary>Details</summary>
Motivation: To develop regularity conditions for non-Newtonian fluid flows in three-dimensional space, addressing the mathematical challenges of establishing when solutions remain smooth and avoiding potential singularities.

Method: Uses weighted gradient of velocity field approach, leveraging the Caffarelli-Kohn-Nirenberg inequality to establish regularity criteria for non-Newtonian fluid equations in ℝ³.

Result: Establishes sufficient conditions for regularity of non-Newtonian fluid flows in terms of weighted velocity gradients, providing mathematical criteria to ensure smooth solutions.

Conclusion: The Caffarelli-Kohn-Nirenberg inequality provides effective framework for establishing regularity criteria for non-Newtonian fluids in three dimensions through weighted gradient conditions.

Abstract: It establishes a regularity criterion for non-Newtonian fluids in $\mathbb{R}^3$ in terms of the weighted gradient of the velocity field, based on the Caffarelli--Kohn--Nirenberg inequality.

</details>


### [39] [Nash-Stackelberg controllability for coupled systems of degenerate equations in non-cylindrical domains](https://arxiv.org/abs/2602.07582)
*Alfredo S. Gamboa,Juan Limaco,Luis P. Yapu*

Main category: math.AP

TL;DR: Local null controllability of coupled degenerate semilinear parabolic equations in moving domains is proven using Liusternik's inverse function theorem and Carleman estimates.


<details>
  <summary>Details</summary>
Motivation: To investigate hierarchical null controllability for coupled degenerate semilinear parabolic equations in time-varying domains, addressing the challenge of controllability in degenerate systems with moving boundaries.

Method: Uses Liusternik's inverse function theorem for local null controllability, adapts Carleman estimates for the linearized optimality system, and applies previously obtained Carleman inequalities for degenerate non-autonomous equations.

Result: Demonstrates local null controllability of the semilinear system in moving domains, overcoming the main difficulty of adapting Carleman estimates for degenerate non-autonomous equations.

Conclusion: Successfully establishes hierarchical null controllability for coupled degenerate semilinear parabolic equations in moving domains through a combination of Liusternik's theorem and adapted Carleman estimates.

Abstract: In this paper we investigate the Hierarchical null controllability of a coupled degenerate semilinear parabolic equation in domains which are moving in time. We show the local null controllability of the semilinear system using Liusternik's inverse function theorem. Nevertheless, the main difficulty is to adapt a Carleman estimate for the controllability of the linearized otimality system, using a Carleman inequality for degenerate non-autonomous equation obtanied by the authors previously.

</details>


### [40] [Integral Harnack estimates and the rate of extinction of singular fractional diffusion](https://arxiv.org/abs/2602.07647)
*Filippo M. Cassanello,Simone Ciani,Antonio Iannizzotto*

Main category: math.AP

TL;DR: The paper proves integral Harnack inequalities for solutions to singular s-fractional p-Laplacian parabolic equations and uses these to analyze extinction behavior and decay rates near extinction times.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous quantitative estimates for the extinction phenomenon in singular fractional p-Laplacian diffusion problems, which requires developing new analytical tools beyond classical approaches.

Method: Proves integral Harnack-type inequalities for weak solutions of parabolic equations with measurable bounded coefficients, then applies these estimates to analyze decay rates near extinction times using an approximation procedure that avoids requiring integrable time derivatives.

Result: Obtains integral Harnack inequalities for singular s-fractional p-Laplacian diffusion, derives decay rates for local mass and supremum of solutions approaching extinction times, and demonstrates consistency through analysis of Cauchy-Dirichlet problems.

Conclusion: The developed integral Harnack inequalities provide powerful tools for analyzing extinction phenomena in singular fractional p-Laplacian diffusion, with applications to decay rate estimation and consistency verification for weak solutions.

Abstract: We prove several integral Harnack-type inequalities for local weak solutions of parabolic equations with measurable and bounded coefficients, describing singular s-fractional p-Laplacian diffusion. Then we apply the aforementioned estimates to evaluate the decay rate of the local mass and supremum of the solutions as they approach a possible extinction time. Yet we show consistency of our general decay estimates by studying the extinction phenomenon for weak solutions of the Cauchy-Dirichlet problem, by means of an approximation procedure that carefully avoids the use of an integrable time derivative.

</details>


### [41] [Finding the convex envelope of a boundary datum using random geometric graphs](https://arxiv.org/abs/2602.07696)
*Aurelia Deshayes,Nicolás Frevenza,Alfredo Miranda,Julio D. Rossi*

Main category: math.AP

TL;DR: The paper approximates the convex envelope of boundary data using random graphs with proximity connections and solves an equation that approximates the first eigenvalue of the Hessian, showing convergence to the convex envelope as points increase.


<details>
  <summary>Details</summary>
Motivation: To develop a computational method for approximating convex envelopes of boundary data in bounded domains, which is important in optimization, PDEs, and geometric analysis where convex envelopes play a crucial role.

Method: Construct random graphs from uniformly distributed points connected by proximity (edges when |x-y|<r). Solve an equation on the graph that approximates the first eigenvalue of the Hessian of a smooth function with given exterior boundary data.

Result: Under appropriate assumptions on the connection radius r, the unique solution to the graph equation converges to the convex envelope of the boundary datum as the number of points goes to infinity.

Conclusion: The random graph method provides a valid approximation scheme for convex envelopes, establishing a connection between discrete graph-based computations and continuous convex analysis.

Abstract: In this paper we approximate the convex envelope of a boundary datum inside a bounded domain in the Euclidean space. We work with a random graph that is obtained as random points with uniform distribution that are connected by proximity ($x\sim y$ when $|x-y|<r$). On the graph we solve an equation (that approximate the first eigenvalue of the Hessian of a smooth function) with an exterior datum. Under appropriate assumptions on $r$ we show that the unique solution to the equation in the graph converges to the convex envelope of the boundary datum as the number of points goes to infinity.

</details>


### [42] [Instability of shear flows with neutral embedded eigenvalues](https://arxiv.org/abs/2602.07807)
*Hui Li,Siqi Ren,Yuxi Wang,Guoqing Zhang*

Main category: math.AP

TL;DR: Monotone shear flows can exhibit arbitrarily large growth in L∞ and L² norms when Rayleigh operator has neutral embedded eigenvalues; stronger linear growth occurs with multiple eigenvalues due to non-normality.


<details>
  <summary>Details</summary>
Motivation: To understand the linear stability properties of monotone shear flows, particularly how neutral embedded eigenvalues in the Rayleigh operator can lead to significant growth despite apparent stability.

Method: Analyze the Rayleigh operator associated with monotone shear flows, study cases with neutral embedded eigenvalues, examine non-normality effects, and construct explicit solutions for multiple eigenvalue cases.

Result: Solutions can exhibit arbitrarily large growth in both L∞ and L² norms when neutral embedded eigenvalues exist. With multiple eigenvalues, stronger instability occurs with linear time growth.

Conclusion: Non-normality of the Rayleigh operator drives instability in monotone shear flows, with neutral embedded eigenvalues leading to significant growth, challenging conventional linear stability analysis.

Abstract: We study the linear stability of a class of monotone shear flows. When the associated Rayleigh operator possesses a neutral embedded eigenvalue, we show that solutions of the linearized system may exhibit arbitrarily large growth in both the $L^\infty$ and $L^2$ norms. Moreover, when the embedded eigenvalue is multiple, we prove that the instability becomes stronger and explicitly construct solutions that grow linearly in time. This instability originates from the non-normality of the Rayleigh operator.

</details>


### [43] [Relative entropy and slightly compressible Navier-Stokes dynamics of the Boltzmann equation](https://arxiv.org/abs/2602.07957)
*Yuhan Chen,Ning Jiang*

Main category: math.AP

TL;DR: The paper establishes formal convergence of Boltzmann equation solutions to compressible Navier-Stokes solutions with small Mach number in 3D periodic domains, using relative entropy methods to quantify convergence rates.


<details>
  <summary>Details</summary>
Motivation: To rigorously characterize the hydrodynamic limit from kinetic theory (Boltzmann equation) to fluid dynamics (Navier-Stokes equations) in the small Mach number regime, providing mathematical justification for this fundamental connection.

Method: Uses the relative entropy method developed by Bardos, Golse, Levermore and Yau, analyzing entropy relative to local Maxwellians governed by slightly compressible Navier-Stokes solutions.

Result: Demonstrates formal convergence of Boltzmann equation solutions to compressible Navier-Stokes solutions with small Mach number over 3D periodic domains, and characterizes the convergence rate to incompressible Navier-Stokes system.

Conclusion: The relative entropy method provides a powerful framework for establishing and quantifying the hydrodynamic limit from kinetic theory to fluid dynamics in the small Mach number regime.

Abstract: This paper shows that, in the formal level, the convergence of solutions of Boltzmann equation to solutions of the compressible Navier-Stokes system with small Mach number over the three-dimensional periodic domain $\mathbb{T}^3$,
  using the relative entropy method originated from Bardos, Golse, Levermore [{\em Comm. Pure Appl. Math.} {\bf 46} (1993) 667--753] and Yau [{\em Lett. Math. Phys.} {\bf 22} (1991) 63--80]. We discuss the evolution of the entropy which is relative to the local Maxwellian governed by the solution of slightly compressible Navier-Stokes system. This characterizes the convergence rate from Boltzmann equation to the incompressible Navier-Stokes system.

</details>


### [44] [Transport-diffusion equations with irregular data and applications to stability estimates for second-order Hamilton-Jacobi PDEs](https://arxiv.org/abs/2602.07969)
*Gianmarco Giovannardi,Alessandro Goffi*

Main category: math.AP

TL;DR: The paper studies quantitative uniqueness properties in L^p spaces for Fokker-Planck and transport-diffusion equations under new assumptions on velocity fields, proving stability estimates and applying them to obtain continuous dependence estimates for viscous Hamilton-Jacobi equations.


<details>
  <summary>Details</summary>
Motivation: To establish quantitative uniqueness properties for Fokker-Planck and transport-diffusion equations under new assumptions on velocity fields, particularly focusing on cases where divergence conditions are less restrictive than previous work.

Method: The authors first prove L^p-stability estimates for advection-diffusion PDEs when div(b) satisfies specific integrability conditions. Then they prove L^∞ stability for viscous transport equations when div(b(t)) fails to be integrable in time. These results are applied using integral methods to obtain continuous dependence estimates for viscous Hamilton-Jacobi equations.

Result: The paper establishes new stability estimates under weaker divergence conditions, obtains explicit constants in estimates for viscous Hamilton-Jacobi equations, and derives new uniqueness properties for diffusive Hamilton-Jacobi equations without relying on viscosity solutions theory.

Conclusion: The paper provides new quantitative uniqueness results for transport-diffusion equations under relaxed assumptions on velocity fields, with applications to Hamilton-Jacobi equations that yield explicit constants and alternative uniqueness proofs independent of viscosity solutions theory.

Abstract: This paper studies quantitative uniqueness properties in $L^p$ spaces for Fokker-Planck and transport-diffusion equations under two new assumptions on their velocity field $b=b(x,t)$. We first prove $L^p$-stability estimates for advection-diffusion PDEs when $\mathrm{div}(b)\in L^r_t(L^q_x)$ with $r\in[2,\infty]$ and $q\in[n/2,\infty)$ satisfying the compatibility condition $n/(2q)+1/r\leq 1$. We then prove a stability result in $L^\infty$ for solutions of viscous transport equations when $\mathrm{div}(b(t))$ fails to be integrable in time. We apply these properties to obtain new continuous dependence estimates for viscous Hamilton-Jacobi equations via integral methods. One of the main novelties in this latter setting is that the constants of the estimates are all explicit with respect to the data of the problem. These imply new uniqueness properties for diffusive Hamilton-Jacobi equations without relying on the theory of viscosity solutions.

</details>


### [45] [Observability properties of the singular Grushin equation](https://arxiv.org/abs/2602.08044)
*Roman Vanlaere*

Main category: math.AP

TL;DR: Study of observability properties for Grushin equation with inverse square potential, determining exact minimal observation times in specific configurations using Carleman estimates.


<details>
  <summary>Details</summary>
Motivation: To understand observability properties of the Grushin equation with singular inverse square potential, which occurs at boundaries in 2D or interior in higher dimensions, and to determine minimal observation times for control theory applications.

Method: Relies on recent Carleman estimates by Beauchard, Dardé, and Ervedoza to analyze observability properties in specific observation set configurations, with applications to heat equations on almost-Riemannian manifolds.

Result: Obtained exact minimal time of observability for specific observation set configurations, and observed dependence of minimal observation time on dimension of singularity for heat equation on almost-Riemannian manifolds.

Conclusion: The analysis provides precise observability results for Grushin equations with singular potentials, revealing dimensional dependence of minimal observation times and connecting to control problems on geometric structures with singularities.

Abstract: We study the observability properties of the Grushin equation with an inverse square potential, whose singularity occurs at the boundary of two-dimensional rectangular domains or in the interior of the domain in higher dimensions. In some specific configurations of the observation set, we obtain the exact minimal time of observability. The analysis we present relies on recent Carleman estimates obtained by K. Beauchard, J. Dardé, and S. Ervedoza. As a byproduct of these results, we observe, for the heat equation associated to the Laplace-Beltrami operator on almost-Riemannian manifolds, a dependence of the minimal time of observability on the dimension of the singularity.

</details>


### [46] [Enhanced lifespan bounds for 1D quasilinear Klein-Gordon flows](https://arxiv.org/abs/2602.08055)
*Hongjing Huang,Mihaela Ifrim,Daniel Tataru*

Main category: math.AP

TL;DR: Small data solutions to 1D quasilinear Klein-Gordon equations persist on cubic timescale ε⁻² with sharp energy estimates, extendable to quartic ε⁻⁴ on ℝ using dispersion.


<details>
  <summary>Details</summary>
Motivation: The paper aims to extend lifespan bounds for small data solutions to quasilinear Klein-Gordon equations beyond the standard linear timescale, generalizing earlier semilinear results by Delort.

Method: Uses refined modified-energy framework of Ifrim and Tataru to analyze long-time behavior, with additional dispersion techniques for the ℝ case to achieve extended lifespan.

Result: Solutions with small initial data of size ε persist on cubic timescale |t| ≲ ε⁻² with sharp cubic energy estimates, and on ℝ extend to quartic timescale ε⁻⁴ using dispersion.

Conclusion: The refined modified-energy framework successfully establishes improved lifespan bounds for quasilinear Klein-Gordon equations, generalizing semilinear results and demonstrating the effectiveness of dispersion for extended timescales on ℝ.

Abstract: In this article we consider one-dimensional scalar quasilinear Klein--Gordon equations with general nonlinearities, on both $\mathbb{R}$ and $\mathbb{T}$. By employing a refined modified-energy framework of Ifrim and Tataru, we investigate long time lifespan bounds for small data solutions. Our main result asserts that solutions with small initial data of size $ε$ persist on the improved cubic timescale $|t| \lesssim ε^{-2}$ and satisfy sharp cubic energy estimates throughout this interval. We also establish difference bounds on the same time scale. In the case of $\mathbb{R}$, we are further able to use dispersion in order to extend the lifespan to $ε^{-4}$. This generalizes earlier results obtained by Delort in the semilinear case.

</details>


### [47] [On the well-posedness of a certain model with two kernels appearing in the mathematical biology](https://arxiv.org/abs/2602.08102)
*Messoud Efendiev,Vitali Vougalter*

Main category: math.AP

TL;DR: Global well-posedness established for integro-differential model with nonlocal diffusion and production terms in biological cell population dynamics.


<details>
  <summary>Details</summary>
Motivation: To analyze cell population dynamics in mathematical biology using integro-differential models with nonlocal diffusion and production terms, which are relevant for biological systems.

Method: Fixed point technique applied to prove global well-posedness in the Sobolev space W^{(1,2),2}(R×R⁺).

Result: Successfully established global well-posedness for the integro-differential problem with transport term in the specified function space.

Conclusion: The fixed point approach provides rigorous mathematical foundation for studying cell population dynamics models with nonlocal diffusion and production mechanisms.

Abstract: The work is devoted to establishing the global well-posedness in $W^{(1,2),2}(R\times R^{+})$ of the integro-differential problem involving the two nonlocal terms describing the diffusion and the production in the biological system in the presence of the transport term. Such model is relevant to the cell population dynamics in the Mathematical Biology. The proof is based on a fixed point technique.

</details>


### [48] [A bifurcation theory approach to the nonlocal Kuramoto-Sivashinsky equation](https://arxiv.org/abs/2602.08107)
*Pablo Cubillos,Rafael Granero-Belinchón y Juan Carlos Sampedro*

Main category: math.AP

TL;DR: The paper analyzes the nonlocal Kuramoto-Sivashinsky equation on a torus, proving well-posedness, studying bifurcations of steady states from trivial solutions, and establishing global continuation of bifurcation branches with numerical verification.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics and steady-state structure of the nonlocal Kuramoto-Sivashinsky equation, which generalizes classical models and appears in various physical applications like flame propagation and fluid dynamics.

Method: Combines analytical PDE techniques (local/global well-posedness in Sobolev spaces), bifurcation theory (Crandall-Rabinowitz theorem), topological degree methods (Fitzpatrick-Pejsachowicz-Rabier degree), and numerical continuation to study steady states.

Result: Proves local/global well-posedness in H³, identifies bifurcation points at ε_k = k^(r-s), obtains local curves of nontrivial equilibria, establishes global continuation with parameter range (2^(r-s),1) for first branch, and provides numerical verification.

Conclusion: The nonlocal Kuramoto-Sivashinsky equation exhibits rich bifurcation structure with nontrivial steady states emerging from trivial solutions, with global existence established for parameter ranges using analytical and topological methods complemented by numerical evidence.

Abstract: We study the nonlocal Kuramoto-Sivashinsky equation on the one-dimensional torus,
  \[
  u_t+u u_x=Λ^{r}u-\varepsilon Λ^{s}u,\qquad x\in\mathbb T,
  \] where $\varepsilon>0$, $s>1$, $r\in[-1,s)$. We first prove local and global well-posedness for initial data in $H^{3}(\mathbb T)$. We then investigate the steady-state problem and show that the trivial branch undergoes bifurcation at the critical values $\varepsilon_k=k^{\,r-s}$, $k\in\mathbb N$. Using the Crandall-Rabinowitz theorem we obtain smooth local curves of nontrivial equilibria emanating from each $(\varepsilon_k,0)$ and compute the bifurcation direction. To address the global continuation of these branches we derive global a priori bounds and apply a global alternative based on the Fitzpatrick-Pejsachowicz-Rabier degree for Fredholm maps of index zero. In particular, for the component bifurcating from the first critical point we prove that its $\varepsilon$-projection contains the interval $(2^{r-s},1)$, yielding the existence of nontrivial steady states for that parameter range. We complement the theory with numerical continuation results illustrating the bifurcation diagram and solution profiles.

</details>


### [49] [Stability of $L^p$ Dirichlet solvability under small bi-Lipschitz transformations of domains](https://arxiv.org/abs/2602.08115)
*Joseph Feneuil,Linhan Li,Jinping Zhuge*

Main category: math.AP

TL;DR: Small bi-Lipschitz deformations preserve L^p Dirichlet problem solvability for Laplacian, unifying convex and C^1 domain results.


<details>
  <summary>Details</summary>
Motivation: To unify two fundamentally different settings where L^p Dirichlet problem solvability was previously known: convex domains and C^1 domains, by showing that small Lipschitz perturbations preserve solvability properties.

Method: Construct a change of variables based on a non-constant basis derived from the Green function, which encodes the geometry of the base domain. This allows handling small bi-Lipschitz deformations of Lipschitz domains.

Result: Small bi-Lipschitz deformations preserve solvability of Dirichlet problem for Laplacian with L^p boundary data for same p>1. Solvability established for small Lipschitz perturbations of convex domains for all p∈(1,∞).

Conclusion: The approach unifies convex and C^1 domain theories through geometric encoding via Green function basis, providing robust framework for perturbation analysis of Dirichlet problems.

Abstract: We show that small bi-Lipschitz deformations of a Lipschitz domain (with possibly large Lipschitz constant) preserve the solvability of the Dirichlet problem for the Laplacian with boundary data in $L^p$, for the same value of $p>1$. As a consequence, for all $p\in(1,\infty)$, we obtain the solvability of the $L^p$ Dirichlet problem for small Lipschitz perturbations of convex domains, thereby unifying two fundamentally different settings in which such results were previously known: convex and $C^1$ domains. The key ingredient and novelty of our approach is a construction of a change of variables based on a non-constant basis derived from the Green function, which encodes the geometry of the base domain.

</details>


### [50] [Forced oscillation of a damped BBM equation posed on whole line in low regularity spaces](https://arxiv.org/abs/2602.08327)
*Chun Ho Lau,Taige Wang*

Main category: math.AP

TL;DR: Establishes existence and stability of time-periodic solutions for 1D forced damped BBM equation in low regularity spaces H^ℓ (ℓ∈[0,1)) using I-energy method.


<details>
  <summary>Details</summary>
Motivation: To prove existence and stability of time-periodic solutions for the 1D Cauchy problem of forced damped Benjamin-Bona-Mahony equation in low regularity spaces, extending results beyond classical smooth function spaces.

Method: Uses I-energy method to derive estimates in H^ℓ spaces for the linearized problem, then treats the convection term as a perturbation of the linear problem to solve the original Cauchy problem.

Result: Establishes existence and stability results for time-periodic solutions in low regularity spaces H^ℓ where ℓ∈[0,1).

Conclusion: The I-energy method successfully proves existence and stability of time-periodic solutions for forced damped BBM equation in low regularity function spaces, handling the convection term as a perturbation.

Abstract: In this manuscript, we would established in low regularity spaces $H^\ell, \ell\in [0,1)$, the existence and stability results of time-periodic solution of 1D Cauchy problem of forced damped Benjamin-Bona-Mahony equation (BBM). We use estimates from I-energy method to derive needed estimates in $H^\ell$ for the linearized problem, then convection term will be treated as perturbation of linear problem such that original Cauchy problem is solved.

</details>


### [51] [Remainder terms and sharp quantitative stability for a nonlocal Sobolev inequality on the Heisenberg group](https://arxiv.org/abs/2602.08375)
*Wenjing Chen,Zexi Wang*

Main category: math.AP

TL;DR: The paper studies a nonlocal Sobolev inequality on the Heisenberg group, establishes a gradient-type remainder term for certain parameter ranges, and proves quantitative stability of critical points for the associated Euler-Lagrange equation in the multi-bubble case.


<details>
  <summary>Details</summary>
Motivation: To investigate the stability properties of the Hardy-Littlewood-Sobolev type inequality on the Heisenberg group and understand the behavior of critical points for the associated nonlocal Euler-Lagrange equation.

Method: Mathematical analysis on the Heisenberg group, including establishing remainder terms for the inequality under specific parameter conditions (Q≥4, μ∈(0,4]) and proving quantitative stability results for critical points in the multi-bubble case when Q=4 and μ∈(2,4).

Result: Proved existence of gradient-type remainder term for the inequality, derived weak norm remainder term on bounded domains, and established quantitative stability of critical points for the Euler-Lagrange equation in specific parameter ranges.

Conclusion: The paper provides stability results for the nonlocal Sobolev inequality on the Heisenberg group, including remainder terms and quantitative stability of critical points, contributing to the understanding of functional inequalities in sub-Riemannian geometry.

Abstract: In this paper, we study the following nonlocal Sobolev inequality on the Heisenberg group \begin{equation}\label{eq:HLS}
  S_{HL}(Q,μ) \left(\int_{\mathbb{H}^{n}}\int_{\mathbb{H}^{n}}\frac{|u(ξ)|^{Q^{\ast}_μ}|u(η)|^{Q^{\ast}_μ}}{|η^{-1}ξ|^μ}{d}ξ{d}η\right)^{\frac{1}{Q^{\ast}_μ}}\leq \int_{\mathbb{H}^{n}}|\nabla_{\mathbb{H}}u|^{2}dξ,\quad \forall \, u\in S^{1,2}(\mathbb{H}^{n}), \end{equation} where $Q=2n+2$ is the homogeneous dimension of the Heisenberg group $\mathbb{H}^{n}$, $n\geq1$, $μ\in(0,Q)$, $Q^{\ast}_μ=\frac{2Q-μ}{Q-2}$ is the upper critical exponent in the sense of the Hardy-Littlewood-Sobolev inequality and the Folland-Stein-Sobolev inequality on the Heisenberg group, $S_{HL}(Q,μ)$ is the sharp constant of \eqref{eq:HLS}, and $S^{1,2}(\mathbb{H}^{n})$ is the Folland-Stein-Sobolev space. %of the nonlocal-Sobolev inequality. It is well-known that, up to a translation and suitable scaling, \begin{equation}\label{eq:abs}
  -Δ_{\mathbb{H}} u=\left(\int_{\mathbb{H}^{n}}\frac{|u(η)| ^{Q^{\ast}_μ}}{|η^{-1}ξ|^μ}{d}η\right)|u|^{Q_μ^*-2}u,~~u\in S^{1,2}(\mathbb{H}^{n}) \end{equation} is the Euler-Lagrange equation corresponding to the associated minimization problem.
  On the one hand, we show the existence of a gradient-type remainder term for inequality \eqref{eq:HLS} when $Q\geq4$, $μ\in (0,4]$, and as a corollary, derive the existence of a remainder term in the weak $L^{\frac{Q}{Q-2}}$-norm on bounded domains. On the other hand, we establish the quantitative stability of critical points for equation \eqref{eq:abs} in the multi-bubble case when $Q=4$ and $μ\in (2,4)$.

</details>


### [52] [$C^{1,α}$-regularity for Mixed Local and Nonlocal Degenerate Elliptic Equations in the Heisenberg Group](https://arxiv.org/abs/2602.08398)
*Junli Zhang*

Main category: math.AP

TL;DR: The paper proves C^{1,α}-regularity of weak solutions to mixed local and nonlocal degenerate elliptic equations on the Heisenberg group using Morrey-type iteration schemes and Hölder continuity techniques.


<details>
  <summary>Details</summary>
Motivation: The regularity theory for equations combining both local and nonlocal operators in sub-Riemannian geometries presents significant challenges, particularly on the Heisenberg group where degenerate elliptic equations arise.

Method: Develops a sophisticated Morrey-type iteration scheme using horizontal difference quotient combined with Nirenberg difference quotient and fractional Sobolev-type inequality. Establishes Hölder continuity through local boundedness, iteration schemes, iterative methods, and Morrey inequality. Then uses this Hölder continuity with existing results to prove C^{1,α}-regularity.

Result: Successfully proves C^{1,α}-regularity of weak solutions to mixed local and nonlocal degenerate elliptic equations on the Heisenberg group.

Conclusion: The paper provides a comprehensive regularity theory for mixed local-nonlocal degenerate elliptic equations in sub-Riemannian geometry, specifically establishing C^{1,α}-regularity on the Heisenberg group through novel iteration techniques.

Abstract: The regularity theory for equations combining both local and nonlocal operators in sub-Riemannian geometries is a huge challenge. In this paper, we investigate the $C^{1,α}$-regularity of weak solutions to mixed local and nonlocal degenerate elliptic equations on the Heisenberg group. We first derive a sophisticated iteration scheme of Morrey-type by leveraging horizontal difference quotient combined with the Nirenberg difference quotient and fractional Sobolev-type inequality on the Heisenberg group. Then, the Hölder continuity of the weak solutions is established by applying the local boundedness, the iteration scheme of Morrey-type, an iterative method and the Morrey inequality. Finally, we use the Hölder continuity in conjunction with Theorem 1.2 from Mukherjee and Zhong[18] to prove the $C^{1,α}$-regularity of weak solutions.

</details>


### [53] [Wave propagation in the frequency regime in one-dimensional quasiperiodic media -Limiting absorption principle](https://arxiv.org/abs/2602.08442)
*Pierre Amenoagbadji,Sonia Fliss,Patrick Joly*

Main category: math.AP

TL;DR: The paper addresses the well-posedness issue of the 1D Helmholtz equation with quasiperiodic coefficients using the limiting absorption principle, replacing Dirichlet-to-Neumann with Robin-to-Robin boundary conditions to enable the absorption limit to zero.


<details>
  <summary>Details</summary>
Motivation: The Helmholtz equation with quasiperiodic coefficients is generally not well-posed for real-valued frequencies - existence in L² is not guaranteed and uniqueness in L∞ may fail. This well-known difficulty has never been addressed in the quasiperiodic case.

Method: Uses the limiting absorption principle (adding imaginary part to frequency to make equation well-posed in L², then taking absorption to zero). Replaces previous Dirichlet-to-Neumann boundary conditions with Robin-to-Robin boundary conditions to facilitate the limiting process. Proposes numerical method to compute physical solution.

Result: Proves that under technical assumptions on the frequency, the limiting absorption principle holds for the 1D Helmholtz equation with quasiperiodic coefficients. Provides a numerical method to compute the physically relevant solution.

Conclusion: The limiting absorption principle with Robin-to-Robin boundary conditions successfully addresses the well-posedness issue of Helmholtz equations with quasiperiodic coefficients, providing both theoretical foundation and practical numerical approach.

Abstract: We study the one-dimensional Helmholtz equation with (possibly perturbed) quasiperiodic coefficients. Quasiperiodic functions are the restriction of higher dimensional periodic functions along a certain (irrational) direction. In classical settings, for real-valued frequencies, this equation is generally not well-posed: existence of solutions in L 2 is not guaranteed and uniqueness in L $\infty$ may fail. This is a well-known difficulty of Helmholtz equations, but it has never been addressed in the quasiperiodic case. We tackle this issue by using the limiting absorption principle, which consists in adding some imaginary part (also called absorption) to the frequency in order to make the equation well-posed in L 2 , and then defining the physically relevant solution by making the absorption tend to zero. In previous work, we introduced a definition of the solution of the equation with absorption based on Dirichlet-to-Neumann (DtN) boundary conditions. This approach offers two key advantages: it facilitates the limiting process and has a direct numerical counterpart. In this work, we first explain why the DtN boundary conditions have to be replaced by Robin-to-Robin boundary conditions to make the absorption go to zero. We then prove, under technical assumptions on the frequency, that the limiting absorption principle holds and we propose a numerical method to compute the physical solution.

</details>


### [54] [Existence and Regularity of Minimizers for a Plateau Approximation Problem](https://arxiv.org/abs/2602.08476)
*Eve Machefert*

Main category: math.AP

TL;DR: The paper establishes existence and Hölder regularity of minimizers for a functional approximating Plateau's problem, generalizing previous 1D Steiner problem approximations to higher dimensions.


<details>
  <summary>Details</summary>
Motivation: To develop a functional approximation method for Plateau's problem (minimal surfaces) that extends previous work on approximating the Steiner problem from 1D to higher-dimensional surfaces.

Method: Study a functional introduced in previous collaborative work, analyze its properties, and prove mathematical results about its minimizers.

Result: Proved existence of a minimizer for the functional and established Hölder regularity of the minimizer.

Conclusion: Successfully generalized the 1D Steiner problem approximation framework to higher-dimensional Plateau problem approximation, providing rigorous mathematical foundations for the approach.

Abstract: In this paper, we study the functional introduced by the author in collaboration with Bonnivard, Bretin, and Lemenant, which is designed to approximate Plateau's problem. We establish the existence of a minimizer and prove its H{ö}lder regularity. Our results may be viewed as a generalization to higher-dimensional surfaces of the one-dimensional work of Bonnivard, Lemenant, and Millot on the approximation of the Steiner problem.

</details>


### [55] [Construction of two-bubble solutions for the energy-critical Hartree equation](https://arxiv.org/abs/2602.08490)
*Jacek Jendrej,Xuemei Li,Guixiang Xu*

Main category: math.AP

TL;DR: Constructed a pure two-bubble solution for focusing energy-critical Hartree equation in N≥7 dimensions, featuring two ground states with scaling ratio converging to 0 and orthogonal phases.


<details>
  <summary>Details</summary>
Motivation: To understand multi-bubble dynamics in nonlocal equations, extending existing constructions from local to nonlocal interactions which are more complex to analyze.

Method: Modulation analysis, bootstrap argument, and topological argument applied to the spherically symmetric, energy-critical Hartree equation.

Result: Successfully constructed a global (at least backward in time) two-bubble solution with bubbles centered at origin, length scale ratio converging to 0, and orthogonal phases.

Conclusion: Demonstrates existence of pure two-bubble solutions for nonlocal Hartree equations, overcoming complexity of nonlocal interactions compared to local cases.

Abstract: We construct a pure two-bubble solution for the focusing, energy-critical Hartree equation in space dimension $N \geq 7$. The constructed solution is spherically symmetric, global in (at least) the negative time direction and asymptotically behaves as a superposition of two ground states (or bubbles) both centered at the origin, with the ratio of their length scales converging to $0$ and the phases of the two bubbles form the right angle. The main arguments are the modulation analysis, the bootstrap argument and the topological argument. The main novelty with respect to existing constructions of pure two-bubble solutions is the nonlocal interaction, which is more complex to analyze.

</details>


### [56] [Viscous Burgers equation driven by point source: a formula for the weak limit](https://arxiv.org/abs/2602.08496)
*Smritikana Pal,Manas R. Sahoo*

Main category: math.AP

TL;DR: Study of viscous Burgers equation with point source term, analyzing weak limit as viscosity approaches zero, revealing connection to unusual variational problem with three functional types.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of solutions to viscous Burgers equation with point source term as viscosity vanishes, which differs from the standard case without source term.

Method: Analyze weak limit of solutions to viscous Burgers equation driven by point source term as viscosity coefficient tends to zero, connecting it to variational problems.

Result: The weak limit is related to a variational problem involving three types of functionals, which is unusual compared to the case without source term.

Conclusion: The presence of point source term significantly changes the limiting behavior of viscous Burgers equation, leading to different variational structure with multiple functional types.

Abstract: In this article, we obtain the weak limit of the solutions of the viscous Burgers equation driven by a point source term, as the coefficient of viscosity tends to zero. The weak limit is related to the variational problem that consists of three types of functional, which is not usual in the absence of the source term.

</details>


### [57] [Influence of the Reynolds number on non-Newtonian flow in thin porous media](https://arxiv.org/abs/2602.08555)
*Maria Anguiano,Matthieu Bonnivard,Francisco J. Suarez-Grau*

Main category: math.AP

TL;DR: Study of Reynolds number effects on generalized Newtonian fluid flow through thin porous media, identifying critical Reynolds number scaling and deriving effective Darcy laws.


<details>
  <summary>Details</summary>
Motivation: To understand how Reynolds number affects flow behavior in thin porous media with non-Newtonian fluids, particularly when inertial forces become significant relative to viscous forces.

Method: Use homogenization theory for thin porous media with periodic microstructure, analyze Navier-Stokes system with Carreau viscosity law, consider Reynolds number scaling as ε^{-γ}, and develop numerical methods for solving derived nonlinear Darcy laws.

Result: Identified critical Reynolds number of order 1/ε where inertial term becomes significant; derived linear/nonlinear Darcy laws for subcritical cases; proposed numerical method for solving effective flow equations.

Conclusion: Inertial effects in thin porous media flow become important only when Reynolds number exceeds critical scaling of 1/ε; below this threshold, effective flow follows Darcy-type laws that can be solved numerically.

Abstract: We study the effect of the Reynolds number on the flow of a generalized Newtonian fluid through a thin porous medium in $\mathbb{R}^3$. This medium is a domain of thickness $\varepsilon \ll 1$, perforated by periodically distributed solid cylinders of size $\varepsilon$. We consider the nonlinear stationary Navier-Stokes system with viscosity following the Carreau law. Using tools from homogenization theory and assuming that the Reynolds number scales as $\varepsilon^{-γ}$, where $γ$ is a real constant, we prove the existence of a critical Reynolds number of order $1/\varepsilon$, in the sense that the inertial term in the Navier-Stokes system has no influence in the limit if the Reynolds number is of order smaller than or equal to $1/\varepsilon$ (i.e. $γ= 1$). In this case, we derive linear or nonlinear Darcy laws connecting velocity to pressure gradient. Conversely, we expect a contribution from the inertial term in the homogenized problem if the Reynolds number is greater than $1/\varepsilon$. Finally, we propose a numerical method to solve nonlinear Darcy laws describing effective flow in the critical case and demonstrate its practical applicability on several examples.

</details>


### [58] [Inverse problem for the geometric Navier-Stokes equations](https://arxiv.org/abs/2602.08644)
*Yavar Kian,Lauri Oksanen,Ziyao Zhao*

Main category: math.AP

TL;DR: Inverse problem: Reconstruct Riemannian manifold from Navier-Stokes observations on small spatial subset using Boundary Control method.


<details>
  <summary>Details</summary>
Motivation: Determine geometric structure (Riemannian manifold) from limited observational data of fluid flow, connecting fluid dynamics with geometric inverse problems.

Method: 1. Reduce Navier-Stokes inverse problem to hyperbolic Stokes system via linearization and spectral techniques. 2. Solve using new generalization of Boundary Control method.

Result: Developed framework to reconstruct compact Riemannian manifold with boundary from fixed-time Navier-Stokes observations on small spatial subset.

Conclusion: Successfully connects fluid dynamics inverse problems with geometric reconstruction using innovative reduction to hyperbolic Stokes system and Boundary Control method.

Abstract: We consider the inverse problem of determining a compact Riemannian manifold with boundary from fixed time observations of the solution, restricted to a small subset in space, for the Navier-Stokes system with a local source on the manifold. Our approach is based on a reduction to an inverse problem for an auxiliary hyperbolic Stokes system, via linearization and spectral techniques. We solve the resulting inverse problem by a new generalization of the Boundary Control method.

</details>


### [59] [Abstract integrodifferential equations and applications](https://arxiv.org/abs/2602.08691)
*Bruno de Andrade,Marcos Gabriel de Santana*

Main category: math.AP

TL;DR: The paper studies abstract integrodifferential equations in interpolation scales, proving local existence, uniqueness, continuation, and blow-up alternatives for mild solutions, with applications to Navier-Stokes equations with hereditary viscosity and reaction-diffusion problems with memory.


<details>
  <summary>Details</summary>
Motivation: To develop a general theory for abstract integrodifferential equations in interpolation scales, which can be applied to important PDE problems with memory effects, such as Navier-Stokes equations with hereditary viscosity and reaction-diffusion systems with memory.

Method: Uses interpolation scales and abstract integrodifferential equation framework to prove local-in-time existence, uniqueness, continuation, and blow-up alternatives for regular mild solutions. Applies this abstract theory to specific PDE problems.

Result: Proves local existence, uniqueness, continuation, and blow-up alternatives for mild solutions to abstract integrodifferential equations. Successfully applies the theory to Navier-Stokes equations with hereditary viscosity (using fractional power spaces of Stokes operator) and reaction-diffusion problems with memory (considering super-linear and gradient-type nonlinearities in Lebesgue and Besov spaces).

Conclusion: The developed abstract theory provides a unified framework for studying integrodifferential equations with memory effects, successfully covering important applications in fluid dynamics and reaction-diffusion systems with various types of nonlinearities and initial data spaces.

Abstract: In this work, we study the initial value problem associated with an abstract integrodifferential equation in interpolation scales. We prove local-in-time existence, uniqueness, continuation, and a blow-up alternative for regular mild solutions to the problem. Additionally, we apply this theory to the Navier-Stokes equations with hereditary viscosity, taking initial data in the scale of fractional power spaces associated with the Stokes operator. We also explore reaction-diffusion problems with memory, considering the effects of super-linear and gradient-type nonlinearities, and initial data in Lebesgue and Besov spaces, respectively.

</details>


### [60] [Weighted Hardy-Sobolev type inequalities with boundary terms](https://arxiv.org/abs/2602.08702)
*João Marcos do Ò,Marcelo Furtado,Everaldo Medeiros,Jesse Ratzkin*

Main category: math.AP

TL;DR: Establishes new weighted Hardy-Sobolev inequalities under mild monotonicity conditions on weights, leading to weighted Sobolev and trace-type inequalities for elliptic problems with Neumann/Robin conditions in unbounded domains.


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for studying elliptic partial differential equations with Neumann or Robin boundary conditions in unbounded domains, which require weighted functional inequalities to handle the lack of compactness and boundary behavior.

Method: Establishes a new class of weighted Hardy-Sobolev type inequalities under mild monotonicity assumptions on the weight function, then derives corresponding weighted Sobolev and trace-type inequalities as consequences.

Result: Obtains new weighted functional inequalities that are applicable to elliptic problems in unbounded domains, providing essential analytical tools for Neumann and Robin boundary value problems.

Conclusion: The developed weighted Hardy-Sobolev inequalities and their corollaries provide important analytical foundations for studying elliptic problems with Neumann or Robin boundary conditions in unbounded domains.

Abstract: In this paper we establish a new class of weighted Hardy-Sobolev type inequalities under mild monotonicity assumptions on the weight function. As a consequence, we derive the corresponding weighted Sobolev and trace-type inequalities. These results play an important role in the analysis of elliptic problems with Neumann or Robin boundary conditions in unbounded domains.

</details>


### [61] [Derivation and analysis of a Stokes-transport system in evolving vessels modeling thermoregulation in human skin](https://arxiv.org/abs/2602.08788)
*Kilian Hacker,Maria Neuss-Radu*

Main category: math.AP

TL;DR: Stokes flow with advection-diffusion in evolving domain models thermal control of blood flow in skin, with existence/uniqueness proofs via fixed point method.


<details>
  <summary>Details</summary>
Motivation: To model thermal control of blood flow in human skin, accounting for temperature-dependent biochemical production, vessel dilation/constriction, and convective heat transfer changes.

Method: Coupled Stokes flow with advective-diffusive transport in evolving domain with inflow/outflow boundary conditions. Uses fixed point method to handle nonlinear coupling between fluid flow, transport, and domain evolution.

Result: Proves existence and uniqueness of weak solutions for the fully coupled nonlinear problem.

Conclusion: The mathematical framework successfully models thermal regulation in skin vasculature and provides rigorous solution existence/uniqueness guarantees for the coupled system.

Abstract: We consider a Stokes flow coupled with advective-diffusive transport in an evolving domain with boundary conditions allowing for inflow and outflow. The evolution of the domain is induced by the transport process, leading to a fully coupled problem. Our aim is to model the thermal control of blood flow in human skin. To this end, the model takes into account the temperature-dependent production of biochemical substances, the subsequent dilation and constriction of blood vessels, and the resulting changes in convective heat transfer. We prove existence and uniqueness of weak solutions using a fixed point method that allows us to treat the nonlinear coupling.

</details>


### [62] [Rigidity of homogeneous Lamé systems](https://arxiv.org/abs/2602.08860)
*Joonas Ilmavirta,Teemu Saksala,Lili Yan*

Main category: math.AP

TL;DR: The paper proves that if a Lamé system has the same Dirichlet-to-Neumann map as a homogeneous Lamé system, then it must actually be homogeneous, with no additional assumptions needed on the Lamé coefficients.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness in inverse problems for elastic wave equations, specifically showing that matching Dirichlet-to-Neumann maps forces Lamé systems to be homogeneous.

Method: Uses geometric properties: the homogeneous system creates a geometry that is both simple and admits a strictly convex foliation. This geometric structure is leveraged to prove the uniqueness result.

Result: Any Lamé system with Dirichlet-to-Neumann map identical to that of a homogeneous Lamé system must itself be homogeneous, regardless of any assumptions on the Lamé coefficients being recovered.

Conclusion: The Dirichlet-to-Neumann map uniquely determines homogeneity in Lamé systems for elastic wave equations, using geometric simplicity and convex foliation properties of homogeneous systems.

Abstract: In this short paper, we show that any Lamé system whose Dirichlet-to-Neumann map for the elastic wave equation agrees with the one arising from the homogeneous Lamé system must actually be homogeneous. We do not need to impose any assumptions for the Lamé coefficients that we aim to recover. We use the fact that the homogeneous system gives rise to a geometry that is both simple and admits a strictly convex foliation.

</details>


### [63] [Global well-posedness for one-dimensional compressible Navier--Stokes system in dynamic combustion with small $BV\cap L^1$ initial data](https://arxiv.org/abs/2602.08867)
*Siran Li,Haitao Wang,Jianing Yang*

Main category: math.AP

TL;DR: Global well-posedness of small BV weak solutions for 1D compressible Navier-Stokes model of reacting gas mixtures in dynamic combustion.


<details>
  <summary>Details</summary>
Motivation: Extend recent global well-posedness theory for BV weak solutions from isentropic Navier-Stokes and Navier-Stokes-Fourier systems to reacting gas mixtures in combustion.

Method: Use iterative scheme for local existence, analyze Green's function of linearized system for global existence, study stability/uniqueness of local weak solutions.

Result: Establish local and global existence of weak solutions for small BV perturbations around equilibrium, characterize large-time behavior.

Conclusion: Successfully extend BV weak solution theory to reacting gas mixtures in combustion, providing comprehensive well-posedness framework.

Abstract: We establish the global well-posedness theory of small BV weak solutions to a one-dimensional compressible Navier--Stokes model for reacting gas mixtures in dynamic combustion. The unknowns of the PDE system consist of the specific volume, velocity, temperature, and mass fraction of the reactant. For initial data that are small perturbations around the constant equilibrium state $(1, 0, 1, 0)$ in the $L^1(\mathbb{R}) \cap {\rm BV}(\mathbb{R})$-norm, we establish the local-in-time existence of weak solutions via an iterative scheme, show the stability and uniqueness of local weak solutions, and prove the global-in-time existence of solutions for initial data with small BV-norm via an analysis of the Green's function of the linearised system. The large-time behaviour of the global BV weak solutions is also characterised. This work is motivated by and extends the recent global well-posedness theory for BV weak solutions to the one-dimensional isentropic Navier--Stokes and Navier--Stokes--Fourier systems developed in [T.-P. Liu, S.-H. Yu, Commun. Pure Appl. Math. 75 (2022), 223--348] and [H. Wang, S.-H. Yu, X. Zhang, Arch. Ration. Mech. Anal. 245 (2022), 375--477].

</details>


### [64] [Existence of expanding harmonic map flows to hemispheres](https://arxiv.org/abs/2602.08932)
*Xuanyu Li*

Main category: math.AP

TL;DR: Existence of multiple weak harmonic map flow solutions from non-minimizing 0-homogeneous maps to balls/hemispheres, answering Struwe's question.


<details>
  <summary>Details</summary>
Motivation: Address Struwe's question about existence of self-expanding harmonic map flows from non-energy-minimizing 0-homogeneous maps, particularly exploring multiplicity of solutions from stationary but non-minimizing initial data.

Method: Construct infinitely many different weak solutions to harmonic map flow starting from non-minimizing stationary 0-homogeneous harmonic maps to closed hemispheres, all satisfying parabolic monotonicity formula.

Result: Proved existence of non-trivial self-expanding harmonic map flows from non-energy-minimizing 0-homogeneous maps to regular balls or closed hemispheres, with infinite multiplicity of solutions from given initial data.

Conclusion: Answers Struwe's question affirmatively by demonstrating rich solution structure for harmonic map flow from non-minimizing stationary initial data, with all constructed solutions satisfying key monotonicity properties.

Abstract: We show the existence of non-trivial self-expanding harmonic map flows starting from non-energy-minimizing 0-homogeneous maps to a regular ball or a closed hemisphere. In particular, given a non-minimizing but stationary 0-homogeneous harmonic map $u_0$ to a closed hemisphere, we construct infinitely many different weak solutions to harmonic map flow starting from $u_0$, all of which satisfy the parabolic monotonicity formula. This answers a question of Struwe.

</details>


### [65] [Sharp gradient integrability for $(s,p)$-Poisson type equations](https://arxiv.org/abs/2602.08944)
*Verena Bögelein,Frank Duzaar,Naian Liao,Kristian Moring*

Main category: math.AP

TL;DR: Local W^{1,q} regularity for fractional p-Laplacian equations with optimal exponent q, confirmed by counterexample.


<details>
  <summary>Details</summary>
Motivation: To establish local regularity results for weak solutions to fractional p-Laplacian type equations with right-hand side in L^r, determining the optimal Sobolev space exponent q.

Method: Prove local W^{1,q}-regularity for weak solutions using quantitative local gradient estimates involving nonlocal tail terms, with assumptions p>1, s∈(0,1), and sp'>1.

Result: Solutions belong to W^{1,q}_{loc}(Ω) for optimal exponent q=q(n,p,s,r), with quantitative gradient estimates and confirmation of optimality via counterexample.

Conclusion: The paper establishes optimal local regularity for fractional p-Laplacian equations, providing complete characterization of Sobolev regularity with explicit dependence on parameters.

Abstract: We prove local $W^{1,q}$-regularity for weak solutions to fractional $p$-Laplacian type equations with right-hand side $f\in L^r_{\mathrm{loc}}(Ω)$. Assuming $p>1$, $s\in(0,1)$, and $sp'>1$, solutions belong to $W^{1,q}_{\mathrm{loc}}(Ω)$ for the optimal exponent $q=q(n,p,s,r)$. We obtain quantitative local gradient estimates involving nonlocal tail terms. The optimality of $q$ is confirmed by a counterexample.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [66] [DISCOVER: A Physics-Informed, GPU-Accelerated Symbolic Regression Framework](https://arxiv.org/abs/2602.06986)
*Udaykumar Gajera,Mohsen Sotoudeh,Kanchan Sarkar,Axel Groß*

Main category: physics.comp-ph

TL;DR: DISCOVER is a new open-source symbolic regression package designed for materials science that offers modular, physics-informed design with GPU acceleration for scalable discovery of interpretable mathematical descriptors.


<details>
  <summary>Details</summary>
Motivation: Existing symbolic regression tools have limitations: poor integration with modern Python workflows, limited control over symbolic search space, and computational inefficiency for large-scale studies, especially in materials science where interpretable physical descriptors are crucial.

Method: DISCOVER uses a modular, physics-motivated design that allows users to guide symbolic search using domain knowledge, explicitly constrain feature space, and leverage optional GPU acceleration for computational efficiency in data-intensive workflows.

Result: The paper introduces DISCOVER as an open-source package that enables reproducible and scalable symbolic regression workflows, addressing the limitations of existing tools while emphasizing discovery of physically meaningful models.

Conclusion: DISCOVER complements general-purpose symbolic regression frameworks by focusing on physically consistent, interpretable models for computational physics, chemistry, and materials science applications where execution time and physical meaning are critical.

Abstract: Symbolic Regression (SR) enables the discovery of interpretable mathematical relationships from experimental and simulation data. These relationships are often coined descriptors which are defined as a fundamental materials property that is directly correlated to a desired or undesired functional property of the material. Although established approaches such as Sure Independence Screening and Sparsifying Operator (SISSO) have successfully identified low-dimensional descriptors within large feature spaces many existing SR tools integrate poorly with modern Python workflows, offer limited control over the symbolic search space, or struggle with the computational demands of large-scale studies. This paper introduces DISCOVER (Data-Informed Symbolic Combination of Operators for Variable Equation Regression), an open-source symbolic regression package developed to address these challenges through a modular, physics-motivated design. DISCOVER allows users to guide the symbolic search using domain knowledge, constrain the feature space explicitly, and take advantage of optional GPU acceleration to improve computational efficiency in data-intensive workflows, enabling reproducible and scalable SR workflows. The software is intended for applications in computational physics, computational chemistry, and materials science, where interpretability, physical consistency, and execution time are especially important, and it complements general-purpose SR frameworks by emphasizing the discovery of physically meaningful models.

</details>


### [67] [diffpy.morph: Python tools for model independent comparisons between sets of 1D functions](https://arxiv.org/abs/2602.06987)
*Andrew Yang,Christopher L. Farrow,Pavol Juhás,Luis Kitsu Iglesias,Chia-Hao Liu,Samuel D. Marks,Vivian R. K. Wall,Joshua Safin,Sean M. Drewry,Caden Myers,Dillon F. Hanlon,Nicholas Leonard,Cedomir Petrovic,Ahhyun Jeong,Dmitri V. Talapin,Linda F. Nazar,Haidong Zhou,Samuel W. Teitelbaum,Tim B. van Driel,Soham Banerjee,Emil S. Bozin,Michael F. Toney,Katharine Page,Naomi S. Ginsberg,Simon J. L. Billinge*

Main category: physics.comp-ph

TL;DR: diffpy.morph is a Python package for analyzing 1D scientific spectra by applying transformations to remove uninteresting differences between spectra, revealing meaningful changes.


<details>
  <summary>Details</summary>
Motivation: Researchers need model-independent ways to gain scientific insights from 1D spectra by analyzing differences between pairs of spectra, but difference curves often contain uninteresting experimental inconsistencies and benign physical changes that obscure meaningful modifications.

Method: The package allows researchers to apply simple transformations ("morphs") to one dataset to remove unwanted differences, revealing non-trivial differences when present. It's an open-source Python package available on PyPI and conda-forge.

Result: The paper describes diffpy.morph's functionality and demonstrates its application to solve experimental challenges on diffraction and PDF data from x-rays and neutrons, though it can be applied to any 1D function in principle.

Conclusion: diffpy.morph provides a practical tool for model-independent analysis of 1D scientific spectra by enabling researchers to isolate meaningful differences between spectra through targeted transformations.

Abstract: diffpy.morph addresses a need to gain scientific insights from 1D scientific spectra in model independent ways. A powerful approach for this is to take differences between pairs of spectra and look for meaningful changes that might indicate underlying chemical, structural, or other modifications. The challenge is that the difference curve may contain uninteresting differences such as experimental inconsistencies and benign physical changes such as the effects of thermal expansion. diffpy.morph allows researchers to apply simple transformations, or "morphs", to one of the datasets to remove the unwanted differences revealing, when they are present, non-trivial differences. diffpy.morph is an open-source Python package available on the Python Package Index and conda-forge. Here, we describe its functionality and apply it to solve a range of experimental challenges on diffraction and PDF data from x-rays and neutrons, though we note that it may be applied to any 1D function in principle.

</details>


### [68] [Event-Chain Monte Carlo: The global-balance breakthrough](https://arxiv.org/abs/2602.07199)
*E. A. J. F. Peters*

Main category: physics.comp-ph

TL;DR: The paper reviews the 2009 Event-Chain Monte Carlo algorithm that revolutionized Monte Carlo sampling by using global balance instead of detailed balance, enabling rejection-free deterministic sampling for hard spheres and generalizing to continuous potentials.


<details>
  <summary>Details</summary>
Motivation: The motivation was to overcome limitations of traditional Monte Carlo methods that rely on detailed balance, which imposes restrictive conditions. The 2009 paper sought to develop more efficient sampling algorithms by using the more fundamental principle of global balance instead.

Method: The method introduced Event-Chain Monte Carlo (ECMC) which abandons detailed balance in favor of global balance. It achieves rejection-free, deterministic sampling for hard spheres using persistent, directional dynamics. The algorithm was later generalized through the Event-Driven Monte Carlo (EDMC) framework to handle continuous potentials and lifted Markov chains.

Result: The algorithm dramatically accelerated equilibration in dense particle systems by enabling rejection-free sampling. The original hard-sphere concept was successfully generalized to continuous potentials and modern lifted Markov chain formalisms, transforming a specific result into a powerful general class of sampling algorithms.

Conclusion: The 2009 ECMC paper represented a paradigm shift in Monte Carlo sampling by demonstrating that abandoning detailed balance in favor of global balance enables more efficient, rejection-free algorithms. This foundational work has evolved into a powerful general framework for sampling algorithms applicable to various systems beyond hard spheres.

Abstract: The seminal 2009 paper by Bernard, Krauth, and Wilson marked a paradigm shift in Monte Carlo sampling. By abandoning the restrictive condition of detailed balance in favor of the more fundamental principle of global balance, they introduced the Event-Chain Monte Carlo (ECMC) algorithm, which achieves rejection-free, deterministic sampling for hard spheres. This breakthrough demonstrated that persistent, directional dynamics could dramatically accelerate equilibration in dense particle systems. In this commentary, we review this foundational work and elucidate its underlying mechanism using the broader Event-Driven Monte Carlo (EDMC) framework developed in subsequent years. We show how the original hard-sphere concept naturally generalizes to continuous potentials and modern lifted Markov chain formalisms, transforming a surprising specific result into a powerful general class of sampling algorithms.

</details>


### [69] [Compressed Sensing Methods for Memory Reduction in Monte Carlo Simulations](https://arxiv.org/abs/2602.07771)
*Ethan Lame,Camille Palmer,Todd Palmer,Ilham Variansyah*

Main category: physics.comp-ph

TL;DR: Compressed sensing with overlapping cells reduces memory usage in Monte Carlo neutron simulations by 81-96% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo simulations for neutronic systems are computationally intensive and memory-hungry, requiring more efficient methods for high-fidelity modeling.

Method: Uses compressed sensing with overlapping cells to collect tallies, applying basis pursuit denoising with sparsity parameter optimization for signal reconstruction.

Result: Achieved memory reductions of 81.25% for 2D and 96.25% for 3D reconstructions, with reconstruction errors within 1 standard deviation of high-fidelity reference results.

Conclusion: Compressed sensing with overlapping cells enables significant memory savings in Monte Carlo neutron simulations while preserving reconstruction accuracy.

Abstract: Monte Carlo simulations of neutronic systems are computationally intensive and demand significant memory resources for high-fidelity modeling. Compressed sensing enables accurate reconstruction of signals from significantly fewer samples than traditional methods. The specific implementation of compressed sensing investigated here involves the use of overlapping cells to collect tallies. Increasing the number of samples improves the reconstruction accuracy, although the marginal gains diminish with more samples. Reconstruction quality is strongly influenced by the sparsity parameter used in basis pursuit denoising. Across the three test cases considered, memory reductions of up to 81.25% (96.25%) are demonstrated for 2D (3D) reconstructions, with select scenarios achieving reconstruction errors within 1 standard deviation of the corresponding high-fidelity reference results.

</details>


### [70] [dewi-kadita: A Python Library for Idealized Fish Schooling Simulation with Entropy-Based Diagnostics](https://arxiv.org/abs/2602.07948)
*Sandy H. S. Herho,Iwan P. Anwar,Faruq Khadami,Alfita P. Handayani,Karina A. Sujatmiko,Kamaluddin Kasim,Rusmawan Suwarman,Dasapta E. Irawan*

Main category: physics.comp-ph

TL;DR: dewi-kadita is an open-source Python library for 3D fish school simulation using Couzin zone-based model with novel entropy metrics for collective behavior analysis.


<details>
  <summary>Details</summary>
Motivation: Current computational tools for simulating and analyzing fish school collective motion are fragmented across research groups, lacking standardized, reproducible infrastructure similar to established molecular dynamics codes.

Method: Implements 3D Couzin zone-based model with Numba JIT compilation for acceleration. Introduces seven information-theoretic entropy metrics and combines them into Oceanic Schooling Index (OSI). Outputs in NetCDF4 format for interoperability.

Result: Library successfully reproduces known phase behaviors across four configurations. Numba JIT provides 10-100x speedup, enabling simulations of 150-250 agents over 1000-2000 time steps within 5 minutes. Entropy framework discriminates between configurations with similar order parameters.

Conclusion: dewi-kadita provides standardized, reproducible infrastructure for collective behavior modeling with novel entropy diagnostics that capture organizational features inaccessible to classical order parameters.

Abstract: Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Couzin zone-based model with comprehensive entropy diagnostics tailored for marine collective behavior research. The library introduces seven information-theoretic metrics -- school cohesion entropy, polarization entropy, depth stratification entropy, angular momentum entropy, nearest-neighbor entropy, velocity correlation entropy, and school shape entropy -- that characterize distinct organizational features inaccessible to classical order parameters. These metrics combine into an Oceanic Schooling Index (OSI) providing a single scalar measure of collective disorder. Validation across four canonical configurations (swarm, torus, dynamic parallel, highly parallel) confirms correct reproduction of known phase behaviors: the swarm maintains disorder with polarization $P < 0.1$ and OSI $\approx 0.71$, while the highly parallel state achieves $P = 0.998$ with OSI $= 0.24$ and velocity correlation entropy vanishing to zero. The entropy framework successfully discriminates the torus and dynamic parallel configurations that exhibit comparable order parameter magnitudes through different organizational mechanisms. Numba just-in-time (JIT) compilation accelerates pairwise interaction calculations by $10$--$100\times$, enabling simulations of $150$--$250$ agents over $1000$--$2000$ time steps within five minutes on standard workstation hardware. NetCDF4 output ensures interoperability with oceanographic analysis tools. The library addresses the need for standardized, reproducible infrastructure in collective behavior modeling analogous to established molecular dynamics codes.

</details>


### [71] [An intramembranous ossification model for the in-silico analysis of bone tissue formation in tooth extraction sites](https://arxiv.org/abs/2602.08492)
*Jennifer Paola Corredor-Gómez,Andrés Mauricio Rueda-Ramírez,Miguel Alejandro Gamboa-Márquez,Carolina Torres-Rodríguez,Carlos Julio Cortés-Rodríguez*

Main category: physics.comp-ph

TL;DR: A computational model for bone healing in tooth extraction sites using mathematical descriptions of cell interactions, angiogenesis, and oxygen-dependent effects, validated with animal experiments.


<details>
  <summary>Details</summary>
Motivation: To develop a computer model for bone healing in dental procedures that can predict tissue behavior, reduce costs, and avoid ethical issues of animal testing, specifically for intramembranous ossification in tooth extraction sites.

Method: Mathematical model describing cell interactions, extracellular matrix synthesis/degradation, angiogenesis, oxygen-dependent effects, and growth factor-induced apoptosis. Includes functional description of cell distribution in periodontal ligament. Implemented using finite element method (FEM).

Result: Model successfully validated by simulating animal experiments on dogs from literature, achieving good fit with experimental data (mean absolute error of 3.04%).

Conclusion: The mathematical framework provides an important tool for designing future experiments and serves as a precedent for computational studies on osseointegration and mechanobiology.

Abstract: The accurate modeling of biological processes allows to predict the spatio-temporal behavior of living tissues by computer-aided (in-silico) testing, a useful tool for the development of medical strategies, avoiding the expenses and potential ethical implications of in-vivo experimentation. A model for bone healing in mouth would be useful for selecting proper surgical techniques in dental procedures. In this paper, the formulation and implementation of a model for Intramembranous Ossification is presented aiming to describe the complex process of bone tissue formation in tooth extraction sites. The model consists in a mathematical description of the mechanisms in which different types of cells interact, synthesize and degrade extra-cellular matrices under the influence of biochemical factors. Special attention is given to angiogenesis, oxygen-dependent effects and growth factor-induced apoptosis of fibroblasts. Furthermore, considering the depth-dependent vascularization of mandibular bone and its influence on bone healing, a functional description of the cell distribution on the severed periodontal ligament (PDL) is proposed. The developed model was implemented using the finite element method (FEM) and successfully validated by simulating an animal in-vivo experiment on dogs reported in the literature. A good fit between model outcome and experimental data was obtained with a mean absolute error of 3.04%. The mathematical framework presented here may represent an important tool for the design of future in-vitro and in-vivo tests, as well as a precedent for future in-silico studies on osseointegration and mechanobiology.

</details>


### [72] [Tikhonov regularization-based reconstruction of partial scattering functions obtained from contrast variation small-angle neutron scattering](https://arxiv.org/abs/2602.08601)
*Manabu Machida,Koichi Mayumi*

Main category: physics.comp-ph

TL;DR: Proposes Tikhonov regularization to stabilize partial scattering function reconstruction in contrast variation small-angle neutron scattering analysis.


<details>
  <summary>Details</summary>
Motivation: CV-SANS uses SVD to decompose scattering intensities into partial scattering functions, but estimation of functions with small absolute values suffers from instability due to significant differences in singular values.

Method: Introduces Tikhonov regularization to ensure more stable reconstruction of partial scattering functions in CV-SANS analysis.

Result: The proposed method provides a remedy for instability in partial scattering function estimation, particularly for functions with small absolute values.

Conclusion: Tikhonov regularization offers a solution to the instability problem in CV-SANS analysis, enabling more reliable structural analysis of multicomponent systems.

Abstract: Contrast variation small-angle neutron scattering (CV-SANS) has been widely employed for nano structural analysis of multicomponent systems. In CV-SANS experiments, scattering intensities of samples with different scattering co\ ntrasts are decomposed into partial scattering functions, corresponding to structure of each component and cross-correlation between different components, by singular value decomposition (SVD). However, the estimation of partial scattering functions with small absolute values often suffers from instability due to the significant differences in the singular values. In this paper, we propose a remedy for this instability by introducing the Tikhonov regularization, which ensures more stable reconstruction of the partial scattering functions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [73] [Topological Arrest of Ballooning Modes in Non-Axisymmetric Plasmas](https://arxiv.org/abs/2602.07329)
*Amitava Bhattacharjee*

Main category: physics.plasm-ph

TL;DR: Ballooning instabilities in non-axisymmetric plasmas exhibit Anderson-like localization that transforms nonlinear stability into a connectivity phase transition on a Ginzburg-Landau network, with topological threshold ηc determining whether instabilities remain benign or become disruptive.


<details>
  <summary>Details</summary>
Motivation: To understand why certain ballooning instabilities in non-axisymmetric plasma equilibria saturate benignly in experiments while others become disruptive, and to explain this through fundamental topological principles rather than traditional stability analysis.

Method: The paper draws analogies between nonlinear ballooning instabilities and Anderson localization in disordered lattices, then models the system as a connectivity phase transition on a Ginzburg-Landau network. It introduces a dimensionless topological threshold ηc derived from continuum percolation theory to characterize the transition.

Result: Identifies ηc as a topological threshold that determines nonlinear stability: below ηc, global instability is topologically arrested as isolated scintillations (explaining benign saturation); above ηc, a spanning cluster path forms, enabling rapid nonlinear growth that can be disruptive.

Conclusion: The localization of ballooning instabilities fundamentally alters nonlinear stability through topological mechanisms, providing a rigorous explanation for experimental observations of benign saturation versus disruptive crashes based on a percolation theory framework.

Abstract: Nonlinear ballooning instabilities in non-axisymmetric equilibria exhibit spatial localization along field lines that mirrors Anderson localization in disordered lattices. We demonstrate that this localization fundamentally alters nonlinear stability, transforming a global ballooning crash into a connectivity phase transition on a Ginzburg-Landau network. We identify a dimensionless number, denoted by ηc, as a topological threshold derived from continuum percolation theory. Below this threshold, global instability is topologically arrested as isolated scintillations, providing a rigorous explanation for the robust, benign saturation observed in experiments. Above this threshold, a spanning cluster path forms, unlocking rapid and potentially disruptive nonlinear growth.

</details>


### [74] [2DESR: a two-dimensional Fourier-space gyrokinetic eigenvalue code for the ion-temperature-gradient modes in tokamaks](https://arxiv.org/abs/2602.07626)
*Haochuan Wang,Jie Wang,Yuefeng Qiu,Shaojie Wang,Zihao Wang,Tiannan Wu,Yuesong Li,Yicheng Cai,Shiqi Xiao*

Main category: physics.plasm-ph

TL;DR: Development of 2DESR, a 2D gyrokinetic eigenvalue solver for ITG modes in tokamaks, benchmarking well against established codes and revealing two coexisting ITG mode branches.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient eigenvalue solver for 2D gyrokinetic problems in tokamaks, specifically targeting ion-temperature-gradient (ITG) modes, which are important for plasma turbulence and transport studies.

Method: Developed 2DESR code that solves 2D gyrokinetic eigenvalue equations in poloidal Fourier space with full kinetic ion effects. Derived equations and implemented numerical solver, then benchmarked against established gyrokinetic initial-value codes GENE and NLT using linear ITG Cyclone test with adiabatic electrons.

Result: The 2DESR code successfully benchmarks against GENE and NLT codes. The analysis reveals that two distinct branches of ITG modes coexist in the system, which is a significant finding about the ITG mode structure.

Conclusion: 2DESR is a validated 2D gyrokinetic eigenvalue solver that provides new insights into ITG mode structure, revealing the coexistence of two mode branches, which could impact understanding of plasma turbulence and transport in tokamaks.

Abstract: A two-dimensional (2D) gyrokinetic eigenvalue solver, 2DESR, has been developed to solve the 2D gyrokinetic eigenvalue problem in the poloidal Fourier space for the ion-temperature-gradient (ITG) modes in tokamaks. With full kinetic effects of ions retained, the 2D gyrokinetic eigenvalue equations in the poloidal Fourier space have been derived and numerically solved in the 2DESR code. In the linear ITG Cyclone test with adiabatic electrons, the 2DESR code benchmarks well against the gyrokinetic initial-value codes GENE and NLT. It is found that two branches of ITG modes coexist in the system.

</details>


### [75] [Assessing the impact of Open Research Information Infrastructures using NLP driven full-text Scientometrics: A case study of the LXCat open-access platform](https://arxiv.org/abs/2602.07664)
*Kalp Pandya,Khushi Shah,Nirmal Shah,Nakshi Shah,Bhaskar Chaudhury*

Main category: physics.plasm-ph

TL;DR: This paper presents an NLP-driven scientometric framework to quantify the impact of open research information (ORI) infrastructures beyond citation counts, using the LXCat platform for plasma research as a case study.


<details>
  <summary>Details</summary>
Motivation: Current assessment of ORI infrastructures relies heavily on citation-based metrics, which don't capture the full impact of these platforms. There's a need for more comprehensive methods to quantify how ORI infrastructures shape research practices and knowledge production.

Method: Developed a full-text NLP pipeline analyzing research articles citing LXCat publications. The framework integrates chemical entity recognition, dataset/solver mention extraction, geographic mapping, and topic modeling to extract fine-grained patterns of data usage and research practices.

Result: The framework successfully extracted detailed patterns of data usage, revealing implicit research priorities, differential reliance on specific databases, evolving data reuse practices, and thematic evolution in low temperature plasma research over the past decade.

Conclusion: The proposed NLP-driven methodology is domain-agnostic and transferable to other ORI contexts, offering a scalable framework for evidence-based evaluation of research infrastructures that can inform design, governance, and policy decisions.

Abstract: Open research information (ORI) play a central role in shaping how scientific knowledge is produced, disseminated, validated, and reused across the research lifecycle. While the visibility of such ORI infrastructures is often assessed through citation-based metrics, in this study, we present a full-text, natural language processing (NLP) driven scientometric framework to systematically quantify the impact of ORI infrastructures beyond citation counts, using the LXCat platform for low temperature plasma (LTP) research as a representative case study. The modeling of LTPs and interpretation of LTP experiments rely heavily on accurate data, much of which is hosted on LXCat, a community-driven, open-access platform central to the LTP research ecosystem. To investigate the scholarly impact of the LXCat platform over the past decade, we analyzed a curated corpus of full-text research articles citing three foundational LXCat publications. We present a comprehensive pipeline that integrates chemical entity recognition, dataset and solver mention extraction, affiliation based geographic mapping and topic modeling to extract fine-grained patterns of data usage that reflect implicit research priorities, data practices, differential reliance on specific databases, evolving modes of data reuse and coupling within scientific workflows, and thematic evolution. Importantly, our proposed methodology is domain-agnostic and transferable to other ORI contexts, and highlights the utility of NLP in quantifying the role of scientific data infrastructures and offers a data-driven reflection on how open-access platforms like LXCat contribute to shaping research directions. This work presents a scalable scientometric framework that has the potential to support evidence based evaluation of ORI platforms and to inform infrastructure design, governance, sustainability, and policy for future development.

</details>


### [76] [Azimuthally polarized terahertz radiation generation using radially polarized laser pulse in magnetized plasma](https://arxiv.org/abs/2602.08438)
*Shivani Aggarwal,Dinkar Mishra,Saumya Singh,Bhupesh Kumar,Pallavi Jha*

Main category: physics.plasm-ph

TL;DR: Analytical formulation of radially polarized laser pulse in magnetized plasma generates THz radiation fields, validated by PIC simulations.


<details>
  <summary>Details</summary>
Motivation: To develop an analytical understanding of THz radiation generation from radially polarized laser pulses propagating in magnetized plasmas, and to study how plasma parameters control the radiation amplitude.

Method: Used Lorentz force, continuity, and Maxwell's equations with perturbation technique and quasi-static approximation (QSA) to derive analytical formulation. Validated with Particle-in-Cell (PIC) simulations using FBPIC code.

Result: Generated slow, oscillating transverse electric and magnetic fields with equal amplitude in THz frequency range. Radiation propagates beyond plasma boundary, indicating coherent electromagnetic emission. Field amplitude scales nonlinearly with plasma density and linearly with external magnetic field strength.

Conclusion: Radially polarized laser pulses in magnetized plasmas can generate coherent THz radiation, with amplitude controllable through plasma density and external magnetic field parameters.

Abstract: An analytical formulation of a radially polarized laser pulse propagating in a homogeneous, magnetized plasma is presented using Lorentz force, continuity and Maxwells equations. Perturbation technique and quasi-static approximation (QSA) have been used to study the generated fields in nonlinear regime. The generated slow, oscillating, transverse electric and magnetic fields having equal amplitude, constitute a radiation field having frequency in the terahertz (THz) range. Particle-in-cell (PIC) simulation code FBPIC is used to validate analytical findings. Simulation studies also show that the generated THz radiation field propagates beyond the plasma boundary, indicating coherent electromagnetic radiation emission. Furthermore, the field amplitude scales nonlinearly with plasma density and increases linearly with external magnetic field strength, highlighting the role of these parameters in controlling radiation amplitude.

</details>


### [77] [JOREK simulations of the X-point radiator formation and its movement in ASDEX Upgrade](https://arxiv.org/abs/2602.08614)
*Y. C. Liang,A. Cathey,M. Hoelzl,S. Q. Korving,M. Szucs,O. Pan,D. Maris,F. Antlitz,the JOREK Team,the ASDEX Upgrade Team*

Main category: physics.plasm-ph

TL;DR: Simulations show JOREK MHD code can model X-point radiator (XPR) formation and movement in fusion reactors by adjusting impurity seeding rates, providing baseline for 3D studies of MHD-XPR interactions.


<details>
  <summary>Details</summary>
Motivation: Future fusion reactors need to avoid extreme heat fluxes on plasma-facing components. The X-point radiator (XPR) regime offers a solution by creating a radiative, cold plasma volume above the X-point through impurity seeding, but requires better understanding of its dynamics and control.

Method: Used axisymmetric (2D) simulations with the nonlinear MHD code JOREK extended with kinetic particle framework for main species neutrals and nitrogen impurities. Simulated progression from attached divertors to complete detachment with XPR formation, then studied stationary XPR behavior and response to seeding rate changes.

Result: Successfully simulated XPR formation at 6.8 cm height and showed that increasing seeding rate moves XPR upward while decreasing rate moves it downward. Observed formation and loss of high-field-side high-density during detachment progression. Demonstrated JOREK's capability to simulate time-varying XPR behavior.

Conclusion: JOREK simulations provide a validated baseline for studying XPR dynamics and control, enabling future transition to 3D simulations to investigate MHD activities and their interactions with the XPR regime in fusion reactors.

Abstract: Future large-scale magnetic confinement fusion reactors require operational regimes that can avoid extreme heat fluxes onto the plasma-facing components. One promising regime is the X-point radiator (XPR), which relies on a highly radiative, cold and dense plasma volume forming above the X-point, and which can be accessed via impurity seeding. Experimentally, the height of the XPR can be controlled by adjusting the seeding rate and heating power. This contribution presents axisymmetric (2D) simulations of the XPR regime in ASDEX Upgrade using the nonlinear MHD code JOREK extended with a kinetic particle framework for the main species neutrals and nitrogen impurities. With the time-dependent simulations, the progression from attached divertors to a complete detachment with the XPR formation is shown, highlighting the effects of the neutrals and impurities separately. Amidst this progression, the formation and the loss of the high-field-side high-density are observed. After the XPR is well-formed at the height of 6.8 cm, the fuelling and seeding rates are adjusted so that the XPR remains stationary. From the stationary case, the seeding rate is then changed to see how the XPR location reacts. By increasing and decreasing the seeding rate, the XPR responds by moving upwards and downwards, respectively. These simulations show JOREK's capability of simulating time-varying XPR, which will provide a baseline for the transition to 3D simulations, so the MHD activities and their interaction with the XPR can be studied.

</details>


### [78] [Preprint: Sheath thickness measurements with the biased plasma impedance probe, Agreement with Child Langmuir scaling](https://arxiv.org/abs/2602.08743)
*John Whitlock Brooks,Richeek Dutta*

Main category: physics.plasm-ph

TL;DR: PIP with DC bias enables direct sheath thickness measurement, validating Child-Langmuir scaling with α≈0.74 correction factor, and extends to floating-PIP for electron temperature and plasma potential estimation.


<details>
  <summary>Details</summary>
Motivation: Plasma sheath thickness is crucial for plasma-surface interactions but difficult to measure experimentally. Existing diagnostics are either indirect (Langmuir probes) or invasive/demanding. There's a need for more direct, straightforward measurement methods.

Method: Uses plasma impedance probe (PIP) with controlled DC bias for direct sheath thickness measurements. Validates against Child-Langmuir sheath model across various discharge conditions. Extends to floating (unbiased) PIP analysis using empirical α factor to estimate electron temperature and plasma potential without biasing.

Result: Biased-PIP measurements closely follow Child-Langmuir scaling with consistent empirical correction factor α≈0.74. Probe biasing doesn't significantly perturb bulk plasma density. Floating-PIP results for electron temperature and plasma potential show close agreement with biased Langmuir probe measurements.

Conclusion: PIP serves as complementary diagnostic to Langmuir probe, enabling direct sheath thickness measurements and expanding accessible plasma measurements while providing experimental validation for classical sheath models.

Abstract: Plasma sheaths play a central role in plasma-surface interactions, yet their thickness remains challenging to measure experimentally. Although classical analytical models such as the Child-Langmuir (CL) sheath model provide clear predictions for sheath thickness, experimental validation has been limited because most diagnostics either rely on indirect, multi-step inference (e.g., Langmuir probes) or require invasive and technically demanding techniques. In this work, we demonstrate that the plasma impedance probe (PIP), when operated with a controlled DC bias, enables relatively direct, model-informed measurements of sheath thickness that are reasonably straightforward to implement experimentally. Across a range of discharge conditions, biased-PIP sheath thickness measurements are found to follow CL scaling closely, requiring a single, consistent empirical correction factor of $α\approx 0.74$ to reconcile the measured thickness with CL predictions. Concurrent measurements of plasma density and electron damping show that probe biasing does not significantly perturb the bulk plasma density, supporting the validity of the biased-PIP approach. Building on this validation, we leverage the empirically determined $α$ factor to extend the floating (unbiased) PIP analysis to obtain model-dependent estimates of electron temperature and plasma potential without electrical biasing. A side-by-side comparison demonstrates close agreement between floating-PIP results and those obtained from a biased Langmuir probe. Taken together, these results establish the PIP as a complementary diagnostic to the Langmuir probe, expanding the range of accessible plasma measurements while providing experimental support for classical sheath models.

</details>


### [79] [Weak and reversed magnetic shear effects on internal kink and fishbone modes](https://arxiv.org/abs/2602.08884)
*Weikang Cai,Ping Zhu,Zhi Zhang,Shiwei Xue,Sui Wan*

Main category: physics.plasm-ph

TL;DR: Reversed magnetic shear in tokamaks stabilizes internal kink modes but can trigger double kink/fishbone modes with energetic particles. Narrow shear regions and ITB profiles significantly affect mode behavior and EP stabilization.


<details>
  <summary>Details</summary>
Motivation: Advanced tokamak scenarios with reversed magnetic shear configurations need investigation of their effects on internal kink and fishbone instabilities, particularly with energetic particles, to understand stability in fusion plasmas.

Method: Used hybrid kinetic-MHD model in NIMROD code to simulate reversed magnetic shear effects on internal kink and fishbone modes in circular limiter tokamak, analyzing both with and without energetic particles.

Result: Reversed magnetic shear stabilizes internal kink modes; with energetic particles, narrow shear regions cause transition to double kink/fishbone modes. EP beta fraction increases with q_min, and ITB width/temperature gradients affect mode suppression differently.

Conclusion: Reversed magnetic shear provides stabilizing effects on internal kink modes but can lead to mode transitions with energetic particles. ITB characteristics significantly influence mode behavior, offering insights for advanced tokamak stability optimization.

Abstract: Advanced tokamak scenarios often feature weak or reversed magnetic shear configurations. In this study, the hybrid kinetic-MHD model implemented in the NIMROD code is used to investigate the effects of reversed magnetic shear on internal kink and fishbone mode in a circular shaped limiter tokamak. In the absence of energetic particles (EPs), the mode growth rate initially increases and then decreases as the magnetic shear changes from positive to negative, indicating stabilizing effects of the reversed magnetic shear on the internal kink mode. In the presence of EPs, when the reversed magnetic shear region is sufficiently narrow, the transition from internal kink/fishbone modes to double kink/fishbone modes takes place, and the stabilizing effects of the reversed magnetic shear can significantly dominate the destabilization of EPs. For non-resonant modes, the EP beta fraction $β_f$ for excitation increases with $q_{min}$, concurrent with progressively lower growth rates in non-resonant fishbone modes. When the equilibrium profile has an internal transport barrier (ITB), broader ITB widths suppress internal kink modes more effectively, whereas steeper temperature gradients strengthen EP stabilization.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [80] [FPIC: a new Particle-In-Cell code for stationary and axisymmetric black-hole spacetimes](https://arxiv.org/abs/2602.07452)
*Claudio Meringolo,Luciano Rezzolla*

Main category: astro-ph.HE

TL;DR: FPIC is a new GRPIC code framework for simulating plasma physics around black holes using spherical Kerr-Schild coordinates, featuring a hybrid particle pusher that dynamically switches schemes based on Hamiltonian energy conservation.


<details>
  <summary>Details</summary>
Motivation: To develop a robust particle-in-cell code framework specifically designed for general relativistic plasma simulations around black holes, addressing the challenges of coordinate singularities at event horizons and improving energy conservation in particle integration schemes.

Method: Fortran-based parallel code using MPI for both fields and particles, employing spherical Kerr-Schild coordinates to avoid horizon singularities. Uses finite-difference time-domain solver with leapfrog scheme for Maxwell equations, and implements multiple particle pushers including a novel hybrid method that dynamically switches between schemes based on Hamiltonian energy violation.

Result: Successfully tested on neutral particles orbiting black holes (Schwarzschild and Kerr metrics), showing improved energy conservation with reduced computational cost. Applied to Wald solution in electrovacuum and plasma-filled configurations, detecting negative-energy particles indicating active Penrose process. Reproduced Blandford-Znajek luminosity in plasma-filled split-monopole solution with excellent agreement to analytical predictions.

Conclusion: FPIC provides an effective framework for general relativistic plasma simulations around black holes, with the hybrid particle pusher offering better energy conservation at lower computational cost, enabling reliable investigation of astrophysical phenomena like the Penrose process and Blandford-Znajek mechanism.

Abstract: In this paper we present a newly developed GRPIC code framework called FPIC, providing a detailed description of the Maxwell-equations solver, of the particle ``pushers'', and of the other algorithms that are needed in this approach. We describe in detail the code, which is written in Fortran and exploits parallel architectures using MPI directives both for the fields and particles. FPIC adopts spherical Kerr-Schild coordinates, which encode the overall spherical topology of the problem while remaining regular at the event horizon. The Maxwell equations are evolved using a finite-difference time-domain solver with a leapfrog scheme, while multiple particle ``pushers'' are implemented for the evolution of the particles. In addition to well-known algorithms, we introduce a novel hybrid method that dynamically switches between the most appropriate scheme based on the violation of the Hamiltonian energy. We first present results for neutral particles orbiting around black holes, both in the Schwarzschild and Kerr metrics, monitoring the evolution of the Hamiltonian error across different integration schemes. We apply our hybrid approach, showing that it is capable of achieving improved energy conservation at reduced computational cost. We apply FPIC to investigate the Wald solution, first in electrovacuum and subsequently in plasma-filled configurations. In the latter case, particles with negative energy at infinity are present inside the ergosphere, indicating that the Penrose process is active. Finally, we present the split-monopole solution in a plasma-filled environment and successfully reproduce the Blandford-Znajek luminosity, finding very good agreement with analytical predictions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [81] [Energy-Controllable Time Integration for Elastodynamic Contact](https://arxiv.org/abs/2602.08094)
*Kevin You,Juntian Zheng,Minchen Li*

Main category: cs.GR

TL;DR: A-search: A novel energy-controllable time integrator for elastic body simulation that modifies implicit Euler to follow user-specified energy targets, balancing stability with flexible energy control.


<details>
  <summary>Details</summary>
Motivation: Existing numerical integrators for elastic body simulation have trade-offs: implicit Euler/BDF2 are stable but dissipate energy uncontrollably, while symplectic methods conserve energy but lack unconditional stability on stiff problems. There's a need for methods that combine stability with controllable energy behavior.

Method: Proposes a general class of Hamiltonian integrators that are symplectic on linear problems but have superior stability on nonlinear problems. From this, derives A-search - a simple modification of implicit Euler that can follow user-specified energy targets, enabling flexible control over energy dissipation or conservation while maintaining stability.

Result: A-search maintains stability and physical fidelity while allowing energy control. It integrates seamlessly with barrier-type energies, provides inversion-free and penetration-free guarantees, and is well-suited for large deformations and complex collisions. Extensive evaluations show it biases energy toward low-frequency motion rather than dissipation, and outperforms BDF2 at similar running times by maintaining energy for more visually desirable simulations.

Conclusion: A-search addresses the fundamental trade-off between stability and energy conservation in elastic body simulation, providing a practical, controllable integrator that combines the stability of implicit methods with flexible energy management for improved visual quality in simulations.

Abstract: Dynamic simulation of elastic bodies is a longstanding task in engineering and computer graphics. In graphics, numerical integrators like implicit Euler and BDF2 are preferred due to their stability at large time steps, but they tend to dissipate energy uncontrollably. In contrast, symplectic methods like implicit midpoint can conserve energy but are not unconditionally stable and fail on moderately stiff problems. To address these limitations, we propose a general class of numerical integrators for Hamiltonian problems which are symplectic on linear problems, yet have superior stability on nonlinear problems. With this, we derive a novel energy-controllable time integrator, A-search, a simple modification of implicit Euler that can follow user-specified energy targets, enabling flexible control over energy dissipation or conservation while maintaining stability and physical fidelity. Our method integrates seamlessly with barrier-type energies and allows for inversion-free and penetration-free guarantees, making it well-suited for handling large deformations and complex collisions. Extensive evaluations over a wide range of material parameters and scenes demonstrate that A-search has biases to keep energy in low frequency motion rather than dissipation, and A-search outperforms traditional methods such as BDF2 at similar total running times by maintaining energy and leading to more visually desirable simulations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis](https://arxiv.org/abs/2602.07031)
*Dong Li,Shuai Huang,Yapeng Cao,Yujun Cui,Xiaobin Wei,Hongtao Cao*

Main category: cs.LG

TL;DR: LBC-PINN framework developed for simulating 1D unsaturated soil consolidation under long-term loading, using logarithmic time segmentation and lagged compatibility to handle multi-scale time domains.


<details>
  <summary>Details</summary>
Motivation: Address challenges of coupled air and water pressure dissipation across multi-scale time domains in unsaturated soil consolidation under long-term loading.

Method: Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) with logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.

Result: Accurate pore pressure predictions validated against FEM with mean absolute errors below 1e-2 for up to 1e10 seconds; simplified segmentation improves efficiency; robust across wide permeability ratios (1e-3 to 1e3).

Conclusion: LBC-PINN framework effectively simulates unsaturated soil consolidation across multi-scale time domains with high accuracy and computational efficiency.

Abstract: This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.
  In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.

</details>


### [83] [Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling](https://arxiv.org/abs/2602.07192)
*Xiaolong He,Haoyan Wei,Wei Hu,Henan Mao,C. T. Wu*

Main category: cs.LG

TL;DR: Comprehensive evaluation of Deep Material Networks (DMNs) shows systematic performance analysis across offline training and online prediction, with rotation-free IMN achieving 3.4-4.7x faster training while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Despite growing adoption of DMNs for multiscale material modeling, there's limited systematic evaluation of their performance across the full offline-online pipeline, including effects of training choices on generalization.

Method: Comparative assessment of DMNs focusing on prediction accuracy, computational efficiency, and training robustness. Investigated effects of initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty.

Result: Prediction error and variance decrease with increasing training data size. Initialization and batch size significantly influence performance. Activation regularization controls network complexity and generalization. Rotation-free IMN achieves 3.4-4.7x speed-up in offline training while maintaining comparable online accuracy.

Conclusion: The study clarifies trade-offs between model expressivity and efficiency in structure-preserving material networks, providing practical guidance for deployment in multiscale material modeling, with IMN offering significant training speed improvements.

Abstract: Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.

</details>


### [84] [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*Woojin Cho,Junghwan Park*

Main category: cs.LG

TL;DR: ELM-INR: A backpropagation-free implicit neural representation method using Extreme Learning Machines with local closed-form solutions and adaptive mesh refinement for handling non-uniform frequency content.


<details>
  <summary>Details</summary>
Motivation: Traditional INR training with iterative backpropagation suffers from spectral bias when dealing with highly non-uniform frequency content, making it difficult to capture fine-scale details efficiently.

Method: Decomposes domain into overlapping subdomains, fits each local problem using Extreme Learning Machines (ELMs) with closed-form linear least-squares solutions (no backpropagation), combines local predictors via partition of unity, and introduces BEAM adaptive mesh refinement to balance spectral complexity across subdomains.

Result: Achieves fast and numerically robust reconstruction by replacing iterative optimization with stable linear solutions, with spectral Barron norm analysis showing global error dominated by high spectral complexity regions, addressed by adaptive refinement.

Conclusion: ELM-INR provides an efficient backpropagation-free alternative to traditional INRs, with theoretical analysis guiding adaptive refinement strategies for handling non-uniform frequency distributions in capacity-constrained settings.

Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.

</details>


### [85] [Approximating Matrix Functions with Deep Neural Networks and Transformers](https://arxiv.org/abs/2602.07800)
*Rahul Padmanabhan,Simone Brugiapaglia*

Main category: cs.LG

TL;DR: Transformers can approximate matrix functions like matrix exponential with theoretical bounds for ReLU networks and experimental success for transformers using proper numerical encodings.


<details>
  <summary>Details</summary>
Motivation: Transformers excel in NLP but their use for numerical computation, particularly matrix functions (like matrix exponential and matrix sign function) which are crucial in scientific computing, has been underexplored.

Method: Two approaches: 1) Theoretical analysis of ReLU network width/depth needed to approximate matrix exponential; 2) Experimental transformer encoder-decoder with various numerical encoding schemes to approximate matrix functions.

Result: Proved theoretical bounds for ReLU network approximation of matrix exponential; Experimentally achieved ~5% relative error for certain matrix functions using transformers, with encoding scheme being a critical performance factor.

Conclusion: Transformers can effectively approximate matrix functions with proper numerical encodings, though performance depends heavily on the encoding scheme, opening new possibilities for neural networks in scientific computing.

Abstract: Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.

</details>


### [86] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

TL;DR: V-ABFT is a variance-based adaptive threshold algorithm for detecting silent data corruptions in matrix multiplication that achieves 6-48× tighter error bounds than previous methods while maintaining zero false positives across multiple precisions.


<details>
  <summary>Details</summary>
Motivation: Existing threshold determination methods for Algorithm-Based Fault Tolerance (ABFT) in matrix multiplication have critical limitations: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT produce thresholds 160-4200× larger than actual rounding errors, making them ineffective for precise error detection.

Method: V-ABFT uses variance-based adaptive thresholding that directly models the verification difference through statistical variance estimation. It achieves tighter error bounds with only O(n) complexity using max/min/mean statistics, compared to A-ABFT's O(pn) complexity for finding p largest values.

Result: V-ABFT reduces threshold-to-actual-error ratio to 7-20× for FP32/FP64 and 48-158× for BF16, representing 6-48× improvement over A-ABFT while maintaining zero false positive rate. For fused-kernel implementations, it enables ~1000× finer detection granularity compared to offline verification.

Conclusion: V-ABFT provides a superior threshold determination method for ABFT that offers significantly tighter error bounds, better detection granularity, and lower computational complexity than existing approaches, making it effective for fault-tolerant matrix multiplication across diverse hardware platforms and precision formats.

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [87] [Radial Müntz-Szász Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities](https://arxiv.org/abs/2602.08419)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: RMN (Radial Müntz-Szász Networks) overcome limitations of coordinate-separable neural architectures in modeling radial singular fields like 1/r and log r by using learnable radial powers and log-primitives, achieving superior accuracy with far fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Radial singular fields (1/r, log r, crack-tip profiles) are difficult for coordinate-separable neural architectures to model. The paper proves that any C² function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models.

Method: Introduces Radial Müntz-Szász Networks (RMN) that represent fields as linear combinations of learnable radial powers r^μ (including negative exponents) with a limit-stable log-primitive for exact log r behavior. The method admits closed-form spatial gradients and Laplacians for physics-informed learning. Extensions include RMN-Angular for angular dependence and RMN-MC for multiple sources with learnable centers.

Result: RMN achieves 1.5×-51× lower RMSE than MLPs and 10×-100× lower RMSE than SIREN while using only 27 parameters vs. 33,537 for MLPs and 8,577 for SIREN. Source-center recovery errors fall below 10⁻⁴ when optimization converges. The paper also reports controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

Conclusion: RMN provides an effective neural architecture for modeling radial singular fields with superior accuracy and parameter efficiency compared to standard neural networks, while maintaining mathematical rigor through closed-form gradients and Laplacians for physics-informed applications.

Abstract: Radial singular fields, such as $1/r$, $\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models. Motivated by this result, we introduce Radial Müntz-Szász Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^μ$, including negative exponents, together with a limit-stable log-primitive for exact $\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains. Across ten 2D and 3D benchmarks, RMN achieves 1.5$\times$--51$\times$ lower RMSE than MLPs and 10$\times$--100$\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$. We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

</details>


### [88] [Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics](https://arxiv.org/abs/2602.08478)
*Albert Alcalde,Markus Widhalm,Emre Yılmaz*

Main category: cs.LG

TL;DR: TD-TF is a simplified transformer architecture that bridges linear operator methods and deep sequence models, showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD).


<details>
  <summary>Details</summary>
Motivation: To create a data-driven model for unsteady spatio-temporal dynamics that combines the interpretability and efficiency of linear operator-based methods with the expressive power of deep sequence models, addressing limitations in modeling nonlinear and chaotic systems.

Method: Proposes a deliberately minimal transformer architecture with one self-attention layer (single query per prediction) and one feedforward layer, resulting in linear computational complexity and small parameter count. The architecture is interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition.

Result: TD-TF matches performance of strong linear baselines on near-linear systems while significantly outperforming them in nonlinear and chaotic regimes, accurately capturing long-term dynamics. Validated on synthetic signals, unsteady aerodynamics, Lorenz '63 system, and reaction-diffusion model.

Conclusion: TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics, successfully bridging linear operator methods and deep sequence models for spatio-temporal modeling.

Abstract: We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [89] [Electron-Informed Coarse-Graining Molecular Representation Learning for Real-World Molecular Physics](https://arxiv.org/abs/2602.07087)
*Gyoung S. Na,Chanyoung Park*

Main category: physics.chem-ph

TL;DR: HEDMoL: A method that learns electron-informed molecular representations by transferring electron-level information from small molecules to large ones without additional computational costs, achieving state-of-the-art accuracy on molecular physics prediction benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing molecular representation learning methods are limited to atom-level information, which is insufficient for describing real-world molecular physics. Electron-level information provides fundamental chemical knowledge but is computationally impractical to obtain for large molecules.

Method: The method transfers readily accessible electron-level information from small molecules to large molecules without additional computation costs, creating electron-informed molecular representations.

Result: Achieved state-of-the-art prediction accuracy on extensive benchmark datasets containing experimentally observed molecular physics.

Conclusion: HEDMoL successfully bridges the gap between computationally expensive electron-level calculations and practical molecular representation learning by transferring knowledge from small to large molecules, enabling more accurate molecular physics predictions.

Abstract: Various representation learning methods for molecular structures have been devised to accelerate data-driven chemistry. However, the representation capabilities of existing methods are essentially limited to atom-level information, which is not sufficient to describe real-world molecular physics. Although electron-level information can provide fundamental knowledge about chemical compounds beyond the atom-level information, obtaining the electron-level information in real-world molecules is computationally impractical and sometimes infeasible. We propose a method for learning electron-informed molecular representations without additional computation costs by transferring readily accessible electron-level information about small molecules to large molecules of our interest. The proposed method achieved state-of-the-art prediction accuracy on extensive benchmark datasets containing experimentally observed molecular physics. The source code for HEDMoL is available at https://github.com/ngs00/HEDMoL.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [90] [Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity](https://arxiv.org/abs/2602.07970)
*Zheyuan Hu,Weitao Chen,Cengiz Öztireli,Chenliang Zhou,Fangcheng Zhong*

Main category: cs.CE

TL;DR: Survey of neural PDE solvers with extension of CNF framework to multi-variable nonlinear settings and applications to scientific simulation problems.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE numerical methods suffer from curse of dimensionality, high computation costs, and domain-specific discretization limitations. Need for more efficient, flexible PDE solvers for scientific simulations.

Method: 1) Comprehensive survey of neural PDE solvers; 2) Extension of CNF (NeurIPS 2023) framework to multi-dependent-variable and nonlinear settings; 3) Implementation of selected methods with self-tuning techniques; 4) Application to forward solutions, inverse problems, and equation discovery.

Result: Implementation of methods, self-tuning techniques developed, evaluation on benchmark problems conducted, comprehensive survey of neural PDE solvers and scientific simulation applications produced.

Conclusion: Neural PDE solvers offer promising alternatives to traditional methods by addressing dimensionality and computational challenges, with extended CNF framework enabling broader application to complex scientific simulation problems.

Abstract: Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [91] [Optimal Quantum Speedups for Repeatedly Nested Expectation Estimation](https://arxiv.org/abs/2602.08120)
*Yihang Sun,Guanyang Wang,Jose Blanchet*

Main category: quant-ph

TL;DR: Quantum algorithm achieves $\tilde O(\varepsilon^{-1})$ cost for estimating repeatedly nested expectations, providing almost quadratic speedup over classical methods.


<details>
  <summary>Details</summary>
Motivation: Extend quantum speedups from single nested expectations to repeated nesting to cover broader applications including optimal stopping problems.

Method: Develop quantum algorithm using derandomized variant of classical randomized Multilevel Monte Carlo (rMLMC) to overcome variable-time issues in quantized randomized algorithms.

Result: Achieves $\varepsilon$-error with cost $\tilde O(\varepsilon^{-1})$, which is essentially optimal and provides almost quadratic speedup over best classical algorithm.

Conclusion: Successfully extends quantum advantages to repeatedly nested expectations, enabling faster solutions for a wider range of applications including optimal stopping.

Abstract: We study the estimation of repeatedly nested expectations (RNEs) with a constant horizon (number of nestings) using quantum computing. We propose a quantum algorithm that achieves $\varepsilon$-error with cost $\tilde O(\varepsilon^{-1})$, up to logarithmic factors. Standard lower bounds show this scaling is essentially optimal, yielding an almost quadratic speedup over the best classical algorithm. Our results extend prior quantum speedups for single nested expectations to repeated nesting, and therefore cover a broader range of applications, including optimal stopping. This extension requires a new derandomized variant of the classical randomized Multilevel Monte Carlo (rMLMC) algorithm. Careful de-randomization is key to overcoming a variable-time issue that typically increases quantized versions of classical randomized algorithms.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [92] [Constructive conditional normalizing flows](https://arxiv.org/abs/2602.08606)
*Borjan Geshkovski,Domènec Ruiz-Balet*

Main category: math.OC

TL;DR: The paper presents two neural network constructions for approximating diffeomorphisms and their pushforward measures using continuity equations with piecewise constant velocity fields.


<details>
  <summary>Details</summary>
Motivation: Motivated by applications in conditional sampling, the paper aims to simultaneously approximate diffeomorphisms φ and their pushforward measures φ#μ using neural network flows.

Method: Two approaches: 1) Explicit construction based on polar-like decomposition of Lagrange interpolant involving compressible (gradient of convex function) and incompressible (shear flows via permutations) components; 2) Probabilistic construction inspired by Maurey empirical method for more regular maps like Knöthe-Rosenblatt rearrangement.

Result: Provides constructive methods where the number of discontinuities in neural network weights doesn't scale inversely with ambient dimension for regular maps.

Conclusion: The paper offers practical neural network implementations for approximating diffeomorphisms and pushforward measures in conditional sampling applications, with improved scaling properties for regular maps.

Abstract: Motivated by applications in conditional sampling, given a probability measure $μ$ and a diffeomorphism $φ$, we consider the problem of simultaneously approximating $φ$ and the pushforward $φ_{\#}μ$ by means of the flow of a continuity equation whose velocity field is a perceptron neural network with piecewise constant weights. We provide an explicit construction based on a polar-like decomposition of the Lagrange interpolant of $φ$. The latter involves a compressible component, given by the gradient of a particular convex function, which can be realized exactly, and an incompressible component, which -- after approximating via permutations -- can be implemented through shear flows intrinsic to the continuity equation. For more regular maps $φ$ -- such as the Knöthe-Rosenblatt rearrangement -- we provide an alternative, probabilistic construction inspired by the Maurey empirical method, in which the number of discontinuities in the weights doesn't scale inversely with the ambient dimension.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [93] [Phenomenological energy exchange of diatomic gases: Comparison of Pullin and Borgnakke-Larsen models in direct simulation Monte Carlo method](https://arxiv.org/abs/2602.07409)
*Hao Jin,Sha Liu,Ningchao Ding,Sirui Yang,Huahua Cui,Congshan Zhuo,Chengwen Zhong*

Main category: physics.flu-dyn

TL;DR: Comparative DSMC study shows Pullin model matches BL model performance for rotational energy exchange in hypersonic rarefied flows, with better theoretical foundation.


<details>
  <summary>Details</summary>
Motivation: The widely-used Borgnakke-Larsen (BL) model for translational-rotational energy exchange in DSMC simulations lacks rigorous theoretical foundation and assumes only a fraction of collisions cause relaxation. Need for more physically consistent model.

Method: Comparative DSMC investigation of BL vs Pullin models using Beta function for energy partitioning. Tests include: 0D rotational relaxation of nitrogen, 1D Couette flow and normal shock, 2D hypersonic flow past cylinder, 3D hypersonic flow around X38-like vehicle.

Result: Pullin model shows consistency with BL model performance. In highly rarefied flows (Kn > 1, altitudes > 100 km), simplified Pullin model performs comparably to BL model.

Conclusion: Pullin model provides rigorous theoretical foundation and accurate physical representation, offering substantial support for future theoretical studies and numerical simulations in hypersonic rarefied flows.

Abstract: In hypersonic rarefied flows, insufficient intermolecular collisions cause significant deviations between translational and rotational temperatures, leading to strong thermal nonequilibrium. For diatomic gases such as nitrogen and oxygen, the direct simulation Monte Carlo (DSMC) method commonly employs the Borgnakke-Larsen (BL) model to simulate translational-rotational energy exchange (relaxation) processes. Although widely used, the BL model lacks a rigorous theoretical foundation and assumes that only a fraction of collisions lead to rotational relaxation. To address these shortcomings, Pullin introduced a kinetically consistent relaxation model into the gas kinetic theory. By employing the Beta function for energy partitioning, a concrete collision cross section that satisfies the detailed balance condition is constructed. In this study, a comparative investigation of the BL and Pullin models is performed within the DSMC framework, where both original and simplified equations are considered and parameterized by physical accommodated coefficient in the Beta function. A series of test cases--including zero-dimensional rotational relaxation of nitrogen, one-dimensional planar Couette flow and normal shock wave, two-dimensional hypersonic flow past a cylinder, and three-dimensional hypersonic flow around an X38-like vehicle--are performed to assess the accuracy and efficiency of these models. The results confirm the consistency between the Pullin and BL models. Owing to its rigorous theoretical foundation and accurate physical representation, the Pullin model is expected to provide substantial support for the extension of subsequent theoretical studies and numerical simulations. Moreover, in the highly rarefied flow regime (Knudsen number greater than 1, or altitudes above 100 km), the simplified Pullin model exhibits performance comparable to that of the BL model.

</details>


### [94] [Fully coupled implicit finite-volume algorithm for viscoelastic interfacial flows](https://arxiv.org/abs/2602.08645)
*Ayman Mazloum,Gabriele Gennari,Fabian Denner,Berend van Wachem*

Main category: physics.flu-dyn

TL;DR: A fully coupled implicit finite-volume algorithm for incompressible viscoelastic interfacial flows using upper-convected Maxwell model, solved in single linear system without log-conformation approach.


<details>
  <summary>Details</summary>
Motivation: Previous segregated algorithms for viscoelastic interfacial flows require log-conformation approaches and struggle with high Weissenberg numbers. Need robust framework for strongly elastic flows.

Method: Fully coupled implicit finite-volume algorithm with front-tracking method. All governing equations (continuity, momentum, constitutive model) discretized and solved in single linear system for pressure, velocity, and polymer stress tensor simultaneously.

Result: Successfully simulates viscoelastic droplet deformation at Wi up to 10^4 and bubble rise with negative wake discontinuity. No log-conformation approach needed, unlike previous segregated methods.

Conclusion: The fully implicit coupled front-tracking formulation provides robust framework for reliable numerical predictions of strongly elastic interfacial flows at large Weissenberg numbers.

Abstract: A fully coupled implicit finite-volume algorithm for incompressible viscoelastic interfacial flows is proposed, whereby the viscoelasticity of the flow is described by an upper-convected Maxwell constitutive model, including limited extensibility and shear-thinning behaviour. The governing equations describing the conservation of continuity and momentum, as well as the constitutive model are discretized using standard finite-volume methods and are solved for pressure, velocity and the polymer stress tensor in a single linear system of equations. Treating all terms of the linearized and discretized governing equations implicit in velocity, pressure and/or the components of the polymer stress tensor, a tightly coupled system of equations is obtained. The interface separating the interacting bulk phases and the surface tension acting at the fluid interface are modelled using a state-of-the-art front-tracking method. We demonstrate the capabilities of the proposed numerical framework with four representative test cases, including the deformation of a viscoelastic droplet in shear flow at large Weissenberg numbers of up to Wi=10^4, and the jump discontinuity of the rise velocity of a bubble rising in a viscoelastic liquid as a result of a "negative wake". Contrary to previous studies using segregated algorithms, the proposed fully coupled implicit algorithm does not apply or require a log-conformation approach to predict these flows. Overall, the fully implicit coupled front-tracking formulation provides a robust framework to reliable numerical predictions of strongly elastic interfacial flows at large Weissenberg numbers.

</details>


### [95] [StabOp: A Data-Driven Stabilization Operator for Reduced Order Modeling](https://arxiv.org/abs/2602.07745)
*Ping-Hsuan Tsai,Anna Ivagnes,Annalisa Quaini,Traian Iliescu,Gianluigi Rozza*

Main category: physics.flu-dyn

TL;DR: Proposes data-driven stabilization operators (StabOp) to replace traditional spatial filters in reduced order models, achieving significantly better accuracy than classical filter-based approaches.


<details>
  <summary>Details</summary>
Motivation: Addresses open questions about which spatial filters work best for stabilization/closure in under-resolved regimes and how to determine filter parameters, aiming to improve reliability of filter-based stabilization strategies.

Method: Replaces traditional spatial filters with data-driven StabOp learned through PDE-constrained optimization. Postulates StabOp as linear, quadratic, or nonlinear mapping and solves optimization problem to minimize loss function.

Result: StabOp-L-ROM shows orders of magnitude better accuracy than classical L-ROM with optimal filter radius in predictive regime across four flow test cases (2D cylinder, lid-driven cavity, 3D hemisphere, minimal channel flow).

Conclusion: Data-driven stabilization operators outperform traditional spatial filters for ROM stabilization, offering a fundamentally different approach that learns optimal smoothing mechanisms from data rather than relying on predefined filter forms.

Abstract: Spatial filters have played a central role in large eddy simulation and, more recently, in reduced order model (ROM) stabilization for convection-dominated flows. Nevertheless, important open questions remain: in under-resolved regimes, which filter is most suitable for a given stabilization or closure model? Moreover, once a filter is selected, how should its parameters, such as the filter radius, be determined? Addressing these questions is essential for the reliable design and performance of filter-based stabilization strategies. To answer these questions, we propose a novel strategy that differs fundamentally from current filter-based approaches: we replace traditional spatial filters with a data-driven stabilization operator (StabOp) that yields accurate results for a given resolution, quantity of interest, and stabilization strategy. Although the new StabOp can be used for both classical discretizations and ROMs, and for different types of filter-based stabilization or closure, for clarity we focus on ROMs and the Leray stabilization. To build the new StabOp, we postulate its model form as a linear, quadratic, or nonlinear mapping, and then solve a PDE-constrained optimization problem to minimize a given loss function. Using the resulting StabOp in the Leray ROM (L-ROM) yields a new stabilized ROM, StabOp-L-ROM. To assess the StabOp-L-ROM, we compare it with the L-ROM and the standard ROM in numerical simulations of four flows: 2D flow past a cylinder at Re=500, lid-driven cavity at Re=10000, 3D flow past a hemisphere at Re=2200, and minimal channel flow at Re=5000. Our numerical results show that the StabOp-L-ROM can be orders of magnitude more accurate than the classical L-ROM tuned with an optimal filter radius in the predictive regime. Furthermore, while the new StabOp smooths the input flow fields, its smoothing mechanism differs from that of classical spatial filters.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [96] [Analyzing Band Gaps in Ensemble Density Functional Theory using Thermodynamic Limits of Finite One-Dimensional Model Systems](https://arxiv.org/abs/2602.07317)
*Gregory G. V. Kenning,Remi J. Leano,David A. Strubbe*

Main category: cond-mat.mtrl-sci

TL;DR: EDFT shows promise for calculating band gaps in periodic systems, with finite Kronig-Penney models demonstrating reasonable corrections to Kohn-Sham gaps in the thermodynamic limit.


<details>
  <summary>Details</summary>
Motivation: While Ensemble Density Functional Theory (EDFT) has shown success for excited states in finite systems, it's unclear if it can calculate band gaps in periodic systems and what theoretical formulation would be appropriate.

Method: Used finite versions of 1D Kronig-Penney periodic model to estimate thermodynamic limit, implemented in Octopus real-space DFT code with ensemblized LDA approximation, carefully identifying valence band maximum and conduction band minimum states.

Result: Kohn-Sham gaps approach same periodic limit for different termination methods; EDFT provides reasonable nonzero correction to bandgap in periodic limit, indicating promise for periodic systems.

Conclusion: EDFT is promising for calculating band gaps in periodic systems, motivating further development of suitable formalism for periodic applications.

Abstract: Ensemble Density Functional Theory (EDFT) is a promising extension to Density Functional Theory (DFT) for calculating excited states. While Kohn-Sham eigenvalue differences underestimate gaps, EDFT has been shown to provide more accurate excitation energies in atoms, molecules and isolated model systems. However, it is unclear whether EDFT is capable of calculating band gaps of periodic systems -- and what an appropriate theoretical formulation would be to describe periodic systems. We explored how EDFT could calculate band gaps by estimating the thermodynamic limit with increasingly wide finite versions of the one-dimensional Kronig-Penney (KP) periodic model. We use Octopus, an ab initio, open-source, real-space DFT code, as in our previous work [R. J. Leano et al., Electron. Struct. 6, 035003 (2024)] in which we found with "particle in a box" models that EDFT can provide a reasonable effective mass correction for the homogeneous electron gas. Now, we use a periodic reference that is gapped. We find that the finite systems' Kohn-Sham gap approaches the same periodic limit for each of three ways of terminating the finite system, though the appropriate states corresponding to the valence band maximum and conduction band minimum have to be carefully identified in each case. Finally, our EDFT results, using a simple ensemblized LDA approximation, have a reasonable nonzero correction to the bandgap in the periodic limit. The results indicate that EDFT is promising for periodic systems, to motivate further work on developing a suitable formalism.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [97] [A Machine Learning accelerated geophysical fluid solver](https://arxiv.org/abs/2602.08670)
*Yang Bai*

Main category: cs.CV

TL;DR: This thesis implements classic solvers for shallow water and Euler equations that outperform Pyclaw, then proposes four deep neural network approaches for ML-based PDE solvers, with two showing satisfactory results.


<details>
  <summary>Details</summary>
Motivation: Machine learning has succeeded in many domains but its application to mathematically constrained areas like solving PDEs remains underexplored. Data-driven discretization methods offer promising ways to accelerate and improve existing PDE solvers on structured grids by predicting coefficients for quasi-linear stencils.

Method: Implemented classic solvers for shallow water and Euler equations under different frameworks, then proposed four different deep neural network architectures for ML-based PDE solvers. The ML approaches use data-driven discretization methods that predict coefficients of quasi-linear stencils for computing values or derivatives at given positions.

Result: The classic solvers performed much better than the Pyclaw solver. Among the four proposed deep neural network approaches, two were able to output satisfactory solutions for the PDE problems.

Conclusion: ML-based data-driven discretization methods show promise for improving PDE solvers, with certain neural network architectures capable of producing satisfactory solutions while potentially benefiting from traditional numerical scheme properties like conservation laws.

Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [98] [Real Bers embedding on the line: Fisher-Rao linearization, Schwarzian curvature, and scattering coordinates](https://arxiv.org/abs/2602.07373)
*Hy Lam*

Main category: math.DG

TL;DR: The paper develops a real-analytic Bers embedding for diffeomorphism groups, connects it to L^p Fisher-Rao geometry, introduces an L^p-Schwarzian family, and establishes connections between geometric structures on diffeomorphisms and probability densities.


<details>
  <summary>Details</summary>
Motivation: To develop a real-analytic counterpart of the classical Bers embedding for diffeomorphism groups, establish connections between geometric structures on diffeomorphisms and Fisher-Rao information geometry on probability densities, and unify various geometric frameworks through an interpolating L^p-Schwarzian family.

Method: Uses p-root maps and logarithmic coordinates to linearize Finsler metrics on diffeomorphism groups, constructs real Bers maps via Schwarzian derivatives, employs Sturm-Liouville spectral theory to characterize images, develops L^p-Schwarzian interpolations, and transfers structures to density manifolds through Jacobian correspondence.

Result: Established isometric linearization of homogeneous Finsler metrics, explicit geodesics, canonical flat connections, real-analytic Bers embeddings as Fréchet-smooth injective immersions, full asymptotic expansions for L^p-Schwarzian family, and connections between diffeomorphism geometry and Fisher information geometry.

Conclusion: The paper provides a unified geometric framework connecting diffeomorphism group geometry with information geometry, revealing deep connections between Schwarzian derivatives, Fisher information, and geometric structures through interpolating L^p-Schwarzian families and real-analytic Bers embeddings.

Abstract: We develop a real-analytic counterpart of the Bers embedding for the Fréchet Lie group $\Diff^{-\infty}(\R)$ of decay-controlled diffeomorphisms of the line, and establish its connection to $L^p$ Fisher-Rao geometry on densities. For $p\in[1,\infty)$, the $p$-root map $\varphi\mapsto p(\varphi'^{1/p}-1)$ isometrically linearizes the homogeneous $\dot W^{1,p}$ Finsler metric on $\Diff^{-\infty}(\R)$, yielding explicit geodesics and a canonical flat connection whose Eulerian geodesic equation is the generalized Hunter-Saxton equation; for $p=\infty$, logarithmic coordinates $\varphi\mapsto\log\varphi'$ provide a global isometry and the Schwarzian derivative emerges as the projective curvature. We construct a real Bers map $β^{-\infty}\colon\Diff^{-\infty}(\R)/\Aff(\R)\to W^{\infty,1}(\R)$ via this Schwarzian, prove it is a Fréchet-smooth injective immersion whose linearization admits a tame right inverse given by an explicit Volterra operator, and characterize its image through Sturm-Liouville spectral theory. We introduce an $L^p$-Schwarzian family $S_p$ that interpolates between affine and projective cocycles, establish full asymptotic expansions as $p\to\infty$ in Fréchet and Orlicz-Sobolev scales, and extend the Bers embedding to Orlicz diffeomorphism groups. Through the Jacobian correspondence, these structures transfer to a manifold of densities asymptotic to Lebesgue measure, where the nonlinear Eulerian transport reduces to a pointwise Riccati law and the Schwarzian becomes the score curvature governing Fisher information. The compact-manifold $L^p$ Fisher-Rao linearization of Bauer, Bruveris, Harms, and Michor is recalled as a guiding framework.

</details>


### [99] [Sharp estimates for the Robin Laplacian under a perimeter constraint in hyperbolic space](https://arxiv.org/abs/2602.07510)
*Daguang Chen,Shan Li*

Main category: math.DG

TL;DR: Lower bound for first Robin eigenvalue with negative parameter on horospherically convex hyperbolic domains shows geodesic ball maximizes it; upper bounds derived for positive parameter case.


<details>
  <summary>Details</summary>
Motivation: To establish eigenvalue bounds for Robin Laplacian on horospherically convex domains in hyperbolic space, extending known results from Euclidean to hyperbolic geometry.

Method: Using isoperimetric deficit to derive lower bound for negative boundary parameter case; developing techniques for upper bounds with positive parameter on horospherically convex domains.

Result: Proved geodesic ball maximizes first Robin eigenvalue among horospherically convex domains for negative parameter; obtained upper bounds for positive parameter case.

Conclusion: Geodesic balls are extremal domains for Robin eigenvalues in hyperbolic space, with isoperimetric-type inequalities connecting geometry to spectral properties.

Abstract: In this paper, we establish a lower bound, in terms of the isoperimetric deficit, for the first eigenvalue of the Robin Laplacian with negative boundary parameter on horospherically convex bounded domains in hyperbolic space, which implies that the geodesic ball maximizes this eigenvalue among all such domains. Furthermore, we derive upper bounds for the first eigenvalue of the Robin Laplacian with positive boundary parameter on horospherically convex bounded domains in hyperbolic space.

</details>


### [100] [Existence of the classical solution to the fractional mean curvature flow with capillary-type boundary conditions](https://arxiv.org/abs/2602.07989)
*Linlin Fan,Peibiao Zhao*

Main category: math.DG

TL;DR: Short-time existence of fractional mean curvature flow for C¹,¹ capillary hypersurfaces using fixed point methods.


<details>
  <summary>Details</summary>
Motivation: Previous work by Wang, Weng & Xia (2024) studied classical mean curvature flow for smooth capillary hypersurfaces using PDE theory. This paper extends to fractional mean curvature flow for less regular (C¹,¹) hypersurfaces with capillary boundary conditions.

Method: Fixed point argument to prove short-time existence of fractional mean curvature flow for C¹,¹ capillary hypersurfaces.

Result: Established short-time existence of solutions for fractional mean curvature flow with capillary-type boundary conditions on C¹,¹ hypersurfaces.

Conclusion: Successfully extended existence theory from classical to fractional mean curvature flow for capillary hypersurfaces, handling lower regularity (C¹,¹ instead of smooth) using fixed point methods rather than standard PDE theory.

Abstract: Wang, Weng and Xia[Math. Ann. 388 (2024), no. 2] studied a mean curvature type flow for the smooth, embedded capillary hypersurfaces with a constant contact angle $θ\in(0,π)$ and confirmed the existence of solutions by the standard PDE theory. In the present paper, we study a fractional mean curvature flow for $C^{1,1}$-regular hypersurfaces with a capillary-type boundary condition and obtain the short time existence by the fixed point argument.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [101] [Two-Dimensional Kelvin-Helmholtz Instability with Anisotropic Pressure](https://arxiv.org/abs/2602.08806)
*Shishir Biswas,Masaru Nakanotani,Dinshaw S. Balsara,Vladimir Florinski,Merav Opher*

Main category: astro-ph.SR

TL;DR: The paper analyzes the Kelvin-Helmholtz instability in collisionless plasmas using CGL equations, finding that MHD limit has larger growth rates, stronger magnetic effects, and more current density than CGL limit.


<details>
  <summary>Details</summary>
Motivation: KH instability occurs widely in heliospheric and interstellar environments, but previous studies focused on MHD limit. Limited research exists on collisionless regime where anisotropic pressures develop, which is important for understanding real astrophysical plasmas.

Method: Used linearised analysis and numerical simulations comparing CGL (Chew-Goldberger-Low) equations (which include anisotropic pressure tensor) with MHD equations to study KH instability in collisionless regime.

Result: MHD limit shows largest growth rates, strongest magnetic effects, highest current densities, and largest magnetic islands. In CGL limit, energy goes into pressure anisotropies instead of bending field lines. MHD also shows strongest intermittency formation.

Conclusion: The study provides insights into KH instability in collisionless plasmas, showing MHD limit dominates over CGL in key instability characteristics. Results have implications for understanding turbulence and reconnection in heliosheath and other astrophysical environments.

Abstract: The Kelvin-Helmholtz (KH) instability occurs in multiple heliospheric (solar-wind stream interfaces, planetary magnetospheres, cometary tails, heliopause flanks) and interstellar (protoplanetary disks, relativistic jets, neutron star accretion disks) environments. While the KH instability has been well-studied in the magnetohydrodynamic (MHD) limit, only limited studies were performed in the collisionless regime, which is conducive to development of anisotropic pressures. Collisionless plasmas are often described using the Chew Goldberger and Low (CGL) equations which feature an anisotropic pressure tensor. This paper presents a comprehensive analysis of the CGL version of the KH instability using linearised and numerical techniques. We find that the largest growth rates and the greatest incidence of magnetic effects occur in the MHD limit. In the large relaxation time CGL limit, part of the energy goes into the formation of pressure anisotropies, resulting in smaller amounts of energy being available for bending the field lines. Consequently, when we cross-compare CGL and MHD simulations that are otherwise identical, the current densities are largest in the MHD limit, and the largest magnetic islands also form in that limit. Early and late time formation of pressure anisotropies have also been studied. We also find that the strongest trend for forming intermittencies in the flow also occurs in the MHD limit. The paper also discusses possible consequences of our results for turbulence and reconnection in the heliosheath (the layer between the solar wind termination shock and the heliopause).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: Verify-RL: A framework for mathematically verified decomposition of complex math problems using symbolic differentiation rules, ensuring valid parent-child relationships that improve training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing decomposition methods for training language models on math problems are heuristic and lack guarantees about subproblem simplicity, solution relevance, or mathematical grounding. Many decompositions are invalid, hindering effective curriculum learning.

Method: Uses symbolic differentiation rules to create verified decompositions where every parent-child relationship satisfies three conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. This enables "verification by construction" through symbolic computation.

Result: Eliminating invalid decompositions yields significant gains - accuracy on hardest problems more than doubles from 32% to 68%, with 40% relative improvement overall compared to heuristic decomposition methods.

Conclusion: Symbolic differentiation provides a mathematically grounded structure for verified decomposition in curriculum learning, and ensuring decomposition validity through formal verification substantially improves language model performance on complex mathematical problems.

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [103] [Green--Wasserstein Inequality on Compact Surfaces](https://arxiv.org/abs/2602.07843)
*Maja Gwozdz*

Main category: math.PR

TL;DR: The paper proves it's impossible to remove the √log n factor from the 2D Green-Wasserstein inequality while keeping the unrenormalized off-diagonal Green term, showing no such inequality can hold uniformly over point sets with O(n^{-1/2}) remainder.


<details>
  <summary>Details</summary>
Motivation: To answer Steinerberger's question about whether the √log n factor in the two-dimensional Green-Wasserstein inequality can be removed while maintaining the unrenormalized off-diagonal Green term.

Method: Proof by contradiction combining a second-moment estimate for the random Green energy of i.i.d. samples with the semi-discrete random matching asymptotics of Ambrosio-Glaudo.

Result: Shows impossibility: no inequality of the same form can hold uniformly over point sets with O(n^{-1/2}) remainder for all n on any compact connected surface.

Conclusion: The √log n factor in the 2D Green-Wasserstein inequality is essential and cannot be removed while keeping the unrenormalized off-diagonal Green term.

Abstract: Let $(M,g)$ be a compact connected two-dimensional Riemannian manifold without boundary. In this note, we answer a question posed by Steinerberger: can one remove the $\sqrt{\log n}$ factor in the two-dimensional Green--Wasserstein inequality while keeping the unrenormalized off-diagonal Green term? We show that this is impossible on any compact connected surface: there is no inequality of the same form that holds uniformly over point sets with an $O(n^{-1/2})$ remainder for all $n$. We argue by contradiction and combine a second-moment estimate for the random Green energy of i.i.d. samples with the semi-discrete random matching asymptotics of Ambrosio--Glaudo.

</details>


### [104] [Averaging Dynamics and Wong-Zakai approximations for a Fast-Slow Navier-Stokes System Driven by fractional Brownian Motion](https://arxiv.org/abs/2602.08680)
*Eliseo Luongo,Francesco Triggiano*

Main category: math.PR

TL;DR: Analysis of a slow-fast Navier-Stokes system with fractional Brownian noise, showing different limiting behaviors based on Hurst parameter H.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of coupled slow-fast Navier-Stokes systems under fractional Brownian noise perturbations, particularly how different noise regimes (Hurst parameter values) affect the effective dynamics of the slow component.

Method: Using rough path theory to analyze a system of coupled 2D and 3D Navier-Stokes equations where the fast component is perturbed by additive fractional Brownian noise with Hurst parameter H > 1/3.

Result: For H < 1/2, the slow component converges in law to a Navier-Stokes system with additional Itô-Stokes drift. For H ∈ (1/2, 1), the limit equation features only transport noise driven by a rough path.

Conclusion: The limiting behavior of the slow-fast Navier-Stokes system exhibits a phase transition at H = 1/2, with qualitatively different effective equations emerging in the subcritical (H < 1/2) and supercritical (H > 1/2) regimes.

Abstract: We study a slow-fast system of coupled two- and three-dimensional Navier-Stokes equations in which the fast component is perturbed by an additive fractional Brownian noise with Hurst parameter $H>\frac{1}{3}$. The system is analyzed using rough path theory, and the limiting behaviour strongly depends on the value of $H$. We prove convergence in law of the slow component to a Navier-Stokes system with an additional Itô-Stokes drift when $H<\frac{1}{2}$. In contrast, for $H\in (\frac{1}{2},1)$, the limit equation features only a transport noise driven by a rough path.

</details>
